# Comparing `tmp/pyquokka-0.2.9-py3-none-any.whl.zip` & `tmp/pyquokka-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,41 +1,42 @@
-Zip file size: 288009 bytes, number of entries: 39
--rw-rw-r--  2.0 unx      309 b- defN 23-May-09 16:28 pyquokka/__init__.py
--rw-rw-r--  2.0 unx     4291 b- defN 23-Apr-21 22:04 pyquokka/catalog.py
--rw-rw-r--  2.0 unx      372 b- defN 23-Apr-21 22:04 pyquokka/common_startup.sh
--rw-rw-r--  2.0 unx    28239 b- defN 23-Apr-21 22:04 pyquokka/coordinator.py
--rw-rw-r--  2.0 unx    48179 b- defN 23-May-09 05:45 pyquokka/core.py
--rw-rw-r--  2.0 unx    48426 b- defN 23-Apr-21 22:04 pyquokka/dataset.py
--rw-rw-r--  2.0 unx    98944 b- defN 23-May-09 05:06 pyquokka/datastream.py
--rw-rw-r--  2.0 unx     1438 b- defN 23-Apr-21 22:04 pyquokka/debugger.py
--rw-rw-r--  2.0 unx    76119 b- defN 23-May-09 16:27 pyquokka/df.py
--rw-rw-r--  2.0 unx    37360 b- defN 23-Apr-21 22:04 pyquokka/executors.py
--rw-rw-r--  2.0 unx    12657 b- defN 23-Apr-21 22:04 pyquokka/expression.py
--rw-rw-r--  2.0 unx    17483 b- defN 23-Apr-21 22:04 pyquokka/flight.py
--rw-rw-r--  2.0 unx     3098 b- defN 23-May-09 03:48 pyquokka/hbq.py
--rwxrwxr-x  2.0 unx   368480 b- defN 23-Apr-21 22:04 pyquokka/ldb.so
--rw-rw-r--  2.0 unx      274 b- defN 23-Apr-21 22:04 pyquokka/leader_start_ray.sh
--rw-rw-r--  2.0 unx      619 b- defN 23-Apr-21 22:04 pyquokka/leader_startup.sh
--rw-rw-r--  2.0 unx    29383 b- defN 23-May-09 05:30 pyquokka/logical.py
--rw-rw-r--  2.0 unx     9100 b- defN 23-Apr-21 22:04 pyquokka/orderedstream.py
--rw-rw-r--  2.0 unx      886 b- defN 23-Apr-21 22:04 pyquokka/placement_strategy.py
--rw-rw-r--  2.0 unx     3717 b- defN 23-Apr-21 22:04 pyquokka/quokka_dataset.py
--rw-rw-r--  2.0 unx    19735 b- defN 23-May-09 03:48 pyquokka/quokka_runtime.py
--rw-rw-r--  2.0 unx    93718 b- defN 23-Apr-21 22:04 pyquokka/redis.conf
--rw-rw-r--  2.0 unx    16471 b- defN 23-Apr-21 22:04 pyquokka/sql_utils.py
--rw-rw-r--  2.0 unx     2371 b- defN 23-Apr-21 22:04 pyquokka/state.py
--rw-rw-r--  2.0 unx    11695 b- defN 23-Apr-21 22:04 pyquokka/tables.py
--rw-rw-r--  2.0 unx     2351 b- defN 23-Apr-21 22:04 pyquokka/target_info.py
--rw-rw-r--  2.0 unx     5752 b- defN 23-Apr-21 22:04 pyquokka/task.py
--rw-rw-r--  2.0 unx    39151 b- defN 23-May-09 15:31 pyquokka/utils.py
--rw-rw-r--  2.0 unx     4081 b- defN 23-Apr-21 22:04 pyquokka/windowtypes.py
--rw-rw-r--  2.0 unx      146 b- defN 23-May-09 04:03 pyquokka/executors/__init__.py
--rw-rw-r--  2.0 unx      811 b- defN 23-May-08 15:34 pyquokka/executors/base_executor.py
--rw-rw-r--  2.0 unx    17511 b- defN 23-May-09 04:03 pyquokka/executors/sql_executors.py
--rw-rw-r--  2.0 unx    17185 b- defN 23-May-09 04:03 pyquokka/executors/ts_executors.py
--rw-rw-r--  2.0 unx     2870 b- defN 23-May-09 05:34 pyquokka/executors/vector_executors.py
--rw-rw-r--  2.0 unx    11357 b- defN 23-May-09 16:30 pyquokka-0.2.9.dist-info/LICENSE
--rw-rw-r--  2.0 unx     1020 b- defN 23-May-09 16:30 pyquokka-0.2.9.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-May-09 16:30 pyquokka-0.2.9.dist-info/WHEEL
--rwxrwxr-x  2.0 unx        9 b- defN 23-May-09 16:30 pyquokka-0.2.9.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     3116 b- defN 23-May-09 16:30 pyquokka-0.2.9.dist-info/RECORD
-39 files, 1038816 bytes uncompressed, 283143 bytes compressed:  72.7%
+Zip file size: 292216 bytes, number of entries: 40
+-rw-rw-r--  2.0 unx      309 b- defN 23-May-11 13:24 pyquokka/__init__.py
+-rw-rw-r--  2.0 unx     4291 b- defN 23-Apr-19 05:39 pyquokka/catalog.py
+-rw-rw-r--  2.0 unx      372 b- defN 23-Apr-12 19:11 pyquokka/common_startup.sh
+-rw-rw-r--  2.0 unx    28239 b- defN 23-Apr-16 23:17 pyquokka/coordinator.py
+-rw-rw-r--  2.0 unx    48407 b- defN 23-May-11 12:30 pyquokka/core.py
+-rw-rw-r--  2.0 unx    49168 b- defN 23-May-11 12:22 pyquokka/dataset.py
+-rw-rw-r--  2.0 unx    98957 b- defN 23-May-11 12:50 pyquokka/datastream.py
+-rw-rw-r--  2.0 unx     1438 b- defN 22-Nov-11 18:24 pyquokka/debugger.py
+-rw-rw-r--  2.0 unx    81396 b- defN 23-May-11 13:14 pyquokka/df.py
+-rw-rw-r--  2.0 unx    37360 b- defN 23-Apr-16 23:19 pyquokka/executors.py
+-rw-rw-r--  2.0 unx    12657 b- defN 23-Mar-28 22:05 pyquokka/expression.py
+-rw-rw-r--  2.0 unx    17483 b- defN 23-Mar-26 21:54 pyquokka/flight.py
+-rw-rw-r--  2.0 unx     3098 b- defN 23-May-11 02:45 pyquokka/hbq.py
+-rwxrwxr-x  2.0 unx   368480 b- defN 23-Apr-11 05:32 pyquokka/ldb.so
+-rw-rw-r--  2.0 unx      274 b- defN 23-Mar-25 18:43 pyquokka/leader_start_ray.sh
+-rw-rw-r--  2.0 unx      619 b- defN 23-Mar-25 18:43 pyquokka/leader_startup.sh
+-rw-rw-r--  2.0 unx    30992 b- defN 23-May-11 07:12 pyquokka/logical.py
+-rw-rw-r--  2.0 unx     9100 b- defN 23-May-11 02:45 pyquokka/orderedstream.py
+-rw-rw-r--  2.0 unx      886 b- defN 23-Mar-15 22:44 pyquokka/placement_strategy.py
+-rw-rw-r--  2.0 unx     3717 b- defN 23-Mar-29 00:48 pyquokka/quokka_dataset.py
+-rw-rw-r--  2.0 unx    19735 b- defN 23-May-11 02:45 pyquokka/quokka_runtime.py
+-rw-rw-r--  2.0 unx    93718 b- defN 22-Oct-04 03:50 pyquokka/redis.conf
+-rw-rw-r--  2.0 unx    10625 b- defN 23-May-11 02:45 pyquokka/sql.py
+-rw-rw-r--  2.0 unx    16471 b- defN 23-Mar-27 22:47 pyquokka/sql_utils.py
+-rw-rw-r--  2.0 unx     2371 b- defN 22-Jul-15 20:59 pyquokka/state.py
+-rw-rw-r--  2.0 unx    11695 b- defN 23-Apr-09 03:05 pyquokka/tables.py
+-rw-rw-r--  2.0 unx     2351 b- defN 23-Jan-28 04:17 pyquokka/target_info.py
+-rw-rw-r--  2.0 unx     5752 b- defN 23-Mar-20 21:30 pyquokka/task.py
+-rw-rw-r--  2.0 unx    39151 b- defN 23-May-11 02:45 pyquokka/utils.py
+-rw-rw-r--  2.0 unx     4081 b- defN 23-Jan-25 17:17 pyquokka/windowtypes.py
+-rw-rw-r--  2.0 unx      146 b- defN 23-May-11 02:45 pyquokka/executors/__init__.py
+-rw-rw-r--  2.0 unx      811 b- defN 23-May-11 02:45 pyquokka/executors/base_executor.py
+-rw-rw-r--  2.0 unx    17620 b- defN 23-May-11 02:45 pyquokka/executors/sql_executors.py
+-rw-rw-r--  2.0 unx    17185 b- defN 23-May-11 02:45 pyquokka/executors/ts_executors.py
+-rw-rw-r--  2.0 unx     4503 b- defN 23-May-11 06:14 pyquokka/executors/vector_executors.py
+-rw-rw-r--  2.0 unx    11357 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx      999 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx        9 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3188 b- defN 23-May-11 13:24 pyquokka-0.3.0.dist-info/RECORD
+40 files, 1059103 bytes uncompressed, 287244 bytes compressed:  72.9%
```

## zipnote {}

```diff
@@ -60,14 +60,17 @@
 
 Filename: pyquokka/quokka_runtime.py
 Comment: 
 
 Filename: pyquokka/redis.conf
 Comment: 
 
+Filename: pyquokka/sql.py
+Comment: 
+
 Filename: pyquokka/sql_utils.py
 Comment: 
 
 Filename: pyquokka/state.py
 Comment: 
 
 Filename: pyquokka/tables.py
@@ -96,23 +99,23 @@
 
 Filename: pyquokka/executors/ts_executors.py
 Comment: 
 
 Filename: pyquokka/executors/vector_executors.py
 Comment: 
 
-Filename: pyquokka-0.2.9.dist-info/LICENSE
+Filename: pyquokka-0.3.0.dist-info/LICENSE
 Comment: 
 
-Filename: pyquokka-0.2.9.dist-info/METADATA
+Filename: pyquokka-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: pyquokka-0.2.9.dist-info/WHEEL
+Filename: pyquokka-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: pyquokka-0.2.9.dist-info/top_level.txt
+Filename: pyquokka-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: pyquokka-0.2.9.dist-info/RECORD
+Filename: pyquokka-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## pyquokka/__init__.py

```diff
@@ -8,8 +8,8 @@
 from . import orderedstream
 from . import tables
 from . import task
 from . import hbq
 
 from .df import QuokkaContext
 
-__version__ = '0.2.9'
+__version__ = '0.3.0'
```

## pyquokka/core.py

```diff
@@ -149,21 +149,28 @@
 
         self.mappings[target_actor_id] = mapping
 
         def partition_fn(predicate_fn, partitioner_fn, batch_funcs, projection, num_target_channels, x, source_channel):
 
             start = time.time()
             # x could be either a pyarrow table of a polars dataframe
-            # print(predicate_fn)
             if type(predicate_fn) == str:
                 con = duckdb.connect().execute('PRAGMA threads=%d' % 8)
                 batch_arrow = x.to_arrow() if type(x) == polars.DataFrame else x
-                x = polars.from_arrow(con.execute(predicate_fn).arrow())
+                x = con.execute(predicate_fn).arrow()
+                try:
+                    x = polars.from_arrow(x)
+                except:
+                    x = polars.from_pandas(x.to_pandas())
             else:
-                x = x if type(x) == polars.DataFrame else polars.from_arrow(x)
+                if type(x) != polars.DataFrame:
+                    try:
+                        x = polars.from_arrow(x)
+                    except:
+                        x = polars.from_pandas(x.to_pandas())
                 x = x.filter(predicate_fn)
             # print("filter time", time.time() - start)
 
             for func in batch_funcs:
                 x = func(x)
                 if x is None or len(x) == 0:
                     return {}
```

## pyquokka/dataset.py

```diff
@@ -18,14 +18,15 @@
 import concurrent.futures
 import time
 import warnings
 import random
 import ray
 import math
 import asyncio
+import numpy as np
 
 # computes the overlap of two intervals (a1, a2) and (b1, b2)
 def overlap(a, b):
     return max(-1, min(a[1], b[1]) - max(a[0], b[0]))
 
 # this is actually fault tolerant, since the dataframe is pickled and stored in Redis
 # this is obviously not efficient in a cloud environment, but the intended use is for local development anyways
@@ -138,55 +139,76 @@
             return ret
         
         results = asyncio.run(main(self.arguments[state[0]: state[1]]))
         # find the first non-None result
         results = polars.concat([i for i in results if i is not None]).select(self.projection)
         return None, results
 
-class InputLanceQVDataset:
+class InputLanceDataset:
 
     """
     Let's hope this works!
     """
 
-    def __init__(self, uri, vec_column, query_vectors, k, columns = None, filters = None, batch_size = 100) -> None:
+    def __init__(self, uris, vec_column, columns = None, filters = None) -> None:
         
-        assert type(columns) == list, "columns must be a list of strings"
+        import lance
+        if columns is not None:
+            assert type(columns) == list, "columns must be a list of strings"
         self.columns = columns
-        assert type(filters) == str, "sql predicate supported"
+        if filters is not None:
+            assert type(filters) == str, "sql predicate supported"
         self.filters = filters
-        self.query_vector = None
+
+        self.probe_df = None
+        self.probe_df_col = None
         self.k = None
-        import numpy as np
-        assert type(query_vectors) == np.ndarray
-        assert len(query_vectors.shape) == 2, "must supply 2d query_vectors"
-        assert type(k) == int
-        self.query_vectors = query_vectors
-        self.k = k
-        self.uri = uri
-        self.batch_size = batch_size
+
+        assert type(uris) == list
+        self.uris = uris
         self.vec_column = vec_column
-        
+
+        for uri in self.uris:
+            dataset = lance.dataset(uri)
+            if not dataset.has_index:
+                print("Warning: dataset {} does not have an index. Expect slow performance and maybe crashes.".format(uri))
+            assert self.vec_column in dataset.schema.names, "vector column not found in schema"
+        
+    def set_probe_df(self, probe_df, probe_df_col, k):
+
+        assert probe_df is not None and probe_df_col is not None and k is not None, "must provide probe_df, probe_df_col, and k"
+        
+        assert type(probe_df) == polars.DataFrame
+        self.probe_df = probe_df
+        assert type(probe_df_col) == str and probe_df_col in probe_df.columns, "probe_df_col must be a string and in probe_df"
+        self.probe_df_col = probe_df_col
+        self.probe_df_vecs = np.stack(self.probe_df[self.probe_df_col].to_numpy())
+        assert type(k) == int and k >= 1
+        self.k = k
+
     def get_own_state(self, num_channels):
 
-        self.num_channels = num_channels
-        vectors_per_channel = len(self.query_vectors) // num_channels + 1
-        channel_infos = {}
+        # split the urls across the channels
+        channel_info = {}
         for channel in range(num_channels):
-            my_query_vectors = self.query_vectors[channel * vectors_per_channel : (channel + 1) * vectors_per_channel]
-            channel_infos[channel] = []
-            for pos in range(0, len(my_query_vectors), self.batch_size):
-                channel_infos[channel].append(my_query_vectors[pos:pos+self.batch_size])
-        return channel_infos
+            channel_info[channel] = self.uris[channel::num_channels]
+        print(channel_info)
+        return channel_info
     
-    def execute(self, mapper_id, query_vec_batch):
+    def execute(self, mapper_id, uri):
 
         import lance
-        dataset = lance.dataset(self.uri)
-        return dataset.to_table(columns = self.columns, filters = self.filters, nearest={"column": self.vec_column, "k": self.k, "q": query_vec_batch})
+        dataset = lance.dataset(uri)
+        if self.k is not None:
+            results = pa.concat_tables( [dataset.to_table(columns = self.columns, filter = self.filters, 
+                                nearest={"column": self.vec_column, "k": self.k, "q": self.probe_df_vecs[i]}) for i in range(len(self.probe_df_vecs))]) 
+        else:
+            results = dataset.to_table(columns = self.columns, filter = self.filters)
+        
+        return None, results
 
 class InputEC2ParquetDataset:
 
     # filter pushdown could be profitable in the future, especially when you can skip entire Parquet files
     # but when you can't it seems like you still read in the entire thing anyways
     # might as well do the filtering at the Pandas step. Also you need to map filters to the DNF form of tuples, which could be
     # an interesting project in itself. Time for an intern?
```

## pyquokka/datastream.py

```diff
@@ -426,20 +426,20 @@
                     probe_vec_col = col_name + suffix
                 new_schema.append(col_name + suffix)
                 schema_mapping[col_name + suffix] = {-1: col_name}
             else:
                 new_schema.append(col_name)
                 schema_mapping[col_name] = {-1: col_name}
 
-        node = NearestNeighborFilterNode(new_schema, schema_mapping, vec_column, probe_df, probe_vec_col,  k)
+        node = NearestNeighborFilterNode(new_schema, schema_mapping, vec_column_left, probe_df, probe_vec_col,  k)
 
         return self.quokka_context.new_stream(sources={0: self}, partitioners={0: PassThroughPartitioner()}, node=node,
                                               schema=new_schema, sorted = self.sorted)
 
-    def ann_join(self, vec_column = None, vec_column_left = None, vec_column_right = None, probe_side = "left", k = 1):
+    def _ann_join(self, vec_column = None, vec_column_left = None, vec_column_right = None, probe_side = "left", k = 1):
 
         """
         This will perform a nearest neighbor join between two Quokka DataStreams.
         The plan is to convert this to a NearestNeighborFilterNode after you propagate cardinality and figure out which side is smaller.
         
         """
 
@@ -1071,38 +1071,39 @@
         assert "__len__" not in self.schema
 
         class AggExecutor(Executor):
             def __init__(self) -> None:
                 self.state = None
             def execute(self,batches,stream_id, executor_id):
                 psum = polars.from_arrow(pa.concat_tables(batches)).select(columns + ["__len__"]).to_numpy().sum(axis=0)
-                for batch in batches:
-                    #print(batch)
-                    if self.state is None:
-                        self.state = psum
-                    else:
-                        self.state += psum
+                if self.state is None:
+                    self.state = psum
+                else:
+                    self.state += psum
             def done(self,executor_id):
-                return polars.from_numpy(np.expand_dims(self.state,0), schema = columns + ["__len__"])
+                x = polars.from_numpy(np.expand_dims(self.state,0), schema = columns + ["__len__"])
+                # print(x)
+                return x
             
         def udf2(x):
             x =  polars.from_numpy(np.expand_dims(x.select(columns).to_numpy().sum(axis = 0),0) , schema = columns).hstack(polars.from_dict({"__len__": [len(x)]}))
+            # print(x)
             return x
 
         stream = self.transform( udf2, new_schema = columns + ["__len__"], required_columns = set(columns), foldable=True)
         local_agg_executor = AggExecutor()
         agg_executor = AggExecutor()
         stream = stream.stateful_transform( local_agg_executor , columns + ["__len__"], required_columns = set(columns + ["__len__"]),
                             partitioner=PassThroughPartitioner(), placement_strategy = CustomChannelsStrategy(1))
         mean = stream.stateful_transform( agg_executor , columns + ["__len__"], required_columns = set(columns + ["__len__"]),
                             partitioner=BroadcastPartitioner(), placement_strategy = SingleChannelStrategy()).collect()
         count = mean["__len__"][0]
-        print(count)
+        # print("COUNT", count)
         mean = mean.select(columns)
-        print(mean)
+        # print("MEAN", mean)
         mean /= count
         
         return self.gramian(columns, demean = np.squeeze(mean.to_numpy())).collect() / count
 
     def with_columns_sql(self, new_columns: str, foldable = True):
 
         """
@@ -1876,15 +1877,15 @@
             
             >>> lineitem = qc.read_csv("lineitem.csv")
             
             >>> d = lineitem.filter("l_shipdate <= date '1998-12-01' - interval '90' day")
             
             >>> d = d.with_column("disc_price", lambda x:x["l_extendedprice"] * (1 - x["l_discount"]), required_columns ={"l_extendedprice", "l_discount"})
             
-            I want the sum and average of the l_quantity column and the l_extendedprice colum, the sum of the disc_price column, the minimum of the l_discount
+            I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount
             column, and oh give me the total row count as well.
             
             >>> f = d.agg({"l_quantity":["sum","avg"], "l_extendedprice":["sum","avg"], "disc_price":"sum", "l_discount":"min","*":"count"})
             
         """
 
         return self._grouped_aggregate([], aggregations, None)
@@ -2105,15 +2106,15 @@
             
             >>> lineitem = qc.read_csv("lineitem.csv")
             
             >>> d = lineitem.filter("l_shipdate <= date '1998-12-01' - interval '90' day")
             
             >>> d = d.with_column("disc_price", lambda x:x["l_extendedprice"] * (1 - x["l_discount"]), required_columns ={"l_extendedprice", "l_discount"})
             
-            I want the sum and average of the l_quantity column and the l_extendedprice colum, the sum of the disc_price column, the minimum of the l_discount
+            I want the sum and average of the l_quantity column and the l_extendedprice column, the sum of the disc_price column, the minimum of the l_discount
             column, and oh give me the total row count as well, of each unique combination of l_returnflag and l_linestatus
             
             >>> f = d.groupby(["l_returnflag", "l_linestatus"]).agg({"l_quantity":["sum","avg"], "l_extendedprice":["sum","avg"], "disc_price":"sum", "l_discount":"min","*":"count"})
         """
 
         return self.source_data_stream._grouped_aggregate(self.groupby, aggregations, self.orderby)
```

## pyquokka/df.py

```diff
@@ -529,14 +529,107 @@
                 schema = [k.name for k in f.schema_arrow]
                 token = ray.get(self.catalog.register_disk_parquet_source.remote(table_location))
                 self.nodes[self.latest_node_id] = InputDiskParquetNode(table_location, schema)
                 self.nodes[self.latest_node_id].set_catalog_id(token)
 
         self.latest_node_id += 1
         return DataStream(self, schema, self.latest_node_id - 1)
+
+    def read_lance(self, table_location: str, vec_column: str):
+
+        """
+        
+        """
+
+        try:
+            import lance
+        except:
+            raise Exception("lance not installed. pylance must be installed on each machine in cluster.")
+
+        s3 = boto3.client('s3')
+        if table_location[:5] == "s3://":
+
+            if type(self.cluster) == LocalCluster:
+                print("Warning: trying to read S3 dataset on local machine. This assumes high network bandwidth.")
+
+            table_location = table_location[5:]
+            bucket = table_location.split("/")[0]
+            if "*" in table_location:
+                assert "*" not in table_location[:-1], "wildcard can only be the last character in address string"
+                table_location = table_location[:-1]
+                prefix = "/".join(table_location[:-1].split("/")[1:])
+
+                z = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
+                if 'Contents' not in z:
+                    raise Exception("Wrong S3 path")
+                files = [bucket + "/" + i['Key'] for i in z['Contents'] if i['Key'].endswith(".lance")]
+                while 'NextContinuationToken' in z.keys():
+                    z = s3.list_objects_v2(
+                        Bucket=bucket, Prefix=prefix, ContinuationToken=z['NextContinuationToken'])
+                    files.extend([bucket + "/" + i['Key'] for i in z['Contents']
+                                    if i['Key'].endswith(".parquet")])
+
+                assert len(files) > 0, "could not find any parquet files. make sure they end with .parquet"
+                
+                try:
+                    schema = lance.dataset(files[0]).schema.names
+                    assert vec_column in schema, "vector column not found in schema"
+                except:
+                    raise Exception("""schema discovery failed for Parquet dataset at location {}. Please raise Github issue.
+                                    Lance dataset with S3 does not work with creds set with AWS configure, must use env variables.""".format(table_location))
+                
+                # token = ray.get(self.catalog.register_s3_lance_source.remote(files[0], len(sizes)))
+                self.nodes[self.latest_node_id] = InputLanceNode(files, schema, vec_column)
+                # self.nodes[self.latest_node_id].set_catalog_id(token)
+            else:
+                try:
+                    schema = lance.dataset(files[0]).schema.names
+                    assert vec_column in schema, "vector column not found in schema"
+                except:
+                    raise Exception("""schema discovery failed for Parquet dataset at location {}. 
+                                    Lance dataset with S3 does not work with creds set with AWS configure, must use env variables.
+                                    Note if you are specifying a prefix to many parquet files, must use asterix. E.g.
+                                    qc.read_parquet("s3://rottnest/happy.parquet/*")""".format(table_location))
+                # key = "/".join(table_location.split("/")[1:])
+                # response = s3.head_object(Bucket= bucket, Key=key)
+                # token = ray.get(self.catalog.register_s3_parquet_source.remote(bucket + "/" + key, 1))
+                self.nodes[self.latest_node_id] = InputLanceNode([table_location], schema, vec_column)
+                # self.nodes[self.latest_node_id].set_catalog_id(token)
+
+            # self.nodes[self.latest_node_id].set_placement_strategy(CustomChannelsStrategy(2))
+        else:
+            if type(self.cluster) == EC2Cluster:
+                raise NotImplementedError("Does not support reading local dataset with S3 cluster. Must use S3 bucket.")
+            
+            if "*" in table_location:
+                table_location = table_location[:-1]
+                assert table_location[-1] == "/", "must specify * with entire directory, doesn't support prefixes yet"
+                try:
+                    files = [i for i in os.listdir(table_location) if i.endswith(".lance")]
+                except:
+                    raise Exception("Tried to get list of parquet files at ", table_location, " failed. Make sure specify absolute path and filenames end with .parquet")
+                assert len(files) > 0
+                schema = lance.dataset(files[0]).schema.names
+                # token = ray.get(self.catalog.register_disk_parquet_source.remote(table_location))
+                self.nodes[self.latest_node_id] = InputLanceNode(files, schema, vec_column)
+                # self.nodes[self.latest_node_id].set_catalog_id(token)
+
+            else:
+                
+                
+                try:
+                    schema = lance.dataset(table_location).schema.names
+                except:
+                    raise Exception("schema discovery failed for Parquet dataset at location {}. Please raise Github issue.".format(table_location))
+                # token = ray.get(self.catalog.register_disk_parquet_source.remote(table_location))
+                self.nodes[self.latest_node_id] = InputLanceNode([table_location], schema, vec_column)
+                # self.nodes[self.latest_node_id].set_catalog_id(token)
+
+        self.latest_node_id += 1
+        return DataStream(self, schema, self.latest_node_id - 1)
     
 
     def read_rest_post(self, url, data, schema, headers = {}, batch_size = 10000):
         self.nodes[self.latest_node_id] = InputRestPostAPINode(url, data, headers, schema, batch_size = batch_size)
         self.latest_node_id += 1
         return DataStream(self, schema, self.latest_node_id - 1)
     
@@ -1174,17 +1267,15 @@
                         curr_target_id = self.execution_nodes[curr_node_id]
             
             # curr_node_id and col_name should now be the source node and the column name of the vector
 
             if issubclass(type(self.execution_nodes[curr_node_id]), InputLanceNode):
                 
                 lance_node = self.execution_nodes[curr_node_id]
-                lance_node.query_vectors = copy.deepcopy(node.query_vectors)
-                lance_node.k = node.k
-                lance_node.vec_column = curr_col_name
+                lance_node.set_probe_df(copy.deepcopy(node.probe_df), node.probe_vector_col , node.k)
                 
                 # delete yourself
                 parent = self.execution_nodes[node.parents[0]]
                 for target_id in targets:
                     parent.targets[target_id] = targets[target_id]
                 del parent.targets[node_id]
                 del self.execution_nodes[node_id]
```

## pyquokka/logical.py

```diff
@@ -269,24 +269,57 @@
     def __str__(self):
         result = str(type(self)) + '\nPredicate: ' + str(self.predicate) + '\nProjection: ' + str(self.projection) + '\nTargets:' 
         for target in self.targets:
             result += "\n\t" + str(target) + " " + str(self.targets[target])
         return result
     
 class InputLanceNode(SourceNode):
-    def __init__(self, uri, schema, predicate = None, projection = None, query_vectors = None, k = None) -> None:
+    def __init__(self, uris, schema, vec_column, predicate = None, projection = None) -> None:
         super().__init__(schema)
-        self.uri = uri
+        self.uris = uris
         self.predicate = predicate
         self.projection = projection
-        self.query_vectors = query_vectors
-        self.k = k
+        self.vec_column = vec_column
+
+        self.probe_df = None
+        self.probe_df_col = None
+        self.k = None
     
+    def set_probe_df(self, probe_df, probe_df_col, k):
+
+        assert probe_df is not None and probe_df_col is not None and k is not None, "must provide probe_df, probe_df_col, and k"
+        
+        assert type(probe_df) == polars.DataFrame
+        self.probe_df = probe_df
+        assert type(probe_df_col) == str and probe_df_col in probe_df.columns, "probe_df_col must be a string and in probe_df"
+        self.probe_df_col = probe_df_col
+        self.probe_df_vecs = np.stack(self.probe_df[self.probe_df_col].to_numpy())
+        assert type(k) == int and k >= 1
+        self.k = k
+
     def set_cardinality(self, catalog):
-        return super().set_cardinality(catalog)
+        
+        # assert self.catalog_id is not None
+        for target in self.targets:
+            self.cardinality[target] = None
+    
+    def lower(self, task_graph):
+        lance_reader = InputLanceDataset(self.uris, self.vec_column, columns = list(self.projection) if self.projection is not None else None, filters = self.predicate)
+        if self.probe_df is not None:
+            lance_reader.set_probe_df(self.probe_df, self.probe_df_col, self.k)
+        node = task_graph.new_input_reader_node(lance_reader, self.stage, self.placement_strategy)
+        if self.probe_df is not None:
+            operator = DFProbeDataStreamNNExecutor2(self.vec_col, self.probe_df_vecs, self.probe_df_col, self.k)
+            target_info = TargetInfo(BroadcastPartitioner(), sqlglot.exp.TRUE, None, [])
+            node = task_graph.new_non_blocking_node({0: node}, 
+                        operator, self.stage, SingleChannelStrategy(), 
+                        source_target_info={0: target_info})
+
+        return node
+        
 
 class InputS3ParquetNode(SourceNode):
     def __init__(self, files, schema, predicate = None, projection = None) -> None:
         super().__init__(schema)
         
         self.files = files
         self.predicate = predicate
```

## pyquokka/executors/sql_executors.py

```diff
@@ -412,25 +412,27 @@
     
     def done(self, executor_id):
         return
 
 class SQLAggExecutor(Executor):
     def __init__(self, groupby_keys, orderby_keys, sql_statement) -> None:
         assert type(groupby_keys) == list
+        if orderby_keys is not None:
+            assert type(orderby_keys) == list
         if len(groupby_keys) > 0:
             self.agg_clause = "select " + ",".join(groupby_keys) + ", " + sql_statement + " from batch_arrow"
         else:
             self.agg_clause = "select " + sql_statement + " from batch_arrow"
         if len(groupby_keys) > 0:
             self.agg_clause += " group by "
             for key in groupby_keys:
                 self.agg_clause += key + ","
             self.agg_clause = self.agg_clause[:-1]
 
-        if orderby_keys is not None:
+        if orderby_keys is not None and len(orderby_keys) > 0:
             self.agg_clause += " order by "
             for key, dir in orderby_keys:
                 if dir == "desc":
                     self.agg_clause += key + " desc,"
                 else:
                     self.agg_clause += key + ","
             self.agg_clause = self.agg_clause[:-1]
```

## pyquokka/executors/vector_executors.py

```diff
@@ -17,17 +17,18 @@
         self.query_vecs = query_vecs / np.linalg.norm(query_vecs, axis = 1, keepdims = True)
         self.k = k
 
     def execute(self, batches, stream_id, executor_id):
         batch = pa.concat_tables(batches)
         vectors = np.stack(batch[self.vec_col].to_numpy())
         normalized_vectors = vectors / np.linalg.norm(vectors, axis = 1, keepdims = True)
-        distances = np.dot(normalized_vectors, self.query_vecs.T)
-        indices = np.argsort(distances, axis = 0)[-self.k:].flatten()
-
+        with threadpool_limits(limits=8, user_api='blas'):
+            distances = np.dot(normalized_vectors, self.query_vecs.T)
+        indices = np.argsort(distances, axis = 0)[-self.k:].T.flatten()
+        print(indices)
         # you could be smarter here and keep track of separate candidate sets for each probe, but let's leave this for an intern.
         return batch.take(indices)
     
     def done(self, executor_id):
         return
 
 
@@ -40,27 +41,57 @@
         query_vecs = np.stack(query_df[query_vec_col].to_numpy())
         self.query_df = query_df
         self.query_vecs = query_vecs / np.linalg.norm(query_vecs, axis = 1, keepdims = True)
         self.k = k
         self.state = None
 
     def execute(self, batches, stream_id, executor_id):
+
+        # each batch should have shape len(query_df) * k
+        # you could start throwing away stuff as each batch comes in, but you can just wait until the end if the number of files is reasonably small
+
         batch = pa.concat_tables(batches)
         if self.state is None:
             self.state = batch
         else:
             self.state = pa.concat_tables([self.state, batch])
 
     def done(self, executor_id):
+
         vectors = np.stack(self.state[self.vec_col].to_numpy())
         normalized_vectors = vectors / np.linalg.norm(vectors, axis = 1, keepdims = True)
+
+        # the length of normalized_vectors is N * len(probe_df) * k, where N is the number of Parquet files.
+        # each probe vector should now probe only its corersponding vectors
+        # this is going to be some complicated math, but I used to be good at this stuff, so let's see how it goes.
+
+        vector_dim = normalized_vectors.shape[-1]
+        normalized_vectors = normalized_vectors.reshape(-1, len(self.query_df), self.k, vector_dim)
+        N = normalized_vectors.shape[0]
+        normalized_vectors = normalized_vectors.transpose(1, 0, 2, 3).reshape(len(self.query_df), N * self.k, vector_dim)
+
+        # now normalized_vectors has shape len(probe_df) * (N * k) * dim, query_vecs has shape len(probe_df) * dim
+        # we can now do the batch matrix multiplication
+        query_vecs = self.query_vecs[:, np.newaxis, :]
         with threadpool_limits(limits=8, user_api='blas'):
-            distances = np.dot(normalized_vectors, self.query_vecs.T)
-        indices = np.argsort(distances, axis = 0)[-self.k:]
-        # indices will have shape k * num_queries
-        flat_indices = indices.T.flatten()
-        matched_vectors = polars.from_arrow(self.state.take(flat_indices))
-        join_indices = np.repeat(np.arange(len(self.query_df)), self.k)
-        matched_queries = self.query_df[join_indices]
+            distances = np.matmul(normalized_vectors, query_vecs.transpose(0, 2, 1)).squeeze()
+    
+        # distances has shape len(probe_df) * (N * k)
+        indices = np.argsort(distances, axis = 1)[:, -self.k:]
+        # indices has shape len(probe_df) * k
+
+        # print(indices)
+        # print(self.state)
+
+        results = []
+        # each index has a value between 0 and N * k - 1, you need to translate that to an index into self.state
+        for i in range(len(indices)):
+            index_group = indices[i] // self.k 
+            index_in_group = indices[i] % self.k
+            new_indices = index_group * len(self.query_df) * self.k + i * self.k + index_in_group
+            results.append(self.state.take(new_indices))
         
-        # upstream should already make sure there is no overlapping column names here
-        return polars.concat([matched_queries, matched_vectors], how = "horizontal")
+        matched_vectors = pa.concat_tables(results)
+        join_indices = np.repeat(np.arange(len(self.query_df)), self.k)
+        matched_queries = self.query_df[join_indices].to_arrow()
+    
+        return pa.Table.from_arrays(matched_vectors.columns + matched_queries.columns, names=matched_vectors.column_names + matched_queries.column_names)
```

## Comparing `pyquokka-0.2.9.dist-info/LICENSE` & `pyquokka-0.3.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyquokka-0.2.9.dist-info/METADATA` & `pyquokka-0.3.0.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 Metadata-Version: 2.1
 Name: pyquokka
-Version: 0.2.9
+Version: 0.3.0
 Summary: Quokka
 Author: Tony Wang
 Author-email: zihengw@stanford.edu
 License: http://www.apache.org/licenses/LICENSE-2.0
 Keywords: python
+Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
 Classifier: Programming Language :: Python :: 3
 Classifier: Operating System :: MacOS :: MacOS X
 Classifier: Operating System :: POSIX :: Linux
 License-File: LICENSE
 Requires-Dist: cffi
 Requires-Dist: pyarrow
@@ -17,25 +18,25 @@
 Requires-Dist: redis
 Requires-Dist: boto3
 Requires-Dist: numpy
 Requires-Dist: pandas
 Requires-Dist: protobuf
 Requires-Dist: ray (>=2.0.0)
 Requires-Dist: psutil
-Requires-Dist: h5py
 Requires-Dist: polars (>=0.17.0)
 Requires-Dist: sqlglot (>=11.4.2)
 Requires-Dist: graphviz
 Requires-Dist: tqdm
 Requires-Dist: aiohttp
 Requires-Dist: botocore
-Requires-Dist: ldbpy
 Requires-Dist: threadpoolctl
 Requires-Dist: parallel-ssh
 
 
 Dope way to do cloud analytics
 
 Check out https://github.com/marsupialtail/quokka
 
 or https://marsupialtail.github.io/quokka/
 
+
+
```

## Comparing `pyquokka-0.2.9.dist-info/RECORD` & `pyquokka-0.3.0.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,39 +1,40 @@
-pyquokka/__init__.py,sha256=wDw5IDuVPjlYJ2iMpyYDOZRhteJtZsDQSbpyb9B_qdg,309
+pyquokka/__init__.py,sha256=pBsjKUKy3Fdy47TuKyvWUGKAppifFP4F64l_ZuSwV80,309
 pyquokka/catalog.py,sha256=XczH9nRAjYE0eWvPdoI_sQE1N85vflPouzjpIe0lqOA,4291
 pyquokka/common_startup.sh,sha256=Rst9lyiO_Fx34swl6_-Z8w7lRHQM-8GDhsUFCbS_kP8,372
 pyquokka/coordinator.py,sha256=s7yjNB_F6bEoWoko_5cTfN0BAyH9AcYm2PKIbqJexhI,28239
-pyquokka/core.py,sha256=BQoZLOBXUpI2peE8rIgp1a0gC-Pk0nMCzlz8V1dkq6s,48179
-pyquokka/dataset.py,sha256=vgKT07HkV_P0LVu5JHijEHvYUkeQZElbgLnMC8yw_jg,48426
-pyquokka/datastream.py,sha256=X34J074YncZqikKloV4wHGdpTB5kGoBv6TluPSVv1nY,98944
+pyquokka/core.py,sha256=lvMrsMc11UTuDE29eri3m4FDHq1wxxVj5n2WQRTPR3k,48407
+pyquokka/dataset.py,sha256=C9khUFsw2uvqDrw066Sc9nXS_JiRcY22JbJRcocpjyI,49168
+pyquokka/datastream.py,sha256=3nR-fmvqIYB0-9Iy1Zp9RjZzjP7VmKFOSEDdsEpVSxI,98957
 pyquokka/debugger.py,sha256=Yi1CqGHbV2y2bszUhKuxcQ561vugc6hs6xfnpJ8HIjU,1438
-pyquokka/df.py,sha256=C3vAduJYuMW-bM8uPl6UGpiuM8hslQ7unGeMfKx9ji8,76119
+pyquokka/df.py,sha256=DJXVI-kFwxs7WswQlJHRPWTHVL3JLZFO6GXvWRC57S8,81396
 pyquokka/executors.py,sha256=dVzy3_WbsMtbAWay2UlEgE1GmxEVbT6Fz1X9i2elboY,37360
 pyquokka/expression.py,sha256=SmJxvGrSgW3ra8EU5SLGbQ8AxbJ2bPxKHLvbXpQ-8WY,12657
 pyquokka/flight.py,sha256=ri1aEXtn4s2_ACD8PfU5EIy_fA7FlNrM61WuUjQ9Pl8,17483
 pyquokka/hbq.py,sha256=V3nE2IcIIclxxdDyXRdzuV7_y_DRa90B_aVaa6aU9tg,3098
 pyquokka/ldb.so,sha256=SO70uhXN219Ns5RNIEePv1fs0qRSksc6yq2eN0tucw8,368480
 pyquokka/leader_start_ray.sh,sha256=vyZi-Utmj7r8Qr43YmOQq3uIuAN5Nn4F61MIszzdit4,274
 pyquokka/leader_startup.sh,sha256=yOP5vjuLS9D2WtcozcFewXQB84p8jm8IcyltYcuWNss,619
-pyquokka/logical.py,sha256=pSxywcLUYY60w8XMZEa7PgOokGpl8vQW_yij6hZVohE,29383
+pyquokka/logical.py,sha256=ydgvSN800xiuuy41p9pqpBe4m3DbvVtiP-k_h8LTjvg,30992
 pyquokka/orderedstream.py,sha256=ciirQMgndCbWOurDGtaZBD0TRF8txdnNB0cIad_wxrc,9100
 pyquokka/placement_strategy.py,sha256=KX1hEDHCTBUfUoQ64Zv5e7iVjTKRh4oAFFs03HIxDqQ,886
 pyquokka/quokka_dataset.py,sha256=_b7L8zWCimtiR3kMbCZWLH4g-w3SZ-J8gw8bjqhv-lA,3717
 pyquokka/quokka_runtime.py,sha256=LG8kBwWlR20AkVZBHyRcP280qwKr_NAB-TFLyIvinv0,19735
 pyquokka/redis.conf,sha256=Hk0GU-BnDDpMZ6Gmit1-Ct_iyO7ttCvzyfz5PLFVkJY,93718
+pyquokka/sql.py,sha256=npTL6_m57gP9NRzLX9SYGP74_Vy0jhy5zR4zo0LINN0,10625
 pyquokka/sql_utils.py,sha256=1vWMrQft37so5KtOLlsqLk8MpaFB0XO9Ib9XwQEuw90,16471
 pyquokka/state.py,sha256=wGt5uh_ZS-xV-HqVgWmdTZUnQoabpvSOHiQXOejg7L4,2371
 pyquokka/tables.py,sha256=58vsFPBKOEKZ-7Ei7GbhXKsAgw6JguwdfQhayRJrjEI,11695
 pyquokka/target_info.py,sha256=RVldOYWGgxv9YXMSN3J3S-UbSEJxeUMoYY1LcHT3edg,2351
 pyquokka/task.py,sha256=_O_itxDxzXnjojys8___CKSaApTnEpUxVrAvWaU-AVI,5752
 pyquokka/utils.py,sha256=sn_Rs0BIGfkmtN8rNfWGOSKJ_1btLTDk8-SW9SazJQ0,39151
 pyquokka/windowtypes.py,sha256=zPh9QgwSQ3E00eNQM8jk2iQZNMuzAQ3eoaFDm1H5NGk,4081
 pyquokka/executors/__init__.py,sha256=48lCF53HMa4Od5yyQfV8T2X0aLL5q9LZ0XVO_0-_MtA,146
 pyquokka/executors/base_executor.py,sha256=clVkP0wf03OMG7x9vMdzRPTviT5DE6rQv3n6E81bZM4,811
-pyquokka/executors/sql_executors.py,sha256=35xL26SqKQQCwqmCVyWxHKtdkHpWCbGoSjvoufJqHFk,17511
+pyquokka/executors/sql_executors.py,sha256=L9y3_mXtWXON1IsSHzuhY_kyyf3cit0CSisG8cz5NzU,17620
 pyquokka/executors/ts_executors.py,sha256=KPRjeMap_PogESYfqYpCBrLyVgRKVrGxWQYICQRehEo,17185
-pyquokka/executors/vector_executors.py,sha256=hs7mrwyXf3Dcvn0pvG9p_IFaJgX0xbCfxUaFj7-WCGQ,2870
-pyquokka-0.2.9.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-pyquokka-0.2.9.dist-info/METADATA,sha256=D9BHEUARLypwBwZ2Nfi21vXS-uv5ieTWIv-z8AdPb9U,1020
-pyquokka-0.2.9.dist-info/WHEEL,sha256=2wepM1nk4DS4eFpYrW1TTqPcoGNfHhhO_i5m4cOimbo,92
-pyquokka-0.2.9.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
-pyquokka-0.2.9.dist-info/RECORD,,
+pyquokka/executors/vector_executors.py,sha256=M_yRDAJkQvVC-IeDDWLTczabcFAvMqWKCPiCLn_W9dc,4503
+pyquokka-0.3.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+pyquokka-0.3.0.dist-info/METADATA,sha256=IygPZwrh1a1nuy1PXMGzIBtxaPtbBKYGrYCwGG6Z9Is,999
+pyquokka-0.3.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+pyquokka-0.3.0.dist-info/top_level.txt,sha256=u5sX_ng3imCHha6-wOUpEO0V2TufF_OHADKxb38hwHg,9
+pyquokka-0.3.0.dist-info/RECORD,,
```

