# Comparing `tmp/deep_training-0.1.4-py3-none-any.whl.zip` & `tmp/deep_training-0.1.5rc0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,175 +1,179 @@
-Zip file size: 310384 bytes, number of entries: 173
--rw-rw-rw-  2.0 fat       47 b- defN 23-Apr-10 11:44 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      905 b- defN 23-May-02 16:26 deep_training/setup.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-10 11:44 deep_training/cv/__init__.py
--rw-rw-rw-  2.0 fat      195 b- defN 23-Apr-10 11:44 deep_training/data_helper/__init__.py
--rw-rw-rw-  2.0 fat    17724 b- defN 23-May-02 04:53 deep_training/data_helper/data_helper.py
--rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-28 11:47 deep_training/data_helper/data_module.py
--rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-28 11:47 deep_training/data_helper/training_args.py
--rw-rw-rw-  2.0 fat       70 b- defN 23-Apr-10 11:44 deep_training/nlp/__init__.py
--rw-rw-rw-  2.0 fat       56 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/__init__.py
--rw-rw-rw-  2.0 fat      241 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/activate.py
--rw-rw-rw-  2.0 fat    13271 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/crf.py
--rw-rw-rw-  2.0 fat     4653 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/handshakingkernel.py
--rw-rw-rw-  2.0 fat      435 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/mask.py
--rw-rw-rw-  2.0 fat     1319 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/mhslayer.py
--rw-rw-rw-  2.0 fat     5911 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/norm.py
--rw-rw-rw-  2.0 fat     1406 b- defN 23-Apr-28 11:47 deep_training/nlp/layers/ppo.py
--rw-rw-rw-  2.0 fat     1220 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/prefix_encoder.py
--rw-rw-rw-  2.0 fat     7259 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/seq_pointer.py
--rw-rw-rw-  2.0 fat     3550 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/w2ner.py
--rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/lora_v1/__init__.py
--rw-rw-rw-  2.0 fat    15095 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/lora_v1/layers.py
--rw-rw-rw-  2.0 fat     1819 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/lora_v1/utils.py
--rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-10 15:59 deep_training/nlp/layers/lora_v2/__init__.py
--rw-rw-rw-  2.0 fat    15465 b- defN 23-Apr-11 15:24 deep_training/nlp/layers/lora_v2/adalora.py
--rw-rw-rw-  2.0 fat     8565 b- defN 23-Apr-11 15:24 deep_training/nlp/layers/lora_v2/layers.py
--rw-rw-rw-  2.0 fat     9106 b- defN 23-Apr-10 11:44 deep_training/nlp/layers/lora_v2/utils.py
--rw-rw-rw-  2.0 fat       80 b- defN 23-May-02 03:45 deep_training/nlp/layers/prompt/__init__.py
--rw-rw-rw-  2.0 fat    15541 b- defN 23-May-02 04:41 deep_training/nlp/layers/prompt/adaption_prompt.py
--rw-rw-rw-  2.0 fat     5463 b- defN 23-May-02 04:41 deep_training/nlp/layers/prompt/p_tuning.py
--rw-rw-rw-  2.0 fat     3053 b- defN 23-May-02 03:45 deep_training/nlp/layers/prompt/prefix_tuning.py
--rw-rw-rw-  2.0 fat     3523 b- defN 23-May-02 04:42 deep_training/nlp/layers/prompt/prompt_tuning.py
--rw-rw-rw-  2.0 fat     9106 b- defN 23-May-02 03:45 deep_training/nlp/layers/prompt/utils.py
--rw-rw-rw-  2.0 fat     3662 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/BatchAllTripletLoss.py
--rw-rw-rw-  2.0 fat     3880 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/BatchHardSoftMarginTripletLoss.py
--rw-rw-rw-  2.0 fat     8358 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/BatchHardTripletLoss.py
--rw-rw-rw-  2.0 fat     4552 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/BatchSemiHardTripletLoss.py
--rw-rw-rw-  2.0 fat     2255 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/ContrastiveLoss.py
--rw-rw-rw-  2.0 fat     4573 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/ContrastiveTensionLoss.py
--rw-rw-rw-  2.0 fat     1359 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/CosineSimilarityLoss.py
--rw-rw-rw-  2.0 fat      742 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/MSELoss.py
--rw-rw-rw-  2.0 fat     1315 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/MarginMSELoss.py
--rw-rw-rw-  2.0 fat     5302 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/MegaBatchMarginLoss.py
--rw-rw-rw-  2.0 fat     2420 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/MultipleNegativesRankingLoss.py
--rw-rw-rw-  2.0 fat     2905 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/MultipleNegativesSymmetricRankingLoss.py
--rw-rw-rw-  2.0 fat     1863 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/OnlineContrastiveLoss.py
--rw-rw-rw-  2.0 fat     2880 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/SoftmaxLoss.py
--rw-rw-rw-  2.0 fat     2306 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/TripletLoss.py
--rw-rw-rw-  2.0 fat      599 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/__init__.py
--rw-rw-rw-  2.0 fat      661 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/bce_loss.py
--rw-rw-rw-  2.0 fat     1397 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/center_loss.py
--rw-rw-rw-  2.0 fat     1772 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/circle_loss.py
--rw-rw-rw-  2.0 fat     1056 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/contrast.py
--rw-rw-rw-  2.0 fat      619 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/dice_loss.py
--rw-rw-rw-  2.0 fat      710 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/focal_loss.py
--rw-rw-rw-  2.0 fat      882 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/label_smoothing.py
--rw-rw-rw-  2.0 fat      547 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/lm_loss.py
--rw-rw-rw-  2.0 fat     2149 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_arcface.py
--rw-rw-rw-  2.0 fat      962 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_casrel.py
--rw-rw-rw-  2.0 fat     1496 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_cosent.py
--rw-rw-rw-  2.0 fat     1912 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_cosface.py
--rw-rw-rw-  2.0 fat     2223 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_globalpointer.py
--rw-rw-rw-  2.0 fat     6020 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_infonce.py
--rw-rw-rw-  2.0 fat      884 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_kl.py
--rw-rw-rw-  2.0 fat     1270 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_mhslinker.py
--rw-rw-rw-  2.0 fat      617 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_r-drop.py
--rw-rw-rw-  2.0 fat     2656 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_sphereface.py
--rw-rw-rw-  2.0 fat      562 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_splinker.py
--rw-rw-rw-  2.0 fat    10822 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_spn4re.py
--rw-rw-rw-  2.0 fat     5644 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/loss_tplinker.py
--rw-rw-rw-  2.0 fat     2466 b- defN 23-Apr-10 11:44 deep_training/nlp/losses/utils.py
--rw-rw-rw-  2.0 fat       71 b- defN 23-Apr-10 11:44 deep_training/nlp/metrics/__init__.py
--rw-rw-rw-  2.0 fat      655 b- defN 23-Apr-10 11:44 deep_training/nlp/metrics/pointer.py
--rw-rw-rw-  2.0 fat       58 b- defN 23-Apr-10 11:44 deep_training/nlp/models/__init__.py
--rw-rw-rw-  2.0 fat     6826 b- defN 23-Apr-28 11:47 deep_training/nlp/models/casrel.py
--rw-rw-rw-  2.0 fat     5093 b- defN 23-Apr-28 11:47 deep_training/nlp/models/crf_cascad.py
--rw-rw-rw-  2.0 fat     1588 b- defN 23-Apr-28 11:47 deep_training/nlp/models/crf_model.py
--rw-rw-rw-  2.0 fat    12985 b- defN 23-Apr-28 11:47 deep_training/nlp/models/diffcse.py
--rw-rw-rw-  2.0 fat     5395 b- defN 23-Apr-28 11:47 deep_training/nlp/models/esimcse.py
--rw-rw-rw-  2.0 fat     4209 b- defN 23-Apr-28 11:47 deep_training/nlp/models/gec_model.py
--rw-rw-rw-  2.0 fat    10854 b- defN 23-Apr-28 11:47 deep_training/nlp/models/gplinker.py
--rw-rw-rw-  2.0 fat     3814 b- defN 23-Apr-28 11:47 deep_training/nlp/models/infonce.py
--rw-rw-rw-  2.0 fat     2459 b- defN 23-Apr-28 11:47 deep_training/nlp/models/mhs_ner.py
--rw-rw-rw-  2.0 fat     5991 b- defN 23-Apr-28 11:47 deep_training/nlp/models/mhslinker.py
--rw-rw-rw-  2.0 fat     4661 b- defN 23-Apr-28 11:47 deep_training/nlp/models/onerel_model.py
--rw-rw-rw-  2.0 fat     2750 b- defN 23-Apr-28 11:47 deep_training/nlp/models/pointer.py
--rw-rw-rw-  2.0 fat    13392 b- defN 23-Apr-28 11:47 deep_training/nlp/models/prefixtuning.py
--rw-rw-rw-  2.0 fat    15915 b- defN 23-Apr-28 11:47 deep_training/nlp/models/prgc_model.py
--rw-rw-rw-  2.0 fat    16115 b- defN 23-Apr-28 11:47 deep_training/nlp/models/promptbert_cse.py
--rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-28 11:47 deep_training/nlp/models/pure_model.py
--rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-28 11:47 deep_training/nlp/models/simcse.py
--rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-28 11:47 deep_training/nlp/models/span_ner.py
--rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-28 11:47 deep_training/nlp/models/spn4re.py
--rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-28 11:47 deep_training/nlp/models/tplinker.py
--rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-28 11:47 deep_training/nlp/models/tplinkerplus.py
--rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-28 11:47 deep_training/nlp/models/transformer.py
--rw-rw-rw-  2.0 fat    26238 b- defN 23-May-02 15:42 deep_training/nlp/models/transformer_base.py
--rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-28 11:47 deep_training/nlp/models/tsdae_model.py
--rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-28 11:47 deep_training/nlp/models/w2ner.py
--rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 11:47 deep_training/nlp/models/LLaMA/__init__.py
--rw-rw-rw-  2.0 fat     5087 b- defN 23-Apr-10 11:44 deep_training/nlp/models/LLaMA/configuration.py
--rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 11:47 deep_training/nlp/models/LLaMA_parallel/__init__.py
--rw-rw-rw-  2.0 fat     5087 b- defN 23-Apr-10 11:44 deep_training/nlp/models/LLaMA_parallel/configuration.py
--rw-rw-rw-  2.0 fat    31627 b- defN 23-Apr-10 11:44 deep_training/nlp/models/PaLM/__init__.py
--rw-rw-rw-  2.0 fat     5890 b- defN 23-Apr-10 11:44 deep_training/nlp/models/PaLM/configuration.py
--rw-rw-rw-  2.0 fat    60510 b- defN 23-May-02 03:45 deep_training/nlp/models/chatglm/__init__.py
--rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 11:44 deep_training/nlp/models/chatglm/configuration.py
--rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-10 11:44 deep_training/nlp/models/chatglm/quantization.py
--rw-rw-rw-  2.0 fat    16642 b- defN 23-Apr-18 11:05 deep_training/nlp/models/chatglm/tokenization.py
--rw-rw-rw-  2.0 fat    34123 b- defN 23-Apr-10 11:44 deep_training/nlp/models/laMDA/__init__.py
--rw-rw-rw-  2.0 fat     5981 b- defN 23-Apr-10 11:44 deep_training/nlp/models/laMDA/configuration.py
--rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 10:52 deep_training/nlp/models/lora/__init__.py
--rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-10 16:34 deep_training/nlp/models/lora/v1/__init__.py
--rw-rw-rw-  2.0 fat     7054 b- defN 23-Apr-10 11:44 deep_training/nlp/models/lora/v1/configuration.py
--rw-rw-rw-  2.0 fat    13688 b- defN 23-May-02 06:57 deep_training/nlp/models/lora/v1/lora_wrapper.py
--rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 10:52 deep_training/nlp/models/lora/v2/__init__.py
--rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-10 16:38 deep_training/nlp/models/lora/v2/adalora_model.py
--rw-rw-rw-  2.0 fat    11285 b- defN 23-Apr-28 11:47 deep_training/nlp/models/lora/v2/configuration.py
--rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-18 11:05 deep_training/nlp/models/lora/v2/lora_model.py
--rw-rw-rw-  2.0 fat    10415 b- defN 23-May-02 06:52 deep_training/nlp/models/lora/v2/lora_wrapper.py
--rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-10 12:59 deep_training/nlp/models/lora/v2/save_and_load.py
--rw-rw-rw-  2.0 fat      467 b- defN 23-Apr-28 11:47 deep_training/nlp/models/moss/__init__.py
--rw-rw-rw-  2.0 fat     5097 b- defN 23-Apr-28 11:47 deep_training/nlp/models/moss/configuration_moss.py
--rw-rw-rw-  2.0 fat     6735 b- defN 23-Apr-28 11:47 deep_training/nlp/models/moss/custom_autotune.py
--rw-rw-rw-  2.0 fat    31079 b- defN 23-Apr-28 11:47 deep_training/nlp/models/moss/modeling_moss.py
--rw-rw-rw-  2.0 fat    18744 b- defN 23-May-02 16:30 deep_training/nlp/models/moss/quantization.py
--rw-rw-rw-  2.0 fat    15939 b- defN 23-Apr-28 11:47 deep_training/nlp/models/moss/tokenization_moss.py
--rw-rw-rw-  2.0 fat      203 b- defN 23-May-02 06:07 deep_training/nlp/models/prompt/__init__.py
--rw-rw-rw-  2.0 fat    12062 b- defN 23-May-02 16:25 deep_training/nlp/models/prompt/configuration.py
--rw-rw-rw-  2.0 fat    52102 b- defN 23-May-02 16:22 deep_training/nlp/models/prompt/prompt_model.py
--rw-rw-rw-  2.0 fat     3551 b- defN 23-May-02 15:49 deep_training/nlp/models/prompt/save_and_load.py
--rw-rw-rw-  2.0 fat     1917 b- defN 23-May-02 05:50 deep_training/nlp/models/prompt/utils.py
--rw-rw-rw-  2.0 fat      102 b- defN 23-Apr-10 11:44 deep_training/nlp/models/splinker/__init__.py
--rw-rw-rw-  2.0 fat     2866 b- defN 23-Apr-28 11:47 deep_training/nlp/models/splinker/splinker.py
--rw-rw-rw-  2.0 fat    14478 b- defN 23-Apr-10 11:44 deep_training/nlp/models/t5decoder/__init__.py
--rw-rw-rw-  2.0 fat     6646 b- defN 23-Apr-10 11:44 deep_training/nlp/models/t5encoder/__init__.py
--rw-rw-rw-  2.0 fat       56 b- defN 23-Apr-10 11:44 deep_training/nlp/optimizer/__init__.py
--rw-rw-rw-  2.0 fat     5225 b- defN 23-Apr-10 11:44 deep_training/nlp/optimizer/lamb.py
--rw-rw-rw-  2.0 fat       99 b- defN 23-Apr-10 11:44 deep_training/nlp/optimizer/lion/__init__.py
--rw-rw-rw-  2.0 fat     2295 b- defN 23-Apr-10 11:44 deep_training/nlp/optimizer/lion/lion.py
--rw-rw-rw-  2.0 fat     2198 b- defN 23-Apr-10 11:44 deep_training/nlp/optimizer/lion/triton.py
--rw-rw-rw-  2.0 fat       79 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/__init__.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo/__init__.py
--rw-rw-rw-  2.0 fat     2238 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo/configuration.py
--rw-rw-rw-  2.0 fat     2691 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo/data_type.py
--rw-rw-rw-  2.0 fat     8063 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo/ppo.py
--rw-rw-rw-  2.0 fat    16348 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo/ppo_dataset.py
--rw-rw-rw-  2.0 fat    10496 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo/utils.py
--rw-rw-rw-  2.0 fat       54 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo_fabric/__init__.py
--rw-rw-rw-  2.0 fat     8621 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo_fabric/engine.py
--rw-rw-rw-  2.0 fat     5736 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo_fabric/ppo.py
--rw-rw-rw-  2.0 fat     5977 b- defN 23-Apr-28 11:47 deep_training/nlp/rl/ppo_fabric/ppo_agent.py
--rw-rw-rw-  2.0 fat     2868 b- defN 23-Apr-10 11:44 deep_training/nlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat     6986 b- defN 23-Apr-28 11:47 deep_training/nlp/utils/__init__.py
--rw-rw-rw-  2.0 fat     6323 b- defN 23-Apr-10 11:44 deep_training/nlp/utils/adversarial.py
--rw-rw-rw-  2.0 fat    15256 b- defN 23-Apr-10 11:44 deep_training/nlp/utils/nlputils.py
--rw-rw-rw-  2.0 fat      795 b- defN 23-Apr-10 11:44 deep_training/nlp/utils/spearman.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/layers/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/losses/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/metrics/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/models/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/optimizer/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/scheduler/__init__.py
--rw-rw-rw-  2.0 fat       53 b- defN 23-Apr-10 11:44 deep_training/tfnlp/utils/__init__.py
--rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-10 11:44 deep_training/utils/__init__.py
--rw-rw-rw-  2.0 fat     1941 b- defN 23-Apr-10 11:44 deep_training/utils/distributed.py
--rw-rw-rw-  2.0 fat     1724 b- defN 23-Apr-10 11:44 deep_training/utils/func.py
--rw-rw-rw-  2.0 fat     5117 b- defN 23-Apr-10 11:44 deep_training/utils/maskedlm.py
--rw-rw-rw-  2.0 fat     7430 b- defN 23-Apr-28 11:47 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      590 b- defN 23-May-02 16:46 deep_training-0.1.4.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-02 16:46 deep_training-0.1.4.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-May-02 16:46 deep_training-0.1.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    16765 b- defN 23-May-02 16:46 deep_training-0.1.4.dist-info/RECORD
-173 files, 1072252 bytes uncompressed, 283222 bytes compressed:  73.6%
+Zip file size: 338483 bytes, number of entries: 177
+-rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
+-rw-rw-rw-  2.0 fat      900 b- defN 23-May-10 06:45 deep_training/setup.py
+-rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
+-rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
+-rw-rw-rw-  2.0 fat    17724 b- defN 23-May-04 00:28 deep_training/data_helper/data_helper.py
+-rw-rw-rw-  2.0 fat     5041 b- defN 23-Apr-28 06:13 deep_training/data_helper/data_module.py
+-rw-rw-rw-  2.0 fat    12121 b- defN 23-Apr-27 00:33 deep_training/data_helper/training_args.py
+-rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
+-rw-rw-rw-  2.0 fat       56 b- defN 22-Nov-10 08:28 deep_training/nlp/layers/__init__.py
+-rw-rw-rw-  2.0 fat      241 b- defN 23-Mar-13 05:48 deep_training/nlp/layers/activate.py
+-rw-rw-rw-  2.0 fat    13271 b- defN 22-Nov-14 00:17 deep_training/nlp/layers/crf.py
+-rw-rw-rw-  2.0 fat     4653 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/handshakingkernel.py
+-rw-rw-rw-  2.0 fat      435 b- defN 22-Dec-02 00:22 deep_training/nlp/layers/mask.py
+-rw-rw-rw-  2.0 fat     1319 b- defN 22-Dec-12 00:12 deep_training/nlp/layers/mhslayer.py
+-rw-rw-rw-  2.0 fat     5911 b- defN 22-Dec-08 00:08 deep_training/nlp/layers/norm.py
+-rw-rw-rw-  2.0 fat     1406 b- defN 23-Apr-28 00:24 deep_training/nlp/layers/ppo.py
+-rw-rw-rw-  2.0 fat     1220 b- defN 22-Jul-21 00:57 deep_training/nlp/layers/prefix_encoder.py
+-rw-rw-rw-  2.0 fat     7259 b- defN 22-Dec-14 02:36 deep_training/nlp/layers/seq_pointer.py
+-rw-rw-rw-  2.0 fat     3550 b- defN 22-Dec-15 00:57 deep_training/nlp/layers/w2ner.py
+-rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/__init__.py
+-rw-rw-rw-  2.0 fat    15095 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/layers.py
+-rw-rw-rw-  2.0 fat     1819 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/utils.py
+-rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v2/__init__.py
+-rw-rw-rw-  2.0 fat    15465 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/adalora.py
+-rw-rw-rw-  2.0 fat     8565 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/layers.py
+-rw-rw-rw-  2.0 fat     9287 b- defN 23-May-10 01:12 deep_training/nlp/layers/lora_v2/utils.py
+-rw-rw-rw-  2.0 fat       80 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/__init__.py
+-rw-rw-rw-  2.0 fat    15541 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/adaption_prompt.py
+-rw-rw-rw-  2.0 fat     5463 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/p_tuning.py
+-rw-rw-rw-  2.0 fat     3053 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/prefix_tuning.py
+-rw-rw-rw-  2.0 fat     3523 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/prompt_tuning.py
+-rw-rw-rw-  2.0 fat     9106 b- defN 23-May-04 00:28 deep_training/nlp/layers/prompt/utils.py
+-rw-rw-rw-  2.0 fat     3662 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchAllTripletLoss.py
+-rw-rw-rw-  2.0 fat     3880 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchHardSoftMarginTripletLoss.py
+-rw-rw-rw-  2.0 fat     8358 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchHardTripletLoss.py
+-rw-rw-rw-  2.0 fat     4552 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchSemiHardTripletLoss.py
+-rw-rw-rw-  2.0 fat     2255 b- defN 22-Nov-18 01:05 deep_training/nlp/losses/ContrastiveLoss.py
+-rw-rw-rw-  2.0 fat     4573 b- defN 22-Nov-16 07:04 deep_training/nlp/losses/ContrastiveTensionLoss.py
+-rw-rw-rw-  2.0 fat     1359 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/CosineSimilarityLoss.py
+-rw-rw-rw-  2.0 fat      742 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/MSELoss.py
+-rw-rw-rw-  2.0 fat     1315 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/MarginMSELoss.py
+-rw-rw-rw-  2.0 fat     5302 b- defN 22-Nov-16 07:11 deep_training/nlp/losses/MegaBatchMarginLoss.py
+-rw-rw-rw-  2.0 fat     2420 b- defN 23-Jan-18 08:08 deep_training/nlp/losses/MultipleNegativesRankingLoss.py
+-rw-rw-rw-  2.0 fat     2905 b- defN 22-Nov-16 07:11 deep_training/nlp/losses/MultipleNegativesSymmetricRankingLoss.py
+-rw-rw-rw-  2.0 fat     1863 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/OnlineContrastiveLoss.py
+-rw-rw-rw-  2.0 fat     2880 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/SoftmaxLoss.py
+-rw-rw-rw-  2.0 fat     2306 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/TripletLoss.py
+-rw-rw-rw-  2.0 fat      599 b- defN 22-Nov-17 00:20 deep_training/nlp/losses/__init__.py
+-rw-rw-rw-  2.0 fat      661 b- defN 22-Dec-02 08:50 deep_training/nlp/losses/bce_loss.py
+-rw-rw-rw-  2.0 fat     1397 b- defN 22-Nov-17 05:38 deep_training/nlp/losses/center_loss.py
+-rw-rw-rw-  2.0 fat     1772 b- defN 22-Nov-17 07:28 deep_training/nlp/losses/circle_loss.py
+-rw-rw-rw-  2.0 fat     1056 b- defN 23-Jan-13 07:26 deep_training/nlp/losses/contrast.py
+-rw-rw-rw-  2.0 fat      619 b- defN 22-Aug-22 01:11 deep_training/nlp/losses/dice_loss.py
+-rw-rw-rw-  2.0 fat      710 b- defN 22-Nov-16 06:49 deep_training/nlp/losses/focal_loss.py
+-rw-rw-rw-  2.0 fat      882 b- defN 22-Aug-22 01:11 deep_training/nlp/losses/label_smoothing.py
+-rw-rw-rw-  2.0 fat      547 b- defN 23-Jan-29 01:07 deep_training/nlp/losses/lm_loss.py
+-rw-rw-rw-  2.0 fat     2149 b- defN 22-Dec-29 06:05 deep_training/nlp/losses/loss_arcface.py
+-rw-rw-rw-  2.0 fat      962 b- defN 22-Dec-05 00:45 deep_training/nlp/losses/loss_casrel.py
+-rw-rw-rw-  2.0 fat     1496 b- defN 22-Dec-21 00:45 deep_training/nlp/losses/loss_cosent.py
+-rw-rw-rw-  2.0 fat     1912 b- defN 22-Dec-29 06:03 deep_training/nlp/losses/loss_cosface.py
+-rw-rw-rw-  2.0 fat     2223 b- defN 22-Dec-08 00:49 deep_training/nlp/losses/loss_globalpointer.py
+-rw-rw-rw-  2.0 fat     6020 b- defN 23-Jan-09 00:29 deep_training/nlp/losses/loss_infonce.py
+-rw-rw-rw-  2.0 fat      884 b- defN 22-Dec-23 00:05 deep_training/nlp/losses/loss_kl.py
+-rw-rw-rw-  2.0 fat     1270 b- defN 23-Mar-14 03:03 deep_training/nlp/losses/loss_mhslinker.py
+-rw-rw-rw-  2.0 fat      617 b- defN 22-Dec-14 08:37 deep_training/nlp/losses/loss_r-drop.py
+-rw-rw-rw-  2.0 fat     2656 b- defN 22-Dec-29 06:04 deep_training/nlp/losses/loss_sphereface.py
+-rw-rw-rw-  2.0 fat      562 b- defN 22-Nov-25 06:44 deep_training/nlp/losses/loss_splinker.py
+-rw-rw-rw-  2.0 fat    10822 b- defN 23-Jan-09 09:00 deep_training/nlp/losses/loss_spn4re.py
+-rw-rw-rw-  2.0 fat     5644 b- defN 22-Dec-12 00:12 deep_training/nlp/losses/loss_tplinker.py
+-rw-rw-rw-  2.0 fat     2466 b- defN 23-Jan-18 09:12 deep_training/nlp/losses/utils.py
+-rw-rw-rw-  2.0 fat       71 b- defN 22-Dec-13 03:17 deep_training/nlp/metrics/__init__.py
+-rw-rw-rw-  2.0 fat      655 b- defN 22-Dec-02 00:22 deep_training/nlp/metrics/pointer.py
+-rw-rw-rw-  2.0 fat       58 b- defN 22-Nov-22 08:00 deep_training/nlp/models/__init__.py
+-rw-rw-rw-  2.0 fat     6826 b- defN 23-Apr-25 03:34 deep_training/nlp/models/casrel.py
+-rw-rw-rw-  2.0 fat     5093 b- defN 23-Apr-25 03:34 deep_training/nlp/models/crf_cascad.py
+-rw-rw-rw-  2.0 fat     1588 b- defN 23-Apr-25 03:34 deep_training/nlp/models/crf_model.py
+-rw-rw-rw-  2.0 fat    12985 b- defN 23-Apr-25 03:34 deep_training/nlp/models/diffcse.py
+-rw-rw-rw-  2.0 fat     5395 b- defN 23-Apr-27 00:33 deep_training/nlp/models/esimcse.py
+-rw-rw-rw-  2.0 fat     4209 b- defN 23-Apr-25 03:34 deep_training/nlp/models/gec_model.py
+-rw-rw-rw-  2.0 fat    10854 b- defN 23-Apr-25 03:34 deep_training/nlp/models/gplinker.py
+-rw-rw-rw-  2.0 fat     3814 b- defN 23-Apr-25 03:34 deep_training/nlp/models/infonce.py
+-rw-rw-rw-  2.0 fat     2459 b- defN 23-Apr-25 03:34 deep_training/nlp/models/mhs_ner.py
+-rw-rw-rw-  2.0 fat     5991 b- defN 23-Apr-25 03:34 deep_training/nlp/models/mhslinker.py
+-rw-rw-rw-  2.0 fat     4661 b- defN 23-Apr-25 03:34 deep_training/nlp/models/onerel_model.py
+-rw-rw-rw-  2.0 fat     2750 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pointer.py
+-rw-rw-rw-  2.0 fat    13392 b- defN 23-Apr-25 03:34 deep_training/nlp/models/prefixtuning.py
+-rw-rw-rw-  2.0 fat    15915 b- defN 23-Apr-25 03:34 deep_training/nlp/models/prgc_model.py
+-rw-rw-rw-  2.0 fat    16115 b- defN 23-Apr-25 03:34 deep_training/nlp/models/promptbert_cse.py
+-rw-rw-rw-  2.0 fat     5149 b- defN 23-Apr-25 03:34 deep_training/nlp/models/pure_model.py
+-rw-rw-rw-  2.0 fat     3949 b- defN 23-Apr-25 03:34 deep_training/nlp/models/simcse.py
+-rw-rw-rw-  2.0 fat     6022 b- defN 23-Apr-25 03:34 deep_training/nlp/models/span_ner.py
+-rw-rw-rw-  2.0 fat    14454 b- defN 23-Apr-25 03:34 deep_training/nlp/models/spn4re.py
+-rw-rw-rw-  2.0 fat    11383 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinker.py
+-rw-rw-rw-  2.0 fat     8157 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tplinkerplus.py
+-rw-rw-rw-  2.0 fat     6624 b- defN 23-Apr-25 03:34 deep_training/nlp/models/transformer.py
+-rw-rw-rw-  2.0 fat    26238 b- defN 23-May-10 01:43 deep_training/nlp/models/transformer_base.py
+-rw-rw-rw-  2.0 fat     7968 b- defN 23-Apr-25 03:34 deep_training/nlp/models/tsdae_model.py
+-rw-rw-rw-  2.0 fat     9040 b- defN 23-Apr-25 03:34 deep_training/nlp/models/w2ner.py
+-rw-rw-rw-  2.0 fat    16524 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA/__init__.py
+-rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-13 01:04 deep_training/nlp/models/LLaMA/configuration.py
+-rw-rw-rw-  2.0 fat    19207 b- defN 23-Apr-28 07:32 deep_training/nlp/models/LLaMA_parallel/__init__.py
+-rw-rw-rw-  2.0 fat     5087 b- defN 23-Mar-10 00:30 deep_training/nlp/models/LLaMA_parallel/configuration.py
+-rw-rw-rw-  2.0 fat    31627 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/__init__.py
+-rw-rw-rw-  2.0 fat     5890 b- defN 23-Mar-13 06:15 deep_training/nlp/models/PaLM/configuration.py
+-rw-rw-rw-  2.0 fat    60510 b- defN 23-May-04 00:28 deep_training/nlp/models/chatglm/__init__.py
+-rw-rw-rw-  2.0 fat     4575 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/configuration.py
+-rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-03 00:32 deep_training/nlp/models/chatglm/quantization.py
+-rw-rw-rw-  2.0 fat    16642 b- defN 23-Apr-17 00:24 deep_training/nlp/models/chatglm/tokenization.py
+-rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-13 06:18 deep_training/nlp/models/laMDA/__init__.py
+-rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-13 06:15 deep_training/nlp/models/laMDA/configuration.py
+-rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 07:19 deep_training/nlp/models/lora/__init__.py
+-rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
+-rw-rw-rw-  2.0 fat     7054 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/configuration.py
+-rw-rw-rw-  2.0 fat    13688 b- defN 23-May-04 00:28 deep_training/nlp/models/lora/v1/lora_wrapper.py
+-rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/__init__.py
+-rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/adalora_model.py
+-rw-rw-rw-  2.0 fat    11285 b- defN 23-Apr-26 08:14 deep_training/nlp/models/lora/v2/configuration.py
+-rw-rw-rw-  2.0 fat    11745 b- defN 23-Apr-18 01:24 deep_training/nlp/models/lora/v2/lora_model.py
+-rw-rw-rw-  2.0 fat    10644 b- defN 23-May-09 00:34 deep_training/nlp/models/lora/v2/lora_wrapper.py
+-rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/save_and_load.py
+-rw-rw-rw-  2.0 fat      467 b- defN 23-Apr-21 04:29 deep_training/nlp/models/moss/__init__.py
+-rw-rw-rw-  2.0 fat     5097 b- defN 23-Apr-23 01:11 deep_training/nlp/models/moss/configuration_moss.py
+-rw-rw-rw-  2.0 fat     6735 b- defN 23-Apr-23 01:05 deep_training/nlp/models/moss/custom_autotune.py
+-rw-rw-rw-  2.0 fat    31079 b- defN 23-Apr-23 02:08 deep_training/nlp/models/moss/modeling_moss.py
+-rw-rw-rw-  2.0 fat    18744 b- defN 23-May-04 00:28 deep_training/nlp/models/moss/quantization.py
+-rw-rw-rw-  2.0 fat    15939 b- defN 23-Apr-24 00:32 deep_training/nlp/models/moss/tokenization_moss.py
+-rw-rw-rw-  2.0 fat      203 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/__init__.py
+-rw-rw-rw-  2.0 fat    12062 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/configuration.py
+-rw-rw-rw-  2.0 fat    52102 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/prompt_model.py
+-rw-rw-rw-  2.0 fat     3551 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/save_and_load.py
+-rw-rw-rw-  2.0 fat     1917 b- defN 23-May-04 00:28 deep_training/nlp/models/prompt/utils.py
+-rw-rw-rw-  2.0 fat       54 b- defN 23-May-11 01:00 deep_training/nlp/models/rl/__init__.py
+-rw-rw-rw-  2.0 fat    40248 b- defN 23-May-11 02:57 deep_training/nlp/models/rl/modeling.py
+-rw-rw-rw-  2.0 fat     8026 b- defN 23-May-11 02:43 deep_training/nlp/models/rl/utils.py
+-rw-rw-rw-  2.0 fat      102 b- defN 22-Nov-22 08:00 deep_training/nlp/models/splinker/__init__.py
+-rw-rw-rw-  2.0 fat     2866 b- defN 23-Apr-25 03:34 deep_training/nlp/models/splinker/splinker.py
+-rw-rw-rw-  2.0 fat    14478 b- defN 23-Feb-11 09:07 deep_training/nlp/models/t5decoder/__init__.py
+-rw-rw-rw-  2.0 fat     6646 b- defN 23-Feb-09 00:28 deep_training/nlp/models/t5encoder/__init__.py
+-rw-rw-rw-  2.0 fat       56 b- defN 22-Dec-14 08:02 deep_training/nlp/optimizer/__init__.py
+-rw-rw-rw-  2.0 fat     5225 b- defN 23-Mar-08 00:14 deep_training/nlp/optimizer/lamb.py
+-rw-rw-rw-  2.0 fat       99 b- defN 23-Mar-02 05:27 deep_training/nlp/optimizer/lion/__init__.py
+-rw-rw-rw-  2.0 fat     2516 b- defN 23-May-10 03:20 deep_training/nlp/optimizer/lion/lion.py
+-rw-rw-rw-  2.0 fat     2499 b- defN 23-May-10 03:18 deep_training/nlp/optimizer/lion/triton.py
+-rw-rw-rw-  2.0 fat       79 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/__init__.py
+-rw-rw-rw-  2.0 fat       55 b- defN 23-Apr-27 00:33 deep_training/nlp/rl/ppo/__init__.py
+-rw-rw-rw-  2.0 fat     6685 b- defN 23-May-11 03:00 deep_training/nlp/rl/ppo/configuration.py
+-rw-rw-rw-  2.0 fat    23235 b- defN 23-May-09 08:10 deep_training/nlp/rl/ppo/ppo_dataset.py
+-rw-rw-rw-  2.0 fat     9421 b- defN 23-May-10 09:04 deep_training/nlp/rl/ppo/ppo_module.py
+-rw-rw-rw-  2.0 fat    43801 b- defN 23-May-11 00:39 deep_training/nlp/rl/ppo/ppo_trainer.py
+-rw-rw-rw-  2.0 fat    13669 b- defN 23-May-09 00:34 deep_training/nlp/rl/ppo/utils/__init__.py
+-rw-rw-rw-  2.0 fat     9844 b- defN 23-Mar-14 00:17 deep_training/nlp/rl/ppo/utils/logging.py
+-rw-rw-rw-  2.0 fat       54 b- defN 23-Apr-27 01:46 deep_training/nlp/rl/ppo_fabric/__init__.py
+-rw-rw-rw-  2.0 fat     8457 b- defN 23-May-04 07:37 deep_training/nlp/rl/ppo_fabric/engine.py
+-rw-rw-rw-  2.0 fat    23909 b- defN 23-May-05 00:39 deep_training/nlp/rl/ppo_fabric/ppo_trainer.py
+-rw-rw-rw-  2.0 fat     2088 b- defN 23-May-04 07:31 deep_training/nlp/rl/ppo_fabric/utils.py
+-rw-rw-rw-  2.0 fat     2868 b- defN 22-Dec-14 08:00 deep_training/nlp/scheduler/__init__.py
+-rw-rw-rw-  2.0 fat     6986 b- defN 23-Apr-26 05:39 deep_training/nlp/utils/__init__.py
+-rw-rw-rw-  2.0 fat     6323 b- defN 23-Jan-29 01:07 deep_training/nlp/utils/adversarial.py
+-rw-rw-rw-  2.0 fat    15256 b- defN 23-Jan-03 01:54 deep_training/nlp/utils/nlputils.py
+-rw-rw-rw-  2.0 fat      795 b- defN 23-Jan-11 07:02 deep_training/nlp/utils/spearman.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/layers/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/losses/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/metrics/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/models/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/optimizer/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
+-rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
+-rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
+-rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
+-rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
+-rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
+-rw-rw-rw-  2.0 fat    14500 b- defN 23-May-11 00:39 deep_training/utils/trainer.py
+-rw-rw-rw-  2.0 fat      605 b- defN 23-May-11 03:22 deep_training-0.1.5rc0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-11 03:22 deep_training-0.1.5rc0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-11 03:22 deep_training-0.1.5rc0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    17184 b- defN 23-May-11 03:22 deep_training-0.1.5rc0.dist-info/RECORD
+177 files, 1209950 bytes uncompressed, 310639 bytes compressed:  74.3%
```

## zipnote {}

```diff
@@ -387,14 +387,23 @@
 
 Filename: deep_training/nlp/models/prompt/save_and_load.py
 Comment: 
 
 Filename: deep_training/nlp/models/prompt/utils.py
 Comment: 
 
+Filename: deep_training/nlp/models/rl/__init__.py
+Comment: 
+
+Filename: deep_training/nlp/models/rl/modeling.py
+Comment: 
+
+Filename: deep_training/nlp/models/rl/utils.py
+Comment: 
+
 Filename: deep_training/nlp/models/splinker/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/models/splinker/splinker.py
 Comment: 
 
 Filename: deep_training/nlp/models/t5decoder/__init__.py
@@ -423,36 +432,39 @@
 
 Filename: deep_training/nlp/rl/ppo/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/rl/ppo/configuration.py
 Comment: 
 
-Filename: deep_training/nlp/rl/ppo/data_type.py
+Filename: deep_training/nlp/rl/ppo/ppo_dataset.py
 Comment: 
 
-Filename: deep_training/nlp/rl/ppo/ppo.py
+Filename: deep_training/nlp/rl/ppo/ppo_module.py
 Comment: 
 
-Filename: deep_training/nlp/rl/ppo/ppo_dataset.py
+Filename: deep_training/nlp/rl/ppo/ppo_trainer.py
+Comment: 
+
+Filename: deep_training/nlp/rl/ppo/utils/__init__.py
 Comment: 
 
-Filename: deep_training/nlp/rl/ppo/utils.py
+Filename: deep_training/nlp/rl/ppo/utils/logging.py
 Comment: 
 
 Filename: deep_training/nlp/rl/ppo_fabric/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/rl/ppo_fabric/engine.py
 Comment: 
 
-Filename: deep_training/nlp/rl/ppo_fabric/ppo.py
+Filename: deep_training/nlp/rl/ppo_fabric/ppo_trainer.py
 Comment: 
 
-Filename: deep_training/nlp/rl/ppo_fabric/ppo_agent.py
+Filename: deep_training/nlp/rl/ppo_fabric/utils.py
 Comment: 
 
 Filename: deep_training/nlp/scheduler/__init__.py
 Comment: 
 
 Filename: deep_training/nlp/utils/__init__.py
 Comment: 
@@ -501,20 +513,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.4.dist-info/METADATA
+Filename: deep_training-0.1.5rc0.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.4.dist-info/WHEEL
+Filename: deep_training-0.1.5rc0.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.4.dist-info/top_level.txt
+Filename: deep_training-0.1.5rc0.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.4.dist-info/RECORD
+Filename: deep_training-0.1.5rc0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,22 +1,22 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.4',
+    version='0.1.5rc0',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
-    install_requires=['pytorch-lightning>=2',
+    install_requires=['lightning>=2',
                       'numpy-io>=0.0.2 , < 0.1.0',
                       'sentencepiece',
                       'numpy',
                       'transformers >= 4.22',
                       'seqmetric',
                       'scipy',
                       'scikit-learn',
```

## deep_training/nlp/layers/lora_v2/utils.py

```diff
@@ -116,14 +116,15 @@
         self.modules_to_save = torch.nn.ModuleDict({})
         self.update(adapter_name)
         self.active_adapter = adapter_name
 
     def update(self, adapter_name):
         self.modules_to_save.update(torch.nn.ModuleDict({adapter_name: copy.deepcopy(self.original_module)}))
 
+
     def forward(self, *args, **kwargs):
         if self.active_adapter not in self.modules_to_save:
             return self.original_module(*args, **kwargs)
         return self.modules_to_save[self.active_adapter](*args, **kwargs)
 
 
 def _get_submodules(model, key):
@@ -148,14 +149,19 @@
             if isinstance(target, ModulesToSaveWrapper):
                 target.update(adapter_name)
             else:
                 for param in target.parameters():
                     param.requires_grad = True
                 setattr(parent, target_name, ModulesToSaveWrapper(target, adapter_name))
 
+    for k, n in model.named_modules():
+        if isinstance(n,ModulesToSaveWrapper):
+            for p in n.original_module.parameters():
+                p.requires_grad = False
+
 
 def _set_adapter(model, adapter_name):
     for module in model.modules():
         if isinstance(module, ModulesToSaveWrapper):
             module.active_adapter = adapter_name
```

## deep_training/nlp/models/lora/v2/lora_wrapper.py

```diff
@@ -57,14 +57,15 @@
         self.active_adapter = adapter_name
         self.lora_type = lora_config_v2.lora_type
         self.base_model_torch_dtype = getattr(model, "dtype", None)
         self.lora_config_v2[adapter_name] = lora_config_v2
         self.base_model = LORA_TYPE_TO_MODEL_MAPPING[lora_config_v2.lora_type](
             self.base_model, self.lora_config_v2, adapter_name
         )
+        self.set_additional_trainable_modules(lora_config_v2, adapter_name)
 
     def save_pretrained(self, save_directory, **kwargs):
         r"""
         This function saves the adapter model and the adapter configuration files to a directory, so that it can be
         reloaded using the [`LoraModel.from_pretrained`] class method, and also used by the [`LoraModel.push_to_hub`]
         method.
 
@@ -184,16 +185,19 @@
     def add_adapter(self, adapter_name, lora_config):
         if lora_config.lora_type != self.lora_type:
             raise ValueError(
                 f"Cannot combine adapters with different peft types. "
                 f"Found {self.lora_type} and {lora_config.lora_type}."
             )
         self.lora_config_v2[adapter_name] = lora_config
-
         self.base_model.add_adapter(adapter_name, lora_config)
+        self.set_additional_trainable_modules(lora_config, adapter_name)
+
+
+    def set_additional_trainable_modules(self, lora_config, adapter_name):
         if getattr(lora_config, "modules_to_save", None) is not None:
             if self.modules_to_save is None:
                 self.modules_to_save = set(lora_config.modules_to_save)
             else:
                 self.modules_to_save = self.modules_to_save.update(lora_config.modules_to_save)
             _set_trainable(self, adapter_name)
```

## deep_training/nlp/optimizer/lion/lion.py

```diff
@@ -1,11 +1,11 @@
 # @Time    : 2023/3/1 22:37
 # @Author  : tk
 # @FileName: lion.py
-
+from functools import partial
 from typing import Tuple, Optional, Callable
 
 import torch
 from torch.optim.optimizer import Optimizer
 
 
 __all__ = [
@@ -37,44 +37,49 @@
 class Lion(Optimizer):
     def __init__(
         self,
         params,
         lr: float = 1e-4,
         betas: Tuple[float, float] = (0.9, 0.99),
         weight_decay: float = 0.0,
-        use_triton: bool = False
+        use_triton: bool = False,
+        triton_block_size: int = 1024
     ):
         assert lr > 0.
         assert all([0. <= beta <= 1. for beta in betas])
 
         defaults = dict(
-            lr = lr,
-            betas = betas,
-            weight_decay = weight_decay
+            lr=lr,
+            betas=betas,
+            weight_decay=weight_decay
         )
 
         super().__init__(params, defaults)
 
         self.update_fn = update_fn
+        self.use_triton = use_triton
+        self.took_first_step = False
 
         if use_triton:
             from .triton import update_fn as triton_update_fn
-            self.update_fn = triton_update_fn
+            self.update_fn = partial(triton_update_fn, BLOCK_SIZE=triton_block_size)
 
     @torch.no_grad()
     def step(
-        self,
-        closure: Optional[Callable] = None
+            self,
+            closure: Optional[Callable] = None
     ):
 
         loss = None
         if exists(closure):
             with torch.enable_grad():
                 loss = closure()
 
+        # update all parameters
+
         for group in self.param_groups:
             for p in filter(lambda p: exists(p.grad), group['params']):
 
                 grad, lr, wd, beta1, beta2, state = p.grad, group['lr'], group['weight_decay'], *group['betas'], self.state[p]
 
                 # init state - exponential moving average of gradient values
```

## deep_training/nlp/optimizer/lion/triton.py

```diff
@@ -1,25 +1,33 @@
 # @Time    : 2023/3/1 22:38
 # @Author  : tk
 # @FileName: triton.py
 
 import torch
+from torch import Tensor
 
 try:
     import triton
     import triton.language as tl
 except ImportError as e:
     print('triton is not installed, please install by running `pip install triton -U --pre`')
     exit()
 
+# helper functions
+
+def calc_num_warps(block_size):
+    num_warps = 4
+    if block_size >= 2048:
+        num_warps = 8
+    if block_size >= 4096:
+        num_warps = 16
+    return num_warps
+
+# triton cuda kernel
 
-@triton.autotune(configs = [
-    triton.Config({'BLOCK_SIZE': 128}, num_warps = 4),
-    triton.Config({'BLOCK_SIZE': 1024}, num_warps = 8),
-], key = ['n_elements'])
 @triton.jit
 def update_fn_kernel(
     p_ptr,
     grad_ptr,
     exp_avg_ptr,
     lr,
     wd,
@@ -72,30 +80,39 @@
 
     # store new params and momentum running average coefficient
 
     tl.store(offset_p_ptr, p, mask = mask)
     tl.store(offset_exp_avg_ptr, exp_avg, mask = mask)
 
 def update_fn(
-    p: torch.Tensor,
-    grad: torch.Tensor,
-    exp_avg: torch.Tensor,
+    p: Tensor,
+    grad: Tensor,
+    exp_avg: Tensor,
     lr: float,
     wd: float,
     beta1: float,
-    beta2: float
+    beta2: float,
+    inplace: bool = True,
+    BLOCK_SIZE: int = 1024
 ):
     assert all([t.is_cuda for t in (p, grad, exp_avg)])
+
     n_elements = p.numel()
 
-    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
+    block_size = triton.next_power_of_2(BLOCK_SIZE)
+    num_warps = calc_num_warps(block_size)
+    n_rows = triton.cdiv(n_elements, block_size)
+
+    # call triton cuda kernel
 
-    update_fn_kernel[grid](
+    update_fn_kernel[(n_rows,)](
         p,
         grad,
         exp_avg,
         lr,
         wd,
         beta1,
         beta2,
-        n_elements
-    )
+        n_elements,
+        num_warps = num_warps,
+        BLOCK_SIZE = BLOCK_SIZE
+    )
```

## deep_training/nlp/rl/ppo/configuration.py

```diff
@@ -1,87 +1,168 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/4/20 11:03
+import json
+import os
 import warnings
-from dataclasses import field, dataclass
+from dataclasses import field, dataclass, asdict
 from typing import Optional, Dict, Any
-
 import numpy as np
-from transformers.utils import flatten_dict
+from transformers.utils import flatten_dict, PushToHubMixin
+
+CONFIG_NAME = 'ppo_config.json'
 
 @dataclass
-class MethodConfig:
-    """
-    Config for a certain RL method.
+class PPOConfigMixin(PushToHubMixin):
+    r"""
+    This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all
+    PEFT adapter models. This class inherits from `transformers.utils.PushToHubMixin` which contains the methods to
+    push your model to the Hub. The method `save_pretrained` will save the configuration of your adapter model in a
+    directory. The method `from_pretrained` will load the configuration of your adapter model from a directory.
 
-    :param name: Name of the method
-    :type name: str
     """
 
-    name: str
+    @property
+    def __dict__(self):
+        return asdict(self)
+
+    def to_dict(self):
+        return self.__dict__
+
+    def save_pretrained(self, save_directory, **kwargs):
+        r"""
+        This method saves the configuration of your adapter model in a directory.
+
+        Args:
+            save_directory (`str`):
+                The directory where the configuration will be saved.
+            **kwargs:
+                Additional keyword arguments passed along to the `transformers.utils.PushToHubMixin.push_to_hub`
+                method.
+        """
+        if os.path.isfile(save_directory):
+            raise AssertionError(f"Provided path ({save_directory}) should be a directory, not a file")
+
+        os.makedirs(save_directory, exist_ok=True)
+
+        output_dict = self.__dict__
+        output_path = os.path.join(save_directory, CONFIG_NAME)
+
+        # save it
+        with open(output_path, "w") as writer:
+            writer.write(json.dumps(output_dict, indent=2, sort_keys=True))
 
     @classmethod
-    def from_dict(cls, config: Dict[str, Any]):
-        return cls(**config)
+    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
+        r"""
+        This method loads the configuration of your adapter model from a directory.
+
+        Args:
+            pretrained_model_name_or_path (`str`):
+                The directory or the hub-id where the configuration is saved.
+            **kwargs:
+                Additional keyword arguments passed along to the child class initialization.
+        """
+        if os.path.isfile(os.path.join(pretrained_model_name_or_path, CONFIG_NAME)):
+            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)
+        else:
+            raise ValueError(f"Can't find '{CONFIG_NAME}' at '{pretrained_model_name_or_path}'")
+
+        loaded_attributes = cls.from_json_file(config_file)
+
+        config = cls(**kwargs)
+
+        for key, value in loaded_attributes.items():
+            if hasattr(config, key):
+                setattr(config, key, value)
 
+        return config
 
+    @classmethod
+    def from_json_file(cls, path_json_file, **kwargs):
+        r"""
+        Loads a configuration file from a json file.
+
+        Args:
+            path_json_file (`str`):
+                The path to the json file.
+        """
+        with open(path_json_file, "r") as file:
+            json_object = json.load(file)
 
-@dataclass
-class PPOConfig(MethodConfig):
-    """
-    Config for PPO method
+        return json_object
 
-    :param ppo_epochs: Number of updates per batch
-    :type ppo_epochs: int
+    @classmethod
+    def from_memory(cls,json_object: dict, **kwargs):
+        config = cls(**kwargs)
+        loaded_attributes = json_object
+        for key, value in loaded_attributes.items():
+            if hasattr(config, key):
+                setattr(config, key, value)
 
-    :param num_rollouts: Number  of experiences to observe before learning
-    :type num_rollouts: int
+        return config
 
-    :param init_kl_coef: Initial value for KL coefficient
-    :type init_kl_coef: float
 
-    :param target: Target value for KL coefficient
-    :type target: float
 
-    :param horizon: Number of steps for KL coefficient to reach target
-    :type horizon: int
 
-    :param gamma: Discount factor
-    :type gamma: float
 
-    :param lam: GAE lambda
-    :type lam: float
 
-    :param cliprange: Clipping range for PPO policy loss (1 - cliprange, 1 + cliprange)
-    :type cliprange: float
 
-    :param cliprange_value: Clipping range for predicted values
-                            (observed values - cliprange_value, observed values + cliprange_value)
-    :type cliprange_value: float
+@dataclass
+class PPOConfig(PPOConfigMixin):
+    ppo_epochs: int = field(default=4, metadata={"help": "Number of updates per batch"})
+    num_rollouts: int = field(default=128, metadata={"help": "Number  of experiences to observe before learning"})
+    chunk_size: int = field(default=128, metadata={"help": "Number of chunk_size of generate"})
+    init_kl_coef: float = field(default=0.001, metadata={"help": "Initial value for KL coefficient"})
+    target: Optional[float] = field(default=None, metadata={"help": "Target value for KL coefficient"})
+    horizon: int = field(default=10000, metadata={"help": "Number of steps for KL coefficient to reach target"})
+    gamma: float = field(default=1., metadata={"help": "Discount factor"})
+    lam: float = field(default=0.95, metadata={"help": "GAE lambda"})
+    cliprange: float = field(default=0.2, metadata={"help": "Clipping range for PPO policy loss (1 - cliprange, 1 + cliprange)"})
+    cliprange_value: float = field(default=0.2, metadata={"help": "Clipping range for predicted values"
+                            "(observed values - cliprange_value, observed values + cliprange_value)"})
+    vf_coef: float = field(default=1., metadata={"help": "Value loss scale w.r.t policy loss"})
+    scale_reward: Optional[str] = field(default="ignored", metadata={"help": ""})
+    ref_mean: Optional[float] = field(default=None, metadata={"help": "Number of updates per batch"})
+    ref_std: Optional[float] = field(default=None, metadata={"help": "Number of updates per batch"})
+    cliprange_reward: int = field(default=10, metadata={"help": "Additioanl kwargs for the generation"})
+    gen_kwargs: dict = field(default=None,
+                             metadata={"help": "Additioanl kwargs for the generation"})
+    gen_experience_kwargs: Optional[dict] = field(default=None, metadata={"help": "Additioanl kwargs for the gen_experience_kwargs"})
+    model_arch_type: Optional[str] = "causal"  # one of causal, prefixlm,seq2seq
+    minibatch_size: Optional[int] =  field(default=None, metadata={"help": "minibatch_size"})
+
+    def __post_init__(self):
+        if self.gen_kwargs is None:
+            self.gen_kwargs = dict(
+            max_new_tokens=40,
+            top_k=0,
+            top_p=1.0,
+            do_sample=True,
+        )
 
-    :param vf_coef: Value loss scale w.r.t policy loss
-    :type vf_coef: float
 
-    :param gen_kwargs: Additioanl kwargs for the generation
-    :type gen_kwargs: Dict[str, Any]
+@dataclass
+class PPOArguments:
+    ppo: PPOConfig= field(default=None, metadata={"help": "PPOConfig."})
+
+
+    def save_pretrained(self, save_directory, **kwargs):
+        if self.ppo is not None:
+            self.ppo.save_pretrained(save_directory, **kwargs)
+
+
+    @classmethod
+    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
+        config = PPOConfig.from_pretrained(pretrained_model_name_or_path,**kwargs)
+        return config
+
+    @property
+    def config(self) -> Optional[PPOConfig]:
+        if self.ppo is not None:
+            return self.ppo
+        return None
 
-    :param gen_experience_kwargs: if this is not None, then the experience is generated using this
-    :type gen_experience_kwargs: Dict[str, Any]
-    """
 
-    ppo_epochs: int
-    num_rollouts: int
-    chunk_size: int
-    init_kl_coef: float
-    target: float
-    horizon: int
-    gamma: float
-    lam: float
-    cliprange: float
-    cliprange_value: float
-    vf_coef: float
-    scale_reward: Optional[str]
-    ref_mean: Optional[float]
-    ref_std: Optional[float]
-    cliprange_reward: float
-    gen_kwargs: dict
-    gen_experience_kwargs: Optional[dict] = None
+    def __post_init__(self):
+        if self.ppo is not None and isinstance(self.ppo, dict):
+            self.ppo = PPOConfig.from_memory(self.ppo)
```

## deep_training/nlp/rl/ppo/ppo_dataset.py

```diff
@@ -1,365 +1,559 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/4/24 9:34
 
-import logging
+import json
 import os
+from abc import abstractmethod
+from dataclasses import is_dataclass
 from time import time
-from typing import List, Callable, Tuple, Optional
-
+from typing import List, Callable, Tuple
 import torch
-from torch import nn
-from tqdm import tqdm
-from torch.nn import functional as F
 import torch.distributed as dist
-from ....nlp.layers.ppo import AdaptiveKLController, FixedKLController
-from .data_type import PPORLElement, PPORLBatch
+from torch.nn import functional as F
+from torch.nn.utils.rnn import pad_sequence
+from transformers import BatchEncoding
+from typing import Any, Callable, Dict, Iterable
+from torch.utils.data import DataLoader, Dataset
 from .utils import logprobs_of_labels, Clock, gather_dict, RunningMoments, pad_across_processes, _gpu_gather, \
-    get_tensor_stats, flatten_dict, whiten
+    PPORLElement, RLElement, PPORLBatch
+from .utils import logging, logger
 
-logger = logging.get_logger(__name__)
 
-class PPO_dataset:
-    def __init__(self,
-                 model,
-                 ref_model,
-                 tokenizer,
-                 reward_fn: Callable,
-                 ppo_config,
-                 stop_sequences=None,
-                 generate_kwargs = None or {}):
-        self.model = model
-        self.ref_model = ref_model
-        self.tokenizer = tokenizer
-        self.reward_fn = reward_fn
-        self.config = ppo_config
-
-        self.stop_sequences = stop_sequences
-        self.generate_kwargs = generate_kwargs
-
-        # Setup stats tracker
-        self.running_moments = RunningMoments()
-        self.ref_mean = self.config.ref_mean
-        self.ref_std = self.config.ref_std
-
-        # Setup the KL controller
-        # This helps prevent large divergences in the controller (policy)
-        if self.config.target is not None:
-            self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
-        else:
-            self.kl_ctl = FixedKLController(self.config.init_kl_coef)
-
-    @torch.no_grad()
-    def generate(self, input_ids, attention_mask=None, **kwargs):
-        """Wraps hf's `generate` adding some specific method's defaults"""
-        input_ids = input_ids.to(self.model.device)
-        if attention_mask is not None:
-            attention_mask = attention_mask.to(self.model.device)
-        kwargs = dict(self.generate_kwargs, **kwargs)
-        return self.model.generate(
-            input_ids=input_ids, attention_mask=attention_mask, **kwargs
-        )
-
-    def decode(
-            self,
-            prompts: List[torch.LongTensor],
-            samples: List[torch.LongTensor],
-            prompt_sizes: torch.LongTensor = None,
-            append_eos_token: bool = False,
-    ) -> Tuple[List[str], List[str], List[str]]:
+class BaseRolloutStore(Dataset):
+    def __init__(self, capacity=-1):
+        self.history: Iterable[Any] = None
+        self.capacity = capacity
+
+    @abstractmethod
+    def push(self, exps: Iterable[Any]):
         """
-        Decode tensor generations into lists of strings (`samples`: List[str], `prompts`: List[str], `outputs`: List[str])
+        Push experiences to rollout storage
         """
-        if prompt_sizes is None:
-            # Assuming prompts were left-padded
-            prompt_sizes = [prompts.shape[1]] * len(prompts)
-
-        str_samples, str_prompts, str_outputs = [], [], []
-        for prompt, sample, prompt_size in zip(prompts, samples, prompt_sizes):
-            if self.config.model.model_arch_type == "seq2seq":
-                output_start_ix = 0
-            else:
-                output_start_ix = prompt_size
+        pass
 
-            str_prompt = self.tokenizer.decode(prompt[:prompt_size], skip_special_tokens=True)
-            str_output = self.tokenizer.decode(sample[output_start_ix:], skip_special_tokens=True)
-            # Trim outputs up to `self.stop_sequences` if any are present
-            trimmed = False
-            if self.stop_sequences:
-                for stop in self.stop_sequences:
-                    stop_ix = str_output.find(stop)
-                    if stop_ix >= 0:
-                        str_output = str_output[:stop_ix].rstrip()
-                        trimmed = True
-
-            # Recover the last <eos> if it was present in the original sample
-            # or add one if it was trimmed with `self.stop_sequences`.
-            # Only in cases when a generation ended due to `max_new_tokens` exhaustion,
-            # <eos> token would not be present in the original sample
-            if append_eos_token and (trimmed or sample[-1] == self.tokenizer.eos_token_id):
-                str_output += self.tokenizer.eos_token
+    def __getitem__(self, index: int) -> RLElement:
+        return self.history[index]
 
-            str_prompts.append(str_prompt)
-            str_outputs.append(str_output)
+    def __len__(self) -> int:
+        return len(self.history)
 
-            if self.config.model.model_arch_type == "seq2seq":
-                sample = str_prompt + self.tokenizer.sep_token + str_output
-            else:
-                sample = str_prompt + str_output
+    @abstractmethod
+    def create_loader(
+        self,
+        batch_size: int,
+        shuffle: bool,
+        prep_fn: Callable = None,
+        num_workers: int = 0,
+    ) -> DataLoader:
+        """
+        Create a dataloader for the rollout store
 
-            str_samples.append(sample)
+        :param prep_fn: Applied to RLElement after collation (typically tokenizer)
+        :type prep_fn: Callable
+        """
+        pass
 
-        return str_samples, str_prompts, str_outputs
 
-    def make_prompt_dataset(self,prompt_iterator,
-                            num_rollouts: int = 1024,
-                            is_main_process=False,
-                            world_size=dist.get_world_size(),
-                            **kwargs):  # noqa:
-        """Make experiences
-
-        Takes `chunk_size` number of prompts from `prompt_iterator`, samples
-        from the model and then computes the KL against a reference model. Finally it
-        then appends PPOElements to trainer's `store`.
+class MiniBatchIterator:
+    """
+    A custom iterator for generating mini-batches from a PyTorch DataLoader.
+    """
+
+    def __init__(self, data_loader, mb_size, num_mb):
+        """
+        Initializes the MiniBatchIterator.
 
         Args:
-            num_rollouts: Number of rollouts to generate
-            iter_count: Total number of updates run (i.e. number of updates run for all batches & epochs)
+            data_loader (torch.utils.data.DataLoader): The DataLoader to generate mini-batches from.
+            mb_size (int): The size of each mini-batch.
+            num_mb (int): The number of mini-batches to generate for each iteration.
         """
-        logger.info("Collecting rollouts")
-        tbar = logging.tqdm(
-            total=num_rollouts,
-            disable=os.environ.get("RANK", 0) != "0",
-            desc=f"[rollout 0 / {num_rollouts}]",
-            # Lower progress bar by 1 if we're in WARNING mode or above to avoid hiding high priority progress
-            # bars (e.g. loss progress in trainers)
-            position=logging.get_verbosity() >= logging.WARNING,
-            # Leave progress bar if we're in INFO mode or lower to avoid spamming in suppressed verbosity levels
-            leave=logging.get_verbosity() < logging.WARNING,
-        )
-
-        clock = Clock()
-        ppo_rl_elements = []
-        accumulated_stats = []
-
-        while len(ppo_rl_elements) < num_rollouts:
-            stats = {}
-            # Get next batch in prompt dataset
-            batch: dict = next(prompt_iterator)
-
-            rollout_generate_time = time()
-
-            # Generate samples from the language model (similar to using HuggingFace `generate` method)
-            samples = self.generate(batch["input_ids"], batch["attention_mask"],**kwargs)
-            stats["time/rollout_generate"] = time() - rollout_generate_time
-
-            prompt_tensors = batch['input_ids']
-            device = samples.device
-
-            prompt_sizes = torch.tensor([prompt_tensors.shape[1]] * len(prompt_tensors), device=device)
-            padded_samples = pad_across_processes(
-                samples, dim=1, pad_index=self.tokenizer.eos_token_id, pad_first=False
-            )
-            padded_prompts = pad_across_processes(
-                prompt_tensors, dim=1, pad_index=self.tokenizer.eos_token_id, pad_first=False
-            )
-            gathered_samples = _gpu_gather(padded_samples)
-            gathered_prompts = _gpu_gather(padded_prompts)
-            gathered_prompt_sizes = _gpu_gather(prompt_sizes)
-            metadata = gather_dict({k: v for k, v in batch.items() if k != "input_ids" and k != "attention_mask"})
-
-            if is_main_process:
-                all_str_samples, all_str_prompts, all_str_outputs = self.decode(
-                    gathered_prompts, gathered_samples, gathered_prompt_sizes, append_eos_token=True
-                )
+        self.data_loader = data_loader
+        self.data_loader_iter = iter(data_loader)
+        self.mb_size = mb_size
+        self.num_mb = num_mb
+
+    def __iter__(self):
+        return self
+
+    def __next__(self):
+        batch = next(self.data_loader_iter)
+        minibatches = []
+
+        for mbi in range(self.num_mb):
+            sliced_data = {}
+            batch_dict = batch
+            if is_dataclass(batch):
+                batch_dict = batch.__dict__
+            for key, value in batch_dict.items():
+                start_idx = mbi * self.mb_size
+                end_idx = (mbi + 1) * self.mb_size
+                sliced_data[key] = value[start_idx:end_idx]
+
+                if len(sliced_data[key]) == 0:
+                    logger.warning(
+                        "WARNING: MiniBatchIterator generated a minibatch with 0 elements. "
+                        "This may be due to the wrong mb_size and/or num_mb or the last batch"
+                        "in the dataset being smaller."
+                    )
+                    sliced_data.pop(key)
+                    break
+                elif len(sliced_data[key]) < self.mb_size:
+                    logger.warning(
+                        "WARNING: MiniBatchIterator generated a minibatch with fewer elements than mb_size. "
+                        "This may be due to the wrong mb_size and/or num_mb or the last batch in the dataset "
+                        "being smaller."
+                    )
+            if not sliced_data:
+                break
 
-                rollout_score_time = time()
-                all_scores = torch.tensor(
-                    self.reward_fn(
-                        samples=all_str_samples, prompts=all_str_prompts, outputs=all_str_outputs, **metadata
-                    ),
-                    dtype=torch.float,
-                    device=device,
-                )
-                stats["time/rollout_score"] = time() - rollout_score_time
+            if isinstance(batch, BatchEncoding):
+                minibatch = BatchEncoding(sliced_data)
+            elif is_dataclass(batch):
+                minibatch = batch.__class__(**sliced_data)
+            # else:
+            #     minibatch = sliced_data
 
-                all_scores = list(all_scores.reshape(world_size, -1).unbind())
-            else:
-                all_scores = None
+            minibatches.append(minibatch)
 
-            if dist.is_initialized():
-                scores = torch.empty(len(samples), device=device)
-                dist.scatter(scores, all_scores)
-            else:
-                scores = all_scores[0].clone().detach()
+        if not minibatches:
+            raise StopIteration
 
-            str_samples, str_prompts, str_outputs = self.decode(prompt_tensors, samples, append_eos_token=True)
+        return minibatches
 
-            # Pad the sample outputs
-            outputs = self.tokenizer(str_outputs).input_ids
-            if self.config.model.model_arch_type == "seq2seq":
-                # add <pad> to the start of the output
-                for i in range(len(outputs)):
-                    outputs[i] = [self.tokenizer.pad_token_id] + outputs[i]
-
-            outputs = list(map(torch.LongTensor, outputs))
-            maxsize = max(map(len, outputs))
-            outputs = [
-                F.pad(
-                    output,
-                    (0, maxsize - len(output)),
-                    value=self.tokenizer.pad_token_id,
-                )
-                for output in outputs
-            ]
-            sample_outputs = torch.vstack(outputs).to(device)
-
-            if self.config.cliprange_reward:
-                scores = torch.clip(scores, -self.config.cliprange_reward, self.config.cliprange_reward)
-
-            # store statistics of the initial rollout as reference
-            if self.ref_mean is None:
-                self.ref_mean, self.ref_std = scores.mean(), scores.std()
-            all_scores_mean, all_scores_std = self.running_moments.update(scores)
-            stats["rollout_scores/mean"] = all_scores_mean.item()
-            stats["rollout_scores/std"] = all_scores_std.item()
-            stats["rollout_scores/running_mean"] = self.running_moments.mean.item()
-            stats["rollout_scores/running_std"] = self.running_moments.std.item()
-
-            if self.config.scale_reward == "running":
-                scores /= self.running_moments.std
-            elif self.config.scale_reward == "ref":
-                scores /= self.ref_std
-
-            # Precompute logprobs, values
-            if self.config.model.model_arch_type == "seq2seq":
-                attention_mask = batch['attention_mask'].to(device)
-                prompt_tensors = batch['input_ids'].to(device)
-                decoder_attention_mask = sample_outputs.not_equal(self.tokenizer.pad_token_id)
-                decoder_attention_mask[:, 0] = 1
-                with torch.no_grad():
-                    outputs = self.model(
-                        input_ids=prompt_tensors,
-                        attention_mask=attention_mask,
-                        decoder_input_ids=sample_outputs,
-                        decoder_attention_mask=decoder_attention_mask,
-                    )
-                    logits = outputs.logits
-                    values = outputs.value
-                    if hasattr(self.model, "frozen_head"):
-                        ref_logits = self.model.forward_hydra(
-                            input_ids=prompt_tensors,
-                            attention_mask=attention_mask,
-                            decoder_input_ids=sample_outputs,
-                            decoder_attention_mask=decoder_attention_mask,
-                            return_dict=True,
-                        ).logits
-                    else:
-                        ref_logits = self.ref_model(
-                            input_ids=prompt_tensors,
-                            attention_mask=attention_mask,
-                            decoder_input_ids=sample_outputs,
-                            decoder_attention_mask=decoder_attention_mask,
-                            return_dict=True,
-                        ).logits
-            else:
-                all_tokens = torch.cat((prompt_tensors.to(device), sample_outputs), dim=1)
-                attention_mask = all_tokens.not_equal(self.tokenizer.pad_token_id).long().to(device)
-                with torch.no_grad():
-                    logits, *_, values = self.model(
-                        all_tokens,
-                        attention_mask=attention_mask,
-                    )
 
-                    ref_logits = self.ref_model(
-                        all_tokens,
-                        attention_mask=attention_mask,
-                        return_dict=True,
-                    ).logits
-                    ref_logits = ref_logits.to(device)
-
-            if self.config.model.model_arch_type == "seq2seq":
-                logprobs = logprobs_of_labels(logits[:, :-1, :], sample_outputs[:, 1:])
-                ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], sample_outputs[:, 1:])
-            else:
-                logprobs = logprobs_of_labels(logits[:, :-1, :], all_tokens[:, 1:])
-                ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], all_tokens[:, 1:])
 
-            n_samples: int = samples.shape[0]
 
-            # Estimate the KL divergence between the model and reference model
-            if self.config.model.model_arch_type == "seq2seq":
-                attention_mask = sample_outputs != self.tokenizer.pad_token_id
-                start = 0
-            else:
-                start = prompt_tensors.shape[1] - 1
+class PPORolloutStore(BaseRolloutStore):
+    """
+    Rollout storage for training PPO
+    """
 
-            log_ratio = (logprobs - ref_logprobs) * attention_mask[:, :-1]
-            kl = log_ratio.exp() - 1 - log_ratio
-            mean_kl_per_token = kl.mean()
-            mean_kl = kl.sum(1).mean()
-
-            logprobs = logprobs.cpu()
-            ref_logprobs = ref_logprobs.cpu()
-            prompt_tensors = prompt_tensors.cpu()
-            sample_outputs = sample_outputs.cpu()
-            values = values.cpu()[:, :-1]
-
-            # Get the logprobs and values, for tokens that are not padding,
-            # from the start of the prompt up to the <eos> token, while also including the latter
-            # (these are taken from the student model and not the reference model)
-            ends = start + attention_mask[:, start:].sum(1) + 1
-            all_values = [values[ix, start: ends[ix]] for ix in range(n_samples)]
-            all_logprobs = [logprobs[ix, start: ends[ix]] for ix in range(n_samples)]
-
-            kl_penalty = self.kl_ctl.value * -log_ratio.cpu()
-            kl_penalty = [xs[start: ends[ix]] for ix, xs in enumerate(kl_penalty)]
-
-            rollout_count = 0
-
-            for sample_idx in range(n_samples):
-                rewards = kl_penalty[sample_idx]
-                rewards[-1] += scores[sample_idx].cpu()
-
-                ppo_rl_elements.append(
-                    PPORLElement(
-                        query_tensor=prompt_tensors[sample_idx],
-                        response_tensor=sample_outputs[sample_idx],
-                        logprobs=all_logprobs[sample_idx],
-                        values=all_values[sample_idx],
-                        rewards=rewards,
-                    )
+    def __init__(self, pad_token_id, padding_side):
+        super().__init__()
+
+        self.pad_token_id = pad_token_id
+        self.padding_side = padding_side
+        self.history: Iterable[PPORLElement] = [None]
+
+    def push(self, exps: Iterable[PPORLElement]):
+        self.history += exps
+
+    def clear_history(self):
+        self.history = []
+
+    def export_history(self, location: str):
+        assert os.path.exists(location)
+
+        fpath = os.path.join(location, f"epoch-{str(time())}.json")
+
+        def exp_to_dict(exp):
+            return {k: v.cpu().tolist() for k, v in exp.__dict__.items()}
+
+        data = [exp_to_dict(exp) for exp in self.history]
+        with open(fpath, "w") as f:
+            f.write(json.dumps(data, indent=2))
+
+    def __getitem__(self, index: int) -> PPORLElement:
+        return self.history[index]
+
+    def __len__(self) -> int:
+        return len(self.history)
+
+    def create_loader(self,
+        batch_size: int,
+        shuffle: bool,
+        **kwargs,
+    ) -> DataLoader:
+        def collate_fn(elems: Iterable[PPORLElement]):
+            if self.padding_side == "right":
+                # Right padding of already right-padded queries
+                query_tensors = pad_sequence(
+                    [elem.query_tensor for elem in elems],
+                    padding_value=self.pad_token_id,
+                    batch_first=True,
                 )
+            else:
+                # Left padding of already left-padded queries
+                query_tensors = pad_sequence(
+                    [elem.query_tensor.flip(0) for elem in elems],
+                    padding_value=self.pad_token_id,
+                    batch_first=True,
+                ).flip(1)
+
+            return PPORLBatch(
+                query_tensors,
+                # Right pad the rest, to have a single horizontal query/response split
+                pad_sequence(
+                    [elem.response_tensor for elem in elems],
+                    padding_value=self.pad_token_id,
+                    batch_first=True,
+                ),
+                pad_sequence(
+                    [elem.logprobs for elem in elems],
+                    padding_value=0.0,
+                    batch_first=True,
+                ),
+                pad_sequence([elem.values for elem in elems], padding_value=0.0, batch_first=True),
+                pad_sequence(
+                    [elem.rewards for elem in elems],
+                    padding_value=0.0,
+                    batch_first=True,
+                ),
+            )
 
-                rollout_count += 1
+        return DataLoader(self, batch_size, shuffle=shuffle, collate_fn=collate_fn)
 
-            if dist.is_initialized():
-                dist.all_reduce(mean_kl, dist.ReduceOp.AVG)
 
-            stats["time/rollout_time"] = clock.tick()
-            stats["policy/sqrt_kl"] = torch.sqrt(mean_kl).item()
-            stats["policy/kl_per_token"] = torch.sqrt(mean_kl_per_token).item()
-            accumulated_stats.append(stats)
 
-            tbar.set_description(f"[rollout {len(ppo_rl_elements)} / {num_rollouts}]")
-            tbar.update(min(rollout_count, num_rollouts))
-        tbar.close()
 
-        stats = {k: sum([xs[k] for xs in accumulated_stats]) / len(accumulated_stats) for k in stats}
-        stats["kl_ctl_value"] = self.kl_ctl.value
-        self.mean_kl = stats["policy/sqrt_kl"] ** 2
 
 
-        # Push samples and rewards to trainer's rollout storage
-        self.push_to_store(ppo_rl_elements)
 
-    def post_backward_callback(self):
-        self.kl_ctl.update(self.mean_kl.item(), n_steps=self.config.train.batch_size)
 
-    def post_epoch_callback(self):
-        """Post epoch callback
 
-        Clears the store and creates `num_rollouts` new episodes.
-        """
-        self.store.clear_history()
-        # Collect more rollouts for training
-        self.make_experience(self.config.num_rollouts, self.iter_count)
+# class PPOBaseTrainer:
+#     def __init__(self,
+#                  model,
+#                  ref_model,
+#                  tokenizer,
+#                  reward_fn: Callable,
+#                  ppo_config,
+#                  stop_sequences=None,
+#                  generate_kwargs = None or {}):
+#         self.model = model
+#         self.ref_model = ref_model
+#         self.tokenizer = tokenizer
+#         self.reward_fn = reward_fn
+#         self.config = ppo_config
+#
+#         self.stop_sequences = stop_sequences
+#         self.generate_kwargs = generate_kwargs
+#
+#         # Setup stats tracker
+#         self.running_moments = RunningMoments()
+#         self.ref_mean = self.config.ref_mean
+#         self.ref_std = self.config.ref_std
+#
+#         # Setup the KL controller
+#         # This helps prevent large divergences in the controller (policy)
+#         if self.config.target is not None:
+#             self.kl_ctl = AdaptiveKLController(self.config.init_kl_coef, self.config.target, self.config.horizon)
+#         else:
+#             self.kl_ctl = FixedKLController(self.config.init_kl_coef)
+#
+#     @torch.no_grad()
+#     def generate(self, input_ids, attention_mask=None, **kwargs):
+#         """Wraps hf's `generate` adding some specific method's defaults"""
+#         input_ids = input_ids.to(self.model.device)
+#         if attention_mask is not None:
+#             attention_mask = attention_mask.to(self.model.device)
+#         kwargs = dict(self.generate_kwargs, **kwargs)
+#         return self.model.generate(
+#             input_ids=input_ids, attention_mask=attention_mask, **kwargs
+#         )
+#
+#     def decode(
+#             self,
+#             prompts: List[torch.LongTensor],
+#             samples: List[torch.LongTensor],
+#             prompt_sizes: torch.LongTensor = None,
+#             append_eos_token: bool = False,
+#     ) -> Tuple[List[str], List[str], List[str]]:
+#         """
+#         Decode tensor generations into lists of strings (`samples`: List[str], `prompts`: List[str], `outputs`: List[str])
+#         """
+#         if prompt_sizes is None:
+#             # Assuming prompts were left-padded
+#             prompt_sizes = [prompts.shape[1]] * len(prompts)
+#
+#         str_samples, str_prompts, str_outputs = [], [], []
+#         for prompt, sample, prompt_size in zip(prompts, samples, prompt_sizes):
+#             if self.config.model.model_arch_type == "seq2seq":
+#                 output_start_ix = 0
+#             else:
+#                 output_start_ix = prompt_size
+#
+#             str_prompt = self.tokenizer.decode(prompt[:prompt_size], skip_special_tokens=True)
+#             str_output = self.tokenizer.decode(sample[output_start_ix:], skip_special_tokens=True)
+#             # Trim outputs up to `self.stop_sequences` if any are present
+#             trimmed = False
+#             if self.stop_sequences:
+#                 for stop in self.stop_sequences:
+#                     stop_ix = str_output.find(stop)
+#                     if stop_ix >= 0:
+#                         str_output = str_output[:stop_ix].rstrip()
+#                         trimmed = True
+#
+#             # Recover the last <eos> if it was present in the original sample
+#             # or add one if it was trimmed with `self.stop_sequences`.
+#             # Only in cases when a generation ended due to `max_new_tokens` exhaustion,
+#             # <eos> token would not be present in the original sample
+#             if append_eos_token and (trimmed or sample[-1] == self.tokenizer.eos_token_id):
+#                 str_output += self.tokenizer.eos_token
+#
+#             str_prompts.append(str_prompt)
+#             str_outputs.append(str_output)
+#
+#             if self.config.model.model_arch_type == "seq2seq":
+#                 sample = str_prompt + self.tokenizer.sep_token + str_output
+#             else:
+#                 sample = str_prompt + str_output
+#
+#             str_samples.append(sample)
+#
+#         return str_samples, str_prompts, str_outputs
+#
+#     def make_experience(self,prompt_iterator,
+#                             num_rollouts: int = 1024,
+#                             is_main_process=False,
+#                             world_size=dist.get_world_size(),
+#                             **kwargs):  # noqa:
+#         """Make experiences
+#
+#         Takes `chunk_size` number of prompts from `prompt_iterator`, samples
+#         from the model and then computes the KL against a reference model. Finally it
+#         then appends PPOElements to trainer's `store`.
+#
+#         Args:
+#             num_rollouts: Number of rollouts to generate
+#             iter_count: Total number of updates run (i.e. number of updates run for all batches & epochs)
+#         """
+#         logger.info("Collecting rollouts")
+#         tbar = logging.tqdm(
+#             total=num_rollouts,
+#             disable=os.environ.get("RANK", 0) != "0",
+#             desc=f"[rollout 0 / {num_rollouts}]",
+#             # Lower progress bar by 1 if we're in WARNING mode or above to avoid hiding high priority progress
+#             # bars (e.g. loss progress in trainers)
+#             position=logging.get_verbosity() >= logging.WARNING,
+#             # Leave progress bar if we're in INFO mode or lower to avoid spamming in suppressed verbosity levels
+#             leave=logging.get_verbosity() < logging.WARNING,
+#         )
+#
+#         clock = Clock()
+#         ppo_rl_elements = []
+#         accumulated_stats = []
+#
+#         while len(ppo_rl_elements) < num_rollouts:
+#             stats = {}
+#             # Get next batch in prompt dataset
+#             batch: dict = next(prompt_iterator)
+#
+#             rollout_generate_time = time()
+#
+#             # Generate samples from the language model (similar to using HuggingFace `generate` method)
+#             samples = self.generate(batch["input_ids"], batch["attention_mask"],**kwargs)
+#             stats["time/rollout_generate"] = time() - rollout_generate_time
+#
+#             prompt_tensors = batch['input_ids']
+#             device = samples.device
+#
+#             prompt_sizes = torch.tensor([prompt_tensors.shape[1]] * len(prompt_tensors), device=device)
+#             padded_samples = pad_across_processes(
+#                 samples, dim=1, pad_index=self.tokenizer.eos_token_id, pad_first=False
+#             )
+#             padded_prompts = pad_across_processes(
+#                 prompt_tensors, dim=1, pad_index=self.tokenizer.eos_token_id, pad_first=False
+#             )
+#             gathered_samples = _gpu_gather(padded_samples)
+#             gathered_prompts = _gpu_gather(padded_prompts)
+#             gathered_prompt_sizes = _gpu_gather(prompt_sizes)
+#             metadata = gather_dict({k: v for k, v in batch.items() if k != "input_ids" and k != "attention_mask"})
+#
+#             if is_main_process:
+#                 all_str_samples, all_str_prompts, all_str_outputs = self.decode(
+#                     gathered_prompts, gathered_samples, gathered_prompt_sizes, append_eos_token=True
+#                 )
+#
+#                 rollout_score_time = time()
+#                 all_scores = torch.tensor(
+#                     self.reward_fn(
+#                         samples=all_str_samples, prompts=all_str_prompts, outputs=all_str_outputs, **metadata
+#                     ),
+#                     dtype=torch.float,
+#                     device=device,
+#                 )
+#                 stats["time/rollout_score"] = time() - rollout_score_time
+#
+#                 all_scores = list(all_scores.reshape(world_size, -1).unbind())
+#             else:
+#                 all_scores = None
+#
+#             if dist.is_initialized():
+#                 scores = torch.empty(len(samples), device=device)
+#                 dist.scatter(scores, all_scores)
+#             else:
+#                 scores = all_scores[0].clone().detach()
+#
+#             str_samples, str_prompts, str_outputs = self.decode(prompt_tensors, samples, append_eos_token=True)
+#
+#             # Pad the sample outputs
+#             outputs = self.tokenizer(str_outputs).input_ids
+#             if self.config.model.model_arch_type == "seq2seq":
+#                 # add <pad> to the start of the output
+#                 for i in range(len(outputs)):
+#                     outputs[i] = [self.tokenizer.pad_token_id] + outputs[i]
+#
+#             outputs = list(map(torch.LongTensor, outputs))
+#             maxsize = max(map(len, outputs))
+#             outputs = [
+#                 F.pad(
+#                     output,
+#                     (0, maxsize - len(output)),
+#                     value=self.tokenizer.pad_token_id,
+#                 )
+#                 for output in outputs
+#             ]
+#             sample_outputs = torch.vstack(outputs).to(device)
+#
+#             if self.config.cliprange_reward:
+#                 scores = torch.clip(scores, -self.config.cliprange_reward, self.config.cliprange_reward)
+#
+#             # store statistics of the initial rollout as reference
+#             if self.ref_mean is None:
+#                 self.ref_mean, self.ref_std = scores.mean(), scores.std()
+#             all_scores_mean, all_scores_std = self.running_moments.update(scores)
+#             stats["rollout_scores/mean"] = all_scores_mean.item()
+#             stats["rollout_scores/std"] = all_scores_std.item()
+#             stats["rollout_scores/running_mean"] = self.running_moments.mean.item()
+#             stats["rollout_scores/running_std"] = self.running_moments.std.item()
+#
+#             if self.config.scale_reward == "running":
+#                 scores /= self.running_moments.std
+#             elif self.config.scale_reward == "ref":
+#                 scores /= self.ref_std
+#
+#             # Precompute logprobs, values
+#             if self.config.model.model_arch_type == "seq2seq":
+#                 attention_mask = batch['attention_mask'].to(device)
+#                 prompt_tensors = batch['input_ids'].to(device)
+#                 decoder_attention_mask = sample_outputs.not_equal(self.tokenizer.pad_token_id)
+#                 decoder_attention_mask[:, 0] = 1
+#                 with torch.no_grad():
+#                     outputs = self.model(
+#                         input_ids=prompt_tensors,
+#                         attention_mask=attention_mask,
+#                         decoder_input_ids=sample_outputs,
+#                         decoder_attention_mask=decoder_attention_mask,
+#                     )
+#                     logits = outputs.logits
+#                     values = outputs.value
+#                     if hasattr(self.model, "frozen_head"):
+#                         ref_logits = self.model.forward_hydra(
+#                             input_ids=prompt_tensors,
+#                             attention_mask=attention_mask,
+#                             decoder_input_ids=sample_outputs,
+#                             decoder_attention_mask=decoder_attention_mask,
+#                             return_dict=True,
+#                         ).logits
+#                     else:
+#                         ref_logits = self.ref_model(
+#                             input_ids=prompt_tensors,
+#                             attention_mask=attention_mask,
+#                             decoder_input_ids=sample_outputs,
+#                             decoder_attention_mask=decoder_attention_mask,
+#                             return_dict=True,
+#                         ).logits
+#             else:
+#                 all_tokens = torch.cat((prompt_tensors.to(device), sample_outputs), dim=1)
+#                 attention_mask = all_tokens.not_equal(self.tokenizer.pad_token_id).long().to(device)
+#                 with torch.no_grad():
+#                     logits, *_, values = self.model(
+#                         all_tokens,
+#                         attention_mask=attention_mask,
+#                     )
+#
+#                     ref_logits = self.ref_model(
+#                         all_tokens,
+#                         attention_mask=attention_mask,
+#                         return_dict=True,
+#                     ).logits
+#                     ref_logits = ref_logits.to(device)
+#
+#             if self.config.model.model_arch_type == "seq2seq":
+#                 logprobs = logprobs_of_labels(logits[:, :-1, :], sample_outputs[:, 1:])
+#                 ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], sample_outputs[:, 1:])
+#             else:
+#                 logprobs = logprobs_of_labels(logits[:, :-1, :], all_tokens[:, 1:])
+#                 ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], all_tokens[:, 1:])
+#
+#             n_samples: int = samples.shape[0]
+#
+#             # Estimate the KL divergence between the model and reference model
+#             if self.config.model.model_arch_type == "seq2seq":
+#                 attention_mask = sample_outputs != self.tokenizer.pad_token_id
+#                 start = 0
+#             else:
+#                 start = prompt_tensors.shape[1] - 1
+#
+#             log_ratio = (logprobs - ref_logprobs) * attention_mask[:, :-1]
+#             kl = log_ratio.exp() - 1 - log_ratio
+#             mean_kl_per_token = kl.mean()
+#             mean_kl = kl.sum(1).mean()
+#
+#             logprobs = logprobs.cpu()
+#             ref_logprobs = ref_logprobs.cpu()
+#             prompt_tensors = prompt_tensors.cpu()
+#             sample_outputs = sample_outputs.cpu()
+#             values = values.cpu()[:, :-1]
+#
+#             # Get the logprobs and values, for tokens that are not padding,
+#             # from the start of the prompt up to the <eos> token, while also including the latter
+#             # (these are taken from the student model and not the reference model)
+#             ends = start + attention_mask[:, start:].sum(1) + 1
+#             all_values = [values[ix, start: ends[ix]] for ix in range(n_samples)]
+#             all_logprobs = [logprobs[ix, start: ends[ix]] for ix in range(n_samples)]
+#
+#             kl_penalty = self.kl_ctl.value * -log_ratio.cpu()
+#             kl_penalty = [xs[start: ends[ix]] for ix, xs in enumerate(kl_penalty)]
+#
+#             rollout_count = 0
+#
+#             for sample_idx in range(n_samples):
+#                 rewards = kl_penalty[sample_idx]
+#                 rewards[-1] += scores[sample_idx].cpu()
+#
+#                 ppo_rl_elements.append(
+#                     PPORLElement(
+#                         query_tensor=prompt_tensors[sample_idx],
+#                         response_tensor=sample_outputs[sample_idx],
+#                         logprobs=all_logprobs[sample_idx],
+#                         values=all_values[sample_idx],
+#                         rewards=rewards,
+#                     )
+#                 )
+#
+#                 rollout_count += 1
+#
+#             if dist.is_initialized():
+#                 dist.all_reduce(mean_kl, dist.ReduceOp.AVG)
+#
+#             stats["time/rollout_time"] = clock.tick()
+#             stats["policy/sqrt_kl"] = torch.sqrt(mean_kl).item()
+#             stats["policy/kl_per_token"] = torch.sqrt(mean_kl_per_token).item()
+#             accumulated_stats.append(stats)
+#
+#             tbar.set_description(f"[rollout {len(ppo_rl_elements)} / {num_rollouts}]")
+#             tbar.update(min(rollout_count, num_rollouts))
+#         tbar.close()
+#
+#         stats = {k: sum([xs[k] for xs in accumulated_stats]) / len(accumulated_stats) for k in stats}
+#         stats["kl_ctl_value"] = self.kl_ctl.value
+#         self.mean_kl = stats["policy/sqrt_kl"] ** 2
+#
+#
+#         # Push samples and rewards to trainer's rollout storage
+#         self.push_to_store(ppo_rl_elements)
+#
+#     def post_backward_callback(self):
+#         self.kl_ctl.update(self.mean_kl.item(), n_steps=self.config.train.batch_size)
+#
+#     def post_epoch_callback(self):
+#         """Post epoch callback
+#
+#         Clears the store and creates `num_rollouts` new episodes.
+#         """
+#         self.store.clear_history()
+#         # Collect more rollouts for training
+#         self.make_experience(self.config.num_rollouts, self.iter_count)
```

## deep_training/nlp/rl/ppo_fabric/engine.py

```diff
@@ -1,17 +1,21 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/4/27 9:48
 import typing
 import torch
 from torch import nn
-from torch.nn import functional as F
+from .utils import gather_log_probs
 from ...models.transformer_base import TransformerLightningModule
 from ...utils import configure_optimizers
 
 
+__all__ = [
+    'PPOEngine',"PPO_Agent"
+]
+
 class PPOEngine:
     def __init__(self):
         self.actor_model: typing.Optional[nn.Module] = None
         self.ref_model: typing.Optional[nn.Module] = None
         self.critic_model: typing.Optional[nn.Module] = None
         self.reward_model: typing.Optional[nn.Module] = None
 
@@ -24,22 +28,19 @@
         self.critic_model.eval()
         self.reward_model.eval()
         self.ref_model.eval()
 
 
 
 
-def gather_log_probs(logits, labels):
-    log_probs = F.log_softmax(logits, dim=-1)
-    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(-1))
-    return log_probs_labels.squeeze(-1)
 
-class MyTransformer(TransformerLightningModule):
+
+class PPO_Agent(TransformerLightningModule):
     def __init__(self, *args, **kwargs):
-        super(MyTransformer, self).__init__(*args, **kwargs)
+        super(PPO_Agent, self).__init__(*args, **kwargs)
         self.automatic_optimization = False
         self.rlhf_engine = PPOEngine()
         self.actor_model = self.rlhf_engine.actor_model
         self.max_answer_seq_len = kwargs.get('max_answer_seq_len',128)
 
 
     def configure_optimizers(self):
```

## deep_training/utils/trainer.py

```diff
@@ -7,15 +7,16 @@
 import torch
 import lightning as pl
 from lightning.pytorch.callbacks import Checkpoint,ModelCheckpoint,Callback
 from lightning.pytorch.utilities.types import STEP_OUTPUT
 from torch import Tensor
 
 __all__ = [
-    'SimpleModelCheckpoint'
+    'SimpleModelCheckpoint',
+    'SimpleModelCheckpointFabric'
 ]
 
 class SimpleModelCheckpoint(Checkpoint):
     def __init__(self,
                  rank=0,# 执行节点
                  every_n_train_steps: Optional[int] = None,
                  every_n_epochs: Optional[int] = None,
@@ -167,7 +168,171 @@
         if self.__every_n_epochs is not None and self.__every_n_epochs > 0:
             if trainer.current_epoch % self.__every_n_epochs == 0:
                 if self.skip_n_epochs is not None and trainer.current_epoch < self.skip_n_epochs:
                     pass
                 else:
                     self.__on_save_model(trainer, pl_module)
 
+
+
+
+
+
+
+
+
+class SimpleModelCheckpointFabric:
+    def __init__(self,
+                 rank=0,# 执行节点
+                 every_n_train_steps: Optional[int] = None,
+                 every_n_epochs: Optional[int] = None,
+                 skip_n_train_steps :  Optional[int] = None,
+                 skip_n_epochs: Optional[int] = None,
+                 monitor=None,
+                 mode='min',
+                 save_weights_only=False,
+                 weight_file='./best.pt',#保存权重名字
+                 last_weight_file='./last.pt',#每评估一次保存一次权重
+                 **kwargs):
+
+        self.__every_n_train_steps = every_n_train_steps
+        self.__every_n_epochs = every_n_epochs
+        assert not (self.__every_n_epochs is None and self.__every_n_train_steps is None),ValueError('must set value one of [every_n_train_steps,every_n_epochs]')
+        self.best = {}
+        self.monitor = monitor
+        self.mode = mode # min max
+
+        self.save_weights_only = save_weights_only
+        self.rank = rank
+
+        self.last_eval_step = -1
+
+        self.skip_n_train_steps = skip_n_train_steps
+        self.skip_n_epochs = skip_n_epochs
+
+        self.weight_file = weight_file
+        self.last_weight_file = last_weight_file
+        self._external_kwargs = kwargs
+
+    @property
+    def external_kwargs(self):
+        return self._external_kwargs
+
+    def _monitor_candidates(self, trainer: "pl.Trainer") -> Dict[str, Tensor]:
+        monitor_candidates = copy.deepcopy(trainer.callback_metrics)
+        # cast to int if necessary because `self.log("epoch", 123)` will convert it to float. if it's not a tensor
+        # or does not exist we overwrite it as it's likely an error
+        epoch = monitor_candidates.get("epoch")
+        monitor_candidates["epoch"] = epoch.int() if isinstance(epoch, Tensor) else torch.tensor(trainer.current_epoch)
+        step = monitor_candidates.get("step")
+        monitor_candidates["step"] = step.int() if isinstance(step, Tensor) else torch.tensor(trainer.global_step)
+        return monitor_candidates
+
+    def on_get_metric( self, trainer: "pl.Trainer", pl_module: "pl.LightningModule"):
+        return {}
+
+    def update_best(self,val):
+        flag = False
+        if isinstance(val, torch.Tensor):
+            if self.monitor not in self.best:
+                flag = True
+                self.best[self.monitor] = val
+            else:
+                monitor_op = torch.le if self.mode.lower() == 'min' else torch.ge
+                if monitor_op(val, self.best[self.monitor]).bool().cpu().item():
+                    flag = True
+        else:
+            warnings.warn('monitor {} is not tensor'.format(self.monitor))
+
+        if flag:
+            self.best[self.monitor] = val
+        return flag
+
+    def on_save_model(
+        self, trainer: "pl.Trainer", pl_module: "pl.LightningModule"
+    ) -> None:
+
+        monitor_candidates = self._monitor_candidates(trainer)
+        monitor_candidates.update(self.on_get_metric(trainer,pl_module))
+        val = monitor_candidates.get(self.monitor,None)
+        if val is not None:
+            flag = self.update_best(val)
+            if flag:
+                logging.info('epoch {} ,step {} , save best {}, {}\n'.format(monitor_candidates['epoch'],
+                                                                           monitor_candidates['step'],
+                                                                           self.best[self.monitor],
+                                                                           self.weight_file))
+                self._save_checkpoint(trainer,self.weight_file)
+
+            if self.last_weight_file is not None:
+                logging.info('epoch {} ,step {} , save {}\n'.format(monitor_candidates['epoch'],
+                                                                    monitor_candidates['step'],
+                                                                    self.last_weight_file))
+                self._save_checkpoint(trainer,self.weight_file)
+
+        else:
+            warnings.warn('monitor {} is not in metirc , save lastest checkpoint!'.format(self.monitor))
+
+            logging.info('epoch {} ,step {} , save {}\n'.format(monitor_candidates['epoch'],
+                                                                monitor_candidates['step'],
+                                                                self.weight_file))
+            self._save_checkpoint(trainer,self.weight_file)
+
+    def _save_checkpoint(self, trainer: "pl.Trainer", filepath: str) -> None:
+        trainer.save_checkpoint(filepath, self.save_weights_only)
+
+
+
+    def __on_save_model(
+        self, trainer: "pl.Trainer", pl_module: "pl.LightningModule"
+    ) -> None:
+
+        if trainer.world_size > 1:
+            if self.rank >= 0 and trainer.global_rank != self.rank:
+                return
+
+        pl_module.eval()
+        with torch.no_grad():
+            self.on_save_model(trainer,pl_module)
+        pl_module.train()
+
+    def on_train_batch_end(
+            self, trainer: "pl.Trainer", pl_module: "pl.LightningModule", outputs, batch: Any,
+            batch_idx: int
+    ) -> None:
+        """Called when the train batch ends.
+
+        Note:
+            The value ``outputs["loss"]`` here will be the normalized value w.r.t ``accumulate_grad_batches`` of the
+            loss returned from ``training_step``.
+        """
+
+        flag = False
+        if self.__every_n_train_steps is not None and self.__every_n_train_steps > 0:
+            if trainer.global_step !=0 and trainer.global_step  % self.__every_n_train_steps == 0:
+                # 由于梯度积累，已经执行过，跳过
+                if self.last_eval_step != trainer.global_step:
+                    if self.skip_n_train_steps is not None and trainer.global_step < self.skip_n_train_steps:
+                        flag = False
+                    else:
+                        flag = True
+        if flag:
+            self.last_eval_step = trainer.global_step
+            self.__on_save_model(trainer, pl_module)
+
+
+
+    def on_train_epoch_end(self, trainer: "pl.Trainer", pl_module: "pl.LightningModule") -> None:
+        """Called when the train epoch ends.
+
+        To access all batch outputs at the end of the epoch, either:
+
+        1. Implement `training_epoch_end` in the `LightningModule` and access outputs via the module OR
+        2. Cache data across train batch hooks inside the callback implementation to post-process in this hook.
+        """
+
+        if self.__every_n_epochs is not None and self.__every_n_epochs > 0:
+            if trainer.current_epoch % self.__every_n_epochs == 0:
+                if self.skip_n_epochs is not None and trainer.current_epoch < self.skip_n_epochs:
+                    pass
+                else:
+                    self.__on_save_model(trainer, pl_module)
```

## Comparing `deep_training/nlp/rl/ppo/utils.py` & `deep_training/nlp/rl/ppo/utils/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,20 +1,107 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2023/4/23 14:15
+from itertools import repeat
+from .logging import get_logger
 import time
-from typing import Dict, MutableMapping, Union, Tuple, Mapping
+from dataclasses import dataclass
+from typing import Dict, MutableMapping, Union, Tuple, Mapping, Iterable
 
 import numpy as np
 from transformers import PretrainedConfig
 import torch
 import torch.distributed as dist
 from torch import nn
 from torch.nn import functional as F
+logger = get_logger(__name__)
 
+@dataclass
+class RLElement:
+    """
+    Batch element for RL model
+    """
+
+    state: Iterable[str] = None  # Context/prompts
+    action: torch.Tensor = None #TensorType["N"] = None  # Tokens generated by model given prompts
+    reward: float = None  # Reward obtained for that generation
+
+@dataclass
+class PPORLElement:
+    """
+    :param query_tensor: The query tensor i.e. the prompt tokens.
+                         Should be a long tensor.
+    :type query_tensor: torch.Tensor
+
+    :param response_tensor: The response tensor i.e. the output tokens.
+                            Should be a long tensor.
+    :type response_tensor: torch.Tensor
+
+    :param logprobs: The log probabilities over the response tokens generated
+                    by the policy network (i.e. the autoregressive model).
+                    Should be a float tensor of same size as tokens.
+    :type logprobs: torch.Tensor
+
+    :param values: The values for each token generated from the value network or value head.
+                    Should be a float tensor of same size as tokens.
+    :type values: torch.Tensor
+
+    :param rewards: The rewards for each token outputted in response.
+                    Should be a float tensor of same size as tokens.
+    :type rewards: torch.Tensor
+    """
+
+    query_tensor: torch.Tensor
+    response_tensor: torch.Tensor
+    logprobs: torch.Tensor
+    values: torch.Tensor
+    rewards: torch.Tensor
+
+
+@dataclass
+class PromptBatch:
+    """
+    Batched PromptElement
+
+    :param text: An iterable of prompt texts.
+    :type text: Iterable[str]
+
+    :param tokens: A long tensor batch of prompt tokens.
+    :type tokens: torch.Tensor
+    """
 
+    text: Iterable[str]
+    tokens: torch.Tensor  # TensorType["batch_size", "num_tokens"]
+
+
+@dataclass
+class PPORLBatch:
+    """
+    A batched version of the PPORLElement. See PPORLElement for more details on individual fields.
+
+    :param query_tensors: A batch of query tensors. Should be a long tensor.
+    :type query_tensors: torch.Tensor
+
+    :param response_tensors: A batch of response tensors. Should be a long tensor.
+    :type response_tensors: torch.Tensor
+
+    :param logprobs: A batch of log probabilities from policy
+    :type logprobs: torch.Tensor
+
+    :param values: A batch of values from value network
+    :type values: torch.Tensor
+
+    :param rewards: A batch of rewards
+    :type rewards: torch.Tensor
+    """
+
+    query_tensors: torch.Tensor  # TensorType["batch_size", "query_size"]
+    response_tensors: torch.Tensor  # TensorType["batch_size", "response_size"]
+    logprobs: torch.Tensor  # TensorType["batch_size", "response_size"]
+    values: torch.Tensor  # TensorType["batch_size", "response_size"]
+    rewards: torch.Tensor  # TensorType["batch_size", "response_size"]
 
 
 def get_global_statistics(xs: torch.Tensor) -> Tuple[float, float, int]:
     """
     Computes element-wise mean and variance of the tensor across processes
     """
     sum_and_count = torch.tensor([xs.sum(), xs.numel()], device=xs.device)
@@ -72,16 +159,16 @@
     """
     Gather and concatenates key-values from a dictionary, optionally
     trimming them if some of them were out of dataloader's padding
     """
     if not torch.distributed.is_initialized():
         return obj
 
-    objs = [None] * torch.distributed.get_world_size()
-    torch.distributed.all_gather_object(objs, obj)
+    objs = [None] * dist.get_world_size()
+    dist.all_gather_object(objs, obj)
 
     acc, *objs = objs
     for obj in objs:
         for k in obj:
             acc[k].extend(obj[k])
 
     # if grad_state:
@@ -243,28 +330,28 @@
         raise TypeError(
             f"Can't apply {func.__name__} on object of type {type(data)}, only of nested list/tuple/dicts of objects "
             f"that satisfy {test_type.__name__}."
         )
     return data
 
 
-def _gpu_gather(tensor):
+def _gpu_gather(tensor,world_size):
     def _gpu_gather_one(tensor):
         if tensor.ndim == 0:
             tensor = tensor.clone()[None]
-        output_tensors = [tensor.clone() for _ in range(torch.distributed.get_world_size())]
-        torch.distributed.all_gather(output_tensors, tensor)
+        output_tensors = [tensor.clone() for _ in range(world_size)]
+        dist.all_gather(output_tensors, tensor)
         return torch.cat(output_tensors, dim=0)
 
     return recursively_apply(_gpu_gather_one, tensor, error_on_other_type=True)
 
 
 _cpu_gather = _gpu_gather
 
-def pad_across_processes(tensor, dim=0, pad_index=0, pad_first=False):
+def pad_across_processes(tensor,world_size, dim=0, pad_index=0, pad_first=False):
     """
     Recursively pad the tensors in a nested list/tuple/dictionary of tensors from all devices to the same size so they
     can safely be gathered.
 
     Args:
         tensor (nested list/tuple/dictionary of `torch.Tensor`):
             The data to gather.
@@ -278,15 +365,15 @@
 
     def _pad_across_processes(tensor, dim=0, pad_index=0, pad_first=False):
         if dim >= len(tensor.shape):
             return tensor
 
         # Gather all sizes
         size = torch.tensor(tensor.shape, device=tensor.device)[None]
-        sizes = _cpu_gather(size).cpu()
+        sizes = _gpu_gather(size,world_size).cpu()
         # Then pad to the maximum size
         max_size = max(s[dim] for s in sizes)
         if max_size == tensor.shape[dim]:
             return tensor
 
         old_size = tensor.shape
         new_size = list(old_size)
@@ -299,8 +386,17 @@
         else:
             indices = tuple(slice(0, old_size[dim]) if i == dim else slice(None) for i in range(len(new_size)))
         new_tensor[indices] = tensor
         return new_tensor
 
     return recursively_apply(
         _pad_across_processes, tensor, error_on_other_type=True, dim=dim, pad_index=pad_index, pad_first=pad_first
-    )
+    )
+
+
+
+def infinite_dataloader(dataloader: Iterable) -> Iterable:
+    """
+    Returns a cyclic infinite dataloader from a finite dataloader
+    """
+    for _ in repeat(dataloader):
+        yield from dataloader
```

## Comparing `deep_training-0.1.4.dist-info/RECORD` & `deep_training-0.1.5rc0.dist-info/RECORD`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=lfCUnxR3vFX4OlF9DejxOze7_l8xUQqOwZWr-eMqB2Q,905
+deep_training/setup.py,sha256=QlP81pyOiqORDocG_pAUjW3lDU7IIwFIm8cDMD_fMgA,900
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
 deep_training/data_helper/data_helper.py,sha256=Pe45VeRSRl7izzI5TVfsZ6PnzGK3tyfGWMxGsoclnLs,17724
 deep_training/data_helper/data_module.py,sha256=EmXCTU2jLnldgHubQL4lpwzmlJErSVJf7YIotbQBQJU,5041
 deep_training/data_helper/training_args.py,sha256=XGUXdty0SE6n8xqk6J0lySFvaYSGMVo2zuq6paFQ8sM,12121
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
 deep_training/nlp/layers/__init__.py,sha256=zbd9GfR02_YVgsTJSXjfyIcQwj8PmG4PscMdA0p6ONI,56
@@ -19,15 +19,15 @@
 deep_training/nlp/layers/w2ner.py,sha256=fP7hlMHp1NTH6elNMJA-wBOER76VTouSSsKCinLsCyM,3550
 deep_training/nlp/layers/lora_v1/__init__.py,sha256=gmwJqLmiKqPfh5_VGWWr38y8lLgdPWw6JrjNVrvsLEY,72
 deep_training/nlp/layers/lora_v1/layers.py,sha256=JWz9RtqA-hLsTxyhZPBlz82aN_VaPjNoDYRhssKV1H0,15095
 deep_training/nlp/layers/lora_v1/utils.py,sha256=1ouFUmTF9IXzum97eIlrTeT6J4OAnEwIaWkZdgXMjSc,1819
 deep_training/nlp/layers/lora_v2/__init__.py,sha256=dGpWUx0v7UoVgwZY5srCDCBvt_hlI77zA6mQO3CxMaE,72
 deep_training/nlp/layers/lora_v2/adalora.py,sha256=dOqXcfqFfs4tpiwqU52bEzO1WoX9wP2itSBysw9rQ90,15465
 deep_training/nlp/layers/lora_v2/layers.py,sha256=giRQXEolkWGhU0CsNoOMOZBXrgwBbp6_FvOf9_Cjdeg,8565
-deep_training/nlp/layers/lora_v2/utils.py,sha256=Hl3i9r0eP5lUzSqyPAXKnO0oEgDCY8lC3ZGV0scmNRs,9106
+deep_training/nlp/layers/lora_v2/utils.py,sha256=Ea6gcMZ276OWS3dk89Ai4YLlMje5HtadFhw5SziYsoM,9287
 deep_training/nlp/layers/prompt/__init__.py,sha256=J8P_z5Lrd5h9c6mhMG1BY8ZP4Vl5l8Tc5yukwlzg9Ag,80
 deep_training/nlp/layers/prompt/adaption_prompt.py,sha256=J9iDQOQQSE98GN1Tmd9Yjo7v0QtCA6yXpyPsRGRXe1I,15541
 deep_training/nlp/layers/prompt/p_tuning.py,sha256=LTvyxWmSrMq6nIi8v8Ohx8knkvFYqvPUSuQ_hssGOpc,5463
 deep_training/nlp/layers/prompt/prefix_tuning.py,sha256=QwgiIoEPPjKy34gH7t1dHg4sh5RRdcbbMl_iMcYH-uA,3053
 deep_training/nlp/layers/prompt/prompt_tuning.py,sha256=STLP8rRqT1XEjcts3lwgjLWhd0_NZT6YvqYSN9nsMkI,3523
 deep_training/nlp/layers/prompt/utils.py,sha256=Hl3i9r0eP5lUzSqyPAXKnO0oEgDCY8lC3ZGV0scmNRs,9106
 deep_training/nlp/losses/BatchAllTripletLoss.py,sha256=_2Og7Hf3Bjd1GT55UFmbZq5QLxdcKUyv4T00loPrKUo,3662
@@ -112,47 +112,51 @@
 deep_training/nlp/models/lora/v1/__init__.py,sha256=zwGdNKqudVj7c8sMWbmZ9CnnncWXuEapAucWY-VEhLs,123
 deep_training/nlp/models/lora/v1/configuration.py,sha256=MAkg2BQCqL6XvHd2SRXsijtPToNMuN-4Hmm_02HVCmU,7054
 deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=c7X2raQW6tEN1stIIBJxoHA87mHEsDJZjcPDl_8D9g0,13688
 deep_training/nlp/models/lora/v2/__init__.py,sha256=2XorjeFlyNuH6xTXiyNO1A8A3P5acBApOjxVv3YEon0,206
 deep_training/nlp/models/lora/v2/adalora_model.py,sha256=iKfKWnW--iY2gmXkMcBv6QWJr9vu-uSll57r1UNvRrY,13112
 deep_training/nlp/models/lora/v2/configuration.py,sha256=zqzbz7ljON4JH9ozp6m128fV1nvVAv7KR3V1HszGhj8,11285
 deep_training/nlp/models/lora/v2/lora_model.py,sha256=5k09kzj8bSvU-CQm5GJWtg5isXUEUYPmvKuxdLPc_V4,11745
-deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=f5Q6CUVU6jloiyxGIMc1GLZMOSWidVjnBaa_vDbEi2s,10415
+deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=CvjnW80ZB558JiNBioYkIO3LWgujcNdgAyVqNsOQttE,10644
 deep_training/nlp/models/lora/v2/save_and_load.py,sha256=U7_ZaPm8gpg8gQhZei6UG5KvsJXDtSNZfZk1gWo6nWc,4889
 deep_training/nlp/models/moss/__init__.py,sha256=_dQslDggRX8ZsR6RPTUuBzAHotQt8MPAqZ1-nGULPns,467
 deep_training/nlp/models/moss/configuration_moss.py,sha256=Qqp7anpWGnsotkqd5UOfc9e5zhxgx7j5xetSQUOmhaQ,5097
 deep_training/nlp/models/moss/custom_autotune.py,sha256=O-C9w-hZkcrUgDfK4B1iPtKjHQAZwELNefYhlLABHyc,6735
 deep_training/nlp/models/moss/modeling_moss.py,sha256=Trm6zkUFQo9SkgInIuusSFekRvyLa6x2W6WyRE9CswI,31079
 deep_training/nlp/models/moss/quantization.py,sha256=FBkz_BJ0s1AxvpStE6wEklGH7Ot63bGuO5D4XaVsWqc,18744
 deep_training/nlp/models/moss/tokenization_moss.py,sha256=Ft7hwLBfYoAqn33anM0sbkvU7GuXJQW8NJ1Ddko_1hk,15939
 deep_training/nlp/models/prompt/__init__.py,sha256=D8B65xX2WyGoV4PBPmc1NujefRcgOz1DUq9zcYbBE2g,203
 deep_training/nlp/models/prompt/configuration.py,sha256=q08sC6BzMcS8yoyV3aMWCXeeZH2fleIB0rn_i98Iw_Y,12062
 deep_training/nlp/models/prompt/prompt_model.py,sha256=BE3eaxzg-tWCy8PSp4aQo7tJdXZDpYtQs4o2KAudH90,52102
 deep_training/nlp/models/prompt/save_and_load.py,sha256=A0RtSZUp_Cn7A-zWIEZKtTAa_quWhVTTi9AtsWB5VlE,3551
 deep_training/nlp/models/prompt/utils.py,sha256=nvNO26eHmgIuY2WrfX3IyyH6jDwEJmQv7ieZqUA-n-0,1917
+deep_training/nlp/models/rl/__init__.py,sha256=pg2jplYDS8gj_w4iUzDgKNFpklxdtOfw4xcTyjA-3xU,54
+deep_training/nlp/models/rl/modeling.py,sha256=k32Ep8uGJMhmEj3wV6wL3qNMomeNJ9ydqaXES5D7tAE,40248
+deep_training/nlp/models/rl/utils.py,sha256=roKArfIwPDMfjVEVpl720Pgv3rNBD5IArk6jUsvCh6c,8026
 deep_training/nlp/models/splinker/__init__.py,sha256=QtgnpJa78vAq9bzfjN67NmHU3dXU6WH84jeyZoD1sBs,102
 deep_training/nlp/models/splinker/splinker.py,sha256=AhIWyfUtNOLqwZn520J-mv8LJwIoDZpo8yNoc4V5Gss,2866
 deep_training/nlp/models/t5decoder/__init__.py,sha256=R9Op4Ysli9isootQQ2FcjhpbG13fNESlmUROu6cfGH0,14478
 deep_training/nlp/models/t5encoder/__init__.py,sha256=692ChfLf2sZWgzhBM37g1PdpmEmsU1R9RRl_uTHRET0,6646
 deep_training/nlp/optimizer/__init__.py,sha256=c4cmx9ebIdqwXBu3N9QbcNNHb32t2MV6fTK9aC2VBGQ,56
 deep_training/nlp/optimizer/lamb.py,sha256=htvZQHPWHG5GCDgo9xCaZikWwRyaD2PjDioIQvX7qXw,5225
 deep_training/nlp/optimizer/lion/__init__.py,sha256=AvYkLp7sOpRIC3a5ejuniUUKyQmmBA1TPJdt2RA7Nqg,99
-deep_training/nlp/optimizer/lion/lion.py,sha256=kDN4_dz1N6G0q8PhQBpdQRNhtm5MXNSx-kde2JP2FlU,2295
-deep_training/nlp/optimizer/lion/triton.py,sha256=QLdWMW7cgqQU6Zb8uqmESONr7S7Nf1Tk4TR7OwACDQo,2198
+deep_training/nlp/optimizer/lion/lion.py,sha256=D_b3Z8cKI3d2cpEKeFiV5YCCZzC042mZqbjZAf-fY3o,2516
+deep_training/nlp/optimizer/lion/triton.py,sha256=W7aZkc5SSgsiyrQFazpeqEkc_UyW0g-EZiALBpT5a8k,2499
 deep_training/nlp/rl/__init__.py,sha256=lXJsb8d-9R7DshCEdcx3iPyndlf1t5FNXiJsh1SUr0s,79
 deep_training/nlp/rl/ppo/__init__.py,sha256=IqNQicmSmtZVbJIdNZdaQxpx0EbqvTJSUb2Bx1pRdys,55
-deep_training/nlp/rl/ppo/configuration.py,sha256=RjPR9mOa-sxtgD65dCfI5os_L7XADs591fJrxNBdodY,2238
-deep_training/nlp/rl/ppo/data_type.py,sha256=YSz3BPegvAC6Pl3x-dnFRbBV-qSxiJjwVwpUvbmMV7U,2691
-deep_training/nlp/rl/ppo/ppo.py,sha256=rJ8edj86BF5XgVcw9sZMLM629ixElHJXNLMmHrUTDko,8063
-deep_training/nlp/rl/ppo/ppo_dataset.py,sha256=IG96gmT0gD_Icw2mCdw_2OxZrKyVi_5QsoU2dQq79lA,16348
-deep_training/nlp/rl/ppo/utils.py,sha256=9oDHyKiXKXJDpLuVfUyUaA6FhyobLmaU7seY_hupPYU,10496
+deep_training/nlp/rl/ppo/configuration.py,sha256=Q_zMVJPBZWwu8V8oZwWqiGa67bf5ze7O_8KcJkosOJg,6685
+deep_training/nlp/rl/ppo/ppo_dataset.py,sha256=zEcYOaRG4wlIrO1Ee-bdKJ_vv7uCW3IArLGQvj1MEuE,23235
+deep_training/nlp/rl/ppo/ppo_module.py,sha256=irceHNhA28HHMsbyjvwWcu6HuQKnXTO3ZSmeeBiLcK0,9421
+deep_training/nlp/rl/ppo/ppo_trainer.py,sha256=PTWkMOiqozdZE4aO6Ue4MmWjxWPPe_dvCTquPkJMs98,43801
+deep_training/nlp/rl/ppo/utils/__init__.py,sha256=_rHbt6EmYegAOIErT4ZUUNlj23v3UN2TG4IeHCowMsQ,13669
+deep_training/nlp/rl/ppo/utils/logging.py,sha256=mF0eKsv9BqBNymZt_wnkMP04AF8N9B_Lhg5Wpb9iRmc,9844
 deep_training/nlp/rl/ppo_fabric/__init__.py,sha256=t568ZV7B9LwcorfgWdMWOHkoxo7cZJam7lBpzVBD6HY,54
-deep_training/nlp/rl/ppo_fabric/engine.py,sha256=G8GqioYmcFzuV1patyw9lDbX4wTs0hIlSWC8CLPXCt4,8621
-deep_training/nlp/rl/ppo_fabric/ppo.py,sha256=ixg_l8FPFtKliwEgmFUc01IhxQ3Pf_jkvYfNOVL_P14,5736
-deep_training/nlp/rl/ppo_fabric/ppo_agent.py,sha256=hBjLtVI8f8o5BJD7TQUJS4vJ0SL8mekh9bk6uMTjH-Q,5977
+deep_training/nlp/rl/ppo_fabric/engine.py,sha256=eLsAw3qmlPrT8jaeDiZMXuMWWARjlOZ21YdERfftOh0,8457
+deep_training/nlp/rl/ppo_fabric/ppo_trainer.py,sha256=eR14Ric33LC6BL3h6aIe8BgMQsQvfm6lWzfRHJI_RkM,23909
+deep_training/nlp/rl/ppo_fabric/utils.py,sha256=zxIDyCjSDsovtNSCb-EyHMDSkx-bt6g00zUI2PaLS98,2088
 deep_training/nlp/scheduler/__init__.py,sha256=-zaiinwJzOBWypkNodSZO12kqbswVsPy5JCsYpvLbbY,2868
 deep_training/nlp/utils/__init__.py,sha256=DF-_NzNu7n93AwN-hJcae38k5FbtqvxDKh62y8S_Opo,6986
 deep_training/nlp/utils/adversarial.py,sha256=FNZlg8mV23YXRu7aDcu1JZBUGBV01hi_bwRzfFyzEzM,6323
 deep_training/nlp/utils/nlputils.py,sha256=KEmFliU1IqJHy3INNDvOriEMlBkP8GNwe8Y8_c_imZQ,15256
 deep_training/nlp/utils/spearman.py,sha256=tOpaah5bt_65ferL_uI6FMfKvNexi7CQztSYLj-k3yo,795
 deep_training/tfnlp/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
 deep_training/tfnlp/layers/__init__.py,sha256=TRm9uMMO340jiD1ZGdDhtoxQ5BxI-au-RnOwAV13mbM,53
@@ -162,12 +166,12 @@
 deep_training/tfnlp/optimizer/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
-deep_training/utils/trainer.py,sha256=kbojvTgVcYX8bJ141J7VS14-JrD480Oh33P_tnwjlks,7430
-deep_training-0.1.4.dist-info/METADATA,sha256=WtZsIB5gkHwQZrE3exfwuusipVuDxvhZF65mPQGcZzw,590
-deep_training-0.1.4.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-deep_training-0.1.4.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.4.dist-info/RECORD,,
+deep_training/utils/trainer.py,sha256=F1usofzi1lBVHeieDJ7WWdfd1d0Q7tftktwdJgczlg8,14500
+deep_training-0.1.5rc0.dist-info/METADATA,sha256=yk6CnZoG4e9SM4ULY0zqV-TnRIkoDHBZxhZQfFXrUKM,605
+deep_training-0.1.5rc0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.5rc0.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.5rc0.dist-info/RECORD,,
```

