# Comparing `tmp/nemollm-0.2.0-py3-none-any.whl.zip` & `tmp/nemollm-0.3.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,36 +1,12 @@
-Zip file size: 118599 bytes, number of entries: 34
--rw-r--r--  2.0 unx     3635 b- defN 23-Jan-24 21:09 nemollm/__init__.py
--rw-r--r--  2.0 unx    39156 b- defN 23-Jan-24 21:09 nemollm/api_client.py
--rw-r--r--  2.0 unx    18478 b- defN 23-Jan-24 21:09 nemollm/configuration.py
--rw-r--r--  2.0 unx     7242 b- defN 23-Jan-24 21:09 nemollm/exceptions.py
--rw-r--r--  2.0 unx    23631 b- defN 23-Jan-24 21:25 nemollm/handy.py
--rw-r--r--  2.0 unx    82265 b- defN 23-Jan-24 21:09 nemollm/model_utils.py
--rw-r--r--  2.0 unx    16363 b- defN 23-Jan-24 21:09 nemollm/rest.py
--rw-r--r--  2.0 unx      211 b- defN 23-Jan-24 15:17 nemollm/api/__init__.py
--rw-r--r--  2.0 unx    64599 b- defN 23-Jan-24 21:09 nemollm/api/default_api.py
--rw-r--r--  2.0 unx      466 b- defN 23-Jan-24 15:17 nemollm/apis/__init__.py
--rw-r--r--  2.0 unx      341 b- defN 23-Jan-24 15:17 nemollm/model/__init__.py
--rw-r--r--  2.0 unx    14068 b- defN 23-Jan-24 21:09 nemollm/model/classification.py
--rw-r--r--  2.0 unx    20174 b- defN 23-Jan-24 21:09 nemollm/model/completion_request_body.py
--rw-r--r--  2.0 unx    15999 b- defN 23-Jan-24 21:09 nemollm/model/completion_response_body.py
--rw-r--r--  2.0 unx    20027 b- defN 23-Jan-24 21:09 nemollm/model/create_customizations_request_body.py
--rw-r--r--  2.0 unx    16632 b- defN 23-Jan-24 21:09 nemollm/model/create_customizations_request_body_additional_hyperparameters.py
--rw-r--r--  2.0 unx    20117 b- defN 23-Jan-24 21:09 nemollm/model/customization.py
--rw-r--r--  2.0 unx    16620 b- defN 23-Jan-24 21:09 nemollm/model/customization_status.py
--rw-r--r--  2.0 unx    14594 b- defN 23-Jan-24 21:09 nemollm/model/customizations_list_response_body.py
--rw-r--r--  2.0 unx    15068 b- defN 23-Jan-24 21:09 nemollm/model/file.py
--rw-r--r--  2.0 unx    13629 b- defN 23-Jan-24 21:09 nemollm/model/file_format.py
--rw-r--r--  2.0 unx    15076 b- defN 23-Jan-24 21:09 nemollm/model/list_meta.py
--rw-r--r--  2.0 unx    14170 b- defN 23-Jan-24 21:09 nemollm/model/model.py
--rw-r--r--  2.0 unx    15654 b- defN 23-Jan-24 21:09 nemollm/model/model_checkpoint.py
--rw-r--r--  2.0 unx    13850 b- defN 23-Jan-24 21:09 nemollm/model/models_list_response.py
--rw-r--r--  2.0 unx    14388 b- defN 23-Jan-24 21:09 nemollm/model/p_tuning_hyperparms.py
--rw-r--r--  2.0 unx    14545 b- defN 23-Jan-24 21:09 nemollm/model/pagination_links.py
--rw-r--r--  2.0 unx    14391 b- defN 23-Jan-24 21:09 nemollm/model/status_detail.py
--rw-r--r--  2.0 unx     1499 b- defN 23-Jan-24 15:17 nemollm/models/__init__.py
--rw-r--r--  2.0 unx     1097 b- defN 23-Jan-24 21:25 nemollm-0.2.0.dist-info/LICENSE.md
--rw-r--r--  2.0 unx     2735 b- defN 23-Jan-24 21:25 nemollm-0.2.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jan-24 21:25 nemollm-0.2.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        8 b- defN 23-Jan-24 21:25 nemollm-0.2.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2947 b- defN 23-Jan-24 21:25 nemollm-0.2.0.dist-info/RECORD
-34 files, 533767 bytes uncompressed, 113897 bytes compressed:  78.7%
+Zip file size: 10068 bytes, number of entries: 10
+-rw-rw-r--  2.0 unx       40 b- defN 23-Mar-30 01:56 nemollm/__init__.py
+-rw-rw-r--  2.0 unx    18808 b- defN 23-May-04 19:26 nemollm/api.py
+-rw-rw-r--  2.0 unx     9872 b- defN 23-May-04 20:56 nemollm/cli.py
+-rw-rw-r--  2.0 unx     1473 b- defN 23-Mar-30 01:56 nemollm/error.py
+-rw-rw-r--  2.0 unx       22 b- defN 23-Mar-30 01:56 nemollm/version.py
+-rw-rw-r--  2.0 unx     4030 b- defN 23-May-04 20:56 nemollm-0.3.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-04 20:56 nemollm-0.3.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       46 b- defN 23-May-04 20:56 nemollm-0.3.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        8 b- defN 23-May-04 20:56 nemollm-0.3.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx      755 b- defN 23-May-04 20:56 nemollm-0.3.0.dist-info/RECORD
+10 files, 35146 bytes uncompressed, 8788 bytes compressed:  75.0%
```

## zipnote {}

```diff
@@ -1,103 +1,31 @@
 Filename: nemollm/__init__.py
 Comment: 
 
-Filename: nemollm/api_client.py
+Filename: nemollm/api.py
 Comment: 
 
-Filename: nemollm/configuration.py
+Filename: nemollm/cli.py
 Comment: 
 
-Filename: nemollm/exceptions.py
+Filename: nemollm/error.py
 Comment: 
 
-Filename: nemollm/handy.py
+Filename: nemollm/version.py
 Comment: 
 
-Filename: nemollm/model_utils.py
+Filename: nemollm-0.3.0.dist-info/METADATA
 Comment: 
 
-Filename: nemollm/rest.py
+Filename: nemollm-0.3.0.dist-info/WHEEL
 Comment: 
 
-Filename: nemollm/api/__init__.py
+Filename: nemollm-0.3.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: nemollm/api/default_api.py
+Filename: nemollm-0.3.0.dist-info/top_level.txt
 Comment: 
 
-Filename: nemollm/apis/__init__.py
-Comment: 
-
-Filename: nemollm/model/__init__.py
-Comment: 
-
-Filename: nemollm/model/classification.py
-Comment: 
-
-Filename: nemollm/model/completion_request_body.py
-Comment: 
-
-Filename: nemollm/model/completion_response_body.py
-Comment: 
-
-Filename: nemollm/model/create_customizations_request_body.py
-Comment: 
-
-Filename: nemollm/model/create_customizations_request_body_additional_hyperparameters.py
-Comment: 
-
-Filename: nemollm/model/customization.py
-Comment: 
-
-Filename: nemollm/model/customization_status.py
-Comment: 
-
-Filename: nemollm/model/customizations_list_response_body.py
-Comment: 
-
-Filename: nemollm/model/file.py
-Comment: 
-
-Filename: nemollm/model/file_format.py
-Comment: 
-
-Filename: nemollm/model/list_meta.py
-Comment: 
-
-Filename: nemollm/model/model.py
-Comment: 
-
-Filename: nemollm/model/model_checkpoint.py
-Comment: 
-
-Filename: nemollm/model/models_list_response.py
-Comment: 
-
-Filename: nemollm/model/p_tuning_hyperparms.py
-Comment: 
-
-Filename: nemollm/model/pagination_links.py
-Comment: 
-
-Filename: nemollm/model/status_detail.py
-Comment: 
-
-Filename: nemollm/models/__init__.py
-Comment: 
-
-Filename: nemollm-0.2.0.dist-info/LICENSE.md
-Comment: 
-
-Filename: nemollm-0.2.0.dist-info/METADATA
-Comment: 
-
-Filename: nemollm-0.2.0.dist-info/WHEEL
-Comment: 
-
-Filename: nemollm-0.2.0.dist-info/top_level.txt
-Comment: 
-
-Filename: nemollm-0.2.0.dist-info/RECORD
+Filename: nemollm-0.3.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nemollm/__init__.py

```diff
@@ -1,45 +1 @@
-# flake8: noqa
-
-"""
-    NVIDIA NeMo LLM service
-
-    # Introduction NeMo LLM Service offers state-of-the-art LLMs that were pre-trained on internet-scale text corpora. </br> With NeMo LLM Service API users can invoke the services from within their application code. </br> These models can be flexibly adapted to solve almost any language processing task for your use cases. You can conveniently and quickly try them out, and via an API that you can easily integrate into your applications. </br> Further, the NeMo LLM Service also offers customization capabilities, where the models can be effectively adapted to new tasks, using your own uploaded data. ### Models: The NeMo LLM Service is powered by a set of large language models of varying sizes and capabilities. Generally, larger models extend the capabilities of what smaller models can do with text. - Large models, such as the 530B, are excellent for complex tasks that require a deep understanding of human languages and all their nuances, such as text summarization, creative writing, or chatbot applications. - Medium models, such as the 20B, are faster than the large models. They are sufficiently good for many tasks such as writing emails and factual question answering. - Small models, such as the 5B, are the fastest and can perform many simple language tasks, such as text classification and spelling correction. </br></br> Each model can be used for \"Completion\" or text generation with One/Few-Shot Learning as well as \"Customization\" with your custom dataset. </br></br> ### The NeMo LLM Service API  comprises the following features:   - Text Completion. With one of the available and pre-trained model the LLM service responds to an input prompt by generating an extension to the provided intut text, that is, a completion. This technique can be used for solving multiple NLP tasks using Zero/Few-shot learning techniques.   - Model Customization. With which you can finetune an existing model on your own custom data in the form of prompt+completion pairs. This enhances the modelâ€™s ability to adapt to your use cases by ingesting hundreds to thousands of domain-specific examples.   Please **[submit NVBug](https://nvbugswb.nvidia.com/NvBugs5/SWBug.aspx?bugid=3682160&cmtNo=)** (by cloning and submitting) if discovered any issues.   # noqa: E501
-
-    The version of the OpenAPI document: 1.0.0
-    Contact: nvidia-nemollm@nvidia.com
-    Generated by: https://openapi-generator.tech
-"""
-
-
-__version__ = "0.2.0"
-
-from nemollm.api.default_api import DefaultApi
-
-# import ApiClient
-from nemollm.api_client import ApiClient
-
-# import Configuration
-from nemollm.configuration import Configuration
-
-# import exceptions
-from nemollm.exceptions import (
-    ApiAttributeError,
-    ApiException,
-    ApiKeyError,
-    ApiTypeError,
-    ApiValueError,
-    OpenApiException,
-)
-from nemollm.handy import Connection
-from nemollm.model.completion_request_body import CompletionRequestBody
-from nemollm.model.completion_response_body import CompletionResponseBody
-from nemollm.model.create_customizations_request_body import CreateCustomizationsRequestBody
-from nemollm.model.create_customizations_request_body_additional_hyperparameters import (
-    CreateCustomizationsRequestBodyAdditionalHyperparameters,
-)
-from nemollm.model.customization import Customization
-from nemollm.model.customizations_list_response_body import CustomizationsListResponseBody
-from nemollm.model.file import File
-from nemollm.model.file_format import FileFormat
-from nemollm.model.models_list_response import ModelsListResponse
-from nemollm.model.p_tuning_hyperparms import PTuningHyperparms
+from nemollm.version import __version__
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## Comparing `nemollm/rest.py` & `nemollm/api.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,420 +1,461 @@
-"""
-    NVIDIA NeMo LLM service
-
-    # Introduction NeMo LLM Service offers state-of-the-art LLMs that were pre-trained on internet-scale text corpora. </br> With NeMo LLM Service API users can invoke the services from within their application code. </br> These models can be flexibly adapted to solve almost any language processing task for your use cases. You can conveniently and quickly try them out, and via an API that you can easily integrate into your applications. </br> Further, the NeMo LLM Service also offers customization capabilities, where the models can be effectively adapted to new tasks, using your own uploaded data. ### Models: The NeMo LLM Service is powered by a set of large language models of varying sizes and capabilities. Generally, larger models extend the capabilities of what smaller models can do with text. - Large models, such as the 530B, are excellent for complex tasks that require a deep understanding of human languages and all their nuances, such as text summarization, creative writing, or chatbot applications. - Medium models, such as the 20B, are faster than the large models. They are sufficiently good for many tasks such as writing emails and factual question answering. - Small models, such as the 5B, are the fastest and can perform many simple language tasks, such as text classification and spelling correction. </br></br> Each model can be used for \"Completion\" or text generation with One/Few-Shot Learning as well as \"Customization\" with your custom dataset. </br></br> ### The NeMo LLM Service API  comprises the following features:   - Text Completion. With one of the available and pre-trained model the LLM service responds to an input prompt by generating an extension to the provided intut text, that is, a completion. This technique can be used for solving multiple NLP tasks using Zero/Few-shot learning techniques.   - Model Customization. With which you can finetune an existing model on your own custom data in the form of prompt+completion pairs. This enhances the modelâ€™s ability to adapt to your use cases by ingesting hundreds to thousands of domain-specific examples.   Please **[submit NVBug](https://nvbugswb.nvidia.com/NvBugs5/SWBug.aspx?bugid=3682160&cmtNo=)** (by cloning and submitting) if discovered any issues.   # noqa: E501
-
-    The version of the OpenAPI document: 1.0.0
-    Contact: nvidia-nemollm@nvidia.com
-    Generated by: https://openapi-generator.tech
-"""
-
-
-import io
-import ipaddress
-import json
+import copy
 import logging
-import re
-import ssl
-from urllib.parse import urlencode, urlparse
-from urllib.request import proxy_bypass_environment
-
-import urllib3
-
-from nemollm.exceptions import (
-    ApiException,
-    ApiValueError,
-    ForbiddenException,
-    NotFoundException,
-    ServiceException,
-    UnauthorizedException,
+import os
+import sys
+import threading
+import warnings
+from concurrent.futures import as_completed
+from pathlib import Path
+from typing import List, Optional
+
+if sys.version_info.minor < 8:
+    from typing_extensions import Literal
+else:
+    from typing import Literal
+
+import requests
+from requests_futures.sessions import FuturesSession
+from requests_toolbelt import MultipartEncoder, MultipartEncoderMonitor
+from tqdm import tqdm
+
+from nemollm.error import (
+    ApiKeyNotSetError,
+    AuthorizationError,
+    ClientSideError,
+    IncorrectParamsError,
+    ModelOrCustomizationNotFoundError,
+    ServerSideError,
+    TooManyRequestsError,
 )
+from nemollm.version import __version__
 
-logger = logging.getLogger(__name__)
+MAX_CONNECTION_RETRIES = 3
+REQUESTS_TIMEOUT_SECS = 600
 
+# used by generate_future and generate_multiple only
+MAX_CONCURRENT_HTTP_REQUESTS = 3
 
-class RESTResponse(io.IOBase):
-    def __init__(self, resp):
-        self.urllib3_response = resp
-        self.status = resp.status
-        self.reason = resp.reason
-        self.data = resp.data
-
-    def getheaders(self):
-        """Returns a dictionary of the response headers."""
-        return self.urllib3_response.getheaders()
-
-    def getheader(self, name, default=None):
-        """Returns a given response header."""
-        return self.urllib3_response.getheader(name, default)
-
-
-class RESTClientObject(object):
-    def __init__(self, configuration, pools_size=4, maxsize=None):
-        # urllib3.PoolManager will pass all kw parameters to connectionpool
-        # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/poolmanager.py#L75  # noqa: E501
-        # https://github.com/shazow/urllib3/blob/f9409436f83aeb79fbaf090181cd81b784f1b8ce/urllib3/connectionpool.py#L680  # noqa: E501
-        # maxsize is the number of requests to host that are allowed in parallel  # noqa: E501
-        # Custom SSL certificates and client certificates: http://urllib3.readthedocs.io/en/latest/advanced-usage.html  # noqa: E501
-
-        # cert_reqs
-        if configuration.verify_ssl:
-            cert_reqs = ssl.CERT_REQUIRED
-        else:
-            cert_reqs = ssl.CERT_NONE
+# make sure all sessions on single thread to minimize TCP reconnections
+_thread_context = threading.local()
 
-        addition_pool_args = {}
-        if configuration.assert_hostname is not None:
-            addition_pool_args['assert_hostname'] = configuration.assert_hostname  # noqa: E501
 
-        if configuration.retries is not None:
-            addition_pool_args['retries'] = configuration.retries
-
-        if configuration.socket_options is not None:
-            addition_pool_args['socket_options'] = configuration.socket_options
+def create_session():
+    """
+    Create session so that TCP connection does not reset, reducing handshake latency
+    """
+    session = requests.Session()
+    session.mount(
+        "https://", requests.adapters.HTTPAdapter(max_retries=MAX_CONNECTION_RETRIES),
+    )
+    return session
 
-        if maxsize is None:
-            if configuration.connection_pool_maxsize is not None:
-                maxsize = configuration.connection_pool_maxsize
-            else:
-                maxsize = 4
 
-        # https pool manager
-        if configuration.proxy and not should_bypass_proxies(
-            configuration.host, no_proxy=configuration.no_proxy or ''
-        ):
-            self.pool_manager = urllib3.ProxyManager(
-                num_pools=pools_size,
-                maxsize=maxsize,
-                cert_reqs=cert_reqs,
-                ca_certs=configuration.ssl_ca_cert,
-                cert_file=configuration.cert_file,
-                key_file=configuration.key_file,
-                proxy_url=configuration.proxy,
-                proxy_headers=configuration.proxy_headers,
-                **addition_pool_args
-            )
-        else:
-            self.pool_manager = urllib3.PoolManager(
-                num_pools=pools_size,
-                maxsize=maxsize,
-                cert_reqs=cert_reqs,
-                ca_certs=configuration.ssl_ca_cert,
-                cert_file=configuration.cert_file,
-                key_file=configuration.key_file,
-                **addition_pool_args
-            )
+def create_generate_future_session():
+    """
+    Generate needs to be called with multiple concurrent request, hence create new session for generate_future 
+    """
+    session = requests.Session()
+    session.mount(
+        "https://", requests.adapters.HTTPAdapter(max_retries=MAX_CONNECTION_RETRIES),
+    )
+    future_session = FuturesSession(session=session, max_workers=MAX_CONCURRENT_HTTP_REQUESTS)
+    return future_session
 
-    def request(
-        self,
-        method,
-        url,
-        query_params=None,
-        headers=None,
-        body=None,
-        post_params=None,
-        _preload_content=True,
-        _request_timeout=None,
-    ):
-        """Perform requests.
 
-        :param method: http request method
-        :param url: http request url
-        :param query_params: query parameters in the url
-        :param headers: http request headers
-        :param body: request json body, for `application/json`
-        :param post_params: request post parameters,
-                            `application/x-www-form-urlencoded`
-                            and `multipart/form-data`
-        :param _preload_content: if False, the urllib3.HTTPResponse object will
-                                 be returned without reading/decoding response
-                                 data. Default is True.
-        :param _request_timeout: timeout setting for this request. If one
-                                 number provided, it will be total request
-                                 timeout. It can also be a pair (tuple) of
-                                 (connection, read) timeouts.
-        """
-        method = method.upper()
-        assert method in ['GET', 'HEAD', 'DELETE', 'POST', 'PUT', 'PATCH', 'OPTIONS']
+class NemoLLM:
+    """
+    Make calls to NemoLLM API server using a wrapper around requests library
+    """
 
-        if post_params and body:
-            raise ApiValueError("body parameter cannot be used with post_params parameter.")
+    def __init__(self, api_key=None, org_id=None, api_host=None):
+        self.api_key = api_key if api_key is not None else os.getenv('NGC_API_KEY')
+        if not self.api_key:
+            raise ApiKeyNotSetError(
+                "API KEY is not set. Please pass api_key when instantiating NemoLLM or do 'export NGC_API_KEY=<your_ngc_api_key>'"
+            )
 
-        post_params = post_params or {}
-        headers = headers or {}
+        self.org_id = org_id if org_id is not None else os.getenv('NGC_ORG_ID')
+        if not self.org_id:
+            warnings.warn(
+                "ORG ID is not set. Organization id is required if you are in multiple LLM enabled organizations. org_id can be set when instantiating NemoLLM or set with environment variable 'export NGC_ORG_ID=<your_ngc_org_id>'"
+            )
 
-        timeout = None
-        if _request_timeout:
-            if isinstance(_request_timeout, (int, float)):  # noqa: E501,F821
-                timeout = urllib3.Timeout(total=_request_timeout)
-            elif isinstance(_request_timeout, tuple) and len(_request_timeout) == 2:
-                timeout = urllib3.Timeout(connect=_request_timeout[0], read=_request_timeout[1])
-
-        try:
-            # For `POST`, `PUT`, `PATCH`, `OPTIONS`, `DELETE`
-            if method in ['POST', 'PUT', 'PATCH', 'OPTIONS', 'DELETE']:
-                # Only set a default Content-Type for POST, PUT, PATCH and OPTIONS requests
-                if (method != 'DELETE') and ('Content-Type' not in headers):
-                    headers['Content-Type'] = 'application/json'
-                if query_params:
-                    url += '?' + urlencode(query_params)
-                if ('Content-Type' not in headers) or (re.search('json', headers['Content-Type'], re.IGNORECASE)):
-                    request_body = None
-                    if body is not None:
-                        request_body = json.dumps(body)
-                    r = self.pool_manager.request(
-                        method,
-                        url,
-                        body=request_body,
-                        preload_content=_preload_content,
-                        timeout=timeout,
-                        headers=headers,
-                    )
-                elif headers['Content-Type'] == 'application/x-www-form-urlencoded':  # noqa: E501
-                    r = self.pool_manager.request(
-                        method,
-                        url,
-                        fields=post_params,
-                        encode_multipart=False,
-                        preload_content=_preload_content,
-                        timeout=timeout,
-                        headers=headers,
-                    )
-                elif headers['Content-Type'] == 'multipart/form-data':
-                    # must del headers['Content-Type'], or the correct
-                    # Content-Type which generated by urllib3 will be
-                    # overwritten.
-                    del headers['Content-Type']
-                    r = self.pool_manager.request(
-                        method,
-                        url,
-                        fields=post_params,
-                        encode_multipart=True,
-                        preload_content=_preload_content,
-                        timeout=timeout,
-                        headers=headers,
-                    )
-                # Pass a `string` parameter directly in the body to support
-                # other content types than Json when `body` argument is
-                # provided in serialized form
-                elif isinstance(body, str) or isinstance(body, bytes):
-                    request_body = body
-                    r = self.pool_manager.request(
-                        method,
-                        url,
-                        body=request_body,
-                        preload_content=_preload_content,
-                        timeout=timeout,
-                        headers=headers,
-                    )
-                else:
-                    # Cannot generate the request from given parameters
-                    msg = """Cannot prepare a request message for provided
-                             arguments. Please check that your arguments match
-                             declared content type."""
-                    raise ApiException(status=0, reason=msg)
-            # For `GET`, `HEAD`
-            else:
-                r = self.pool_manager.request(
-                    method,
-                    url,
-                    fields=query_params,
-                    preload_content=_preload_content,
-                    timeout=timeout,
-                    headers=headers,
-                )
-        except urllib3.exceptions.SSLError as e:
-            msg = "{0}\n{1}".format(type(e).__name__, str(e))
-            raise ApiException(status=0, reason=msg)
+        self.api_host = api_host if api_host is not None else "https://api.llm.ngc.nvidia.com/v1"
+        self.headers = {"Authorization": f"Bearer {self.api_key}", 'User-Agent': f'python-client:{__version__}'}
+        if self.org_id:
+            self.headers["Organization-ID"] = self.org_id
+
+    @staticmethod
+    def handle_response(response, stream=False):
+        status_code = response.status_code
+
+        is_binary_content = (
+            'content-disposition' in response.headers
+            and response.headers['content-disposition'].startswith('attachment')
+            or response.headers.get('content-type') == 'application/octet-stream'
+        )
 
-        if _preload_content:
-            r = RESTResponse(r)
+        if stream:
+            decoded_content = 'Streaming content'
+        elif is_binary_content:
+            decoded_content = 'Binary content'
+        else:
+            decoded_content = response.content.decode()
+        # successful
+        if status_code < 400:
 
-            # log response body
-            logger.debug("response body: %s", r.data)
+            logging.info(
+                f"Request succeeded with HTTP Status Code {status_code} {response.reason} Full response: {decoded_content}"
+            )
 
-        if not 200 <= r.status <= 299:
-            if r.status == 401:
-                raise UnauthorizedException(http_resp=r)
+        # client_side errors
+        elif status_code < 500:
+            if status_code == 400:
+                raise IncorrectParamsError(
+                    status_code=status_code, reason=response.reason, decoded_content=decoded_content
+                )
+            elif status_code in [401, 403]:
+                raise AuthorizationError(
+                    status_code=status_code, reason=response.reason, decoded_content=decoded_content
+                )
+            elif status_code == 404:
+                raise ModelOrCustomizationNotFoundError(
+                    status_code=status_code, reason=response.reason, decoded_content=decoded_content
+                )
+            elif status_code == 429:
+                raise TooManyRequestsError(
+                    status_code=status_code, reason=response.reason, decoded_content=decoded_content
+                )
+            else:
+                raise ClientSideError(status_code=status_code, reason=response.reason, decoded_content=decoded_content)
 
-            if r.status == 403:
-                raise ForbiddenException(http_resp=r)
+        # server side errors
+        else:
+            raise ServerSideError(status_code=status_code, reason=response.reason, decoded_content=decoded_content)
 
-            if r.status == 404:
-                raise NotFoundException(http_resp=r)
+    def list_models(self):
+        url = f"{self.api_host}/models"
 
-            if 500 <= r.status <= 599:
-                raise ServiceException(http_resp=r)
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
 
-            raise ApiException(http_resp=r)
+        response = _thread_context.session.get(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
 
-        return r
+    def create_customization(
+        self,
+        model: str,
+        training_dataset_file_id: str,
+        name: str,
+        validation_dataset_file_id: Optional[str] = None,
+        batch_size: Optional[int] = None,
+        epochs: Optional[int] = None,
+        learning_rate: Optional[float] = None,
+        num_virtual_tokens: Optional[int] = None,
+    ):
+        url = f"{self.api_host}/models/{model}/customizations"
 
-    def GET(self, url, headers=None, query_params=None, _preload_content=True, _request_timeout=None):
-        return self.request(
-            "GET",
-            url,
-            headers=headers,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            query_params=query_params,
-        )
+        data = {"name": name, "training_dataset_file_id": training_dataset_file_id}
 
-    def HEAD(self, url, headers=None, query_params=None, _preload_content=True, _request_timeout=None):
-        return self.request(
-            "HEAD",
-            url,
-            headers=headers,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            query_params=query_params,
-        )
+        # do not set explicit default params for training,
+        # instead, set param only when set by user, otherwise depending on API defaults
+        # this reduces need to sync with API when API defaults change
+        if validation_dataset_file_id is not None:
+            data["validation_dataset_file_id"] = validation_dataset_file_id
+        if batch_size is not None:
+            data["batch_size"] = batch_size
+        if epochs is not None:
+            data["epochs"] = epochs
+        if epochs is not None:
+            data["learning_rate"] = learning_rate
+        if num_virtual_tokens is not None:
+            data["additional_hyperparameters"] = {}
+            data["additional_hyperparameters"]["num_virtual_tokens"] = num_virtual_tokens
+
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.post(url, headers=self.headers, json=data, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
 
-    def OPTIONS(
+    def generate(
         self,
-        url,
-        headers=None,
-        query_params=None,
-        post_params=None,
-        body=None,
-        _preload_content=True,
-        _request_timeout=None,
+        model: str,
+        prompt: str,
+        customization_id: str = None,
+        return_type: Literal['json', 'text', 'stream', 'future', 'async'] = 'json',
+        tokens_to_generate: Optional[int] = None,
+        logprobs: Optional[bool] = None,
+        temperature: Optional[float] = None,
+        top_p: Optional[float] = None,
+        top_k: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        random_seed: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        beam_search_diversity_rate: Optional[float] = None,
+        beam_width: Optional[int] = None,
+        length_penalty: Optional[float] = None,
+        disable_logging: bool = False,
     ):
-        return self.request(
-            "OPTIONS",
-            url,
-            headers=headers,
-            query_params=query_params,
-            post_params=post_params,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            body=body,
-        )
+        """
+        Beyond API configurations, 
+        
+        Allow users to choose the type of response they would want using return_type
+
+        1. json: JSON representation of the API response content
+        2. text: STR using only the 'text' field of the API response json
+        3. stream: ITER of lines with each line being of json structure containing one token that can be used with json.loads(line) 
+        4. future: FUTURE object based on https://github.com/ross/requests-futures and concurrent.futures.Future. To get json response, use future.result()
+        5. async: alias to future
 
-    def DELETE(self, url, headers=None, query_params=None, body=None, _preload_content=True, _request_timeout=None):
-        return self.request(
-            "DELETE",
-            url,
-            headers=headers,
-            query_params=query_params,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            body=body,
-        )
+        """
+        if customization_id is None:
+            url = f"{self.api_host}/models/{model}/completions"
+        else:
+            url = f"{self.api_host}/models/{model}/customizations/{customization_id}/completions"
 
-    def POST(
-        self,
-        url,
-        headers=None,
-        query_params=None,
-        post_params=None,
-        body=None,
-        _preload_content=True,
-        _request_timeout=None,
-    ):
-        return self.request(
-            "POST",
-            url,
-            headers=headers,
-            query_params=query_params,
-            post_params=post_params,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            body=body,
-        )
+        data = {"prompt": prompt}
 
-    def PUT(
-        self,
-        url,
-        headers=None,
-        query_params=None,
-        post_params=None,
-        body=None,
-        _preload_content=True,
-        _request_timeout=None,
-    ):
-        return self.request(
-            "PUT",
-            url,
-            headers=headers,
-            query_params=query_params,
-            post_params=post_params,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            body=body,
-        )
+        # do not set explicit default params for training,
+        # instead, set param only when set by user, otherwise depending on API defaults
+        # this reduces need to sync with API when API defaults change
+
+        if tokens_to_generate is not None:
+            data["tokens_to_generate"] = tokens_to_generate
+        if logprobs is not None:
+            data["logprobs"] = logprobs
+        if temperature is not None:
+            data["temperature"] = temperature
+        if top_p is not None:
+            data["top_p"] = top_p
+        if top_k is not None:
+            data["top_k"] = top_k
+        if stop is not None:
+            data["stop"] = stop
+        if random_seed is not None:
+            data["random_seed"] = random_seed
+        if repetition_penalty is not None:
+            data["repetition_penalty"] = repetition_penalty
+        if beam_search_diversity_rate is not None:
+            data["beam_search_diversity_rate"] = beam_search_diversity_rate
+        if beam_width is not None:
+            data["beam_width"] = beam_width
+        if length_penalty is not None:
+            data["length_penalty"] = length_penalty
+
+        if return_type not in ['async', 'future'] and not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+
+        if return_type in ['async', 'future'] and not hasattr(_thread_context, 'generate_future_session'):
+            _thread_context.generate_future_session = create_generate_future_session()
+
+        headers = copy.copy(self.headers)
+        if disable_logging:
+            headers["x-disable-logging"] = 'true'
+        if return_type == 'stream':
+            headers["x-stream"] = 'true'
+
+        if return_type in ['async', 'future']:
+            response = _thread_context.generate_future_session.post(
+                url, headers=headers, json=data, timeout=REQUESTS_TIMEOUT_SECS
+            )
+        else:
+            response = _thread_context.session.post(
+                url, headers=headers, json=data, timeout=REQUESTS_TIMEOUT_SECS, stream=(return_type == 'stream')
+            )
 
-    def PATCH(
-        self,
-        url,
-        headers=None,
-        query_params=None,
-        post_params=None,
-        body=None,
-        _preload_content=True,
-        _request_timeout=None,
-    ):
-        return self.request(
-            "PATCH",
-            url,
-            headers=headers,
-            query_params=query_params,
-            post_params=post_params,
-            _preload_content=_preload_content,
-            _request_timeout=_request_timeout,
-            body=body,
-        )
+        if return_type == 'stream':
+            NemoLLM.handle_response(response, stream=True)
+            return response.iter_lines()
+        elif return_type in ['async', 'future']:
+            return response
+        elif return_type == 'text':
+            NemoLLM.handle_response(response)
+            return NemoLLM.post_process_generate_response(response, True)
+        else:
+            NemoLLM.handle_response(response)
+            return NemoLLM.post_process_generate_response(response, False)
 
+    @staticmethod
+    def post_process_generate_response(response, return_text_completion_only):
+        if response.ok:
+            response_json = response.json()
+        else:
+            response_json = {"status": "fail", "msg": str(response.content.decode())}
 
-# end of class RESTClientObject
+        if return_text_completion_only:
+            return response_json['text'] if 'text' in response_json else ''
+        return response_json
 
+    def generate_multiple(
+        self,
+        model: str,
+        prompts: List[str],
+        customization_id: str = None,
+        return_type: Literal['json', 'text'] = 'json',
+        tokens_to_generate: Optional[bool] = None,
+        logprobs: Optional[bool] = None,
+        temperature: Optional[float] = None,
+        top_p: Optional[float] = None,
+        top_k: Optional[int] = None,
+        stop: Optional[List[str]] = None,
+        random_seed: Optional[int] = None,
+        repetition_penalty: Optional[float] = None,
+        beam_search_diversity_rate: Optional[float] = None,
+        beam_width: Optional[int] = None,
+        length_penalty: Optional[int] = None,
+        disable_logging: bool = False,
+    ):
+        futures = []
+        for i, prompt in enumerate(prompts):
+            future = self.generate(
+                model,
+                prompt,
+                return_type='future',
+                customization_id=customization_id,
+                tokens_to_generate=tokens_to_generate,
+                logprobs=logprobs,
+                temperature=temperature,
+                top_p=top_p,
+                top_k=top_k,
+                stop=stop,
+                random_seed=random_seed,
+                repetition_penalty=repetition_penalty,
+                beam_search_diversity_rate=beam_search_diversity_rate,
+                beam_width=beam_width,
+                length_penalty=length_penalty,
+                disable_logging=disable_logging,
+            )
+            # ensure order of responses follows order of prompts
+            future.i = i
+            futures.append(future)
+
+        responses_json = [None] * len(prompts)
+        for future in as_completed(futures):
+            response = future.result()
+            NemoLLM.handle_response(response)
+            responses_json[future.i] = NemoLLM.post_process_generate_response(response, return_type == "text")
+        return responses_json
 
-def is_ipv4(target):
-    """ Test if IPv4 address or not
-    """
-    try:
-        chk = ipaddress.IPv4Address(target)
-        return True
-    except ipaddress.AddressValueError:
-        return False
+    def upload(self, filepath, progress_bar: bool = False):
+        """
+        Allows monitoring of upload progress via tqdm for large files
 
+        Requires requests toolbelt
+        """
+        url = f"{self.api_host}/files"
 
-def in_ipv4net(target, net):
-    """ Test if target belongs to given IPv4 network
-    """
-    try:
-        nw = ipaddress.IPv4Network(net)
-        ip = ipaddress.IPv4Address(target)
-        if ip in nw:
-            return True
-        return False
-    except ipaddress.AddressValueError:
-        return False
-    except ipaddress.NetmaskValueError:
-        return False
+        path = Path(filepath)
+        total_size = path.stat().st_size
+        filename = path.name
+        fields = {}
+
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+
+        with tqdm(
+            desc=filename, total=total_size, unit="B", unit_scale=True, unit_divisor=1024, disable=not progress_bar
+        ) as bar:
+            with open(filepath, "rb") as f:
+                fields["file"] = (filepath, f)
+                fields["format"] = "jsonl"
+                e = MultipartEncoder(fields=fields)
+                m = MultipartEncoderMonitor(e, lambda monitor: bar.update(monitor.bytes_read - bar.n))
+                headers = copy.copy(self.headers)
+                headers["Content-Type"] = m.content_type
+                response = _thread_context.session.post(url, data=m, headers=headers, timeout=REQUESTS_TIMEOUT_SECS)
+                NemoLLM.handle_response(response)
+        return response.json()
 
+    def list_customizations(
+        self,
+        page: Optional[int] = None,
+        page_size: Optional[int] = None,
+        shared: Optional[str] = None,
+        sort_by: Optional[str] = None,
+        order: Optional[str] = None,
+    ):
+        url = f"{self.api_host}/customizations"
 
-def should_bypass_proxies(url, no_proxy=None):
-    """ Yet another requests.should_bypass_proxies
-    Test if proxies should not be used for a particular url.
-    """
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
 
-    parsed = urlparse(url)
+        params = {}
 
-    # special cases
-    if parsed.hostname in [None, '']:
-        return True
-
-    # special cases
-    if no_proxy in [None, '']:
-        return False
-    if no_proxy == '*':
-        return True
-
-    no_proxy = no_proxy.lower().replace(' ', '')
-    entries = (host for host in no_proxy.split(',') if host)
-
-    if is_ipv4(parsed.hostname):
-        for item in entries:
-            if in_ipv4net(parsed.hostname, item):
-                return True
-    return proxy_bypass_environment(parsed.hostname, {'no': no_proxy})
+        if page is not None:
+            params["page"] = page
+        if page_size is not None:
+            params["page_size"] = page_size
+        if shared is not None:
+            params["shared"] = shared
+        if sort_by is not None:
+            params["sort_by"] = sort_by
+        if order is not None:
+            params["order"] = order
+        print(params)
+        response = _thread_context.session.get(url, headers=self.headers, params=params, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
+
+    def list_files(self):
+        url = f"{self.api_host}/files"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.get(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
+
+    def delete_file(self, file_id):
+        url = f"{self.api_host}/files/{file_id}"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.delete(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return f"File {file_id} has{'' if response.ok else ' not'} been deleted"
+
+    def get_info_customization(self, model, customization_id):
+        url = f"{self.api_host}/models/{model}/customizations/{customization_id}"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.get(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
+
+    def delete_customization(self, model, customization_id):
+        url = f"{self.api_host}/models/{model}/customizations/{customization_id}"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.delete(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return f"Customization {customization_id} for model {model} has{'' if response.ok else ' not'} been deleted"
+
+    def download_customization(self, model, customization_id, save_filename):
+        url = f"{self.api_host}/models/{model}/customizations/{customization_id}/fetch"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.get(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+
+        NemoLLM.handle_response(response)
+
+        if os.path.dirname(save_filename):
+            os.makedirs(os.path.dirname(save_filename), exist_ok=True)
+        with open(save_filename, "wb") as fw:
+            fw.write(response.content)
+        return f"Customization {customization_id} for model {model} has{'' if response.ok else ' not'} been saved at {save_filename}"
+
+    def get_info_file(self, file_id):
+        url = f"{self.api_host}/files/{file_id}"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.get(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
+
+    def get_customization_training_metrics(self, model, customization_id):
+        url = f"https://api.llm.ngc.nvidia.com/v1/models/{model}/customizations/{customization_id}/metrics"
+        if not hasattr(_thread_context, 'session'):
+            _thread_context.session = create_session()
+        response = _thread_context.session.get(url, headers=self.headers, timeout=REQUESTS_TIMEOUT_SECS)
+        NemoLLM.handle_response(response)
+        return response.json()
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

