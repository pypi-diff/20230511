# Comparing `tmp/rsatoolbox-0.1.3.dev51.tar.gz` & `tmp/rsatoolbox-0.1.3.dev56.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "rsatoolbox-0.1.3.dev51.tar", last modified: Thu May 11 16:30:45 2023, max compression
+gzip compressed data, was "rsatoolbox-0.1.3.dev56.tar", last modified: Thu May 11 16:36:27 2023, max compression
```

## Comparing `rsatoolbox-0.1.3.dev51.tar` & `rsatoolbox-0.1.3.dev56.tar`

### file list

```diff
@@ -1,91 +1,91 @@
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.229502 rsatoolbox-0.1.3.dev51/
--rw-rw-rw-   0        0        0       28 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/.bandit
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.198306 rsatoolbox-0.1.3.dev51/.conda/
--rw-rw-rw-   0        0        0     1285 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/.conda/meta.yaml
--rw-rw-rw-   0        0        0     1957 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/.gitignore
--rw-rw-rw-   0        0        0    18767 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/.pylintrc
--rw-rw-rw-   0        0        0      531 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/.readthedocs.yml
--rw-rw-rw-   0        0        0      469 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/AUTHORS
--rw-rw-rw-   0        0        0     3708 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/CONTRIBUTING.md
--rw-rw-rw-   0        0        0     1096 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/LICENSE
--rw-rw-rw-   0        0        0      191 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/MANIFEST.in
--rw-rw-rw-   0        0        0     3594 2023-05-11 16:30:45.229502 rsatoolbox-0.1.3.dev51/PKG-INFO
--rw-rw-rw-   0        0        0     1355 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/README.md
--rw-rw-rw-   0        0        0      210 2023-05-11 16:26:42.000000 rsatoolbox-0.1.3.dev51/codecov.yml
--rw-rw-rw-   0        0        0     1686 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/pyproject.toml
--rw-rw-rw-   0        0        0      139 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/requirements.txt
--rw-rw-rw-   0        0        0      101 2023-05-11 16:30:45.229502 rsatoolbox-0.1.3.dev51/setup.cfg
--rw-rw-rw-   0        0        0      540 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/setup.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.182172 rsatoolbox-0.1.3.dev51/src/
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.198306 rsatoolbox-0.1.3.dev51/src/rsatoolbox/
--rw-rw-rw-   0        0        0      283 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/__init__.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.198306 rsatoolbox-0.1.3.dev51/src/rsatoolbox/cengine/
--rw-rw-rw-   0        0        0       37 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/cengine/__init__.pxd
--rw-rw-rw-   0        0        0       46 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/cengine/__init__.py
--rw-rw-rw-   0        0        0    13274 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/cengine/similarity.pyx
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.198306 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/
--rw-rw-rw-   0        0        0      513 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/__init__.py
--rw-rw-rw-   0        0        0     7385 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/base.py
--rw-rw-rw-   0        0        0     1271 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/computations.py
--rw-rw-rw-   0        0        0    34968 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/dataset.py
--rw-rw-rw-   0        0        0      755 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/neurodataset.py
--rw-rw-rw-   0        0        0    16829 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/noise.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.213818 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/
--rw-rw-rw-   0        0        0     1079 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/__init__.py
--rw-rw-rw-   0        0        0     6924 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/boot_testset.py
--rw-rw-rw-   0        0        0     4493 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/bootstrap.py
--rw-rw-rw-   0        0        0    16591 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/crossvalsets.py
--rw-rw-rw-   0        0        0    36349 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/evaluate.py
--rw-rw-rw-   0        0        0     2979 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/noise_ceiling.py
--rw-rw-rw-   0        0        0    12831 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/result.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.213818 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/
--rw-rw-rw-   0        0        0        0 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/__init__.py
--rw-rw-rw-   0        0        0     3829 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/hdf5.py
--rw-rw-rw-   0        0        0     7666 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/meadows.py
--rw-rw-rw-   0        0        0     1542 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/pandas.py
--rw-rw-rw-   0        0        0     6800 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/petnames.py
--rw-rw-rw-   0        0        0     1176 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/pkl.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.213818 rsatoolbox-0.1.3.dev51/src/rsatoolbox/model/
--rw-rw-rw-   0        0        0      350 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/model/__init__.py
--rw-rw-rw-   0        0        0    18280 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/model/fitter.py
--rw-rw-rw-   0        0        0    11812 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/model/model.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.213818 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/
--rw-rw-rw-   0        0        0      960 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/__init__.py
--rw-rw-rw-   0        0        0    21896 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/calc.py
--rw-rw-rw-   0        0        0     8128 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/calc_unbalanced.py
--rw-rw-rw-   0        0        0     7632 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/combine.py
--rw-rw-rw-   0        0        0    21860 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/compare.py
--rw-rw-rw-   0        0        0     1448 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/pairs.py
--rw-rw-rw-   0        0        0    25723 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/rdms.py
--rw-rw-rw-   0        0        0     4185 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/transform.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.213818 rsatoolbox-0.1.3.dev51/src/rsatoolbox/simulation/
--rw-rw-rw-   0        0        0      164 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/simulation/__init__.py
--rw-rw-rw-   0        0        0     8340 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/simulation/sim.py
--rw-rw-rw-   0        0        0     3744 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/test.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.229502 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/
--rw-rw-rw-   0        0        0       22 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/__init__.py
--rw-rw-rw-   0        0        0     1250 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/data_utils.py
--rw-rw-rw-   0        0        0     6365 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/descriptor_utils.py
--rw-rw-rw-   0        0        0      515 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/file_io.py
--rw-rw-rw-   0        0        0    26370 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/inference_util.py
--rw-rw-rw-   0        0        0     8619 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/matrix.py
--rw-rw-rw-   0        0        0     4523 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/pooling.py
--rw-rw-rw-   0        0        0     7234 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/rdm_utils.py
--rw-rw-rw-   0        0        0     7763 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/searchlight.py
--rw-rw-rw-   0        0        0    20006 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/vis_utils.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.229502 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/
--rw-rw-rw-   0        0        0      387 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/__init__.py
--rw-rw-rw-   0        0        0     3197 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/colors.py
--rw-rw-rw-   0        0        0    21019 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/icon.py
--rw-rw-rw-   0        0        0    39490 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/model_map.py
--rw-rw-rw-   0        0        0    41648 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/model_plot.py
--rw-rw-rw-   0        0        0      242 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/rdm.mplstyle
--rw-rw-rw-   0        0        0    24404 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/rdm_comparison.py
--rw-rw-rw-   0        0        0    22172 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/rdm_plot.py
--rw-rw-rw-   0        0        0     8048 2023-05-11 16:26:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/scatter_plot.py
-drwxrwxrwx   0        0        0        0 2023-05-11 16:30:45.198306 rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/
--rw-rw-rw-   0        0        0     3594 2023-05-11 16:30:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/PKG-INFO
--rw-rw-rw-   0        0        0     2228 2023-05-11 16:30:45.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/SOURCES.txt
--rw-rw-rw-   0        0        0        1 2023-05-11 16:30:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/dependency_links.txt
--rw-rw-rw-   0        0        0      134 2023-05-11 16:30:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/requires.txt
--rw-rw-rw-   0        0        0       11 2023-05-11 16:30:43.000000 rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/top_level.txt
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.811930 rsatoolbox-0.1.3.dev56/
+-rw-r--r--   0 runner     (501) staff       (20)       26 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/.bandit
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.770516 rsatoolbox-0.1.3.dev56/.conda/
+-rw-r--r--   0 runner     (501) staff       (20)     1224 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/.conda/meta.yaml
+-rw-r--r--   0 runner     (501) staff       (20)     1810 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/.gitignore
+-rw-r--r--   0 runner     (501) staff       (20)    18173 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/.pylintrc
+-rw-r--r--   0 runner     (501) staff       (20)      507 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/.readthedocs.yml
+-rw-r--r--   0 runner     (501) staff       (20)      457 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/AUTHORS
+-rw-r--r--   0 runner     (501) staff       (20)     3633 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/CONTRIBUTING.md
+-rw-r--r--   0 runner     (501) staff       (20)     1075 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/LICENSE
+-rw-r--r--   0 runner     (501) staff       (20)      183 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/MANIFEST.in
+-rw-r--r--   0 runner     (501) staff       (20)     3511 2023-05-11 16:36:27.812415 rsatoolbox-0.1.3.dev56/PKG-INFO
+-rw-r--r--   0 runner     (501) staff       (20)     1319 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/README.md
+-rw-r--r--   0 runner     (501) staff       (20)      196 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/codecov.yml
+-rw-r--r--   0 runner     (501) staff       (20)     1625 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/pyproject.toml
+-rw-r--r--   0 runner     (501) staff       (20)      130 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/requirements.txt
+-rw-r--r--   0 runner     (501) staff       (20)       92 2023-05-11 16:36:27.813198 rsatoolbox-0.1.3.dev56/setup.cfg
+-rw-r--r--   0 runner     (501) staff       (20)      523 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/setup.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.758986 rsatoolbox-0.1.3.dev56/src/
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.771644 rsatoolbox-0.1.3.dev56/src/rsatoolbox/
+-rw-r--r--   0 runner     (501) staff       (20)      270 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/__init__.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.775703 rsatoolbox-0.1.3.dev56/src/rsatoolbox/cengine/
+-rw-r--r--   0 runner     (501) staff       (20)       36 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/cengine/__init__.pxd
+-rw-r--r--   0 runner     (501) staff       (20)       45 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/cengine/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)    12945 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/cengine/similarity.pyx
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.780748 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/
+-rw-r--r--   0 runner     (501) staff       (20)      500 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     7184 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/base.py
+-rw-r--r--   0 runner     (501) staff       (20)     1228 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/computations.py
+-rw-r--r--   0 runner     (501) staff       (20)    34102 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/dataset.py
+-rw-r--r--   0 runner     (501) staff       (20)      727 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/neurodataset.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    16393 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/noise.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.788903 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/
+-rw-r--r--   0 runner     (501) staff       (20)     1054 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/__init__.py
+-rwxr-xr-x   0 runner     (501) staff       (20)     6764 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/boot_testset.py
+-rw-r--r--   0 runner     (501) staff       (20)     4374 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/bootstrap.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    16205 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/crossvalsets.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    36075 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/evaluate.py
+-rwxr-xr-x   0 runner     (501) staff       (20)     2901 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/noise_ceiling.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    12608 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/result.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.792126 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/
+-rw-r--r--   0 runner     (501) staff       (20)        0 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/__init__.py
+-rwxr-xr-x   0 runner     (501) staff       (20)     3711 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/hdf5.py
+-rw-r--r--   0 runner     (501) staff       (20)     7445 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/meadows.py
+-rw-r--r--   0 runner     (501) staff       (20)     1500 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/pandas.py
+-rw-r--r--   0 runner     (501) staff       (20)     6341 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/petnames.py
+-rwxr-xr-x   0 runner     (501) staff       (20)     1133 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/pkl.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.793459 rsatoolbox-0.1.3.dev56/src/rsatoolbox/model/
+-rw-r--r--   0 runner     (501) staff       (20)      341 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/model/__init__.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    17799 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/model/fitter.py
+-rw-r--r--   0 runner     (501) staff       (20)    11445 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/model/model.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.798877 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/
+-rw-r--r--   0 runner     (501) staff       (20)      935 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)    21356 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/calc.py
+-rwxr-xr-x   0 runner     (501) staff       (20)     7925 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/calc_unbalanced.py
+-rw-r--r--   0 runner     (501) staff       (20)     7427 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/combine.py
+-rw-r--r--   0 runner     (501) staff       (20)    21236 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/compare.py
+-rw-r--r--   0 runner     (501) staff       (20)     1408 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/pairs.py
+-rw-r--r--   0 runner     (501) staff       (20)    25060 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/rdms.py
+-rw-r--r--   0 runner     (501) staff       (20)     4073 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/transform.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.800276 rsatoolbox-0.1.3.dev56/src/rsatoolbox/simulation/
+-rw-r--r--   0 runner     (501) staff       (20)      160 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/simulation/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     8136 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/simulation/sim.py
+-rw-r--r--   0 runner     (501) staff       (20)     3642 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/test.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.805575 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/
+-rw-r--r--   0 runner     (501) staff       (20)       21 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     1207 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/data_utils.py
+-rw-r--r--   0 runner     (501) staff       (20)     6147 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/descriptor_utils.py
+-rwxr-xr-x   0 runner     (501) staff       (20)      495 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/file_io.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    28146 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/inference_util.py
+-rw-r--r--   0 runner     (501) staff       (20)     8353 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/matrix.py
+-rwxr-xr-x   0 runner     (501) staff       (20)     4402 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/pooling.py
+-rw-r--r--   0 runner     (501) staff       (20)     6996 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/rdm_utils.py
+-rw-r--r--   0 runner     (501) staff       (20)     7554 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/searchlight.py
+-rw-r--r--   0 runner     (501) staff       (20)    19491 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/vis_utils.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.811472 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/
+-rw-r--r--   0 runner     (501) staff       (20)      380 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/__init__.py
+-rw-r--r--   0 runner     (501) staff       (20)     3089 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/colors.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    20398 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/icon.py
+-rw-r--r--   0 runner     (501) staff       (20)    38651 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/model_map.py
+-rw-r--r--   0 runner     (501) staff       (20)    41648 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/model_plot.py
+-rw-r--r--   0 runner     (501) staff       (20)      230 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/rdm.mplstyle
+-rw-r--r--   0 runner     (501) staff       (20)    23827 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/rdm_comparison.py
+-rwxr-xr-x   0 runner     (501) staff       (20)    21628 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/rdm_plot.py
+-rw-r--r--   0 runner     (501) staff       (20)     7831 2023-05-11 16:35:49.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/scatter_plot.py
+drwxr-xr-x   0 runner     (501) staff       (20)        0 2023-05-11 16:36:27.774283 rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/
+-rw-r--r--   0 runner     (501) staff       (20)     3511 2023-05-11 16:36:27.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/PKG-INFO
+-rw-r--r--   0 runner     (501) staff       (20)     2190 2023-05-11 16:36:27.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/SOURCES.txt
+-rw-r--r--   0 runner     (501) staff       (20)        1 2023-05-11 16:36:27.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/dependency_links.txt
+-rw-r--r--   0 runner     (501) staff       (20)      134 2023-05-11 16:36:27.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/requires.txt
+-rw-r--r--   0 runner     (501) staff       (20)       11 2023-05-11 16:36:27.000000 rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/top_level.txt
```

### Comparing `rsatoolbox-0.1.3.dev51/.conda/meta.yaml` & `rsatoolbox-0.1.3.dev56/.conda/meta.yaml`

 * *Ordering differences only*

 * *Files 23% similar despite different names*

```diff
@@ -1,61 +1,61 @@
-{% set name = "rsatoolbox" %}
-{% set version = "0.1.2" %}
-
-package:
-  name: {{ name|lower }}
-  version: {{ version }}
-
-source:
-  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/rsatoolbox-{{ version }}.tar.gz
-  sha256: fee6e0134c345f4d7e7f2d7bdde20b13724e9d0d32f89e8b22b7fc17ed21ef9a
-
-build:
-  skip: true  # [py<37 or py>310 or py2k]
-  script: {{ PYTHON }} -m pip install . -vv
-  number: 0
-
-requirements:
-  build:
-    - {{ compiler('c') }}
-  host:
-    - python
-    - cython # >=3.0.0a11
-    - numpy >=1.21.2
-    - scipy
-    - setuptools
-    - setuptools-scm
-    - pip
-  run:
-    - python
-    - {{ pin_compatible('numpy') }}
-    - {{ pin_compatible('scipy') }}
-    - pandas
-    - scikit-learn
-    - scikit-image
-    - matplotlib-base
-    - h5py
-    - joblib
-    - tqdm
-    - coverage
-
-
-test:
-  imports:
-    - rsatoolbox
-  commands:
-    - pip check
-    - python -m unittest -v rsatoolbox.test
-  requires:
-    - pip
-
-about:
-  home: https://github.com/rsagroup/rsatoolbox
-  summary: Representational Similarity Analysis (RSA) in Python
-  license: MIT
-  license_file: LICENSE
-  doc_url: https://rsatoolbox.readthedocs.io/
-  dev_url: https://github.com/rsagroup/rsatoolbox
-
-extra:
-  recipe-maintainers:
-    - ilogue
+{% set name = "rsatoolbox" %}
+{% set version = "0.1.2" %}
+
+package:
+  name: {{ name|lower }}
+  version: {{ version }}
+
+source:
+  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/rsatoolbox-{{ version }}.tar.gz
+  sha256: fee6e0134c345f4d7e7f2d7bdde20b13724e9d0d32f89e8b22b7fc17ed21ef9a
+
+build:
+  skip: true  # [py<37 or py>310 or py2k]
+  script: {{ PYTHON }} -m pip install . -vv
+  number: 0
+
+requirements:
+  build:
+    - {{ compiler('c') }}
+  host:
+    - python
+    - cython # >=3.0.0a11
+    - numpy >=1.21.2
+    - scipy
+    - setuptools
+    - setuptools-scm
+    - pip
+  run:
+    - python
+    - {{ pin_compatible('numpy') }}
+    - {{ pin_compatible('scipy') }}
+    - pandas
+    - scikit-learn
+    - scikit-image
+    - matplotlib-base
+    - h5py
+    - joblib
+    - tqdm
+    - coverage
+
+
+test:
+  imports:
+    - rsatoolbox
+  commands:
+    - pip check
+    - python -m unittest -v rsatoolbox.test
+  requires:
+    - pip
+
+about:
+  home: https://github.com/rsagroup/rsatoolbox
+  summary: Representational Similarity Analysis (RSA) in Python
+  license: MIT
+  license_file: LICENSE
+  doc_url: https://rsatoolbox.readthedocs.io/
+  dev_url: https://github.com/rsagroup/rsatoolbox
+
+extra:
+  recipe-maintainers:
+    - ilogue
```

### Comparing `rsatoolbox-0.1.3.dev51/.pylintrc` & `rsatoolbox-0.1.3.dev56/.pylintrc`

 * *Files 18% similar despite different names*

```diff
@@ -1,1173 +1,1136 @@
-00000000: 5b4d 4153 5445 525d 0d0a 0d0a 2320 4120  [MASTER]....# A 
-00000010: 636f 6d6d 612d 7365 7061 7261 7465 6420  comma-separated 
-00000020: 6c69 7374 206f 6620 7061 636b 6167 6520  list of package 
-00000030: 6f72 206d 6f64 756c 6520 6e61 6d65 7320  or module names 
-00000040: 6672 6f6d 2077 6865 7265 2043 2065 7874  from where C ext
-00000050: 656e 7369 6f6e 7320 6d61 790d 0a23 2062  ensions may..# b
-00000060: 6520 6c6f 6164 6564 2e20 4578 7465 6e73  e loaded. Extens
-00000070: 696f 6e73 2061 7265 206c 6f61 6469 6e67  ions are loading
-00000080: 2069 6e74 6f20 7468 6520 6163 7469 7665   into the active
-00000090: 2050 7974 686f 6e20 696e 7465 7270 7265   Python interpre
-000000a0: 7465 7220 616e 6420 6d61 790d 0a23 2072  ter and may..# r
-000000b0: 756e 2061 7262 6974 7261 7279 2063 6f64  un arbitrary cod
-000000c0: 652e 0d0a 6578 7465 6e73 696f 6e2d 706b  e...extension-pk
-000000d0: 672d 7768 6974 656c 6973 743d 0d0a 0d0a  g-whitelist=....
-000000e0: 2320 5370 6563 6966 7920 6120 7363 6f72  # Specify a scor
-000000f0: 6520 7468 7265 7368 6f6c 6420 746f 2062  e threshold to b
-00000100: 6520 6578 6365 6564 6564 2062 6566 6f72  e exceeded befor
-00000110: 6520 7072 6f67 7261 6d20 6578 6974 7320  e program exits 
-00000120: 7769 7468 2065 7272 6f72 2e0d 0a66 6169  with error...fai
-00000130: 6c2d 756e 6465 723d 3130 2e30 0d0a 0d0a  l-under=10.0....
-00000140: 2320 4164 6420 6669 6c65 7320 6f72 2064  # Add files or d
-00000150: 6972 6563 746f 7269 6573 2074 6f20 7468  irectories to th
-00000160: 6520 626c 6163 6b6c 6973 742e 2054 6865  e blacklist. The
-00000170: 7920 7368 6f75 6c64 2062 6520 6261 7365  y should be base
-00000180: 206e 616d 6573 2c20 6e6f 740d 0a23 2070   names, not..# p
-00000190: 6174 6873 2e0d 0a69 676e 6f72 653d 4356  aths...ignore=CV
-000001a0: 530d 0a0d 0a23 2041 6464 2066 696c 6573  S....# Add files
-000001b0: 206f 7220 6469 7265 6374 6f72 6965 7320   or directories 
-000001c0: 6d61 7463 6869 6e67 2074 6865 2072 6567  matching the reg
-000001d0: 6578 2070 6174 7465 726e 7320 746f 2074  ex patterns to t
-000001e0: 6865 2062 6c61 636b 6c69 7374 2e20 5468  he blacklist. Th
-000001f0: 650d 0a23 2072 6567 6578 206d 6174 6368  e..# regex match
-00000200: 6573 2061 6761 696e 7374 2062 6173 6520  es against base 
-00000210: 6e61 6d65 732c 206e 6f74 2070 6174 6873  names, not paths
-00000220: 2e0d 0a69 676e 6f72 652d 7061 7474 6572  ...ignore-patter
-00000230: 6e73 3d0d 0a0d 0a23 2050 7974 686f 6e20  ns=....# Python 
-00000240: 636f 6465 2074 6f20 6578 6563 7574 652c  code to execute,
-00000250: 2075 7375 616c 6c79 2066 6f72 2073 7973   usually for sys
-00000260: 2e70 6174 6820 6d61 6e69 7075 6c61 7469  .path manipulati
-00000270: 6f6e 2073 7563 6820 6173 0d0a 2320 7079  on such as..# py
-00000280: 6774 6b2e 7265 7175 6972 6528 292e 0d0a  gtk.require()...
-00000290: 2369 6e69 742d 686f 6f6b 3d0d 0a0d 0a23  #init-hook=....#
-000002a0: 2055 7365 206d 756c 7469 706c 6520 7072   Use multiple pr
-000002b0: 6f63 6573 7365 7320 746f 2073 7065 6564  ocesses to speed
-000002c0: 2075 7020 5079 6c69 6e74 2e20 5370 6563   up Pylint. Spec
-000002d0: 6966 7969 6e67 2030 2077 696c 6c20 6175  ifying 0 will au
-000002e0: 746f 2d64 6574 6563 7420 7468 650d 0a23  to-detect the..#
-000002f0: 206e 756d 6265 7220 6f66 2070 726f 6365   number of proce
-00000300: 7373 6f72 7320 6176 6169 6c61 626c 6520  ssors available 
-00000310: 746f 2075 7365 2e0d 0a6a 6f62 733d 310d  to use...jobs=1.
-00000320: 0a0d 0a23 2043 6f6e 7472 6f6c 2074 6865  ...# Control the
-00000330: 2061 6d6f 756e 7420 6f66 2070 6f74 656e   amount of poten
-00000340: 7469 616c 2069 6e66 6572 7265 6420 7661  tial inferred va
-00000350: 6c75 6573 2077 6865 6e20 696e 6665 7272  lues when inferr
-00000360: 696e 6720 6120 7369 6e67 6c65 0d0a 2320  ing a single..# 
-00000370: 6f62 6a65 6374 2e20 5468 6973 2063 616e  object. This can
-00000380: 2068 656c 7020 7468 6520 7065 7266 6f72   help the perfor
-00000390: 6d61 6e63 6520 7768 656e 2064 6561 6c69  mance when deali
-000003a0: 6e67 2077 6974 6820 6c61 7267 6520 6675  ng with large fu
-000003b0: 6e63 7469 6f6e 7320 6f72 0d0a 2320 636f  nctions or..# co
-000003c0: 6d70 6c65 782c 206e 6573 7465 6420 636f  mplex, nested co
-000003d0: 6e64 6974 696f 6e73 2e0d 0a6c 696d 6974  nditions...limit
-000003e0: 2d69 6e66 6572 656e 6365 2d72 6573 756c  -inference-resul
-000003f0: 7473 3d31 3030 0d0a 0d0a 2320 4c69 7374  ts=100....# List
-00000400: 206f 6620 706c 7567 696e 7320 2861 7320   of plugins (as 
-00000410: 636f 6d6d 6120 7365 7061 7261 7465 6420  comma separated 
-00000420: 7661 6c75 6573 206f 6620 7079 7468 6f6e  values of python
-00000430: 206d 6f64 756c 6520 6e61 6d65 7329 2074   module names) t
-00000440: 6f20 6c6f 6164 2c0d 0a23 2075 7375 616c  o load,..# usual
-00000450: 6c79 2074 6f20 7265 6769 7374 6572 2061  ly to register a
-00000460: 6464 6974 696f 6e61 6c20 6368 6563 6b65  dditional checke
-00000470: 7273 2e0d 0a6c 6f61 642d 706c 7567 696e  rs...load-plugin
-00000480: 733d 0d0a 0d0a 2320 5069 636b 6c65 2063  s=....# Pickle c
-00000490: 6f6c 6c65 6374 6564 2064 6174 6120 666f  ollected data fo
-000004a0: 7220 6c61 7465 7220 636f 6d70 6172 6973  r later comparis
-000004b0: 6f6e 732e 0d0a 7065 7273 6973 7465 6e74  ons...persistent
-000004c0: 3d79 6573 0d0a 0d0a 2320 5768 656e 2065  =yes....# When e
-000004d0: 6e61 626c 6564 2c20 7079 6c69 6e74 2077  nabled, pylint w
-000004e0: 6f75 6c64 2061 7474 656d 7074 2074 6f20  ould attempt to 
-000004f0: 6775 6573 7320 636f 6d6d 6f6e 206d 6973  guess common mis
-00000500: 636f 6e66 6967 7572 6174 696f 6e20 616e  configuration an
-00000510: 6420 656d 6974 0d0a 2320 7573 6572 2d66  d emit..# user-f
-00000520: 7269 656e 646c 7920 6869 6e74 7320 696e  riendly hints in
-00000530: 7374 6561 6420 6f66 2066 616c 7365 2d70  stead of false-p
-00000540: 6f73 6974 6976 6520 6572 726f 7220 6d65  ositive error me
-00000550: 7373 6167 6573 2e0d 0a73 7567 6765 7374  ssages...suggest
-00000560: 696f 6e2d 6d6f 6465 3d79 6573 0d0a 0d0a  ion-mode=yes....
-00000570: 2320 416c 6c6f 7720 6c6f 6164 696e 6720  # Allow loading 
-00000580: 6f66 2061 7262 6974 7261 7279 2043 2065  of arbitrary C e
-00000590: 7874 656e 7369 6f6e 732e 2045 7874 656e  xtensions. Exten
-000005a0: 7369 6f6e 7320 6172 6520 696d 706f 7274  sions are import
-000005b0: 6564 2069 6e74 6f20 7468 650d 0a23 2061  ed into the..# a
-000005c0: 6374 6976 6520 5079 7468 6f6e 2069 6e74  ctive Python int
-000005d0: 6572 7072 6574 6572 2061 6e64 206d 6179  erpreter and may
-000005e0: 2072 756e 2061 7262 6974 7261 7279 2063   run arbitrary c
-000005f0: 6f64 652e 0d0a 756e 7361 6665 2d6c 6f61  ode...unsafe-loa
-00000600: 642d 616e 792d 6578 7465 6e73 696f 6e3d  d-any-extension=
-00000610: 6e6f 0d0a 0d0a 0d0a 5b4d 4553 5341 4745  no......[MESSAGE
-00000620: 5320 434f 4e54 524f 4c5d 0d0a 0d0a 2320  S CONTROL]....# 
-00000630: 4f6e 6c79 2073 686f 7720 7761 726e 696e  Only show warnin
-00000640: 6773 2077 6974 6820 7468 6520 6c69 7374  gs with the list
-00000650: 6564 2063 6f6e 6669 6465 6e63 6520 6c65  ed confidence le
-00000660: 7665 6c73 2e20 4c65 6176 6520 656d 7074  vels. Leave empt
-00000670: 7920 746f 2073 686f 770d 0a23 2061 6c6c  y to show..# all
-00000680: 2e20 5661 6c69 6420 6c65 7665 6c73 3a20  . Valid levels: 
-00000690: 4849 4748 2c20 494e 4645 5245 4e43 452c  HIGH, INFERENCE,
-000006a0: 2049 4e46 4552 454e 4345 5f46 4149 4c55   INFERENCE_FAILU
-000006b0: 5245 2c20 554e 4445 4649 4e45 442e 0d0a  RE, UNDEFINED...
-000006c0: 636f 6e66 6964 656e 6365 3d0d 0a0d 0a23  confidence=....#
-000006d0: 2044 6973 6162 6c65 2074 6865 206d 6573   Disable the mes
-000006e0: 7361 6765 2c20 7265 706f 7274 2c20 6361  sage, report, ca
-000006f0: 7465 676f 7279 206f 7220 6368 6563 6b65  tegory or checke
-00000700: 7220 7769 7468 2074 6865 2067 6976 656e  r with the given
-00000710: 2069 6428 7329 2e20 596f 750d 0a23 2063   id(s). You..# c
-00000720: 616e 2065 6974 6865 7220 6769 7665 206d  an either give m
-00000730: 756c 7469 706c 6520 6964 656e 7469 6669  ultiple identifi
-00000740: 6572 7320 7365 7061 7261 7465 6420 6279  ers separated by
-00000750: 2063 6f6d 6d61 2028 2c29 206f 7220 7075   comma (,) or pu
-00000760: 7420 7468 6973 0d0a 2320 6f70 7469 6f6e  t this..# option
-00000770: 206d 756c 7469 706c 6520 7469 6d65 7320   multiple times 
-00000780: 286f 6e6c 7920 6f6e 2074 6865 2063 6f6d  (only on the com
-00000790: 6d61 6e64 206c 696e 652c 206e 6f74 2069  mand line, not i
-000007a0: 6e20 7468 6520 636f 6e66 6967 7572 6174  n the configurat
-000007b0: 696f 6e0d 0a23 2066 696c 6520 7768 6572  ion..# file wher
-000007c0: 6520 6974 2073 686f 756c 6420 6170 7065  e it should appe
-000007d0: 6172 206f 6e6c 7920 6f6e 6365 292e 2059  ar only once). Y
-000007e0: 6f75 2063 616e 2061 6c73 6f20 7573 6520  ou can also use 
-000007f0: 222d 2d64 6973 6162 6c65 3d61 6c6c 2220  "--disable=all" 
-00000800: 746f 0d0a 2320 6469 7361 626c 6520 6576  to..# disable ev
-00000810: 6572 7974 6869 6e67 2066 6972 7374 2061  erything first a
-00000820: 6e64 2074 6865 6e20 7265 656e 6162 6c65  nd then reenable
-00000830: 2073 7065 6369 6669 6320 6368 6563 6b73   specific checks
-00000840: 2e20 466f 7220 6578 616d 706c 652c 2069  . For example, i
-00000850: 660d 0a23 2079 6f75 2077 616e 7420 746f  f..# you want to
-00000860: 2072 756e 206f 6e6c 7920 7468 6520 7369   run only the si
-00000870: 6d69 6c61 7269 7469 6573 2063 6865 636b  milarities check
-00000880: 6572 2c20 796f 7520 6361 6e20 7573 6520  er, you can use 
-00000890: 222d 2d64 6973 6162 6c65 3d61 6c6c 0d0a  "--disable=all..
-000008a0: 2320 2d2d 656e 6162 6c65 3d73 696d 696c  # --enable=simil
-000008b0: 6172 6974 6965 7322 2e20 4966 2079 6f75  arities". If you
-000008c0: 2077 616e 7420 746f 2072 756e 206f 6e6c   want to run onl
-000008d0: 7920 7468 6520 636c 6173 7365 7320 6368  y the classes ch
-000008e0: 6563 6b65 722c 2062 7574 2068 6176 650d  ecker, but have.
-000008f0: 0a23 206e 6f20 5761 726e 696e 6720 6c65  .# no Warning le
-00000900: 7665 6c20 6d65 7373 6167 6573 2064 6973  vel messages dis
-00000910: 706c 6179 6564 2c20 7573 6520 222d 2d64  played, use "--d
-00000920: 6973 6162 6c65 3d61 6c6c 202d 2d65 6e61  isable=all --ena
-00000930: 626c 653d 636c 6173 7365 730d 0a23 202d  ble=classes..# -
-00000940: 2d64 6973 6162 6c65 3d57 222e 0d0a 6469  -disable=W"...di
-00000950: 7361 626c 653d 7072 696e 742d 7374 6174  sable=print-stat
-00000960: 656d 656e 742c 0d0a 2020 2020 2020 2020  ement,..        
-00000970: 7061 7261 6d65 7465 722d 756e 7061 636b  parameter-unpack
-00000980: 696e 672c 0d0a 2020 2020 2020 2020 756e  ing,..        un
-00000990: 7061 636b 696e 672d 696e 2d65 7863 6570  packing-in-excep
-000009a0: 742c 0d0a 2020 2020 2020 2020 6f6c 642d  t,..        old-
-000009b0: 7261 6973 652d 7379 6e74 6178 2c0d 0a20  raise-syntax,.. 
-000009c0: 2020 2020 2020 2062 6163 6b74 6963 6b2c         backtick,
-000009d0: 0d0a 2020 2020 2020 2020 6c6f 6e67 2d73  ..        long-s
-000009e0: 7566 6669 782c 0d0a 2020 2020 2020 2020  uffix,..        
-000009f0: 6f6c 642d 6e65 2d6f 7065 7261 746f 722c  old-ne-operator,
-00000a00: 0d0a 2020 2020 2020 2020 6f6c 642d 6f63  ..        old-oc
-00000a10: 7461 6c2d 6c69 7465 7261 6c2c 0d0a 2020  tal-literal,..  
-00000a20: 2020 2020 2020 696d 706f 7274 2d73 7461        import-sta
-00000a30: 722d 6d6f 6475 6c65 2d6c 6576 656c 2c0d  r-module-level,.
-00000a40: 0a20 2020 2020 2020 206e 6f6e 2d61 7363  .        non-asc
-00000a50: 6969 2d62 7974 6573 2d6c 6974 6572 616c  ii-bytes-literal
-00000a60: 2c0d 0a20 2020 2020 2020 2072 6177 2d63  ,..        raw-c
-00000a70: 6865 636b 6572 2d66 6169 6c65 642c 0d0a  hecker-failed,..
-00000a80: 2020 2020 2020 2020 6261 642d 696e 6c69          bad-inli
-00000a90: 6e65 2d6f 7074 696f 6e2c 0d0a 2020 2020  ne-option,..    
-00000aa0: 2020 2020 6c6f 6361 6c6c 792d 6469 7361      locally-disa
-00000ab0: 626c 6564 2c0d 0a20 2020 2020 2020 2066  bled,..        f
-00000ac0: 696c 652d 6967 6e6f 7265 642c 0d0a 2020  ile-ignored,..  
-00000ad0: 2020 2020 2020 7375 7070 7265 7373 6564        suppressed
-00000ae0: 2d6d 6573 7361 6765 2c0d 0a20 2020 2020  -message,..     
-00000af0: 2020 2075 7365 6c65 7373 2d73 7570 7072     useless-suppr
-00000b00: 6573 7369 6f6e 2c0d 0a20 2020 2020 2020  ession,..       
-00000b10: 2064 6570 7265 6361 7465 642d 7072 6167   deprecated-prag
-00000b20: 6d61 2c0d 0a20 2020 2020 2020 2075 7365  ma,..        use
-00000b30: 2d73 796d 626f 6c69 632d 6d65 7373 6167  -symbolic-messag
-00000b40: 652d 696e 7374 6561 642c 0d0a 2020 2020  e-instead,..    
-00000b50: 2020 2020 6170 706c 792d 6275 696c 7469      apply-builti
-00000b60: 6e2c 0d0a 2020 2020 2020 2020 6261 7365  n,..        base
-00000b70: 7374 7269 6e67 2d62 7569 6c74 696e 2c0d  string-builtin,.
-00000b80: 0a20 2020 2020 2020 2062 7566 6665 722d  .        buffer-
-00000b90: 6275 696c 7469 6e2c 0d0a 2020 2020 2020  builtin,..      
-00000ba0: 2020 636d 702d 6275 696c 7469 6e2c 0d0a    cmp-builtin,..
-00000bb0: 2020 2020 2020 2020 636f 6572 6365 2d62          coerce-b
-00000bc0: 7569 6c74 696e 2c0d 0a20 2020 2020 2020  uiltin,..       
-00000bd0: 2065 7865 6366 696c 652d 6275 696c 7469   execfile-builti
-00000be0: 6e2c 0d0a 2020 2020 2020 2020 6669 6c65  n,..        file
-00000bf0: 2d62 7569 6c74 696e 2c0d 0a20 2020 2020  -builtin,..     
-00000c00: 2020 206c 6f6e 672d 6275 696c 7469 6e2c     long-builtin,
-00000c10: 0d0a 2020 2020 2020 2020 7261 775f 696e  ..        raw_in
-00000c20: 7075 742d 6275 696c 7469 6e2c 0d0a 2020  put-builtin,..  
-00000c30: 2020 2020 2020 7265 6475 6365 2d62 7569        reduce-bui
-00000c40: 6c74 696e 2c0d 0a20 2020 2020 2020 2073  ltin,..        s
-00000c50: 7461 6e64 6172 6465 7272 6f72 2d62 7569  tandarderror-bui
-00000c60: 6c74 696e 2c0d 0a20 2020 2020 2020 2075  ltin,..        u
-00000c70: 6e69 636f 6465 2d62 7569 6c74 696e 2c0d  nicode-builtin,.
-00000c80: 0a20 2020 2020 2020 2078 7261 6e67 652d  .        xrange-
-00000c90: 6275 696c 7469 6e2c 0d0a 2020 2020 2020  builtin,..      
-00000ca0: 2020 636f 6572 6365 2d6d 6574 686f 642c    coerce-method,
-00000cb0: 0d0a 2020 2020 2020 2020 6465 6c73 6c69  ..        delsli
-00000cc0: 6365 2d6d 6574 686f 642c 0d0a 2020 2020  ce-method,..    
-00000cd0: 2020 2020 6765 7473 6c69 6365 2d6d 6574      getslice-met
-00000ce0: 686f 642c 0d0a 2020 2020 2020 2020 7365  hod,..        se
-00000cf0: 7473 6c69 6365 2d6d 6574 686f 642c 0d0a  tslice-method,..
-00000d00: 2020 2020 2020 2020 6e6f 2d61 6273 6f6c          no-absol
-00000d10: 7574 652d 696d 706f 7274 2c0d 0a20 2020  ute-import,..   
-00000d20: 2020 2020 206f 6c64 2d64 6976 6973 696f       old-divisio
-00000d30: 6e2c 0d0a 2020 2020 2020 2020 6469 6374  n,..        dict
-00000d40: 2d69 7465 722d 6d65 7468 6f64 2c0d 0a20  -iter-method,.. 
-00000d50: 2020 2020 2020 2064 6963 742d 7669 6577         dict-view
-00000d60: 2d6d 6574 686f 642c 0d0a 2020 2020 2020  -method,..      
-00000d70: 2020 6e65 7874 2d6d 6574 686f 642d 6361    next-method-ca
-00000d80: 6c6c 6564 2c0d 0a20 2020 2020 2020 206d  lled,..        m
-00000d90: 6574 6163 6c61 7373 2d61 7373 6967 6e6d  etaclass-assignm
-00000da0: 656e 742c 0d0a 2020 2020 2020 2020 696e  ent,..        in
-00000db0: 6465 7869 6e67 2d65 7863 6570 7469 6f6e  dexing-exception
-00000dc0: 2c0d 0a20 2020 2020 2020 2072 6169 7369  ,..        raisi
-00000dd0: 6e67 2d73 7472 696e 672c 0d0a 2020 2020  ng-string,..    
-00000de0: 2020 2020 7265 6c6f 6164 2d62 7569 6c74      reload-built
-00000df0: 696e 2c0d 0a20 2020 2020 2020 206f 6374  in,..        oct
-00000e00: 2d6d 6574 686f 642c 0d0a 2020 2020 2020  -method,..      
-00000e10: 2020 6865 782d 6d65 7468 6f64 2c0d 0a20    hex-method,.. 
-00000e20: 2020 2020 2020 206e 6f6e 7a65 726f 2d6d         nonzero-m
-00000e30: 6574 686f 642c 0d0a 2020 2020 2020 2020  ethod,..        
-00000e40: 636d 702d 6d65 7468 6f64 2c0d 0a20 2020  cmp-method,..   
-00000e50: 2020 2020 2069 6e70 7574 2d62 7569 6c74       input-built
-00000e60: 696e 2c0d 0a20 2020 2020 2020 2072 6f75  in,..        rou
-00000e70: 6e64 2d62 7569 6c74 696e 2c0d 0a20 2020  nd-builtin,..   
-00000e80: 2020 2020 2069 6e74 6572 6e2d 6275 696c       intern-buil
-00000e90: 7469 6e2c 0d0a 2020 2020 2020 2020 756e  tin,..        un
-00000ea0: 6963 6872 2d62 7569 6c74 696e 2c0d 0a20  ichr-builtin,.. 
-00000eb0: 2020 2020 2020 206d 6170 2d62 7569 6c74         map-built
-00000ec0: 696e 2d6e 6f74 2d69 7465 7261 7469 6e67  in-not-iterating
-00000ed0: 2c0d 0a20 2020 2020 2020 207a 6970 2d62  ,..        zip-b
-00000ee0: 7569 6c74 696e 2d6e 6f74 2d69 7465 7261  uiltin-not-itera
-00000ef0: 7469 6e67 2c0d 0a20 2020 2020 2020 2072  ting,..        r
-00000f00: 616e 6765 2d62 7569 6c74 696e 2d6e 6f74  ange-builtin-not
-00000f10: 2d69 7465 7261 7469 6e67 2c0d 0a20 2020  -iterating,..   
-00000f20: 2020 2020 2066 696c 7465 722d 6275 696c       filter-buil
-00000f30: 7469 6e2d 6e6f 742d 6974 6572 6174 696e  tin-not-iteratin
-00000f40: 672c 0d0a 2020 2020 2020 2020 7573 696e  g,..        usin
-00000f50: 672d 636d 702d 6172 6775 6d65 6e74 2c0d  g-cmp-argument,.
-00000f60: 0a20 2020 2020 2020 2065 712d 7769 7468  .        eq-with
-00000f70: 6f75 742d 6861 7368 2c0d 0a20 2020 2020  out-hash,..     
-00000f80: 2020 2064 6976 2d6d 6574 686f 642c 0d0a     div-method,..
-00000f90: 2020 2020 2020 2020 6964 6976 2d6d 6574          idiv-met
-00000fa0: 686f 642c 0d0a 2020 2020 2020 2020 7264  hod,..        rd
-00000fb0: 6976 2d6d 6574 686f 642c 0d0a 2020 2020  iv-method,..    
-00000fc0: 2020 2020 6578 6365 7074 696f 6e2d 6d65      exception-me
-00000fd0: 7373 6167 652d 6174 7472 6962 7574 652c  ssage-attribute,
-00000fe0: 0d0a 2020 2020 2020 2020 696e 7661 6c69  ..        invali
-00000ff0: 642d 7374 722d 636f 6465 632c 0d0a 2020  d-str-codec,..  
-00001000: 2020 2020 2020 7379 732d 6d61 782d 696e        sys-max-in
-00001010: 742c 0d0a 2020 2020 2020 2020 6261 642d  t,..        bad-
-00001020: 7079 7468 6f6e 332d 696d 706f 7274 2c0d  python3-import,.
-00001030: 0a20 2020 2020 2020 2064 6570 7265 6361  .        depreca
-00001040: 7465 642d 7374 7269 6e67 2d66 756e 6374  ted-string-funct
-00001050: 696f 6e2c 0d0a 2020 2020 2020 2020 6465  ion,..        de
-00001060: 7072 6563 6174 6564 2d73 7472 2d74 7261  precated-str-tra
-00001070: 6e73 6c61 7465 2d63 616c 6c2c 0d0a 2020  nslate-call,..  
-00001080: 2020 2020 2020 6465 7072 6563 6174 6564        deprecated
-00001090: 2d69 7465 7274 6f6f 6c73 2d66 756e 6374  -itertools-funct
-000010a0: 696f 6e2c 0d0a 2020 2020 2020 2020 6465  ion,..        de
-000010b0: 7072 6563 6174 6564 2d74 7970 6573 2d66  precated-types-f
-000010c0: 6965 6c64 2c0d 0a20 2020 2020 2020 206e  ield,..        n
-000010d0: 6578 742d 6d65 7468 6f64 2d64 6566 696e  ext-method-defin
-000010e0: 6564 2c0d 0a20 2020 2020 2020 2064 6963  ed,..        dic
-000010f0: 742d 6974 656d 732d 6e6f 742d 6974 6572  t-items-not-iter
-00001100: 6174 696e 672c 0d0a 2020 2020 2020 2020  ating,..        
-00001110: 6469 6374 2d6b 6579 732d 6e6f 742d 6974  dict-keys-not-it
-00001120: 6572 6174 696e 672c 0d0a 2020 2020 2020  erating,..      
-00001130: 2020 6469 6374 2d76 616c 7565 732d 6e6f    dict-values-no
-00001140: 742d 6974 6572 6174 696e 672c 0d0a 2020  t-iterating,..  
-00001150: 2020 2020 2020 6465 7072 6563 6174 6564        deprecated
-00001160: 2d6f 7065 7261 746f 722d 6675 6e63 7469  -operator-functi
-00001170: 6f6e 2c0d 0a20 2020 2020 2020 2064 6570  on,..        dep
-00001180: 7265 6361 7465 642d 7572 6c6c 6962 2d66  recated-urllib-f
-00001190: 756e 6374 696f 6e2c 0d0a 2020 2020 2020  unction,..      
-000011a0: 2020 7872 6561 646c 696e 6573 2d61 7474    xreadlines-att
-000011b0: 7269 6275 7465 2c0d 0a20 2020 2020 2020  ribute,..       
-000011c0: 2064 6570 7265 6361 7465 642d 7379 732d   deprecated-sys-
-000011d0: 6675 6e63 7469 6f6e 2c0d 0a20 2020 2020  function,..     
-000011e0: 2020 2065 7863 6570 7469 6f6e 2d65 7363     exception-esc
-000011f0: 6170 652c 0d0a 2020 2020 2020 2020 636f  ape,..        co
-00001200: 6d70 7265 6865 6e73 696f 6e2d 6573 6361  mprehension-esca
-00001210: 7065 2c0d 0a20 2020 2020 2020 206e 6f2d  pe,..        no-
-00001220: 7365 6c66 2d75 7365 2c0d 0a20 2020 2020  self-use,..     
-00001230: 2020 2075 7365 2d64 6963 742d 6c69 7465     use-dict-lite
-00001240: 7261 6c0d 0a0d 0a23 2045 6e61 626c 6520  ral....# Enable 
-00001250: 7468 6520 6d65 7373 6167 652c 2072 6570  the message, rep
-00001260: 6f72 742c 2063 6174 6567 6f72 7920 6f72  ort, category or
-00001270: 2063 6865 636b 6572 2077 6974 6820 7468   checker with th
-00001280: 6520 6769 7665 6e20 6964 2873 292e 2059  e given id(s). Y
-00001290: 6f75 2063 616e 0d0a 2320 6569 7468 6572  ou can..# either
-000012a0: 2067 6976 6520 6d75 6c74 6970 6c65 2069   give multiple i
-000012b0: 6465 6e74 6966 6965 7220 7365 7061 7261  dentifier separa
-000012c0: 7465 6420 6279 2063 6f6d 6d61 2028 2c29  ted by comma (,)
-000012d0: 206f 7220 7075 7420 7468 6973 206f 7074   or put this opt
-000012e0: 696f 6e0d 0a23 206d 756c 7469 706c 6520  ion..# multiple 
-000012f0: 7469 6d65 2028 6f6e 6c79 206f 6e20 7468  time (only on th
-00001300: 6520 636f 6d6d 616e 6420 6c69 6e65 2c20  e command line, 
-00001310: 6e6f 7420 696e 2074 6865 2063 6f6e 6669  not in the confi
-00001320: 6775 7261 7469 6f6e 2066 696c 6520 7768  guration file wh
-00001330: 6572 650d 0a23 2069 7420 7368 6f75 6c64  ere..# it should
-00001340: 2061 7070 6561 7220 6f6e 6c79 206f 6e63   appear only onc
-00001350: 6529 2e20 5365 6520 616c 736f 2074 6865  e). See also the
-00001360: 2022 2d2d 6469 7361 626c 6522 206f 7074   "--disable" opt
-00001370: 696f 6e20 666f 7220 6578 616d 706c 6573  ion for examples
-00001380: 2e0d 0a65 6e61 626c 653d 632d 6578 7465  ...enable=c-exte
-00001390: 6e73 696f 6e2d 6e6f 2d6d 656d 6265 720d  nsion-no-member.
-000013a0: 0a0d 0a0d 0a5b 5245 504f 5254 535d 0d0a  .....[REPORTS]..
-000013b0: 0d0a 2320 5079 7468 6f6e 2065 7870 7265  ..# Python expre
-000013c0: 7373 696f 6e20 7768 6963 6820 7368 6f75  ssion which shou
-000013d0: 6c64 2072 6574 7572 6e20 6120 7363 6f72  ld return a scor
-000013e0: 6520 6c65 7373 2074 6861 6e20 6f72 2065  e less than or e
-000013f0: 7175 616c 2074 6f20 3130 2e20 596f 750d  qual to 10. You.
-00001400: 0a23 2068 6176 6520 6163 6365 7373 2074  .# have access t
-00001410: 6f20 7468 6520 7661 7269 6162 6c65 7320  o the variables 
-00001420: 2765 7272 6f72 272c 2027 7761 726e 696e  'error', 'warnin
-00001430: 6727 2c20 2772 6566 6163 746f 7227 2c20  g', 'refactor', 
-00001440: 616e 6420 2763 6f6e 7665 6e74 696f 6e27  and 'convention'
-00001450: 0d0a 2320 7768 6963 6820 636f 6e74 6169  ..# which contai
-00001460: 6e20 7468 6520 6e75 6d62 6572 206f 6620  n the number of 
-00001470: 6d65 7373 6167 6573 2069 6e20 6561 6368  messages in each
-00001480: 2063 6174 6567 6f72 792c 2061 7320 7765   category, as we
-00001490: 6c6c 2061 7320 2773 7461 7465 6d65 6e74  ll as 'statement
-000014a0: 270d 0a23 2077 6869 6368 2069 7320 7468  '..# which is th
-000014b0: 6520 746f 7461 6c20 6e75 6d62 6572 206f  e total number o
-000014c0: 6620 7374 6174 656d 656e 7473 2061 6e61  f statements ana
-000014d0: 6c79 7a65 642e 2054 6869 7320 7363 6f72  lyzed. This scor
-000014e0: 6520 6973 2075 7365 6420 6279 2074 6865  e is used by the
-000014f0: 0d0a 2320 676c 6f62 616c 2065 7661 6c75  ..# global evalu
-00001500: 6174 696f 6e20 7265 706f 7274 2028 5250  ation report (RP
-00001510: 3030 3034 292e 0d0a 6576 616c 7561 7469  0004)...evaluati
-00001520: 6f6e 3d31 302e 3020 2d20 2828 666c 6f61  on=10.0 - ((floa
-00001530: 7428 3520 2a20 6572 726f 7220 2b20 7761  t(5 * error + wa
-00001540: 726e 696e 6720 2b20 7265 6661 6374 6f72  rning + refactor
-00001550: 202b 2063 6f6e 7665 6e74 696f 6e29 202f   + convention) /
-00001560: 2073 7461 7465 6d65 6e74 2920 2a20 3130   statement) * 10
-00001570: 290d 0a0d 0a23 2054 656d 706c 6174 6520  )....# Template 
-00001580: 7573 6564 2074 6f20 6469 7370 6c61 7920  used to display 
-00001590: 6d65 7373 6167 6573 2e20 5468 6973 2069  messages. This i
-000015a0: 7320 6120 7079 7468 6f6e 206e 6577 2d73  s a python new-s
-000015b0: 7479 6c65 2066 6f72 6d61 7420 7374 7269  tyle format stri
-000015c0: 6e67 0d0a 2320 7573 6564 2074 6f20 666f  ng..# used to fo
-000015d0: 726d 6174 2074 6865 206d 6573 7361 6765  rmat the message
-000015e0: 2069 6e66 6f72 6d61 7469 6f6e 2e20 5365   information. Se
-000015f0: 6520 646f 6320 666f 7220 616c 6c20 6465  e doc for all de
-00001600: 7461 696c 732e 0d0a 236d 7367 2d74 656d  tails...#msg-tem
-00001610: 706c 6174 653d 0d0a 0d0a 2320 5365 7420  plate=....# Set 
-00001620: 7468 6520 6f75 7470 7574 2066 6f72 6d61  the output forma
-00001630: 742e 2041 7661 696c 6162 6c65 2066 6f72  t. Available for
-00001640: 6d61 7473 2061 7265 2074 6578 742c 2070  mats are text, p
-00001650: 6172 7365 6162 6c65 2c20 636f 6c6f 7269  arseable, colori
-00001660: 7a65 642c 206a 736f 6e0d 0a23 2061 6e64  zed, json..# and
-00001670: 206d 7376 7320 2876 6973 7561 6c20 7374   msvs (visual st
-00001680: 7564 696f 292e 2059 6f75 2063 616e 2061  udio). You can a
-00001690: 6c73 6f20 6769 7665 2061 2072 6570 6f72  lso give a repor
-000016a0: 7465 7220 636c 6173 732c 2065 2e67 2e0d  ter class, e.g..
-000016b0: 0a23 206d 7970 6163 6b61 6765 2e6d 796d  .# mypackage.mym
-000016c0: 6f64 756c 652e 4d79 5265 706f 7274 6572  odule.MyReporter
-000016d0: 436c 6173 732e 0d0a 6f75 7470 7574 2d66  Class...output-f
-000016e0: 6f72 6d61 743d 7465 7874 0d0a 0d0a 2320  ormat=text....# 
-000016f0: 5465 6c6c 7320 7768 6574 6865 7220 746f  Tells whether to
-00001700: 2064 6973 706c 6179 2061 2066 756c 6c20   display a full 
-00001710: 7265 706f 7274 206f 7220 6f6e 6c79 2074  report or only t
-00001720: 6865 206d 6573 7361 6765 732e 0d0a 7265  he messages...re
-00001730: 706f 7274 733d 6e6f 0d0a 0d0a 2320 4163  ports=no....# Ac
-00001740: 7469 7661 7465 2074 6865 2065 7661 6c75  tivate the evalu
-00001750: 6174 696f 6e20 7363 6f72 652e 0d0a 7363  ation score...sc
-00001760: 6f72 653d 7965 730d 0a0d 0a0d 0a5b 5245  ore=yes......[RE
-00001770: 4641 4354 4f52 494e 475d 0d0a 0d0a 2320  FACTORING]....# 
-00001780: 4d61 7869 6d75 6d20 6e75 6d62 6572 206f  Maximum number o
-00001790: 6620 6e65 7374 6564 2062 6c6f 636b 7320  f nested blocks 
-000017a0: 666f 7220 6675 6e63 7469 6f6e 202f 206d  for function / m
-000017b0: 6574 686f 6420 626f 6479 0d0a 6d61 782d  ethod body..max-
-000017c0: 6e65 7374 6564 2d62 6c6f 636b 733d 350d  nested-blocks=5.
-000017d0: 0a0d 0a23 2043 6f6d 706c 6574 6520 6e61  ...# Complete na
-000017e0: 6d65 206f 6620 6675 6e63 7469 6f6e 7320  me of functions 
-000017f0: 7468 6174 206e 6576 6572 2072 6574 7572  that never retur
-00001800: 6e73 2e20 5768 656e 2063 6865 636b 696e  ns. When checkin
-00001810: 6720 666f 720d 0a23 2069 6e63 6f6e 7369  g for..# inconsi
-00001820: 7374 656e 742d 7265 7475 726e 2d73 7461  stent-return-sta
-00001830: 7465 6d65 6e74 7320 6966 2061 206e 6576  tements if a nev
-00001840: 6572 2072 6574 7572 6e69 6e67 2066 756e  er returning fun
-00001850: 6374 696f 6e20 6973 2063 616c 6c65 6420  ction is called 
-00001860: 7468 656e 0d0a 2320 6974 2077 696c 6c20  then..# it will 
-00001870: 6265 2063 6f6e 7369 6465 7265 6420 6173  be considered as
-00001880: 2061 6e20 6578 706c 6963 6974 2072 6574   an explicit ret
-00001890: 7572 6e20 7374 6174 656d 656e 7420 616e  urn statement an
-000018a0: 6420 6e6f 206d 6573 7361 6765 2077 696c  d no message wil
-000018b0: 6c20 6265 0d0a 2320 7072 696e 7465 642e  l be..# printed.
-000018c0: 0d0a 6e65 7665 722d 7265 7475 726e 696e  ..never-returnin
-000018d0: 672d 6675 6e63 7469 6f6e 733d 7379 732e  g-functions=sys.
-000018e0: 6578 6974 0d0a 0d0a 0d0a 5b53 5045 4c4c  exit......[SPELL
-000018f0: 494e 475d 0d0a 0d0a 2320 4c69 6d69 7473  ING]....# Limits
-00001900: 2063 6f75 6e74 206f 6620 656d 6974 7465   count of emitte
-00001910: 6420 7375 6767 6573 7469 6f6e 7320 666f  d suggestions fo
-00001920: 7220 7370 656c 6c69 6e67 206d 6973 7461  r spelling mista
-00001930: 6b65 732e 0d0a 6d61 782d 7370 656c 6c69  kes...max-spelli
-00001940: 6e67 2d73 7567 6765 7374 696f 6e73 3d34  ng-suggestions=4
-00001950: 0d0a 0d0a 2320 5370 656c 6c69 6e67 2064  ....# Spelling d
-00001960: 6963 7469 6f6e 6172 7920 6e61 6d65 2e20  ictionary name. 
-00001970: 4176 6169 6c61 626c 6520 6469 6374 696f  Available dictio
-00001980: 6e61 7269 6573 3a20 6e6f 6e65 2e20 546f  naries: none. To
-00001990: 206d 616b 6520 6974 2077 6f72 6b2c 0d0a   make it work,..
-000019a0: 2320 696e 7374 616c 6c20 7468 6520 7079  # install the py
-000019b0: 7468 6f6e 2d65 6e63 6861 6e74 2070 6163  thon-enchant pac
-000019c0: 6b61 6765 2e0d 0a73 7065 6c6c 696e 672d  kage...spelling-
-000019d0: 6469 6374 3d0d 0a0d 0a23 204c 6973 7420  dict=....# List 
-000019e0: 6f66 2063 6f6d 6d61 2073 6570 6172 6174  of comma separat
-000019f0: 6564 2077 6f72 6473 2074 6861 7420 7368  ed words that sh
-00001a00: 6f75 6c64 206e 6f74 2062 6520 6368 6563  ould not be chec
-00001a10: 6b65 642e 0d0a 7370 656c 6c69 6e67 2d69  ked...spelling-i
-00001a20: 676e 6f72 652d 776f 7264 733d 0d0a 0d0a  gnore-words=....
-00001a30: 2320 4120 7061 7468 2074 6f20 6120 6669  # A path to a fi
-00001a40: 6c65 2074 6861 7420 636f 6e74 6169 6e73  le that contains
-00001a50: 2074 6865 2070 7269 7661 7465 2064 6963   the private dic
-00001a60: 7469 6f6e 6172 793b 206f 6e65 2077 6f72  tionary; one wor
-00001a70: 6420 7065 7220 6c69 6e65 2e0d 0a73 7065  d per line...spe
-00001a80: 6c6c 696e 672d 7072 6976 6174 652d 6469  lling-private-di
-00001a90: 6374 2d66 696c 653d 0d0a 0d0a 2320 5465  ct-file=....# Te
-00001aa0: 6c6c 7320 7768 6574 6865 7220 746f 2073  lls whether to s
-00001ab0: 746f 7265 2075 6e6b 6e6f 776e 2077 6f72  tore unknown wor
-00001ac0: 6473 2074 6f20 7468 6520 7072 6976 6174  ds to the privat
-00001ad0: 6520 6469 6374 696f 6e61 7279 2028 7365  e dictionary (se
-00001ae0: 6520 7468 650d 0a23 202d 2d73 7065 6c6c  e the..# --spell
-00001af0: 696e 672d 7072 6976 6174 652d 6469 6374  ing-private-dict
-00001b00: 2d66 696c 6520 6f70 7469 6f6e 2920 696e  -file option) in
-00001b10: 7374 6561 6420 6f66 2072 6169 7369 6e67  stead of raising
-00001b20: 2061 206d 6573 7361 6765 2e0d 0a73 7065   a message...spe
-00001b30: 6c6c 696e 672d 7374 6f72 652d 756e 6b6e  lling-store-unkn
-00001b40: 6f77 6e2d 776f 7264 733d 6e6f 0d0a 0d0a  own-words=no....
-00001b50: 0d0a 5b56 4152 4941 424c 4553 5d0d 0a0d  ..[VARIABLES]...
-00001b60: 0a23 204c 6973 7420 6f66 2061 6464 6974  .# List of addit
-00001b70: 696f 6e61 6c20 6e61 6d65 7320 7375 7070  ional names supp
-00001b80: 6f73 6564 2074 6f20 6265 2064 6566 696e  osed to be defin
-00001b90: 6564 2069 6e20 6275 696c 7469 6e73 2e20  ed in builtins. 
-00001ba0: 5265 6d65 6d62 6572 2074 6861 740d 0a23  Remember that..#
-00001bb0: 2079 6f75 2073 686f 756c 6420 6176 6f69   you should avoi
-00001bc0: 6420 6465 6669 6e69 6e67 206e 6577 2062  d defining new b
-00001bd0: 7569 6c74 696e 7320 7768 656e 2070 6f73  uiltins when pos
-00001be0: 7369 626c 652e 0d0a 6164 6469 7469 6f6e  sible...addition
-00001bf0: 616c 2d62 7569 6c74 696e 733d 0d0a 0d0a  al-builtins=....
-00001c00: 2320 5465 6c6c 7320 7768 6574 6865 7220  # Tells whether 
-00001c10: 756e 7573 6564 2067 6c6f 6261 6c20 7661  unused global va
-00001c20: 7269 6162 6c65 7320 7368 6f75 6c64 2062  riables should b
-00001c30: 6520 7472 6561 7465 6420 6173 2061 2076  e treated as a v
-00001c40: 696f 6c61 7469 6f6e 2e0d 0a61 6c6c 6f77  iolation...allow
-00001c50: 2d67 6c6f 6261 6c2d 756e 7573 6564 2d76  -global-unused-v
-00001c60: 6172 6961 626c 6573 3d79 6573 0d0a 0d0a  ariables=yes....
-00001c70: 2320 4c69 7374 206f 6620 7374 7269 6e67  # List of string
-00001c80: 7320 7768 6963 6820 6361 6e20 6964 656e  s which can iden
-00001c90: 7469 6679 2061 2063 616c 6c62 6163 6b20  tify a callback 
-00001ca0: 6675 6e63 7469 6f6e 2062 7920 6e61 6d65  function by name
-00001cb0: 2e20 4120 6361 6c6c 6261 636b 0d0a 2320  . A callback..# 
-00001cc0: 6e61 6d65 206d 7573 7420 7374 6172 7420  name must start 
-00001cd0: 6f72 2065 6e64 2077 6974 6820 6f6e 6520  or end with one 
-00001ce0: 6f66 2074 686f 7365 2073 7472 696e 6773  of those strings
-00001cf0: 2e0d 0a63 616c 6c62 6163 6b73 3d63 625f  ...callbacks=cb_
-00001d00: 2c0d 0a20 2020 2020 2020 2020 205f 6362  ,..          _cb
-00001d10: 0d0a 0d0a 2320 4120 7265 6775 6c61 7220  ....# A regular 
-00001d20: 6578 7072 6573 7369 6f6e 206d 6174 6368  expression match
-00001d30: 696e 6720 7468 6520 6e61 6d65 206f 6620  ing the name of 
-00001d40: 6475 6d6d 7920 7661 7269 6162 6c65 7320  dummy variables 
-00001d50: 2869 2e65 2e20 6578 7065 6374 6564 2074  (i.e. expected t
-00001d60: 6f0d 0a23 206e 6f74 2062 6520 7573 6564  o..# not be used
-00001d70: 292e 0d0a 6475 6d6d 792d 7661 7269 6162  )...dummy-variab
-00001d80: 6c65 732d 7267 783d 5f2b 247c 285f 5b61  les-rgx=_+$|(_[a
-00001d90: 2d7a 412d 5a30 2d39 5f5d 2a5b 612d 7a41  -zA-Z0-9_]*[a-zA
-00001da0: 2d5a 302d 395d 2b3f 2429 7c64 756d 6d79  -Z0-9]+?$)|dummy
-00001db0: 7c5e 6967 6e6f 7265 645f 7c5e 756e 7573  |^ignored_|^unus
-00001dc0: 6564 5f0d 0a0d 0a23 2041 7267 756d 656e  ed_....# Argumen
-00001dd0: 7420 6e61 6d65 7320 7468 6174 206d 6174  t names that mat
-00001de0: 6368 2074 6869 7320 6578 7072 6573 7369  ch this expressi
-00001df0: 6f6e 2077 696c 6c20 6265 2069 676e 6f72  on will be ignor
-00001e00: 6564 2e20 4465 6661 756c 7420 746f 206e  ed. Default to n
-00001e10: 616d 650d 0a23 2077 6974 6820 6c65 6164  ame..# with lead
-00001e20: 696e 6720 756e 6465 7273 636f 7265 2e0d  ing underscore..
-00001e30: 0a69 676e 6f72 6564 2d61 7267 756d 656e  .ignored-argumen
-00001e40: 742d 6e61 6d65 733d 5f2e 2a7c 5e69 676e  t-names=_.*|^ign
-00001e50: 6f72 6564 5f7c 5e75 6e75 7365 645f 0d0a  ored_|^unused_..
-00001e60: 0d0a 2320 5465 6c6c 7320 7768 6574 6865  ..# Tells whethe
-00001e70: 7220 7765 2073 686f 756c 6420 6368 6563  r we should chec
-00001e80: 6b20 666f 7220 756e 7573 6564 2069 6d70  k for unused imp
-00001e90: 6f72 7420 696e 205f 5f69 6e69 745f 5f20  ort in __init__ 
-00001ea0: 6669 6c65 732e 0d0a 696e 6974 2d69 6d70  files...init-imp
-00001eb0: 6f72 743d 6e6f 0d0a 0d0a 2320 4c69 7374  ort=no....# List
-00001ec0: 206f 6620 7175 616c 6966 6965 6420 6d6f   of qualified mo
-00001ed0: 6475 6c65 206e 616d 6573 2077 6869 6368  dule names which
-00001ee0: 2063 616e 2068 6176 6520 6f62 6a65 6374   can have object
-00001ef0: 7320 7468 6174 2063 616e 2072 6564 6566  s that can redef
-00001f00: 696e 650d 0a23 2062 7569 6c74 696e 732e  ine..# builtins.
-00001f10: 0d0a 7265 6465 6669 6e69 6e67 2d62 7569  ..redefining-bui
-00001f20: 6c74 696e 732d 6d6f 6475 6c65 733d 7369  ltins-modules=si
-00001f30: 782e 6d6f 7665 732c 7061 7374 2e62 7569  x.moves,past.bui
-00001f40: 6c74 696e 732c 6675 7475 7265 2e62 7569  ltins,future.bui
-00001f50: 6c74 696e 732c 6275 696c 7469 6e73 2c69  ltins,builtins,i
-00001f60: 6f0d 0a0d 0a0d 0a5b 464f 524d 4154 5d0d  o......[FORMAT].
-00001f70: 0a0d 0a23 2045 7870 6563 7465 6420 666f  ...# Expected fo
-00001f80: 726d 6174 206f 6620 6c69 6e65 2065 6e64  rmat of line end
-00001f90: 696e 672c 2065 2e67 2e20 656d 7074 7920  ing, e.g. empty 
-00001fa0: 2861 6e79 206c 696e 6520 656e 6469 6e67  (any line ending
-00001fb0: 292c 204c 4620 6f72 2043 524c 462e 0d0a  ), LF or CRLF...
-00001fc0: 6578 7065 6374 6564 2d6c 696e 652d 656e  expected-line-en
-00001fd0: 6469 6e67 2d66 6f72 6d61 743d 0d0a 0d0a  ding-format=....
-00001fe0: 2320 5265 6765 7870 2066 6f72 2061 206c  # Regexp for a l
-00001ff0: 696e 6520 7468 6174 2069 7320 616c 6c6f  ine that is allo
-00002000: 7765 6420 746f 2062 6520 6c6f 6e67 6572  wed to be longer
-00002010: 2074 6861 6e20 7468 6520 6c69 6d69 742e   than the limit.
-00002020: 0d0a 6967 6e6f 7265 2d6c 6f6e 672d 6c69  ..ignore-long-li
-00002030: 6e65 733d 5e5c 732a 2823 2029 3f3c 3f68  nes=^\s*(# )?<?h
-00002040: 7474 7073 3f3a 2f2f 5c53 2b3e 3f24 0d0a  ttps?://\S+>?$..
-00002050: 0d0a 2320 4e75 6d62 6572 206f 6620 7370  ..# Number of sp
-00002060: 6163 6573 206f 6620 696e 6465 6e74 2072  aces of indent r
-00002070: 6571 7569 7265 6420 696e 7369 6465 2061  equired inside a
-00002080: 2068 616e 6769 6e67 206f 7220 636f 6e74   hanging or cont
-00002090: 696e 7565 6420 6c69 6e65 2e0d 0a69 6e64  inued line...ind
-000020a0: 656e 742d 6166 7465 722d 7061 7265 6e3d  ent-after-paren=
-000020b0: 340d 0a0d 0a23 2053 7472 696e 6720 7573  4....# String us
-000020c0: 6564 2061 7320 696e 6465 6e74 6174 696f  ed as indentatio
-000020d0: 6e20 756e 6974 2e20 5468 6973 2069 7320  n unit. This is 
-000020e0: 7573 7561 6c6c 7920 2220 2020 2022 2028  usually "    " (
-000020f0: 3420 7370 6163 6573 2920 6f72 2022 5c74  4 spaces) or "\t
-00002100: 2220 2831 0d0a 2320 7461 6229 2e0d 0a69  " (1..# tab)...i
-00002110: 6e64 656e 742d 7374 7269 6e67 3d27 2020  ndent-string='  
-00002120: 2020 270d 0a0d 0a23 204d 6178 696d 756d    '....# Maximum
-00002130: 206e 756d 6265 7220 6f66 2063 6861 7261   number of chara
-00002140: 6374 6572 7320 6f6e 2061 2073 696e 676c  cters on a singl
-00002150: 6520 6c69 6e65 2e0d 0a6d 6178 2d6c 696e  e line...max-lin
-00002160: 652d 6c65 6e67 7468 3d31 3030 0d0a 0d0a  e-length=100....
-00002170: 2320 4d61 7869 6d75 6d20 6e75 6d62 6572  # Maximum number
-00002180: 206f 6620 6c69 6e65 7320 696e 2061 206d   of lines in a m
-00002190: 6f64 756c 652e 0d0a 6d61 782d 6d6f 6475  odule...max-modu
-000021a0: 6c65 2d6c 696e 6573 3d31 3030 300d 0a0d  le-lines=1000...
-000021b0: 0a23 2041 6c6c 6f77 2074 6865 2062 6f64  .# Allow the bod
-000021c0: 7920 6f66 2061 2063 6c61 7373 2074 6f20  y of a class to 
-000021d0: 6265 206f 6e20 7468 6520 7361 6d65 206c  be on the same l
-000021e0: 696e 6520 6173 2074 6865 2064 6563 6c61  ine as the decla
-000021f0: 7261 7469 6f6e 2069 6620 626f 6479 0d0a  ration if body..
-00002200: 2320 636f 6e74 6169 6e73 2073 696e 676c  # contains singl
-00002210: 6520 7374 6174 656d 656e 742e 0d0a 7369  e statement...si
-00002220: 6e67 6c65 2d6c 696e 652d 636c 6173 732d  ngle-line-class-
-00002230: 7374 6d74 3d6e 6f0d 0a0d 0a23 2041 6c6c  stmt=no....# All
-00002240: 6f77 2074 6865 2062 6f64 7920 6f66 2061  ow the body of a
-00002250: 6e20 6966 2074 6f20 6265 206f 6e20 7468  n if to be on th
-00002260: 6520 7361 6d65 206c 696e 6520 6173 2074  e same line as t
-00002270: 6865 2074 6573 7420 6966 2074 6865 7265  he test if there
-00002280: 2069 7320 6e6f 0d0a 2320 656c 7365 2e0d   is no..# else..
-00002290: 0a73 696e 676c 652d 6c69 6e65 2d69 662d  .single-line-if-
-000022a0: 7374 6d74 3d6e 6f0d 0a0d 0a0d 0a5b 5459  stmt=no......[TY
-000022b0: 5045 4348 4543 4b5d 0d0a 0d0a 2320 4c69  PECHECK]....# Li
-000022c0: 7374 206f 6620 6465 636f 7261 746f 7273  st of decorators
-000022d0: 2074 6861 7420 7072 6f64 7563 6520 636f   that produce co
-000022e0: 6e74 6578 7420 6d61 6e61 6765 7273 2c20  ntext managers, 
-000022f0: 7375 6368 2061 730d 0a23 2063 6f6e 7465  such as..# conte
-00002300: 7874 6c69 622e 636f 6e74 6578 746d 616e  xtlib.contextman
-00002310: 6167 6572 2e20 4164 6420 746f 2074 6869  ager. Add to thi
-00002320: 7320 6c69 7374 2074 6f20 7265 6769 7374  s list to regist
-00002330: 6572 206f 7468 6572 2064 6563 6f72 6174  er other decorat
-00002340: 6f72 7320 7468 6174 0d0a 2320 7072 6f64  ors that..# prod
-00002350: 7563 6520 7661 6c69 6420 636f 6e74 6578  uce valid contex
-00002360: 7420 6d61 6e61 6765 7273 2e0d 0a63 6f6e  t managers...con
-00002370: 7465 7874 6d61 6e61 6765 722d 6465 636f  textmanager-deco
-00002380: 7261 746f 7273 3d63 6f6e 7465 7874 6c69  rators=contextli
-00002390: 622e 636f 6e74 6578 746d 616e 6167 6572  b.contextmanager
-000023a0: 0d0a 0d0a 2320 4c69 7374 206f 6620 6d65  ....# List of me
-000023b0: 6d62 6572 7320 7768 6963 6820 6172 6520  mbers which are 
-000023c0: 7365 7420 6479 6e61 6d69 6361 6c6c 7920  set dynamically 
-000023d0: 616e 6420 6d69 7373 6564 2062 7920 7079  and missed by py
-000023e0: 6c69 6e74 2069 6e66 6572 656e 6365 0d0a  lint inference..
-000023f0: 2320 7379 7374 656d 2c20 616e 6420 736f  # system, and so
-00002400: 2073 686f 756c 646e 2774 2074 7269 6767   shouldn't trigg
-00002410: 6572 2045 3131 3031 2077 6865 6e20 6163  er E1101 when ac
-00002420: 6365 7373 6564 2e20 5079 7468 6f6e 2072  cessed. Python r
-00002430: 6567 756c 6172 0d0a 2320 6578 7072 6573  egular..# expres
-00002440: 7369 6f6e 7320 6172 6520 6163 6365 7074  sions are accept
-00002450: 6564 2e0d 0a67 656e 6572 6174 6564 2d6d  ed...generated-m
-00002460: 656d 6265 7273 3d0d 0a0d 0a23 2054 656c  embers=....# Tel
-00002470: 6c73 2077 6865 7468 6572 206d 6973 7369  ls whether missi
-00002480: 6e67 206d 656d 6265 7273 2061 6363 6573  ng members acces
-00002490: 7365 6420 696e 206d 6978 696e 2063 6c61  sed in mixin cla
-000024a0: 7373 2073 686f 756c 6420 6265 2069 676e  ss should be ign
-000024b0: 6f72 6564 2e20 410d 0a23 206d 6978 696e  ored. A..# mixin
-000024c0: 2063 6c61 7373 2069 7320 6465 7465 6374   class is detect
-000024d0: 6564 2069 6620 6974 7320 6e61 6d65 2065  ed if its name e
-000024e0: 6e64 7320 7769 7468 2022 6d69 7869 6e22  nds with "mixin"
-000024f0: 2028 6361 7365 2069 6e73 656e 7369 7469   (case insensiti
-00002500: 7665 292e 0d0a 6967 6e6f 7265 2d6d 6978  ve)...ignore-mix
-00002510: 696e 2d6d 656d 6265 7273 3d79 6573 0d0a  in-members=yes..
-00002520: 0d0a 2320 5465 6c6c 7320 7768 6574 6865  ..# Tells whethe
-00002530: 7220 746f 2077 6172 6e20 6162 6f75 7420  r to warn about 
-00002540: 6d69 7373 696e 6720 6d65 6d62 6572 7320  missing members 
-00002550: 7768 656e 2074 6865 206f 776e 6572 206f  when the owner o
-00002560: 6620 7468 6520 6174 7472 6962 7574 650d  f the attribute.
-00002570: 0a23 2069 7320 696e 6665 7272 6564 2074  .# is inferred t
-00002580: 6f20 6265 204e 6f6e 652e 0d0a 6967 6e6f  o be None...igno
-00002590: 7265 2d6e 6f6e 653d 7965 730d 0a0d 0a23  re-none=yes....#
-000025a0: 2054 6869 7320 666c 6167 2063 6f6e 7472   This flag contr
-000025b0: 6f6c 7320 7768 6574 6865 7220 7079 6c69  ols whether pyli
-000025c0: 6e74 2073 686f 756c 6420 7761 726e 2061  nt should warn a
-000025d0: 626f 7574 206e 6f2d 6d65 6d62 6572 2061  bout no-member a
-000025e0: 6e64 2073 696d 696c 6172 0d0a 2320 6368  nd similar..# ch
-000025f0: 6563 6b73 2077 6865 6e65 7665 7220 616e  ecks whenever an
-00002600: 206f 7061 7175 6520 6f62 6a65 6374 2069   opaque object i
-00002610: 7320 7265 7475 726e 6564 2077 6865 6e20  s returned when 
-00002620: 696e 6665 7272 696e 672e 2054 6865 2069  inferring. The i
-00002630: 6e66 6572 656e 6365 0d0a 2320 6361 6e20  nference..# can 
-00002640: 7265 7475 726e 206d 756c 7469 706c 6520  return multiple 
-00002650: 706f 7465 6e74 6961 6c20 7265 7375 6c74  potential result
-00002660: 7320 7768 696c 6520 6576 616c 7561 7469  s while evaluati
-00002670: 6e67 2061 2050 7974 686f 6e20 6f62 6a65  ng a Python obje
-00002680: 6374 2c20 6275 740d 0a23 2073 6f6d 6520  ct, but..# some 
-00002690: 6272 616e 6368 6573 206d 6967 6874 206e  branches might n
-000026a0: 6f74 2062 6520 6576 616c 7561 7465 642c  ot be evaluated,
-000026b0: 2077 6869 6368 2072 6573 756c 7473 2069   which results i
-000026c0: 6e20 7061 7274 6961 6c20 696e 6665 7265  n partial infere
-000026d0: 6e63 652e 2049 6e0d 0a23 2074 6861 7420  nce. In..# that 
-000026e0: 6361 7365 2c20 6974 206d 6967 6874 2062  case, it might b
-000026f0: 6520 7573 6566 756c 2074 6f20 7374 696c  e useful to stil
-00002700: 6c20 656d 6974 206e 6f2d 6d65 6d62 6572  l emit no-member
-00002710: 2061 6e64 206f 7468 6572 2063 6865 636b   and other check
-00002720: 7320 666f 720d 0a23 2074 6865 2072 6573  s for..# the res
-00002730: 7420 6f66 2074 6865 2069 6e66 6572 7265  t of the inferre
-00002740: 6420 6f62 6a65 6374 732e 0d0a 6967 6e6f  d objects...igno
-00002750: 7265 2d6f 6e2d 6f70 6171 7565 2d69 6e66  re-on-opaque-inf
-00002760: 6572 656e 6365 3d79 6573 0d0a 0d0a 2320  erence=yes....# 
-00002770: 4c69 7374 206f 6620 636c 6173 7320 6e61  List of class na
-00002780: 6d65 7320 666f 7220 7768 6963 6820 6d65  mes for which me
-00002790: 6d62 6572 2061 7474 7269 6275 7465 7320  mber attributes 
-000027a0: 7368 6f75 6c64 206e 6f74 2062 6520 6368  should not be ch
-000027b0: 6563 6b65 6420 2875 7365 6675 6c0d 0a23  ecked (useful..#
-000027c0: 2066 6f72 2063 6c61 7373 6573 2077 6974   for classes wit
-000027d0: 6820 6479 6e61 6d69 6361 6c6c 7920 7365  h dynamically se
-000027e0: 7420 6174 7472 6962 7574 6573 292e 2054  t attributes). T
-000027f0: 6869 7320 7375 7070 6f72 7473 2074 6865  his supports the
-00002800: 2075 7365 206f 660d 0a23 2071 7561 6c69   use of..# quali
-00002810: 6669 6564 206e 616d 6573 2e0d 0a69 676e  fied names...ign
-00002820: 6f72 6564 2d63 6c61 7373 6573 3d6f 7074  ored-classes=opt
-00002830: 7061 7273 652e 5661 6c75 6573 2c74 6872  parse.Values,thr
-00002840: 6561 642e 5f6c 6f63 616c 2c5f 7468 7265  ead._local,_thre
-00002850: 6164 2e5f 6c6f 6361 6c0d 0a0d 0a23 204c  ad._local....# L
-00002860: 6973 7420 6f66 206d 6f64 756c 6520 6e61  ist of module na
-00002870: 6d65 7320 666f 7220 7768 6963 6820 6d65  mes for which me
-00002880: 6d62 6572 2061 7474 7269 6275 7465 7320  mber attributes 
-00002890: 7368 6f75 6c64 206e 6f74 2062 6520 6368  should not be ch
-000028a0: 6563 6b65 640d 0a23 2028 7573 6566 756c  ecked..# (useful
-000028b0: 2066 6f72 206d 6f64 756c 6573 2f70 726f   for modules/pro
-000028c0: 6a65 6374 7320 7768 6572 6520 6e61 6d65  jects where name
-000028d0: 7370 6163 6573 2061 7265 206d 616e 6970  spaces are manip
-000028e0: 756c 6174 6564 2064 7572 696e 6720 7275  ulated during ru
-000028f0: 6e74 696d 650d 0a23 2061 6e64 2074 6875  ntime..# and thu
-00002900: 7320 6578 6973 7469 6e67 206d 656d 6265  s existing membe
-00002910: 7220 6174 7472 6962 7574 6573 2063 616e  r attributes can
-00002920: 6e6f 7420 6265 2064 6564 7563 6564 2062  not be deduced b
-00002930: 7920 7374 6174 6963 2061 6e61 6c79 7369  y static analysi
-00002940: 7329 2e20 4974 0d0a 2320 7375 7070 6f72  s). It..# suppor
-00002950: 7473 2071 7561 6c69 6669 6564 206d 6f64  ts qualified mod
-00002960: 756c 6520 6e61 6d65 732c 2061 7320 7765  ule names, as we
-00002970: 6c6c 2061 7320 556e 6978 2070 6174 7465  ll as Unix patte
-00002980: 726e 206d 6174 6368 696e 672e 0d0a 6967  rn matching...ig
-00002990: 6e6f 7265 642d 6d6f 6475 6c65 733d 0d0a  nored-modules=..
-000029a0: 0d0a 2320 5368 6f77 2061 2068 696e 7420  ..# Show a hint 
-000029b0: 7769 7468 2070 6f73 7369 626c 6520 6e61  with possible na
-000029c0: 6d65 7320 7768 656e 2061 206d 656d 6265  mes when a membe
-000029d0: 7220 6e61 6d65 2077 6173 206e 6f74 2066  r name was not f
-000029e0: 6f75 6e64 2e20 5468 6520 6173 7065 6374  ound. The aspect
-000029f0: 0d0a 2320 6f66 2066 696e 6469 6e67 2074  ..# of finding t
-00002a00: 6865 2068 696e 7420 6973 2062 6173 6564  he hint is based
-00002a10: 206f 6e20 6564 6974 2064 6973 7461 6e63   on edit distanc
-00002a20: 652e 0d0a 6d69 7373 696e 672d 6d65 6d62  e...missing-memb
-00002a30: 6572 2d68 696e 743d 7965 730d 0a0d 0a23  er-hint=yes....#
-00002a40: 2054 6865 206d 696e 696d 756d 2065 6469   The minimum edi
-00002a50: 7420 6469 7374 616e 6365 2061 206e 616d  t distance a nam
-00002a60: 6520 7368 6f75 6c64 2068 6176 6520 696e  e should have in
-00002a70: 206f 7264 6572 2074 6f20 6265 2063 6f6e   order to be con
-00002a80: 7369 6465 7265 6420 610d 0a23 2073 696d  sidered a..# sim
-00002a90: 696c 6172 206d 6174 6368 2066 6f72 2061  ilar match for a
-00002aa0: 206d 6973 7369 6e67 206d 656d 6265 7220   missing member 
-00002ab0: 6e61 6d65 2e0d 0a6d 6973 7369 6e67 2d6d  name...missing-m
-00002ac0: 656d 6265 722d 6869 6e74 2d64 6973 7461  ember-hint-dista
-00002ad0: 6e63 653d 310d 0a0d 0a23 2054 6865 2074  nce=1....# The t
-00002ae0: 6f74 616c 206e 756d 6265 7220 6f66 2073  otal number of s
-00002af0: 696d 696c 6172 206e 616d 6573 2074 6861  imilar names tha
-00002b00: 7420 7368 6f75 6c64 2062 6520 7461 6b65  t should be take
-00002b10: 6e20 696e 2063 6f6e 7369 6465 7261 7469  n in considerati
-00002b20: 6f6e 2077 6865 6e0d 0a23 2073 686f 7769  on when..# showi
-00002b30: 6e67 2061 2068 696e 7420 666f 7220 6120  ng a hint for a 
-00002b40: 6d69 7373 696e 6720 6d65 6d62 6572 2e0d  missing member..
-00002b50: 0a6d 6973 7369 6e67 2d6d 656d 6265 722d  .missing-member-
-00002b60: 6d61 782d 6368 6f69 6365 733d 310d 0a0d  max-choices=1...
-00002b70: 0a23 204c 6973 7420 6f66 2064 6563 6f72  .# List of decor
-00002b80: 6174 6f72 7320 7468 6174 2063 6861 6e67  ators that chang
-00002b90: 6520 7468 6520 7369 676e 6174 7572 6520  e the signature 
-00002ba0: 6f66 2061 2064 6563 6f72 6174 6564 2066  of a decorated f
-00002bb0: 756e 6374 696f 6e2e 0d0a 7369 676e 6174  unction...signat
-00002bc0: 7572 652d 6d75 7461 746f 7273 3d0d 0a0d  ure-mutators=...
-00002bd0: 0a0d 0a5b 5349 4d49 4c41 5249 5449 4553  ...[SIMILARITIES
-00002be0: 5d0d 0a0d 0a23 2049 676e 6f72 6520 636f  ]....# Ignore co
-00002bf0: 6d6d 656e 7473 2077 6865 6e20 636f 6d70  mments when comp
-00002c00: 7574 696e 6720 7369 6d69 6c61 7269 7469  uting similariti
-00002c10: 6573 2e0d 0a69 676e 6f72 652d 636f 6d6d  es...ignore-comm
-00002c20: 656e 7473 3d79 6573 0d0a 0d0a 2320 4967  ents=yes....# Ig
-00002c30: 6e6f 7265 2064 6f63 7374 7269 6e67 7320  nore docstrings 
-00002c40: 7768 656e 2063 6f6d 7075 7469 6e67 2073  when computing s
-00002c50: 696d 696c 6172 6974 6965 732e 0d0a 6967  imilarities...ig
-00002c60: 6e6f 7265 2d64 6f63 7374 7269 6e67 733d  nore-docstrings=
-00002c70: 7965 730d 0a0d 0a23 2049 676e 6f72 6520  yes....# Ignore 
-00002c80: 696d 706f 7274 7320 7768 656e 2063 6f6d  imports when com
-00002c90: 7075 7469 6e67 2073 696d 696c 6172 6974  puting similarit
-00002ca0: 6965 732e 0d0a 6967 6e6f 7265 2d69 6d70  ies...ignore-imp
-00002cb0: 6f72 7473 3d6e 6f0d 0a0d 0a23 204d 696e  orts=no....# Min
-00002cc0: 696d 756d 206c 696e 6573 206e 756d 6265  imum lines numbe
-00002cd0: 7220 6f66 2061 2073 696d 696c 6172 6974  r of a similarit
-00002ce0: 792e 0d0a 6d69 6e2d 7369 6d69 6c61 7269  y...min-similari
-00002cf0: 7479 2d6c 696e 6573 3d34 0d0a 0d0a 0d0a  ty-lines=4......
-00002d00: 5b4d 4953 4345 4c4c 414e 454f 5553 5d0d  [MISCELLANEOUS].
-00002d10: 0a0d 0a23 204c 6973 7420 6f66 206e 6f74  ...# List of not
-00002d20: 6520 7461 6773 2074 6f20 7461 6b65 2069  e tags to take i
-00002d30: 6e20 636f 6e73 6964 6572 6174 696f 6e2c  n consideration,
-00002d40: 2073 6570 6172 6174 6564 2062 7920 6120   separated by a 
-00002d50: 636f 6d6d 612e 0d0a 6e6f 7465 733d 4649  comma...notes=FI
-00002d60: 584d 452c 0d0a 2020 2020 2020 5858 582c  XME,..      XXX,
-00002d70: 0d0a 2020 2020 2020 544f 444f 0d0a 0d0a  ..      TODO....
-00002d80: 2320 5265 6775 6c61 7220 6578 7072 6573  # Regular expres
-00002d90: 7369 6f6e 206f 6620 6e6f 7465 2074 6167  sion of note tag
-00002da0: 7320 746f 2074 616b 6520 696e 2063 6f6e  s to take in con
-00002db0: 7369 6465 7261 7469 6f6e 2e0d 0a23 6e6f  sideration...#no
-00002dc0: 7465 732d 7267 783d 0d0a 0d0a 0d0a 5b42  tes-rgx=......[B
-00002dd0: 4153 4943 5d0d 0a0d 0a23 204e 616d 696e  ASIC]....# Namin
-00002de0: 6720 7374 796c 6520 6d61 7463 6869 6e67  g style matching
-00002df0: 2063 6f72 7265 6374 2061 7267 756d 656e   correct argumen
-00002e00: 7420 6e61 6d65 732e 0d0a 6172 6775 6d65  t names...argume
-00002e10: 6e74 2d6e 616d 696e 672d 7374 796c 653d  nt-naming-style=
-00002e20: 736e 616b 655f 6361 7365 0d0a 0d0a 2320  snake_case....# 
-00002e30: 5265 6775 6c61 7220 6578 7072 6573 7369  Regular expressi
-00002e40: 6f6e 206d 6174 6368 696e 6720 636f 7272  on matching corr
-00002e50: 6563 7420 6172 6775 6d65 6e74 206e 616d  ect argument nam
-00002e60: 6573 2e20 4f76 6572 7269 6465 7320 6172  es. Overrides ar
-00002e70: 6775 6d65 6e74 2d0d 0a23 206e 616d 696e  gument-..# namin
-00002e80: 672d 7374 796c 652e 0d0a 2361 7267 756d  g-style...#argum
-00002e90: 656e 742d 7267 783d 0d0a 0d0a 2320 4e61  ent-rgx=....# Na
-00002ea0: 6d69 6e67 2073 7479 6c65 206d 6174 6368  ming style match
-00002eb0: 696e 6720 636f 7272 6563 7420 6174 7472  ing correct attr
-00002ec0: 6962 7574 6520 6e61 6d65 732e 0d0a 6174  ibute names...at
-00002ed0: 7472 2d6e 616d 696e 672d 7374 796c 653d  tr-naming-style=
-00002ee0: 736e 616b 655f 6361 7365 0d0a 0d0a 2320  snake_case....# 
-00002ef0: 5265 6775 6c61 7220 6578 7072 6573 7369  Regular expressi
-00002f00: 6f6e 206d 6174 6368 696e 6720 636f 7272  on matching corr
-00002f10: 6563 7420 6174 7472 6962 7574 6520 6e61  ect attribute na
-00002f20: 6d65 732e 204f 7665 7272 6964 6573 2061  mes. Overrides a
-00002f30: 7474 722d 6e61 6d69 6e67 2d0d 0a23 2073  ttr-naming-..# s
-00002f40: 7479 6c65 2e0d 0a23 6174 7472 2d72 6778  tyle...#attr-rgx
-00002f50: 3d0d 0a0d 0a23 2042 6164 2076 6172 6961  =....# Bad varia
-00002f60: 626c 6520 6e61 6d65 7320 7768 6963 6820  ble names which 
-00002f70: 7368 6f75 6c64 2061 6c77 6179 7320 6265  should always be
-00002f80: 2072 6566 7573 6564 2c20 7365 7061 7261   refused, separa
-00002f90: 7465 6420 6279 2061 2063 6f6d 6d61 2e0d  ted by a comma..
-00002fa0: 0a62 6164 2d6e 616d 6573 3d66 6f6f 2c0d  .bad-names=foo,.
-00002fb0: 0a20 2020 2020 2020 2020 2062 6172 2c0d  .          bar,.
-00002fc0: 0a20 2020 2020 2020 2020 2062 617a 2c0d  .          baz,.
-00002fd0: 0a20 2020 2020 2020 2020 2074 6f74 6f2c  .          toto,
-00002fe0: 0d0a 2020 2020 2020 2020 2020 7475 7475  ..          tutu
-00002ff0: 2c0d 0a20 2020 2020 2020 2020 2074 6174  ,..          tat
-00003000: 610d 0a0d 0a23 2042 6164 2076 6172 6961  a....# Bad varia
-00003010: 626c 6520 6e61 6d65 7320 7265 6765 7865  ble names regexe
-00003020: 732c 2073 6570 6172 6174 6564 2062 7920  s, separated by 
-00003030: 6120 636f 6d6d 612e 2049 6620 6e61 6d65  a comma. If name
-00003040: 7320 6d61 7463 6820 616e 7920 7265 6765  s match any rege
-00003050: 782c 0d0a 2320 7468 6579 2077 696c 6c20  x,..# they will 
-00003060: 616c 7761 7973 2062 6520 7265 6675 7365  always be refuse
-00003070: 640d 0a62 6164 2d6e 616d 6573 2d72 6778  d..bad-names-rgx
-00003080: 733d 0d0a 0d0a 2320 4e61 6d69 6e67 2073  s=....# Naming s
-00003090: 7479 6c65 206d 6174 6368 696e 6720 636f  tyle matching co
-000030a0: 7272 6563 7420 636c 6173 7320 6174 7472  rrect class attr
-000030b0: 6962 7574 6520 6e61 6d65 732e 0d0a 636c  ibute names...cl
-000030c0: 6173 732d 6174 7472 6962 7574 652d 6e61  ass-attribute-na
-000030d0: 6d69 6e67 2d73 7479 6c65 3d61 6e79 0d0a  ming-style=any..
-000030e0: 0d0a 2320 5265 6775 6c61 7220 6578 7072  ..# Regular expr
-000030f0: 6573 7369 6f6e 206d 6174 6368 696e 6720  ession matching 
-00003100: 636f 7272 6563 7420 636c 6173 7320 6174  correct class at
-00003110: 7472 6962 7574 6520 6e61 6d65 732e 204f  tribute names. O
-00003120: 7665 7272 6964 6573 2063 6c61 7373 2d0d  verrides class-.
-00003130: 0a23 2061 7474 7269 6275 7465 2d6e 616d  .# attribute-nam
-00003140: 696e 672d 7374 796c 652e 0d0a 2363 6c61  ing-style...#cla
-00003150: 7373 2d61 7474 7269 6275 7465 2d72 6778  ss-attribute-rgx
-00003160: 3d0d 0a0d 0a23 204e 616d 696e 6720 7374  =....# Naming st
-00003170: 796c 6520 6d61 7463 6869 6e67 2063 6f72  yle matching cor
-00003180: 7265 6374 2063 6c61 7373 206e 616d 6573  rect class names
-00003190: 2e0d 0a63 6c61 7373 2d6e 616d 696e 672d  ...class-naming-
-000031a0: 7374 796c 653d 5061 7363 616c 4361 7365  style=PascalCase
-000031b0: 0d0a 0d0a 2320 5265 6775 6c61 7220 6578  ....# Regular ex
-000031c0: 7072 6573 7369 6f6e 206d 6174 6368 696e  pression matchin
-000031d0: 6720 636f 7272 6563 7420 636c 6173 7320  g correct class 
-000031e0: 6e61 6d65 732e 204f 7665 7272 6964 6573  names. Overrides
-000031f0: 2063 6c61 7373 2d6e 616d 696e 672d 0d0a   class-naming-..
-00003200: 2320 7374 796c 652e 0d0a 2363 6c61 7373  # style...#class
-00003210: 2d72 6778 3d0d 0a0d 0a23 204e 616d 696e  -rgx=....# Namin
-00003220: 6720 7374 796c 6520 6d61 7463 6869 6e67  g style matching
-00003230: 2063 6f72 7265 6374 2063 6f6e 7374 616e   correct constan
-00003240: 7420 6e61 6d65 732e 0d0a 636f 6e73 742d  t names...const-
-00003250: 6e61 6d69 6e67 2d73 7479 6c65 3d55 5050  naming-style=UPP
-00003260: 4552 5f43 4153 450d 0a0d 0a23 2052 6567  ER_CASE....# Reg
-00003270: 756c 6172 2065 7870 7265 7373 696f 6e20  ular expression 
-00003280: 6d61 7463 6869 6e67 2063 6f72 7265 6374  matching correct
-00003290: 2063 6f6e 7374 616e 7420 6e61 6d65 732e   constant names.
-000032a0: 204f 7665 7272 6964 6573 2063 6f6e 7374   Overrides const
-000032b0: 2d6e 616d 696e 672d 0d0a 2320 7374 796c  -naming-..# styl
-000032c0: 652e 0d0a 2363 6f6e 7374 2d72 6778 3d0d  e...#const-rgx=.
-000032d0: 0a0d 0a23 204d 696e 696d 756d 206c 696e  ...# Minimum lin
-000032e0: 6520 6c65 6e67 7468 2066 6f72 2066 756e  e length for fun
-000032f0: 6374 696f 6e73 2f63 6c61 7373 6573 2074  ctions/classes t
-00003300: 6861 7420 7265 7175 6972 6520 646f 6373  hat require docs
-00003310: 7472 696e 6773 2c20 7368 6f72 7465 720d  trings, shorter.
-00003320: 0a23 206f 6e65 7320 6172 6520 6578 656d  .# ones are exem
-00003330: 7074 2e0d 0a64 6f63 7374 7269 6e67 2d6d  pt...docstring-m
-00003340: 696e 2d6c 656e 6774 683d 2d31 0d0a 0d0a  in-length=-1....
-00003350: 2320 4e61 6d69 6e67 2073 7479 6c65 206d  # Naming style m
-00003360: 6174 6368 696e 6720 636f 7272 6563 7420  atching correct 
-00003370: 6675 6e63 7469 6f6e 206e 616d 6573 2e0d  function names..
-00003380: 0a66 756e 6374 696f 6e2d 6e61 6d69 6e67  .function-naming
-00003390: 2d73 7479 6c65 3d73 6e61 6b65 5f63 6173  -style=snake_cas
-000033a0: 650d 0a0d 0a23 2052 6567 756c 6172 2065  e....# Regular e
-000033b0: 7870 7265 7373 696f 6e20 6d61 7463 6869  xpression matchi
-000033c0: 6e67 2063 6f72 7265 6374 2066 756e 6374  ng correct funct
-000033d0: 696f 6e20 6e61 6d65 732e 204f 7665 7272  ion names. Overr
-000033e0: 6964 6573 2066 756e 6374 696f 6e2d 0d0a  ides function-..
-000033f0: 2320 6e61 6d69 6e67 2d73 7479 6c65 2e0d  # naming-style..
-00003400: 0a23 6675 6e63 7469 6f6e 2d72 6778 3d0d  .#function-rgx=.
-00003410: 0a0d 0a23 2047 6f6f 6420 7661 7269 6162  ...# Good variab
-00003420: 6c65 206e 616d 6573 2077 6869 6368 2073  le names which s
-00003430: 686f 756c 6420 616c 7761 7973 2062 6520  hould always be 
-00003440: 6163 6365 7074 6564 2c20 7365 7061 7261  accepted, separa
-00003450: 7465 6420 6279 2061 2063 6f6d 6d61 2e0d  ted by a comma..
-00003460: 0a67 6f6f 642d 6e61 6d65 733d 692c 0d0a  .good-names=i,..
-00003470: 2020 2020 2020 2020 2020 206a 2c0d 0a20             j,.. 
-00003480: 2020 2020 2020 2020 2020 6b2c 0d0a 2020            k,..  
-00003490: 2020 2020 2020 2020 2065 782c 0d0a 2020           ex,..  
-000034a0: 2020 2020 2020 2020 2052 756e 2c0d 0a20           Run,.. 
-000034b0: 2020 2020 2020 2020 2020 6279 2c0d 0a20            by,.. 
-000034c0: 2020 2020 2020 2020 2020 742c 0d0a 2020            t,..  
-000034d0: 2020 2020 2020 2020 205f 2c0d 0a20 2020           _,..   
-000034e0: 2020 2020 2020 2020 6466 2c0d 0a20 2020          df,..   
-000034f0: 2020 2020 2020 2020 6473 0d0a 0d0a 2320          ds....# 
-00003500: 476f 6f64 2076 6172 6961 626c 6520 6e61  Good variable na
-00003510: 6d65 7320 7265 6765 7865 732c 2073 6570  mes regexes, sep
-00003520: 6172 6174 6564 2062 7920 6120 636f 6d6d  arated by a comm
-00003530: 612e 2049 6620 6e61 6d65 7320 6d61 7463  a. If names matc
-00003540: 6820 616e 7920 7265 6765 782c 0d0a 2320  h any regex,..# 
-00003550: 7468 6579 2077 696c 6c20 616c 7761 7973  they will always
-00003560: 2062 6520 6163 6365 7074 6564 0d0a 676f   be accepted..go
-00003570: 6f64 2d6e 616d 6573 2d72 6778 733d 0d0a  od-names-rgxs=..
-00003580: 0d0a 2320 496e 636c 7564 6520 6120 6869  ..# Include a hi
-00003590: 6e74 2066 6f72 2074 6865 2063 6f72 7265  nt for the corre
-000035a0: 6374 206e 616d 696e 6720 666f 726d 6174  ct naming format
-000035b0: 2077 6974 6820 696e 7661 6c69 642d 6e61   with invalid-na
-000035c0: 6d65 2e0d 0a69 6e63 6c75 6465 2d6e 616d  me...include-nam
-000035d0: 696e 672d 6869 6e74 3d6e 6f0d 0a0d 0a23  ing-hint=no....#
-000035e0: 204e 616d 696e 6720 7374 796c 6520 6d61   Naming style ma
-000035f0: 7463 6869 6e67 2063 6f72 7265 6374 2069  tching correct i
-00003600: 6e6c 696e 6520 6974 6572 6174 696f 6e20  nline iteration 
-00003610: 6e61 6d65 732e 0d0a 696e 6c69 6e65 7661  names...inlineva
-00003620: 722d 6e61 6d69 6e67 2d73 7479 6c65 3d61  r-naming-style=a
-00003630: 6e79 0d0a 0d0a 2320 5265 6775 6c61 7220  ny....# Regular 
-00003640: 6578 7072 6573 7369 6f6e 206d 6174 6368  expression match
-00003650: 696e 6720 636f 7272 6563 7420 696e 6c69  ing correct inli
-00003660: 6e65 2069 7465 7261 7469 6f6e 206e 616d  ne iteration nam
-00003670: 6573 2e20 4f76 6572 7269 6465 730d 0a23  es. Overrides..#
-00003680: 2069 6e6c 696e 6576 6172 2d6e 616d 696e   inlinevar-namin
-00003690: 672d 7374 796c 652e 0d0a 2369 6e6c 696e  g-style...#inlin
-000036a0: 6576 6172 2d72 6778 3d0d 0a0d 0a23 204e  evar-rgx=....# N
-000036b0: 616d 696e 6720 7374 796c 6520 6d61 7463  aming style matc
-000036c0: 6869 6e67 2063 6f72 7265 6374 206d 6574  hing correct met
-000036d0: 686f 6420 6e61 6d65 732e 0d0a 6d65 7468  hod names...meth
-000036e0: 6f64 2d6e 616d 696e 672d 7374 796c 653d  od-naming-style=
-000036f0: 736e 616b 655f 6361 7365 0d0a 0d0a 2320  snake_case....# 
-00003700: 5265 6775 6c61 7220 6578 7072 6573 7369  Regular expressi
-00003710: 6f6e 206d 6174 6368 696e 6720 636f 7272  on matching corr
-00003720: 6563 7420 6d65 7468 6f64 206e 616d 6573  ect method names
-00003730: 2e20 4f76 6572 7269 6465 7320 6d65 7468  . Overrides meth
-00003740: 6f64 2d6e 616d 696e 672d 0d0a 2320 7374  od-naming-..# st
-00003750: 796c 652e 0d0a 236d 6574 686f 642d 7267  yle...#method-rg
-00003760: 783d 0d0a 0d0a 2320 4e61 6d69 6e67 2073  x=....# Naming s
-00003770: 7479 6c65 206d 6174 6368 696e 6720 636f  tyle matching co
-00003780: 7272 6563 7420 6d6f 6475 6c65 206e 616d  rrect module nam
-00003790: 6573 2e0d 0a6d 6f64 756c 652d 6e61 6d69  es...module-nami
-000037a0: 6e67 2d73 7479 6c65 3d73 6e61 6b65 5f63  ng-style=snake_c
-000037b0: 6173 650d 0a0d 0a23 2052 6567 756c 6172  ase....# Regular
-000037c0: 2065 7870 7265 7373 696f 6e20 6d61 7463   expression matc
-000037d0: 6869 6e67 2063 6f72 7265 6374 206d 6f64  hing correct mod
-000037e0: 756c 6520 6e61 6d65 732e 204f 7665 7272  ule names. Overr
-000037f0: 6964 6573 206d 6f64 756c 652d 6e61 6d69  ides module-nami
-00003800: 6e67 2d0d 0a23 2073 7479 6c65 2e0d 0a23  ng-..# style...#
-00003810: 6d6f 6475 6c65 2d72 6778 3d0d 0a0d 0a23  module-rgx=....#
-00003820: 2043 6f6c 6f6e 2d64 656c 696d 6974 6564   Colon-delimited
-00003830: 2073 6574 7320 6f66 206e 616d 6573 2074   sets of names t
-00003840: 6861 7420 6465 7465 726d 696e 6520 6561  hat determine ea
-00003850: 6368 206f 7468 6572 2773 206e 616d 696e  ch other's namin
-00003860: 6720 7374 796c 6520 7768 656e 0d0a 2320  g style when..# 
-00003870: 7468 6520 6e61 6d65 2072 6567 6578 6573  the name regexes
-00003880: 2061 6c6c 6f77 2073 6576 6572 616c 2073   allow several s
-00003890: 7479 6c65 732e 0d0a 6e61 6d65 2d67 726f  tyles...name-gro
-000038a0: 7570 3d0d 0a0d 0a23 2052 6567 756c 6172  up=....# Regular
-000038b0: 2065 7870 7265 7373 696f 6e20 7768 6963   expression whic
-000038c0: 6820 7368 6f75 6c64 206f 6e6c 7920 6d61  h should only ma
-000038d0: 7463 6820 6675 6e63 7469 6f6e 206f 7220  tch function or 
-000038e0: 636c 6173 7320 6e61 6d65 7320 7468 6174  class names that
-000038f0: 2064 6f0d 0a23 206e 6f74 2072 6571 7569   do..# not requi
-00003900: 7265 2061 2064 6f63 7374 7269 6e67 2e0d  re a docstring..
-00003910: 0a6e 6f2d 646f 6373 7472 696e 672d 7267  .no-docstring-rg
-00003920: 783d 5e5f 0d0a 0d0a 2320 4c69 7374 206f  x=^_....# List o
-00003930: 6620 6465 636f 7261 746f 7273 2074 6861  f decorators tha
-00003940: 7420 7072 6f64 7563 6520 7072 6f70 6572  t produce proper
-00003950: 7469 6573 2c20 7375 6368 2061 7320 6162  ties, such as ab
-00003960: 632e 6162 7374 7261 6374 7072 6f70 6572  c.abstractproper
-00003970: 7479 2e20 4164 640d 0a23 2074 6f20 7468  ty. Add..# to th
-00003980: 6973 206c 6973 7420 746f 2072 6567 6973  is list to regis
-00003990: 7465 7220 6f74 6865 7220 6465 636f 7261  ter other decora
-000039a0: 746f 7273 2074 6861 7420 7072 6f64 7563  tors that produc
-000039b0: 6520 7661 6c69 6420 7072 6f70 6572 7469  e valid properti
-000039c0: 6573 2e0d 0a23 2054 6865 7365 2064 6563  es...# These dec
-000039d0: 6f72 6174 6f72 7320 6172 6520 7461 6b65  orators are take
-000039e0: 6e20 696e 2063 6f6e 7369 6465 7261 7469  n in considerati
-000039f0: 6f6e 206f 6e6c 7920 666f 7220 696e 7661  on only for inva
-00003a00: 6c69 642d 6e61 6d65 2e0d 0a70 726f 7065  lid-name...prope
-00003a10: 7274 792d 636c 6173 7365 733d 6162 632e  rty-classes=abc.
-00003a20: 6162 7374 7261 6374 7072 6f70 6572 7479  abstractproperty
-00003a30: 0d0a 0d0a 2320 4e61 6d69 6e67 2073 7479  ....# Naming sty
-00003a40: 6c65 206d 6174 6368 696e 6720 636f 7272  le matching corr
-00003a50: 6563 7420 7661 7269 6162 6c65 206e 616d  ect variable nam
-00003a60: 6573 2e0d 0a76 6172 6961 626c 652d 6e61  es...variable-na
-00003a70: 6d69 6e67 2d73 7479 6c65 3d73 6e61 6b65  ming-style=snake
-00003a80: 5f63 6173 650d 0a0d 0a23 2052 6567 756c  _case....# Regul
-00003a90: 6172 2065 7870 7265 7373 696f 6e20 6d61  ar expression ma
-00003aa0: 7463 6869 6e67 2063 6f72 7265 6374 2076  tching correct v
-00003ab0: 6172 6961 626c 6520 6e61 6d65 732e 204f  ariable names. O
-00003ac0: 7665 7272 6964 6573 2076 6172 6961 626c  verrides variabl
-00003ad0: 652d 0d0a 2320 6e61 6d69 6e67 2d73 7479  e-..# naming-sty
-00003ae0: 6c65 2e0d 0a23 7661 7269 6162 6c65 2d72  le...#variable-r
-00003af0: 6778 3d0d 0a0d 0a0d 0a5b 4c4f 4747 494e  gx=......[LOGGIN
-00003b00: 475d 0d0a 0d0a 2320 5468 6520 7479 7065  G]....# The type
-00003b10: 206f 6620 7374 7269 6e67 2066 6f72 6d61   of string forma
-00003b20: 7474 696e 6720 7468 6174 206c 6f67 6769  tting that loggi
-00003b30: 6e67 206d 6574 686f 6473 2064 6f2e 2060  ng methods do. `
-00003b40: 6f6c 6460 206d 6561 6e73 2075 7369 6e67  old` means using
-00003b50: 2025 0d0a 2320 666f 726d 6174 7469 6e67   %..# formatting
-00003b60: 2c20 606e 6577 6020 6973 2066 6f72 2060  , `new` is for `
-00003b70: 7b7d 6020 666f 726d 6174 7469 6e67 2e0d  {}` formatting..
-00003b80: 0a6c 6f67 6769 6e67 2d66 6f72 6d61 742d  .logging-format-
-00003b90: 7374 796c 653d 6f6c 640d 0a0d 0a23 204c  style=old....# L
-00003ba0: 6f67 6769 6e67 206d 6f64 756c 6573 2074  ogging modules t
-00003bb0: 6f20 6368 6563 6b20 7468 6174 2074 6865  o check that the
-00003bc0: 2073 7472 696e 6720 666f 726d 6174 2061   string format a
-00003bd0: 7267 756d 656e 7473 2061 7265 2069 6e20  rguments are in 
-00003be0: 6c6f 6767 696e 670d 0a23 2066 756e 6374  logging..# funct
-00003bf0: 696f 6e20 7061 7261 6d65 7465 7220 666f  ion parameter fo
-00003c00: 726d 6174 2e0d 0a6c 6f67 6769 6e67 2d6d  rmat...logging-m
-00003c10: 6f64 756c 6573 3d6c 6f67 6769 6e67 0d0a  odules=logging..
-00003c20: 0d0a 0d0a 5b53 5452 494e 475d 0d0a 0d0a  ....[STRING]....
-00003c30: 2320 5468 6973 2066 6c61 6720 636f 6e74  # This flag cont
-00003c40: 726f 6c73 2077 6865 7468 6572 2069 6e63  rols whether inc
-00003c50: 6f6e 7369 7374 656e 742d 7175 6f74 6573  onsistent-quotes
-00003c60: 2067 656e 6572 6174 6573 2061 2077 6172   generates a war
-00003c70: 6e69 6e67 2077 6865 6e20 7468 650d 0a23  ning when the..#
-00003c80: 2063 6861 7261 6374 6572 2075 7365 6420   character used 
-00003c90: 6173 2061 2071 756f 7465 2064 656c 696d  as a quote delim
-00003ca0: 6974 6572 2069 7320 7573 6564 2069 6e63  iter is used inc
-00003cb0: 6f6e 7369 7374 656e 746c 7920 7769 7468  onsistently with
-00003cc0: 696e 2061 206d 6f64 756c 652e 0d0a 6368  in a module...ch
-00003cd0: 6563 6b2d 7175 6f74 652d 636f 6e73 6973  eck-quote-consis
-00003ce0: 7465 6e63 793d 6e6f 0d0a 0d0a 2320 5468  tency=no....# Th
-00003cf0: 6973 2066 6c61 6720 636f 6e74 726f 6c73  is flag controls
-00003d00: 2077 6865 7468 6572 2074 6865 2069 6d70   whether the imp
-00003d10: 6c69 6369 742d 7374 722d 636f 6e63 6174  licit-str-concat
-00003d20: 2073 686f 756c 6420 6765 6e65 7261 7465   should generate
-00003d30: 2061 2077 6172 6e69 6e67 0d0a 2320 6f6e   a warning..# on
-00003d40: 2069 6d70 6c69 6369 7420 7374 7269 6e67   implicit string
-00003d50: 2063 6f6e 6361 7465 6e61 7469 6f6e 2069   concatenation i
-00003d60: 6e20 7365 7175 656e 6365 7320 6465 6669  n sequences defi
-00003d70: 6e65 6420 6f76 6572 2073 6576 6572 616c  ned over several
-00003d80: 206c 696e 6573 2e0d 0a63 6865 636b 2d73   lines...check-s
-00003d90: 7472 2d63 6f6e 6361 742d 6f76 6572 2d6c  tr-concat-over-l
-00003da0: 696e 652d 6a75 6d70 733d 6e6f 0d0a 0d0a  ine-jumps=no....
-00003db0: 0d0a 5b44 4553 4947 4e5d 0d0a 0d0a 2320  ..[DESIGN]....# 
-00003dc0: 4d61 7869 6d75 6d20 6e75 6d62 6572 206f  Maximum number o
-00003dd0: 6620 6172 6775 6d65 6e74 7320 666f 7220  f arguments for 
-00003de0: 6675 6e63 7469 6f6e 202f 206d 6574 686f  function / metho
-00003df0: 642e 0d0a 6d61 782d 6172 6773 3d35 0d0a  d...max-args=5..
-00003e00: 0d0a 2320 4d61 7869 6d75 6d20 6e75 6d62  ..# Maximum numb
-00003e10: 6572 206f 6620 6174 7472 6962 7574 6573  er of attributes
-00003e20: 2066 6f72 2061 2063 6c61 7373 2028 7365   for a class (se
-00003e30: 6520 5230 3930 3229 2e0d 0a6d 6178 2d61  e R0902)...max-a
-00003e40: 7474 7269 6275 7465 733d 370d 0a0d 0a23  ttributes=7....#
-00003e50: 204d 6178 696d 756d 206e 756d 6265 7220   Maximum number 
-00003e60: 6f66 2062 6f6f 6c65 616e 2065 7870 7265  of boolean expre
-00003e70: 7373 696f 6e73 2069 6e20 616e 2069 6620  ssions in an if 
-00003e80: 7374 6174 656d 656e 7420 2873 6565 2052  statement (see R
-00003e90: 3039 3136 292e 0d0a 6d61 782d 626f 6f6c  0916)...max-bool
-00003ea0: 2d65 7870 723d 350d 0a0d 0a23 204d 6178  -expr=5....# Max
-00003eb0: 696d 756d 206e 756d 6265 7220 6f66 2062  imum number of b
-00003ec0: 7261 6e63 6820 666f 7220 6675 6e63 7469  ranch for functi
-00003ed0: 6f6e 202f 206d 6574 686f 6420 626f 6479  on / method body
-00003ee0: 2e0d 0a6d 6178 2d62 7261 6e63 6865 733d  ...max-branches=
-00003ef0: 3132 0d0a 0d0a 2320 4d61 7869 6d75 6d20  12....# Maximum 
-00003f00: 6e75 6d62 6572 206f 6620 6c6f 6361 6c73  number of locals
-00003f10: 2066 6f72 2066 756e 6374 696f 6e20 2f20   for function / 
-00003f20: 6d65 7468 6f64 2062 6f64 792e 0d0a 6d61  method body...ma
-00003f30: 782d 6c6f 6361 6c73 3d31 350d 0a0d 0a23  x-locals=15....#
-00003f40: 204d 6178 696d 756d 206e 756d 6265 7220   Maximum number 
-00003f50: 6f66 2070 6172 656e 7473 2066 6f72 2061  of parents for a
-00003f60: 2063 6c61 7373 2028 7365 6520 5230 3930   class (see R090
-00003f70: 3129 2e0d 0a6d 6178 2d70 6172 656e 7473  1)...max-parents
-00003f80: 3d37 0d0a 0d0a 2320 4d61 7869 6d75 6d20  =7....# Maximum 
-00003f90: 6e75 6d62 6572 206f 6620 7075 626c 6963  number of public
-00003fa0: 206d 6574 686f 6473 2066 6f72 2061 2063   methods for a c
-00003fb0: 6c61 7373 2028 7365 6520 5230 3930 3429  lass (see R0904)
-00003fc0: 2e0d 0a6d 6178 2d70 7562 6c69 632d 6d65  ...max-public-me
-00003fd0: 7468 6f64 733d 3230 0d0a 0d0a 2320 4d61  thods=20....# Ma
-00003fe0: 7869 6d75 6d20 6e75 6d62 6572 206f 6620  ximum number of 
-00003ff0: 7265 7475 726e 202f 2079 6965 6c64 2066  return / yield f
-00004000: 6f72 2066 756e 6374 696f 6e20 2f20 6d65  or function / me
-00004010: 7468 6f64 2062 6f64 792e 0d0a 6d61 782d  thod body...max-
-00004020: 7265 7475 726e 733d 360d 0a0d 0a23 204d  returns=6....# M
-00004030: 6178 696d 756d 206e 756d 6265 7220 6f66  aximum number of
-00004040: 2073 7461 7465 6d65 6e74 7320 696e 2066   statements in f
-00004050: 756e 6374 696f 6e20 2f20 6d65 7468 6f64  unction / method
-00004060: 2062 6f64 792e 0d0a 6d61 782d 7374 6174   body...max-stat
-00004070: 656d 656e 7473 3d35 300d 0a0d 0a23 204d  ements=50....# M
-00004080: 696e 696d 756d 206e 756d 6265 7220 6f66  inimum number of
-00004090: 2070 7562 6c69 6320 6d65 7468 6f64 7320   public methods 
-000040a0: 666f 7220 6120 636c 6173 7320 2873 6565  for a class (see
-000040b0: 2052 3039 3033 292e 0d0a 6d69 6e2d 7075   R0903)...min-pu
-000040c0: 626c 6963 2d6d 6574 686f 6473 3d32 0d0a  blic-methods=2..
-000040d0: 0d0a 0d0a 5b49 4d50 4f52 5453 5d0d 0a0d  ....[IMPORTS]...
-000040e0: 0a23 204c 6973 7420 6f66 206d 6f64 756c  .# List of modul
-000040f0: 6573 2074 6861 7420 6361 6e20 6265 2069  es that can be i
-00004100: 6d70 6f72 7465 6420 6174 2061 6e79 206c  mported at any l
-00004110: 6576 656c 2c20 6e6f 7420 6a75 7374 2074  evel, not just t
-00004120: 6865 2074 6f70 206c 6576 656c 0d0a 2320  he top level..# 
-00004130: 6f6e 652e 0d0a 616c 6c6f 772d 616e 792d  one...allow-any-
-00004140: 696d 706f 7274 2d6c 6576 656c 3d0d 0a0d  import-level=...
-00004150: 0a23 2041 6c6c 6f77 2077 696c 6463 6172  .# Allow wildcar
-00004160: 6420 696d 706f 7274 7320 6672 6f6d 206d  d imports from m
-00004170: 6f64 756c 6573 2074 6861 7420 6465 6669  odules that defi
-00004180: 6e65 205f 5f61 6c6c 5f5f 2e0d 0a61 6c6c  ne __all__...all
-00004190: 6f77 2d77 696c 6463 6172 642d 7769 7468  ow-wildcard-with
-000041a0: 2d61 6c6c 3d6e 6f0d 0a0d 0a23 2041 6e61  -all=no....# Ana
-000041b0: 6c79 7365 2069 6d70 6f72 7420 6661 6c6c  lyse import fall
-000041c0: 6261 636b 2062 6c6f 636b 732e 2054 6869  back blocks. Thi
-000041d0: 7320 6361 6e20 6265 2075 7365 6420 746f  s can be used to
-000041e0: 2073 7570 706f 7274 2062 6f74 6820 5079   support both Py
-000041f0: 7468 6f6e 2032 2061 6e64 0d0a 2320 3320  thon 2 and..# 3 
-00004200: 636f 6d70 6174 6962 6c65 2063 6f64 652c  compatible code,
-00004210: 2077 6869 6368 206d 6561 6e73 2074 6861   which means tha
-00004220: 7420 7468 6520 626c 6f63 6b20 6d69 6768  t the block migh
-00004230: 7420 6861 7665 2063 6f64 6520 7468 6174  t have code that
-00004240: 2065 7869 7374 730d 0a23 206f 6e6c 7920   exists..# only 
-00004250: 696e 206f 6e65 206f 7220 616e 6f74 6865  in one or anothe
-00004260: 7220 696e 7465 7270 7265 7465 722c 206c  r interpreter, l
-00004270: 6561 6469 6e67 2074 6f20 6661 6c73 6520  eading to false 
-00004280: 706f 7369 7469 7665 7320 7768 656e 2061  positives when a
-00004290: 6e61 6c79 7365 642e 0d0a 616e 616c 7973  nalysed...analys
-000042a0: 652d 6661 6c6c 6261 636b 2d62 6c6f 636b  e-fallback-block
-000042b0: 733d 6e6f 0d0a 0d0a 2320 4465 7072 6563  s=no....# Deprec
-000042c0: 6174 6564 206d 6f64 756c 6573 2077 6869  ated modules whi
-000042d0: 6368 2073 686f 756c 6420 6e6f 7420 6265  ch should not be
-000042e0: 2075 7365 642c 2073 6570 6172 6174 6564   used, separated
-000042f0: 2062 7920 6120 636f 6d6d 612e 0d0a 6465   by a comma...de
-00004300: 7072 6563 6174 6564 2d6d 6f64 756c 6573  precated-modules
-00004310: 3d6f 7074 7061 7273 652c 746b 696e 7465  =optparse,tkinte
-00004320: 722e 7469 780d 0a0d 0a23 2043 7265 6174  r.tix....# Creat
-00004330: 6520 6120 6772 6170 6820 6f66 2065 7874  e a graph of ext
-00004340: 6572 6e61 6c20 6465 7065 6e64 656e 6369  ernal dependenci
-00004350: 6573 2069 6e20 7468 6520 6769 7665 6e20  es in the given 
-00004360: 6669 6c65 2028 7265 706f 7274 2052 5030  file (report RP0
-00004370: 3430 3220 6d75 7374 0d0a 2320 6e6f 7420  402 must..# not 
-00004380: 6265 2064 6973 6162 6c65 6429 2e0d 0a65  be disabled)...e
-00004390: 7874 2d69 6d70 6f72 742d 6772 6170 683d  xt-import-graph=
-000043a0: 0d0a 0d0a 2320 4372 6561 7465 2061 2067  ....# Create a g
-000043b0: 7261 7068 206f 6620 6576 6572 7920 2869  raph of every (i
-000043c0: 2e65 2e20 696e 7465 726e 616c 2061 6e64  .e. internal and
-000043d0: 2065 7874 6572 6e61 6c29 2064 6570 656e   external) depen
-000043e0: 6465 6e63 6965 7320 696e 2074 6865 0d0a  dencies in the..
-000043f0: 2320 6769 7665 6e20 6669 6c65 2028 7265  # given file (re
-00004400: 706f 7274 2052 5030 3430 3220 6d75 7374  port RP0402 must
-00004410: 206e 6f74 2062 6520 6469 7361 626c 6564   not be disabled
-00004420: 292e 0d0a 696d 706f 7274 2d67 7261 7068  )...import-graph
-00004430: 3d0d 0a0d 0a23 2043 7265 6174 6520 6120  =....# Create a 
-00004440: 6772 6170 6820 6f66 2069 6e74 6572 6e61  graph of interna
-00004450: 6c20 6465 7065 6e64 656e 6369 6573 2069  l dependencies i
-00004460: 6e20 7468 6520 6769 7665 6e20 6669 6c65  n the given file
-00004470: 2028 7265 706f 7274 2052 5030 3430 3220   (report RP0402 
-00004480: 6d75 7374 0d0a 2320 6e6f 7420 6265 2064  must..# not be d
-00004490: 6973 6162 6c65 6429 2e0d 0a69 6e74 2d69  isabled)...int-i
-000044a0: 6d70 6f72 742d 6772 6170 683d 0d0a 0d0a  mport-graph=....
-000044b0: 2320 466f 7263 6520 696d 706f 7274 206f  # Force import o
-000044c0: 7264 6572 2074 6f20 7265 636f 676e 697a  rder to recogniz
-000044d0: 6520 6120 6d6f 6475 6c65 2061 7320 7061  e a module as pa
-000044e0: 7274 206f 6620 7468 6520 7374 616e 6461  rt of the standa
-000044f0: 7264 0d0a 2320 636f 6d70 6174 6962 696c  rd..# compatibil
-00004500: 6974 7920 6c69 6272 6172 6965 732e 0d0a  ity libraries...
-00004510: 6b6e 6f77 6e2d 7374 616e 6461 7264 2d6c  known-standard-l
-00004520: 6962 7261 7279 3d0d 0a0d 0a23 2046 6f72  ibrary=....# For
-00004530: 6365 2069 6d70 6f72 7420 6f72 6465 7220  ce import order 
-00004540: 746f 2072 6563 6f67 6e69 7a65 2061 206d  to recognize a m
-00004550: 6f64 756c 6520 6173 2070 6172 7420 6f66  odule as part of
-00004560: 2061 2074 6869 7264 2070 6172 7479 206c   a third party l
-00004570: 6962 7261 7279 2e0d 0a6b 6e6f 776e 2d74  ibrary...known-t
-00004580: 6869 7264 2d70 6172 7479 3d65 6e63 6861  hird-party=encha
-00004590: 6e74 0d0a 0d0a 2320 436f 7570 6c65 7320  nt....# Couples 
-000045a0: 6f66 206d 6f64 756c 6573 2061 6e64 2070  of modules and p
-000045b0: 7265 6665 7272 6564 206d 6f64 756c 6573  referred modules
-000045c0: 2c20 7365 7061 7261 7465 6420 6279 2061  , separated by a
-000045d0: 2063 6f6d 6d61 2e0d 0a70 7265 6665 7272   comma...preferr
-000045e0: 6564 2d6d 6f64 756c 6573 3d0d 0a0d 0a0d  ed-modules=.....
-000045f0: 0a5b 434c 4153 5345 535d 0d0a 0d0a 2320  .[CLASSES]....# 
-00004600: 4c69 7374 206f 6620 6d65 7468 6f64 206e  List of method n
-00004610: 616d 6573 2075 7365 6420 746f 2064 6563  ames used to dec
-00004620: 6c61 7265 2028 692e 652e 2061 7373 6967  lare (i.e. assig
-00004630: 6e29 2069 6e73 7461 6e63 6520 6174 7472  n) instance attr
-00004640: 6962 7574 6573 2e0d 0a64 6566 696e 696e  ibutes...definin
-00004650: 672d 6174 7472 2d6d 6574 686f 6473 3d5f  g-attr-methods=_
-00004660: 5f69 6e69 745f 5f2c 0d0a 2020 2020 2020  _init__,..      
-00004670: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004680: 5f5f 6e65 775f 5f2c 0d0a 2020 2020 2020  __new__,..      
-00004690: 2020 2020 2020 2020 2020 2020 2020 2020                  
-000046a0: 7365 7455 702c 0d0a 2020 2020 2020 2020  setUp,..        
-000046b0: 2020 2020 2020 2020 2020 2020 2020 5f5f                __
-000046c0: 706f 7374 5f69 6e69 745f 5f0d 0a0d 0a23  post_init__....#
-000046d0: 204c 6973 7420 6f66 206d 656d 6265 7220   List of member 
-000046e0: 6e61 6d65 732c 2077 6869 6368 2073 686f  names, which sho
-000046f0: 756c 6420 6265 2065 7863 6c75 6465 6420  uld be excluded 
-00004700: 6672 6f6d 2074 6865 2070 726f 7465 6374  from the protect
-00004710: 6564 2061 6363 6573 730d 0a23 2077 6172  ed access..# war
-00004720: 6e69 6e67 2e0d 0a65 7863 6c75 6465 2d70  ning...exclude-p
-00004730: 726f 7465 6374 6564 3d5f 6173 6469 6374  rotected=_asdict
-00004740: 2c0d 0a20 2020 2020 2020 2020 2020 2020  ,..             
-00004750: 2020 2020 205f 6669 656c 6473 2c0d 0a20       _fields,.. 
-00004760: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004770: 205f 7265 706c 6163 652c 0d0a 2020 2020   _replace,..    
-00004780: 2020 2020 2020 2020 2020 2020 2020 5f73                _s
-00004790: 6f75 7263 652c 0d0a 2020 2020 2020 2020  ource,..        
-000047a0: 2020 2020 2020 2020 2020 5f6d 616b 650d            _make.
-000047b0: 0a0d 0a23 204c 6973 7420 6f66 2076 616c  ...# List of val
-000047c0: 6964 206e 616d 6573 2066 6f72 2074 6865  id names for the
-000047d0: 2066 6972 7374 2061 7267 756d 656e 7420   first argument 
-000047e0: 696e 2061 2063 6c61 7373 206d 6574 686f  in a class metho
-000047f0: 642e 0d0a 7661 6c69 642d 636c 6173 736d  d...valid-classm
-00004800: 6574 686f 642d 6669 7273 742d 6172 673d  ethod-first-arg=
-00004810: 636c 730d 0a0d 0a23 204c 6973 7420 6f66  cls....# List of
-00004820: 2076 616c 6964 206e 616d 6573 2066 6f72   valid names for
-00004830: 2074 6865 2066 6972 7374 2061 7267 756d   the first argum
-00004840: 656e 7420 696e 2061 206d 6574 6163 6c61  ent in a metacla
-00004850: 7373 2063 6c61 7373 206d 6574 686f 642e  ss class method.
-00004860: 0d0a 7661 6c69 642d 6d65 7461 636c 6173  ..valid-metaclas
-00004870: 732d 636c 6173 736d 6574 686f 642d 6669  s-classmethod-fi
-00004880: 7273 742d 6172 673d 636c 730d 0a0d 0a0d  rst-arg=cls.....
-00004890: 0a5b 4558 4345 5054 494f 4e53 5d0d 0a0d  .[EXCEPTIONS]...
-000048a0: 0a23 2045 7863 6570 7469 6f6e 7320 7468  .# Exceptions th
-000048b0: 6174 2077 696c 6c20 656d 6974 2061 2077  at will emit a w
-000048c0: 6172 6e69 6e67 2077 6865 6e20 6265 696e  arning when bein
-000048d0: 6720 6361 7567 6874 2e20 4465 6661 756c  g caught. Defaul
-000048e0: 7473 2074 6f0d 0a23 2022 4261 7365 4578  ts to..# "BaseEx
-000048f0: 6365 7074 696f 6e2c 2045 7863 6570 7469  ception, Excepti
-00004900: 6f6e 222e 0d0a 6f76 6572 6765 6e65 7261  on"...overgenera
-00004910: 6c2d 6578 6365 7074 696f 6e73 3d42 6173  l-exceptions=Bas
-00004920: 6545 7863 6570 7469 6f6e 2c0d 0a20 2020  eException,..   
-00004930: 2020 2020 2020 2020 2020 2020 2020 2020                  
-00004940: 2020 2020 4578 6365 7074 696f 6e0d 0a        Exception..
+00000000: 5b4d 4153 5445 525d 0a0a 2320 4120 636f  [MASTER]..# A co
+00000010: 6d6d 612d 7365 7061 7261 7465 6420 6c69  mma-separated li
+00000020: 7374 206f 6620 7061 636b 6167 6520 6f72  st of package or
+00000030: 206d 6f64 756c 6520 6e61 6d65 7320 6672   module names fr
+00000040: 6f6d 2077 6865 7265 2043 2065 7874 656e  om where C exten
+00000050: 7369 6f6e 7320 6d61 790a 2320 6265 206c  sions may.# be l
+00000060: 6f61 6465 642e 2045 7874 656e 7369 6f6e  oaded. Extension
+00000070: 7320 6172 6520 6c6f 6164 696e 6720 696e  s are loading in
+00000080: 746f 2074 6865 2061 6374 6976 6520 5079  to the active Py
+00000090: 7468 6f6e 2069 6e74 6572 7072 6574 6572  thon interpreter
+000000a0: 2061 6e64 206d 6179 0a23 2072 756e 2061   and may.# run a
+000000b0: 7262 6974 7261 7279 2063 6f64 652e 0a65  rbitrary code..e
+000000c0: 7874 656e 7369 6f6e 2d70 6b67 2d77 6869  xtension-pkg-whi
+000000d0: 7465 6c69 7374 3d0a 0a23 2053 7065 6369  telist=..# Speci
+000000e0: 6679 2061 2073 636f 7265 2074 6872 6573  fy a score thres
+000000f0: 686f 6c64 2074 6f20 6265 2065 7863 6565  hold to be excee
+00000100: 6465 6420 6265 666f 7265 2070 726f 6772  ded before progr
+00000110: 616d 2065 7869 7473 2077 6974 6820 6572  am exits with er
+00000120: 726f 722e 0a66 6169 6c2d 756e 6465 723d  ror..fail-under=
+00000130: 3130 2e30 0a0a 2320 4164 6420 6669 6c65  10.0..# Add file
+00000140: 7320 6f72 2064 6972 6563 746f 7269 6573  s or directories
+00000150: 2074 6f20 7468 6520 626c 6163 6b6c 6973   to the blacklis
+00000160: 742e 2054 6865 7920 7368 6f75 6c64 2062  t. They should b
+00000170: 6520 6261 7365 206e 616d 6573 2c20 6e6f  e base names, no
+00000180: 740a 2320 7061 7468 732e 0a69 676e 6f72  t.# paths..ignor
+00000190: 653d 4356 530a 0a23 2041 6464 2066 696c  e=CVS..# Add fil
+000001a0: 6573 206f 7220 6469 7265 6374 6f72 6965  es or directorie
+000001b0: 7320 6d61 7463 6869 6e67 2074 6865 2072  s matching the r
+000001c0: 6567 6578 2070 6174 7465 726e 7320 746f  egex patterns to
+000001d0: 2074 6865 2062 6c61 636b 6c69 7374 2e20   the blacklist. 
+000001e0: 5468 650a 2320 7265 6765 7820 6d61 7463  The.# regex matc
+000001f0: 6865 7320 6167 6169 6e73 7420 6261 7365  hes against base
+00000200: 206e 616d 6573 2c20 6e6f 7420 7061 7468   names, not path
+00000210: 732e 0a69 676e 6f72 652d 7061 7474 6572  s..ignore-patter
+00000220: 6e73 3d0a 0a23 2050 7974 686f 6e20 636f  ns=..# Python co
+00000230: 6465 2074 6f20 6578 6563 7574 652c 2075  de to execute, u
+00000240: 7375 616c 6c79 2066 6f72 2073 7973 2e70  sually for sys.p
+00000250: 6174 6820 6d61 6e69 7075 6c61 7469 6f6e  ath manipulation
+00000260: 2073 7563 6820 6173 0a23 2070 7967 746b   such as.# pygtk
+00000270: 2e72 6571 7569 7265 2829 2e0a 2369 6e69  .require()..#ini
+00000280: 742d 686f 6f6b 3d0a 0a23 2055 7365 206d  t-hook=..# Use m
+00000290: 756c 7469 706c 6520 7072 6f63 6573 7365  ultiple processe
+000002a0: 7320 746f 2073 7065 6564 2075 7020 5079  s to speed up Py
+000002b0: 6c69 6e74 2e20 5370 6563 6966 7969 6e67  lint. Specifying
+000002c0: 2030 2077 696c 6c20 6175 746f 2d64 6574   0 will auto-det
+000002d0: 6563 7420 7468 650a 2320 6e75 6d62 6572  ect the.# number
+000002e0: 206f 6620 7072 6f63 6573 736f 7273 2061   of processors a
+000002f0: 7661 696c 6162 6c65 2074 6f20 7573 652e  vailable to use.
+00000300: 0a6a 6f62 733d 310a 0a23 2043 6f6e 7472  .jobs=1..# Contr
+00000310: 6f6c 2074 6865 2061 6d6f 756e 7420 6f66  ol the amount of
+00000320: 2070 6f74 656e 7469 616c 2069 6e66 6572   potential infer
+00000330: 7265 6420 7661 6c75 6573 2077 6865 6e20  red values when 
+00000340: 696e 6665 7272 696e 6720 6120 7369 6e67  inferring a sing
+00000350: 6c65 0a23 206f 626a 6563 742e 2054 6869  le.# object. Thi
+00000360: 7320 6361 6e20 6865 6c70 2074 6865 2070  s can help the p
+00000370: 6572 666f 726d 616e 6365 2077 6865 6e20  erformance when 
+00000380: 6465 616c 696e 6720 7769 7468 206c 6172  dealing with lar
+00000390: 6765 2066 756e 6374 696f 6e73 206f 720a  ge functions or.
+000003a0: 2320 636f 6d70 6c65 782c 206e 6573 7465  # complex, neste
+000003b0: 6420 636f 6e64 6974 696f 6e73 2e0a 6c69  d conditions..li
+000003c0: 6d69 742d 696e 6665 7265 6e63 652d 7265  mit-inference-re
+000003d0: 7375 6c74 733d 3130 300a 0a23 204c 6973  sults=100..# Lis
+000003e0: 7420 6f66 2070 6c75 6769 6e73 2028 6173  t of plugins (as
+000003f0: 2063 6f6d 6d61 2073 6570 6172 6174 6564   comma separated
+00000400: 2076 616c 7565 7320 6f66 2070 7974 686f   values of pytho
+00000410: 6e20 6d6f 6475 6c65 206e 616d 6573 2920  n module names) 
+00000420: 746f 206c 6f61 642c 0a23 2075 7375 616c  to load,.# usual
+00000430: 6c79 2074 6f20 7265 6769 7374 6572 2061  ly to register a
+00000440: 6464 6974 696f 6e61 6c20 6368 6563 6b65  dditional checke
+00000450: 7273 2e0a 6c6f 6164 2d70 6c75 6769 6e73  rs..load-plugins
+00000460: 3d0a 0a23 2050 6963 6b6c 6520 636f 6c6c  =..# Pickle coll
+00000470: 6563 7465 6420 6461 7461 2066 6f72 206c  ected data for l
+00000480: 6174 6572 2063 6f6d 7061 7269 736f 6e73  ater comparisons
+00000490: 2e0a 7065 7273 6973 7465 6e74 3d79 6573  ..persistent=yes
+000004a0: 0a0a 2320 5768 656e 2065 6e61 626c 6564  ..# When enabled
+000004b0: 2c20 7079 6c69 6e74 2077 6f75 6c64 2061  , pylint would a
+000004c0: 7474 656d 7074 2074 6f20 6775 6573 7320  ttempt to guess 
+000004d0: 636f 6d6d 6f6e 206d 6973 636f 6e66 6967  common misconfig
+000004e0: 7572 6174 696f 6e20 616e 6420 656d 6974  uration and emit
+000004f0: 0a23 2075 7365 722d 6672 6965 6e64 6c79  .# user-friendly
+00000500: 2068 696e 7473 2069 6e73 7465 6164 206f   hints instead o
+00000510: 6620 6661 6c73 652d 706f 7369 7469 7665  f false-positive
+00000520: 2065 7272 6f72 206d 6573 7361 6765 732e   error messages.
+00000530: 0a73 7567 6765 7374 696f 6e2d 6d6f 6465  .suggestion-mode
+00000540: 3d79 6573 0a0a 2320 416c 6c6f 7720 6c6f  =yes..# Allow lo
+00000550: 6164 696e 6720 6f66 2061 7262 6974 7261  ading of arbitra
+00000560: 7279 2043 2065 7874 656e 7369 6f6e 732e  ry C extensions.
+00000570: 2045 7874 656e 7369 6f6e 7320 6172 6520   Extensions are 
+00000580: 696d 706f 7274 6564 2069 6e74 6f20 7468  imported into th
+00000590: 650a 2320 6163 7469 7665 2050 7974 686f  e.# active Pytho
+000005a0: 6e20 696e 7465 7270 7265 7465 7220 616e  n interpreter an
+000005b0: 6420 6d61 7920 7275 6e20 6172 6269 7472  d may run arbitr
+000005c0: 6172 7920 636f 6465 2e0a 756e 7361 6665  ary code..unsafe
+000005d0: 2d6c 6f61 642d 616e 792d 6578 7465 6e73  -load-any-extens
+000005e0: 696f 6e3d 6e6f 0a0a 0a5b 4d45 5353 4147  ion=no...[MESSAG
+000005f0: 4553 2043 4f4e 5452 4f4c 5d0a 0a23 204f  ES CONTROL]..# O
+00000600: 6e6c 7920 7368 6f77 2077 6172 6e69 6e67  nly show warning
+00000610: 7320 7769 7468 2074 6865 206c 6973 7465  s with the liste
+00000620: 6420 636f 6e66 6964 656e 6365 206c 6576  d confidence lev
+00000630: 656c 732e 204c 6561 7665 2065 6d70 7479  els. Leave empty
+00000640: 2074 6f20 7368 6f77 0a23 2061 6c6c 2e20   to show.# all. 
+00000650: 5661 6c69 6420 6c65 7665 6c73 3a20 4849  Valid levels: HI
+00000660: 4748 2c20 494e 4645 5245 4e43 452c 2049  GH, INFERENCE, I
+00000670: 4e46 4552 454e 4345 5f46 4149 4c55 5245  NFERENCE_FAILURE
+00000680: 2c20 554e 4445 4649 4e45 442e 0a63 6f6e  , UNDEFINED..con
+00000690: 6669 6465 6e63 653d 0a0a 2320 4469 7361  fidence=..# Disa
+000006a0: 626c 6520 7468 6520 6d65 7373 6167 652c  ble the message,
+000006b0: 2072 6570 6f72 742c 2063 6174 6567 6f72   report, categor
+000006c0: 7920 6f72 2063 6865 636b 6572 2077 6974  y or checker wit
+000006d0: 6820 7468 6520 6769 7665 6e20 6964 2873  h the given id(s
+000006e0: 292e 2059 6f75 0a23 2063 616e 2065 6974  ). You.# can eit
+000006f0: 6865 7220 6769 7665 206d 756c 7469 706c  her give multipl
+00000700: 6520 6964 656e 7469 6669 6572 7320 7365  e identifiers se
+00000710: 7061 7261 7465 6420 6279 2063 6f6d 6d61  parated by comma
+00000720: 2028 2c29 206f 7220 7075 7420 7468 6973   (,) or put this
+00000730: 0a23 206f 7074 696f 6e20 6d75 6c74 6970  .# option multip
+00000740: 6c65 2074 696d 6573 2028 6f6e 6c79 206f  le times (only o
+00000750: 6e20 7468 6520 636f 6d6d 616e 6420 6c69  n the command li
+00000760: 6e65 2c20 6e6f 7420 696e 2074 6865 2063  ne, not in the c
+00000770: 6f6e 6669 6775 7261 7469 6f6e 0a23 2066  onfiguration.# f
+00000780: 696c 6520 7768 6572 6520 6974 2073 686f  ile where it sho
+00000790: 756c 6420 6170 7065 6172 206f 6e6c 7920  uld appear only 
+000007a0: 6f6e 6365 292e 2059 6f75 2063 616e 2061  once). You can a
+000007b0: 6c73 6f20 7573 6520 222d 2d64 6973 6162  lso use "--disab
+000007c0: 6c65 3d61 6c6c 2220 746f 0a23 2064 6973  le=all" to.# dis
+000007d0: 6162 6c65 2065 7665 7279 7468 696e 6720  able everything 
+000007e0: 6669 7273 7420 616e 6420 7468 656e 2072  first and then r
+000007f0: 6565 6e61 626c 6520 7370 6563 6966 6963  eenable specific
+00000800: 2063 6865 636b 732e 2046 6f72 2065 7861   checks. For exa
+00000810: 6d70 6c65 2c20 6966 0a23 2079 6f75 2077  mple, if.# you w
+00000820: 616e 7420 746f 2072 756e 206f 6e6c 7920  ant to run only 
+00000830: 7468 6520 7369 6d69 6c61 7269 7469 6573  the similarities
+00000840: 2063 6865 636b 6572 2c20 796f 7520 6361   checker, you ca
+00000850: 6e20 7573 6520 222d 2d64 6973 6162 6c65  n use "--disable
+00000860: 3d61 6c6c 0a23 202d 2d65 6e61 626c 653d  =all.# --enable=
+00000870: 7369 6d69 6c61 7269 7469 6573 222e 2049  similarities". I
+00000880: 6620 796f 7520 7761 6e74 2074 6f20 7275  f you want to ru
+00000890: 6e20 6f6e 6c79 2074 6865 2063 6c61 7373  n only the class
+000008a0: 6573 2063 6865 636b 6572 2c20 6275 7420  es checker, but 
+000008b0: 6861 7665 0a23 206e 6f20 5761 726e 696e  have.# no Warnin
+000008c0: 6720 6c65 7665 6c20 6d65 7373 6167 6573  g level messages
+000008d0: 2064 6973 706c 6179 6564 2c20 7573 6520   displayed, use 
+000008e0: 222d 2d64 6973 6162 6c65 3d61 6c6c 202d  "--disable=all -
+000008f0: 2d65 6e61 626c 653d 636c 6173 7365 730a  -enable=classes.
+00000900: 2320 2d2d 6469 7361 626c 653d 5722 2e0a  # --disable=W"..
+00000910: 6469 7361 626c 653d 7072 696e 742d 7374  disable=print-st
+00000920: 6174 656d 656e 742c 0a20 2020 2020 2020  atement,.       
+00000930: 2070 6172 616d 6574 6572 2d75 6e70 6163   parameter-unpac
+00000940: 6b69 6e67 2c0a 2020 2020 2020 2020 756e  king,.        un
+00000950: 7061 636b 696e 672d 696e 2d65 7863 6570  packing-in-excep
+00000960: 742c 0a20 2020 2020 2020 206f 6c64 2d72  t,.        old-r
+00000970: 6169 7365 2d73 796e 7461 782c 0a20 2020  aise-syntax,.   
+00000980: 2020 2020 2062 6163 6b74 6963 6b2c 0a20       backtick,. 
+00000990: 2020 2020 2020 206c 6f6e 672d 7375 6666         long-suff
+000009a0: 6978 2c0a 2020 2020 2020 2020 6f6c 642d  ix,.        old-
+000009b0: 6e65 2d6f 7065 7261 746f 722c 0a20 2020  ne-operator,.   
+000009c0: 2020 2020 206f 6c64 2d6f 6374 616c 2d6c       old-octal-l
+000009d0: 6974 6572 616c 2c0a 2020 2020 2020 2020  iteral,.        
+000009e0: 696d 706f 7274 2d73 7461 722d 6d6f 6475  import-star-modu
+000009f0: 6c65 2d6c 6576 656c 2c0a 2020 2020 2020  le-level,.      
+00000a00: 2020 6e6f 6e2d 6173 6369 692d 6279 7465    non-ascii-byte
+00000a10: 732d 6c69 7465 7261 6c2c 0a20 2020 2020  s-literal,.     
+00000a20: 2020 2072 6177 2d63 6865 636b 6572 2d66     raw-checker-f
+00000a30: 6169 6c65 642c 0a20 2020 2020 2020 2062  ailed,.        b
+00000a40: 6164 2d69 6e6c 696e 652d 6f70 7469 6f6e  ad-inline-option
+00000a50: 2c0a 2020 2020 2020 2020 6c6f 6361 6c6c  ,.        locall
+00000a60: 792d 6469 7361 626c 6564 2c0a 2020 2020  y-disabled,.    
+00000a70: 2020 2020 6669 6c65 2d69 676e 6f72 6564      file-ignored
+00000a80: 2c0a 2020 2020 2020 2020 7375 7070 7265  ,.        suppre
+00000a90: 7373 6564 2d6d 6573 7361 6765 2c0a 2020  ssed-message,.  
+00000aa0: 2020 2020 2020 7573 656c 6573 732d 7375        useless-su
+00000ab0: 7070 7265 7373 696f 6e2c 0a20 2020 2020  ppression,.     
+00000ac0: 2020 2064 6570 7265 6361 7465 642d 7072     deprecated-pr
+00000ad0: 6167 6d61 2c0a 2020 2020 2020 2020 7573  agma,.        us
+00000ae0: 652d 7379 6d62 6f6c 6963 2d6d 6573 7361  e-symbolic-messa
+00000af0: 6765 2d69 6e73 7465 6164 2c0a 2020 2020  ge-instead,.    
+00000b00: 2020 2020 6170 706c 792d 6275 696c 7469      apply-builti
+00000b10: 6e2c 0a20 2020 2020 2020 2062 6173 6573  n,.        bases
+00000b20: 7472 696e 672d 6275 696c 7469 6e2c 0a20  tring-builtin,. 
+00000b30: 2020 2020 2020 2062 7566 6665 722d 6275         buffer-bu
+00000b40: 696c 7469 6e2c 0a20 2020 2020 2020 2063  iltin,.        c
+00000b50: 6d70 2d62 7569 6c74 696e 2c0a 2020 2020  mp-builtin,.    
+00000b60: 2020 2020 636f 6572 6365 2d62 7569 6c74      coerce-built
+00000b70: 696e 2c0a 2020 2020 2020 2020 6578 6563  in,.        exec
+00000b80: 6669 6c65 2d62 7569 6c74 696e 2c0a 2020  file-builtin,.  
+00000b90: 2020 2020 2020 6669 6c65 2d62 7569 6c74        file-built
+00000ba0: 696e 2c0a 2020 2020 2020 2020 6c6f 6e67  in,.        long
+00000bb0: 2d62 7569 6c74 696e 2c0a 2020 2020 2020  -builtin,.      
+00000bc0: 2020 7261 775f 696e 7075 742d 6275 696c    raw_input-buil
+00000bd0: 7469 6e2c 0a20 2020 2020 2020 2072 6564  tin,.        red
+00000be0: 7563 652d 6275 696c 7469 6e2c 0a20 2020  uce-builtin,.   
+00000bf0: 2020 2020 2073 7461 6e64 6172 6465 7272       standarderr
+00000c00: 6f72 2d62 7569 6c74 696e 2c0a 2020 2020  or-builtin,.    
+00000c10: 2020 2020 756e 6963 6f64 652d 6275 696c      unicode-buil
+00000c20: 7469 6e2c 0a20 2020 2020 2020 2078 7261  tin,.        xra
+00000c30: 6e67 652d 6275 696c 7469 6e2c 0a20 2020  nge-builtin,.   
+00000c40: 2020 2020 2063 6f65 7263 652d 6d65 7468       coerce-meth
+00000c50: 6f64 2c0a 2020 2020 2020 2020 6465 6c73  od,.        dels
+00000c60: 6c69 6365 2d6d 6574 686f 642c 0a20 2020  lice-method,.   
+00000c70: 2020 2020 2067 6574 736c 6963 652d 6d65       getslice-me
+00000c80: 7468 6f64 2c0a 2020 2020 2020 2020 7365  thod,.        se
+00000c90: 7473 6c69 6365 2d6d 6574 686f 642c 0a20  tslice-method,. 
+00000ca0: 2020 2020 2020 206e 6f2d 6162 736f 6c75         no-absolu
+00000cb0: 7465 2d69 6d70 6f72 742c 0a20 2020 2020  te-import,.     
+00000cc0: 2020 206f 6c64 2d64 6976 6973 696f 6e2c     old-division,
+00000cd0: 0a20 2020 2020 2020 2064 6963 742d 6974  .        dict-it
+00000ce0: 6572 2d6d 6574 686f 642c 0a20 2020 2020  er-method,.     
+00000cf0: 2020 2064 6963 742d 7669 6577 2d6d 6574     dict-view-met
+00000d00: 686f 642c 0a20 2020 2020 2020 206e 6578  hod,.        nex
+00000d10: 742d 6d65 7468 6f64 2d63 616c 6c65 642c  t-method-called,
+00000d20: 0a20 2020 2020 2020 206d 6574 6163 6c61  .        metacla
+00000d30: 7373 2d61 7373 6967 6e6d 656e 742c 0a20  ss-assignment,. 
+00000d40: 2020 2020 2020 2069 6e64 6578 696e 672d         indexing-
+00000d50: 6578 6365 7074 696f 6e2c 0a20 2020 2020  exception,.     
+00000d60: 2020 2072 6169 7369 6e67 2d73 7472 696e     raising-strin
+00000d70: 672c 0a20 2020 2020 2020 2072 656c 6f61  g,.        reloa
+00000d80: 642d 6275 696c 7469 6e2c 0a20 2020 2020  d-builtin,.     
+00000d90: 2020 206f 6374 2d6d 6574 686f 642c 0a20     oct-method,. 
+00000da0: 2020 2020 2020 2068 6578 2d6d 6574 686f         hex-metho
+00000db0: 642c 0a20 2020 2020 2020 206e 6f6e 7a65  d,.        nonze
+00000dc0: 726f 2d6d 6574 686f 642c 0a20 2020 2020  ro-method,.     
+00000dd0: 2020 2063 6d70 2d6d 6574 686f 642c 0a20     cmp-method,. 
+00000de0: 2020 2020 2020 2069 6e70 7574 2d62 7569         input-bui
+00000df0: 6c74 696e 2c0a 2020 2020 2020 2020 726f  ltin,.        ro
+00000e00: 756e 642d 6275 696c 7469 6e2c 0a20 2020  und-builtin,.   
+00000e10: 2020 2020 2069 6e74 6572 6e2d 6275 696c       intern-buil
+00000e20: 7469 6e2c 0a20 2020 2020 2020 2075 6e69  tin,.        uni
+00000e30: 6368 722d 6275 696c 7469 6e2c 0a20 2020  chr-builtin,.   
+00000e40: 2020 2020 206d 6170 2d62 7569 6c74 696e       map-builtin
+00000e50: 2d6e 6f74 2d69 7465 7261 7469 6e67 2c0a  -not-iterating,.
+00000e60: 2020 2020 2020 2020 7a69 702d 6275 696c          zip-buil
+00000e70: 7469 6e2d 6e6f 742d 6974 6572 6174 696e  tin-not-iteratin
+00000e80: 672c 0a20 2020 2020 2020 2072 616e 6765  g,.        range
+00000e90: 2d62 7569 6c74 696e 2d6e 6f74 2d69 7465  -builtin-not-ite
+00000ea0: 7261 7469 6e67 2c0a 2020 2020 2020 2020  rating,.        
+00000eb0: 6669 6c74 6572 2d62 7569 6c74 696e 2d6e  filter-builtin-n
+00000ec0: 6f74 2d69 7465 7261 7469 6e67 2c0a 2020  ot-iterating,.  
+00000ed0: 2020 2020 2020 7573 696e 672d 636d 702d        using-cmp-
+00000ee0: 6172 6775 6d65 6e74 2c0a 2020 2020 2020  argument,.      
+00000ef0: 2020 6571 2d77 6974 686f 7574 2d68 6173    eq-without-has
+00000f00: 682c 0a20 2020 2020 2020 2064 6976 2d6d  h,.        div-m
+00000f10: 6574 686f 642c 0a20 2020 2020 2020 2069  ethod,.        i
+00000f20: 6469 762d 6d65 7468 6f64 2c0a 2020 2020  div-method,.    
+00000f30: 2020 2020 7264 6976 2d6d 6574 686f 642c      rdiv-method,
+00000f40: 0a20 2020 2020 2020 2065 7863 6570 7469  .        excepti
+00000f50: 6f6e 2d6d 6573 7361 6765 2d61 7474 7269  on-message-attri
+00000f60: 6275 7465 2c0a 2020 2020 2020 2020 696e  bute,.        in
+00000f70: 7661 6c69 642d 7374 722d 636f 6465 632c  valid-str-codec,
+00000f80: 0a20 2020 2020 2020 2073 7973 2d6d 6178  .        sys-max
+00000f90: 2d69 6e74 2c0a 2020 2020 2020 2020 6261  -int,.        ba
+00000fa0: 642d 7079 7468 6f6e 332d 696d 706f 7274  d-python3-import
+00000fb0: 2c0a 2020 2020 2020 2020 6465 7072 6563  ,.        deprec
+00000fc0: 6174 6564 2d73 7472 696e 672d 6675 6e63  ated-string-func
+00000fd0: 7469 6f6e 2c0a 2020 2020 2020 2020 6465  tion,.        de
+00000fe0: 7072 6563 6174 6564 2d73 7472 2d74 7261  precated-str-tra
+00000ff0: 6e73 6c61 7465 2d63 616c 6c2c 0a20 2020  nslate-call,.   
+00001000: 2020 2020 2064 6570 7265 6361 7465 642d       deprecated-
+00001010: 6974 6572 746f 6f6c 732d 6675 6e63 7469  itertools-functi
+00001020: 6f6e 2c0a 2020 2020 2020 2020 6465 7072  on,.        depr
+00001030: 6563 6174 6564 2d74 7970 6573 2d66 6965  ecated-types-fie
+00001040: 6c64 2c0a 2020 2020 2020 2020 6e65 7874  ld,.        next
+00001050: 2d6d 6574 686f 642d 6465 6669 6e65 642c  -method-defined,
+00001060: 0a20 2020 2020 2020 2064 6963 742d 6974  .        dict-it
+00001070: 656d 732d 6e6f 742d 6974 6572 6174 696e  ems-not-iteratin
+00001080: 672c 0a20 2020 2020 2020 2064 6963 742d  g,.        dict-
+00001090: 6b65 7973 2d6e 6f74 2d69 7465 7261 7469  keys-not-iterati
+000010a0: 6e67 2c0a 2020 2020 2020 2020 6469 6374  ng,.        dict
+000010b0: 2d76 616c 7565 732d 6e6f 742d 6974 6572  -values-not-iter
+000010c0: 6174 696e 672c 0a20 2020 2020 2020 2064  ating,.        d
+000010d0: 6570 7265 6361 7465 642d 6f70 6572 6174  eprecated-operat
+000010e0: 6f72 2d66 756e 6374 696f 6e2c 0a20 2020  or-function,.   
+000010f0: 2020 2020 2064 6570 7265 6361 7465 642d       deprecated-
+00001100: 7572 6c6c 6962 2d66 756e 6374 696f 6e2c  urllib-function,
+00001110: 0a20 2020 2020 2020 2078 7265 6164 6c69  .        xreadli
+00001120: 6e65 732d 6174 7472 6962 7574 652c 0a20  nes-attribute,. 
+00001130: 2020 2020 2020 2064 6570 7265 6361 7465         deprecate
+00001140: 642d 7379 732d 6675 6e63 7469 6f6e 2c0a  d-sys-function,.
+00001150: 2020 2020 2020 2020 6578 6365 7074 696f          exceptio
+00001160: 6e2d 6573 6361 7065 2c0a 2020 2020 2020  n-escape,.      
+00001170: 2020 636f 6d70 7265 6865 6e73 696f 6e2d    comprehension-
+00001180: 6573 6361 7065 2c0a 2020 2020 2020 2020  escape,.        
+00001190: 6e6f 2d73 656c 662d 7573 652c 0a20 2020  no-self-use,.   
+000011a0: 2020 2020 2075 7365 2d64 6963 742d 6c69       use-dict-li
+000011b0: 7465 7261 6c0a 0a23 2045 6e61 626c 6520  teral..# Enable 
+000011c0: 7468 6520 6d65 7373 6167 652c 2072 6570  the message, rep
+000011d0: 6f72 742c 2063 6174 6567 6f72 7920 6f72  ort, category or
+000011e0: 2063 6865 636b 6572 2077 6974 6820 7468   checker with th
+000011f0: 6520 6769 7665 6e20 6964 2873 292e 2059  e given id(s). Y
+00001200: 6f75 2063 616e 0a23 2065 6974 6865 7220  ou can.# either 
+00001210: 6769 7665 206d 756c 7469 706c 6520 6964  give multiple id
+00001220: 656e 7469 6669 6572 2073 6570 6172 6174  entifier separat
+00001230: 6564 2062 7920 636f 6d6d 6120 282c 2920  ed by comma (,) 
+00001240: 6f72 2070 7574 2074 6869 7320 6f70 7469  or put this opti
+00001250: 6f6e 0a23 206d 756c 7469 706c 6520 7469  on.# multiple ti
+00001260: 6d65 2028 6f6e 6c79 206f 6e20 7468 6520  me (only on the 
+00001270: 636f 6d6d 616e 6420 6c69 6e65 2c20 6e6f  command line, no
+00001280: 7420 696e 2074 6865 2063 6f6e 6669 6775  t in the configu
+00001290: 7261 7469 6f6e 2066 696c 6520 7768 6572  ration file wher
+000012a0: 650a 2320 6974 2073 686f 756c 6420 6170  e.# it should ap
+000012b0: 7065 6172 206f 6e6c 7920 6f6e 6365 292e  pear only once).
+000012c0: 2053 6565 2061 6c73 6f20 7468 6520 222d   See also the "-
+000012d0: 2d64 6973 6162 6c65 2220 6f70 7469 6f6e  -disable" option
+000012e0: 2066 6f72 2065 7861 6d70 6c65 732e 0a65   for examples..e
+000012f0: 6e61 626c 653d 632d 6578 7465 6e73 696f  nable=c-extensio
+00001300: 6e2d 6e6f 2d6d 656d 6265 720a 0a0a 5b52  n-no-member...[R
+00001310: 4550 4f52 5453 5d0a 0a23 2050 7974 686f  EPORTS]..# Pytho
+00001320: 6e20 6578 7072 6573 7369 6f6e 2077 6869  n expression whi
+00001330: 6368 2073 686f 756c 6420 7265 7475 726e  ch should return
+00001340: 2061 2073 636f 7265 206c 6573 7320 7468   a score less th
+00001350: 616e 206f 7220 6571 7561 6c20 746f 2031  an or equal to 1
+00001360: 302e 2059 6f75 0a23 2068 6176 6520 6163  0. You.# have ac
+00001370: 6365 7373 2074 6f20 7468 6520 7661 7269  cess to the vari
+00001380: 6162 6c65 7320 2765 7272 6f72 272c 2027  ables 'error', '
+00001390: 7761 726e 696e 6727 2c20 2772 6566 6163  warning', 'refac
+000013a0: 746f 7227 2c20 616e 6420 2763 6f6e 7665  tor', and 'conve
+000013b0: 6e74 696f 6e27 0a23 2077 6869 6368 2063  ntion'.# which c
+000013c0: 6f6e 7461 696e 2074 6865 206e 756d 6265  ontain the numbe
+000013d0: 7220 6f66 206d 6573 7361 6765 7320 696e  r of messages in
+000013e0: 2065 6163 6820 6361 7465 676f 7279 2c20   each category, 
+000013f0: 6173 2077 656c 6c20 6173 2027 7374 6174  as well as 'stat
+00001400: 656d 656e 7427 0a23 2077 6869 6368 2069  ement'.# which i
+00001410: 7320 7468 6520 746f 7461 6c20 6e75 6d62  s the total numb
+00001420: 6572 206f 6620 7374 6174 656d 656e 7473  er of statements
+00001430: 2061 6e61 6c79 7a65 642e 2054 6869 7320   analyzed. This 
+00001440: 7363 6f72 6520 6973 2075 7365 6420 6279  score is used by
+00001450: 2074 6865 0a23 2067 6c6f 6261 6c20 6576   the.# global ev
+00001460: 616c 7561 7469 6f6e 2072 6570 6f72 7420  aluation report 
+00001470: 2852 5030 3030 3429 2e0a 6576 616c 7561  (RP0004)..evalua
+00001480: 7469 6f6e 3d31 302e 3020 2d20 2828 666c  tion=10.0 - ((fl
+00001490: 6f61 7428 3520 2a20 6572 726f 7220 2b20  oat(5 * error + 
+000014a0: 7761 726e 696e 6720 2b20 7265 6661 6374  warning + refact
+000014b0: 6f72 202b 2063 6f6e 7665 6e74 696f 6e29  or + convention)
+000014c0: 202f 2073 7461 7465 6d65 6e74 2920 2a20   / statement) * 
+000014d0: 3130 290a 0a23 2054 656d 706c 6174 6520  10)..# Template 
+000014e0: 7573 6564 2074 6f20 6469 7370 6c61 7920  used to display 
+000014f0: 6d65 7373 6167 6573 2e20 5468 6973 2069  messages. This i
+00001500: 7320 6120 7079 7468 6f6e 206e 6577 2d73  s a python new-s
+00001510: 7479 6c65 2066 6f72 6d61 7420 7374 7269  tyle format stri
+00001520: 6e67 0a23 2075 7365 6420 746f 2066 6f72  ng.# used to for
+00001530: 6d61 7420 7468 6520 6d65 7373 6167 6520  mat the message 
+00001540: 696e 666f 726d 6174 696f 6e2e 2053 6565  information. See
+00001550: 2064 6f63 2066 6f72 2061 6c6c 2064 6574   doc for all det
+00001560: 6169 6c73 2e0a 236d 7367 2d74 656d 706c  ails..#msg-templ
+00001570: 6174 653d 0a0a 2320 5365 7420 7468 6520  ate=..# Set the 
+00001580: 6f75 7470 7574 2066 6f72 6d61 742e 2041  output format. A
+00001590: 7661 696c 6162 6c65 2066 6f72 6d61 7473  vailable formats
+000015a0: 2061 7265 2074 6578 742c 2070 6172 7365   are text, parse
+000015b0: 6162 6c65 2c20 636f 6c6f 7269 7a65 642c  able, colorized,
+000015c0: 206a 736f 6e0a 2320 616e 6420 6d73 7673   json.# and msvs
+000015d0: 2028 7669 7375 616c 2073 7475 6469 6f29   (visual studio)
+000015e0: 2e20 596f 7520 6361 6e20 616c 736f 2067  . You can also g
+000015f0: 6976 6520 6120 7265 706f 7274 6572 2063  ive a reporter c
+00001600: 6c61 7373 2c20 652e 672e 0a23 206d 7970  lass, e.g..# myp
+00001610: 6163 6b61 6765 2e6d 796d 6f64 756c 652e  ackage.mymodule.
+00001620: 4d79 5265 706f 7274 6572 436c 6173 732e  MyReporterClass.
+00001630: 0a6f 7574 7075 742d 666f 726d 6174 3d74  .output-format=t
+00001640: 6578 740a 0a23 2054 656c 6c73 2077 6865  ext..# Tells whe
+00001650: 7468 6572 2074 6f20 6469 7370 6c61 7920  ther to display 
+00001660: 6120 6675 6c6c 2072 6570 6f72 7420 6f72  a full report or
+00001670: 206f 6e6c 7920 7468 6520 6d65 7373 6167   only the messag
+00001680: 6573 2e0a 7265 706f 7274 733d 6e6f 0a0a  es..reports=no..
+00001690: 2320 4163 7469 7661 7465 2074 6865 2065  # Activate the e
+000016a0: 7661 6c75 6174 696f 6e20 7363 6f72 652e  valuation score.
+000016b0: 0a73 636f 7265 3d79 6573 0a0a 0a5b 5245  .score=yes...[RE
+000016c0: 4641 4354 4f52 494e 475d 0a0a 2320 4d61  FACTORING]..# Ma
+000016d0: 7869 6d75 6d20 6e75 6d62 6572 206f 6620  ximum number of 
+000016e0: 6e65 7374 6564 2062 6c6f 636b 7320 666f  nested blocks fo
+000016f0: 7220 6675 6e63 7469 6f6e 202f 206d 6574  r function / met
+00001700: 686f 6420 626f 6479 0a6d 6178 2d6e 6573  hod body.max-nes
+00001710: 7465 642d 626c 6f63 6b73 3d35 0a0a 2320  ted-blocks=5..# 
+00001720: 436f 6d70 6c65 7465 206e 616d 6520 6f66  Complete name of
+00001730: 2066 756e 6374 696f 6e73 2074 6861 7420   functions that 
+00001740: 6e65 7665 7220 7265 7475 726e 732e 2057  never returns. W
+00001750: 6865 6e20 6368 6563 6b69 6e67 2066 6f72  hen checking for
+00001760: 0a23 2069 6e63 6f6e 7369 7374 656e 742d  .# inconsistent-
+00001770: 7265 7475 726e 2d73 7461 7465 6d65 6e74  return-statement
+00001780: 7320 6966 2061 206e 6576 6572 2072 6574  s if a never ret
+00001790: 7572 6e69 6e67 2066 756e 6374 696f 6e20  urning function 
+000017a0: 6973 2063 616c 6c65 6420 7468 656e 0a23  is called then.#
+000017b0: 2069 7420 7769 6c6c 2062 6520 636f 6e73   it will be cons
+000017c0: 6964 6572 6564 2061 7320 616e 2065 7870  idered as an exp
+000017d0: 6c69 6369 7420 7265 7475 726e 2073 7461  licit return sta
+000017e0: 7465 6d65 6e74 2061 6e64 206e 6f20 6d65  tement and no me
+000017f0: 7373 6167 6520 7769 6c6c 2062 650a 2320  ssage will be.# 
+00001800: 7072 696e 7465 642e 0a6e 6576 6572 2d72  printed..never-r
+00001810: 6574 7572 6e69 6e67 2d66 756e 6374 696f  eturning-functio
+00001820: 6e73 3d73 7973 2e65 7869 740a 0a0a 5b53  ns=sys.exit...[S
+00001830: 5045 4c4c 494e 475d 0a0a 2320 4c69 6d69  PELLING]..# Limi
+00001840: 7473 2063 6f75 6e74 206f 6620 656d 6974  ts count of emit
+00001850: 7465 6420 7375 6767 6573 7469 6f6e 7320  ted suggestions 
+00001860: 666f 7220 7370 656c 6c69 6e67 206d 6973  for spelling mis
+00001870: 7461 6b65 732e 0a6d 6178 2d73 7065 6c6c  takes..max-spell
+00001880: 696e 672d 7375 6767 6573 7469 6f6e 733d  ing-suggestions=
+00001890: 340a 0a23 2053 7065 6c6c 696e 6720 6469  4..# Spelling di
+000018a0: 6374 696f 6e61 7279 206e 616d 652e 2041  ctionary name. A
+000018b0: 7661 696c 6162 6c65 2064 6963 7469 6f6e  vailable diction
+000018c0: 6172 6965 733a 206e 6f6e 652e 2054 6f20  aries: none. To 
+000018d0: 6d61 6b65 2069 7420 776f 726b 2c0a 2320  make it work,.# 
+000018e0: 696e 7374 616c 6c20 7468 6520 7079 7468  install the pyth
+000018f0: 6f6e 2d65 6e63 6861 6e74 2070 6163 6b61  on-enchant packa
+00001900: 6765 2e0a 7370 656c 6c69 6e67 2d64 6963  ge..spelling-dic
+00001910: 743d 0a0a 2320 4c69 7374 206f 6620 636f  t=..# List of co
+00001920: 6d6d 6120 7365 7061 7261 7465 6420 776f  mma separated wo
+00001930: 7264 7320 7468 6174 2073 686f 756c 6420  rds that should 
+00001940: 6e6f 7420 6265 2063 6865 636b 6564 2e0a  not be checked..
+00001950: 7370 656c 6c69 6e67 2d69 676e 6f72 652d  spelling-ignore-
+00001960: 776f 7264 733d 0a0a 2320 4120 7061 7468  words=..# A path
+00001970: 2074 6f20 6120 6669 6c65 2074 6861 7420   to a file that 
+00001980: 636f 6e74 6169 6e73 2074 6865 2070 7269  contains the pri
+00001990: 7661 7465 2064 6963 7469 6f6e 6172 793b  vate dictionary;
+000019a0: 206f 6e65 2077 6f72 6420 7065 7220 6c69   one word per li
+000019b0: 6e65 2e0a 7370 656c 6c69 6e67 2d70 7269  ne..spelling-pri
+000019c0: 7661 7465 2d64 6963 742d 6669 6c65 3d0a  vate-dict-file=.
+000019d0: 0a23 2054 656c 6c73 2077 6865 7468 6572  .# Tells whether
+000019e0: 2074 6f20 7374 6f72 6520 756e 6b6e 6f77   to store unknow
+000019f0: 6e20 776f 7264 7320 746f 2074 6865 2070  n words to the p
+00001a00: 7269 7661 7465 2064 6963 7469 6f6e 6172  rivate dictionar
+00001a10: 7920 2873 6565 2074 6865 0a23 202d 2d73  y (see the.# --s
+00001a20: 7065 6c6c 696e 672d 7072 6976 6174 652d  pelling-private-
+00001a30: 6469 6374 2d66 696c 6520 6f70 7469 6f6e  dict-file option
+00001a40: 2920 696e 7374 6561 6420 6f66 2072 6169  ) instead of rai
+00001a50: 7369 6e67 2061 206d 6573 7361 6765 2e0a  sing a message..
+00001a60: 7370 656c 6c69 6e67 2d73 746f 7265 2d75  spelling-store-u
+00001a70: 6e6b 6e6f 776e 2d77 6f72 6473 3d6e 6f0a  nknown-words=no.
+00001a80: 0a0a 5b56 4152 4941 424c 4553 5d0a 0a23  ..[VARIABLES]..#
+00001a90: 204c 6973 7420 6f66 2061 6464 6974 696f   List of additio
+00001aa0: 6e61 6c20 6e61 6d65 7320 7375 7070 6f73  nal names suppos
+00001ab0: 6564 2074 6f20 6265 2064 6566 696e 6564  ed to be defined
+00001ac0: 2069 6e20 6275 696c 7469 6e73 2e20 5265   in builtins. Re
+00001ad0: 6d65 6d62 6572 2074 6861 740a 2320 796f  member that.# yo
+00001ae0: 7520 7368 6f75 6c64 2061 766f 6964 2064  u should avoid d
+00001af0: 6566 696e 696e 6720 6e65 7720 6275 696c  efining new buil
+00001b00: 7469 6e73 2077 6865 6e20 706f 7373 6962  tins when possib
+00001b10: 6c65 2e0a 6164 6469 7469 6f6e 616c 2d62  le..additional-b
+00001b20: 7569 6c74 696e 733d 0a0a 2320 5465 6c6c  uiltins=..# Tell
+00001b30: 7320 7768 6574 6865 7220 756e 7573 6564  s whether unused
+00001b40: 2067 6c6f 6261 6c20 7661 7269 6162 6c65   global variable
+00001b50: 7320 7368 6f75 6c64 2062 6520 7472 6561  s should be trea
+00001b60: 7465 6420 6173 2061 2076 696f 6c61 7469  ted as a violati
+00001b70: 6f6e 2e0a 616c 6c6f 772d 676c 6f62 616c  on..allow-global
+00001b80: 2d75 6e75 7365 642d 7661 7269 6162 6c65  -unused-variable
+00001b90: 733d 7965 730a 0a23 204c 6973 7420 6f66  s=yes..# List of
+00001ba0: 2073 7472 696e 6773 2077 6869 6368 2063   strings which c
+00001bb0: 616e 2069 6465 6e74 6966 7920 6120 6361  an identify a ca
+00001bc0: 6c6c 6261 636b 2066 756e 6374 696f 6e20  llback function 
+00001bd0: 6279 206e 616d 652e 2041 2063 616c 6c62  by name. A callb
+00001be0: 6163 6b0a 2320 6e61 6d65 206d 7573 7420  ack.# name must 
+00001bf0: 7374 6172 7420 6f72 2065 6e64 2077 6974  start or end wit
+00001c00: 6820 6f6e 6520 6f66 2074 686f 7365 2073  h one of those s
+00001c10: 7472 696e 6773 2e0a 6361 6c6c 6261 636b  trings..callback
+00001c20: 733d 6362 5f2c 0a20 2020 2020 2020 2020  s=cb_,.         
+00001c30: 205f 6362 0a0a 2320 4120 7265 6775 6c61   _cb..# A regula
+00001c40: 7220 6578 7072 6573 7369 6f6e 206d 6174  r expression mat
+00001c50: 6368 696e 6720 7468 6520 6e61 6d65 206f  ching the name o
+00001c60: 6620 6475 6d6d 7920 7661 7269 6162 6c65  f dummy variable
+00001c70: 7320 2869 2e65 2e20 6578 7065 6374 6564  s (i.e. expected
+00001c80: 2074 6f0a 2320 6e6f 7420 6265 2075 7365   to.# not be use
+00001c90: 6429 2e0a 6475 6d6d 792d 7661 7269 6162  d)..dummy-variab
+00001ca0: 6c65 732d 7267 783d 5f2b 247c 285f 5b61  les-rgx=_+$|(_[a
+00001cb0: 2d7a 412d 5a30 2d39 5f5d 2a5b 612d 7a41  -zA-Z0-9_]*[a-zA
+00001cc0: 2d5a 302d 395d 2b3f 2429 7c64 756d 6d79  -Z0-9]+?$)|dummy
+00001cd0: 7c5e 6967 6e6f 7265 645f 7c5e 756e 7573  |^ignored_|^unus
+00001ce0: 6564 5f0a 0a23 2041 7267 756d 656e 7420  ed_..# Argument 
+00001cf0: 6e61 6d65 7320 7468 6174 206d 6174 6368  names that match
+00001d00: 2074 6869 7320 6578 7072 6573 7369 6f6e   this expression
+00001d10: 2077 696c 6c20 6265 2069 676e 6f72 6564   will be ignored
+00001d20: 2e20 4465 6661 756c 7420 746f 206e 616d  . Default to nam
+00001d30: 650a 2320 7769 7468 206c 6561 6469 6e67  e.# with leading
+00001d40: 2075 6e64 6572 7363 6f72 652e 0a69 676e   underscore..ign
+00001d50: 6f72 6564 2d61 7267 756d 656e 742d 6e61  ored-argument-na
+00001d60: 6d65 733d 5f2e 2a7c 5e69 676e 6f72 6564  mes=_.*|^ignored
+00001d70: 5f7c 5e75 6e75 7365 645f 0a0a 2320 5465  _|^unused_..# Te
+00001d80: 6c6c 7320 7768 6574 6865 7220 7765 2073  lls whether we s
+00001d90: 686f 756c 6420 6368 6563 6b20 666f 7220  hould check for 
+00001da0: 756e 7573 6564 2069 6d70 6f72 7420 696e  unused import in
+00001db0: 205f 5f69 6e69 745f 5f20 6669 6c65 732e   __init__ files.
+00001dc0: 0a69 6e69 742d 696d 706f 7274 3d6e 6f0a  .init-import=no.
+00001dd0: 0a23 204c 6973 7420 6f66 2071 7561 6c69  .# List of quali
+00001de0: 6669 6564 206d 6f64 756c 6520 6e61 6d65  fied module name
+00001df0: 7320 7768 6963 6820 6361 6e20 6861 7665  s which can have
+00001e00: 206f 626a 6563 7473 2074 6861 7420 6361   objects that ca
+00001e10: 6e20 7265 6465 6669 6e65 0a23 2062 7569  n redefine.# bui
+00001e20: 6c74 696e 732e 0a72 6564 6566 696e 696e  ltins..redefinin
+00001e30: 672d 6275 696c 7469 6e73 2d6d 6f64 756c  g-builtins-modul
+00001e40: 6573 3d73 6978 2e6d 6f76 6573 2c70 6173  es=six.moves,pas
+00001e50: 742e 6275 696c 7469 6e73 2c66 7574 7572  t.builtins,futur
+00001e60: 652e 6275 696c 7469 6e73 2c62 7569 6c74  e.builtins,built
+00001e70: 696e 732c 696f 0a0a 0a5b 464f 524d 4154  ins,io...[FORMAT
+00001e80: 5d0a 0a23 2045 7870 6563 7465 6420 666f  ]..# Expected fo
+00001e90: 726d 6174 206f 6620 6c69 6e65 2065 6e64  rmat of line end
+00001ea0: 696e 672c 2065 2e67 2e20 656d 7074 7920  ing, e.g. empty 
+00001eb0: 2861 6e79 206c 696e 6520 656e 6469 6e67  (any line ending
+00001ec0: 292c 204c 4620 6f72 2043 524c 462e 0a65  ), LF or CRLF..e
+00001ed0: 7870 6563 7465 642d 6c69 6e65 2d65 6e64  xpected-line-end
+00001ee0: 696e 672d 666f 726d 6174 3d0a 0a23 2052  ing-format=..# R
+00001ef0: 6567 6578 7020 666f 7220 6120 6c69 6e65  egexp for a line
+00001f00: 2074 6861 7420 6973 2061 6c6c 6f77 6564   that is allowed
+00001f10: 2074 6f20 6265 206c 6f6e 6765 7220 7468   to be longer th
+00001f20: 616e 2074 6865 206c 696d 6974 2e0a 6967  an the limit..ig
+00001f30: 6e6f 7265 2d6c 6f6e 672d 6c69 6e65 733d  nore-long-lines=
+00001f40: 5e5c 732a 2823 2029 3f3c 3f68 7474 7073  ^\s*(# )?<?https
+00001f50: 3f3a 2f2f 5c53 2b3e 3f24 0a0a 2320 4e75  ?://\S+>?$..# Nu
+00001f60: 6d62 6572 206f 6620 7370 6163 6573 206f  mber of spaces o
+00001f70: 6620 696e 6465 6e74 2072 6571 7569 7265  f indent require
+00001f80: 6420 696e 7369 6465 2061 2068 616e 6769  d inside a hangi
+00001f90: 6e67 206f 7220 636f 6e74 696e 7565 6420  ng or continued 
+00001fa0: 6c69 6e65 2e0a 696e 6465 6e74 2d61 6674  line..indent-aft
+00001fb0: 6572 2d70 6172 656e 3d34 0a0a 2320 5374  er-paren=4..# St
+00001fc0: 7269 6e67 2075 7365 6420 6173 2069 6e64  ring used as ind
+00001fd0: 656e 7461 7469 6f6e 2075 6e69 742e 2054  entation unit. T
+00001fe0: 6869 7320 6973 2075 7375 616c 6c79 2022  his is usually "
+00001ff0: 2020 2020 2220 2834 2073 7061 6365 7329      " (4 spaces)
+00002000: 206f 7220 225c 7422 2028 310a 2320 7461   or "\t" (1.# ta
+00002010: 6229 2e0a 696e 6465 6e74 2d73 7472 696e  b)..indent-strin
+00002020: 673d 2720 2020 2027 0a0a 2320 4d61 7869  g='    '..# Maxi
+00002030: 6d75 6d20 6e75 6d62 6572 206f 6620 6368  mum number of ch
+00002040: 6172 6163 7465 7273 206f 6e20 6120 7369  aracters on a si
+00002050: 6e67 6c65 206c 696e 652e 0a6d 6178 2d6c  ngle line..max-l
+00002060: 696e 652d 6c65 6e67 7468 3d31 3030 0a0a  ine-length=100..
+00002070: 2320 4d61 7869 6d75 6d20 6e75 6d62 6572  # Maximum number
+00002080: 206f 6620 6c69 6e65 7320 696e 2061 206d   of lines in a m
+00002090: 6f64 756c 652e 0a6d 6178 2d6d 6f64 756c  odule..max-modul
+000020a0: 652d 6c69 6e65 733d 3130 3030 0a0a 2320  e-lines=1000..# 
+000020b0: 416c 6c6f 7720 7468 6520 626f 6479 206f  Allow the body o
+000020c0: 6620 6120 636c 6173 7320 746f 2062 6520  f a class to be 
+000020d0: 6f6e 2074 6865 2073 616d 6520 6c69 6e65  on the same line
+000020e0: 2061 7320 7468 6520 6465 636c 6172 6174   as the declarat
+000020f0: 696f 6e20 6966 2062 6f64 790a 2320 636f  ion if body.# co
+00002100: 6e74 6169 6e73 2073 696e 676c 6520 7374  ntains single st
+00002110: 6174 656d 656e 742e 0a73 696e 676c 652d  atement..single-
+00002120: 6c69 6e65 2d63 6c61 7373 2d73 746d 743d  line-class-stmt=
+00002130: 6e6f 0a0a 2320 416c 6c6f 7720 7468 6520  no..# Allow the 
+00002140: 626f 6479 206f 6620 616e 2069 6620 746f  body of an if to
+00002150: 2062 6520 6f6e 2074 6865 2073 616d 6520   be on the same 
+00002160: 6c69 6e65 2061 7320 7468 6520 7465 7374  line as the test
+00002170: 2069 6620 7468 6572 6520 6973 206e 6f0a   if there is no.
+00002180: 2320 656c 7365 2e0a 7369 6e67 6c65 2d6c  # else..single-l
+00002190: 696e 652d 6966 2d73 746d 743d 6e6f 0a0a  ine-if-stmt=no..
+000021a0: 0a5b 5459 5045 4348 4543 4b5d 0a0a 2320  .[TYPECHECK]..# 
+000021b0: 4c69 7374 206f 6620 6465 636f 7261 746f  List of decorato
+000021c0: 7273 2074 6861 7420 7072 6f64 7563 6520  rs that produce 
+000021d0: 636f 6e74 6578 7420 6d61 6e61 6765 7273  context managers
+000021e0: 2c20 7375 6368 2061 730a 2320 636f 6e74  , such as.# cont
+000021f0: 6578 746c 6962 2e63 6f6e 7465 7874 6d61  extlib.contextma
+00002200: 6e61 6765 722e 2041 6464 2074 6f20 7468  nager. Add to th
+00002210: 6973 206c 6973 7420 746f 2072 6567 6973  is list to regis
+00002220: 7465 7220 6f74 6865 7220 6465 636f 7261  ter other decora
+00002230: 746f 7273 2074 6861 740a 2320 7072 6f64  tors that.# prod
+00002240: 7563 6520 7661 6c69 6420 636f 6e74 6578  uce valid contex
+00002250: 7420 6d61 6e61 6765 7273 2e0a 636f 6e74  t managers..cont
+00002260: 6578 746d 616e 6167 6572 2d64 6563 6f72  extmanager-decor
+00002270: 6174 6f72 733d 636f 6e74 6578 746c 6962  ators=contextlib
+00002280: 2e63 6f6e 7465 7874 6d61 6e61 6765 720a  .contextmanager.
+00002290: 0a23 204c 6973 7420 6f66 206d 656d 6265  .# List of membe
+000022a0: 7273 2077 6869 6368 2061 7265 2073 6574  rs which are set
+000022b0: 2064 796e 616d 6963 616c 6c79 2061 6e64   dynamically and
+000022c0: 206d 6973 7365 6420 6279 2070 796c 696e   missed by pylin
+000022d0: 7420 696e 6665 7265 6e63 650a 2320 7379  t inference.# sy
+000022e0: 7374 656d 2c20 616e 6420 736f 2073 686f  stem, and so sho
+000022f0: 756c 646e 2774 2074 7269 6767 6572 2045  uldn't trigger E
+00002300: 3131 3031 2077 6865 6e20 6163 6365 7373  1101 when access
+00002310: 6564 2e20 5079 7468 6f6e 2072 6567 756c  ed. Python regul
+00002320: 6172 0a23 2065 7870 7265 7373 696f 6e73  ar.# expressions
+00002330: 2061 7265 2061 6363 6570 7465 642e 0a67   are accepted..g
+00002340: 656e 6572 6174 6564 2d6d 656d 6265 7273  enerated-members
+00002350: 3d0a 0a23 2054 656c 6c73 2077 6865 7468  =..# Tells wheth
+00002360: 6572 206d 6973 7369 6e67 206d 656d 6265  er missing membe
+00002370: 7273 2061 6363 6573 7365 6420 696e 206d  rs accessed in m
+00002380: 6978 696e 2063 6c61 7373 2073 686f 756c  ixin class shoul
+00002390: 6420 6265 2069 676e 6f72 6564 2e20 410a  d be ignored. A.
+000023a0: 2320 6d69 7869 6e20 636c 6173 7320 6973  # mixin class is
+000023b0: 2064 6574 6563 7465 6420 6966 2069 7473   detected if its
+000023c0: 206e 616d 6520 656e 6473 2077 6974 6820   name ends with 
+000023d0: 226d 6978 696e 2220 2863 6173 6520 696e  "mixin" (case in
+000023e0: 7365 6e73 6974 6976 6529 2e0a 6967 6e6f  sensitive)..igno
+000023f0: 7265 2d6d 6978 696e 2d6d 656d 6265 7273  re-mixin-members
+00002400: 3d79 6573 0a0a 2320 5465 6c6c 7320 7768  =yes..# Tells wh
+00002410: 6574 6865 7220 746f 2077 6172 6e20 6162  ether to warn ab
+00002420: 6f75 7420 6d69 7373 696e 6720 6d65 6d62  out missing memb
+00002430: 6572 7320 7768 656e 2074 6865 206f 776e  ers when the own
+00002440: 6572 206f 6620 7468 6520 6174 7472 6962  er of the attrib
+00002450: 7574 650a 2320 6973 2069 6e66 6572 7265  ute.# is inferre
+00002460: 6420 746f 2062 6520 4e6f 6e65 2e0a 6967  d to be None..ig
+00002470: 6e6f 7265 2d6e 6f6e 653d 7965 730a 0a23  nore-none=yes..#
+00002480: 2054 6869 7320 666c 6167 2063 6f6e 7472   This flag contr
+00002490: 6f6c 7320 7768 6574 6865 7220 7079 6c69  ols whether pyli
+000024a0: 6e74 2073 686f 756c 6420 7761 726e 2061  nt should warn a
+000024b0: 626f 7574 206e 6f2d 6d65 6d62 6572 2061  bout no-member a
+000024c0: 6e64 2073 696d 696c 6172 0a23 2063 6865  nd similar.# che
+000024d0: 636b 7320 7768 656e 6576 6572 2061 6e20  cks whenever an 
+000024e0: 6f70 6171 7565 206f 626a 6563 7420 6973  opaque object is
+000024f0: 2072 6574 7572 6e65 6420 7768 656e 2069   returned when i
+00002500: 6e66 6572 7269 6e67 2e20 5468 6520 696e  nferring. The in
+00002510: 6665 7265 6e63 650a 2320 6361 6e20 7265  ference.# can re
+00002520: 7475 726e 206d 756c 7469 706c 6520 706f  turn multiple po
+00002530: 7465 6e74 6961 6c20 7265 7375 6c74 7320  tential results 
+00002540: 7768 696c 6520 6576 616c 7561 7469 6e67  while evaluating
+00002550: 2061 2050 7974 686f 6e20 6f62 6a65 6374   a Python object
+00002560: 2c20 6275 740a 2320 736f 6d65 2062 7261  , but.# some bra
+00002570: 6e63 6865 7320 6d69 6768 7420 6e6f 7420  nches might not 
+00002580: 6265 2065 7661 6c75 6174 6564 2c20 7768  be evaluated, wh
+00002590: 6963 6820 7265 7375 6c74 7320 696e 2070  ich results in p
+000025a0: 6172 7469 616c 2069 6e66 6572 656e 6365  artial inference
+000025b0: 2e20 496e 0a23 2074 6861 7420 6361 7365  . In.# that case
+000025c0: 2c20 6974 206d 6967 6874 2062 6520 7573  , it might be us
+000025d0: 6566 756c 2074 6f20 7374 696c 6c20 656d  eful to still em
+000025e0: 6974 206e 6f2d 6d65 6d62 6572 2061 6e64  it no-member and
+000025f0: 206f 7468 6572 2063 6865 636b 7320 666f   other checks fo
+00002600: 720a 2320 7468 6520 7265 7374 206f 6620  r.# the rest of 
+00002610: 7468 6520 696e 6665 7272 6564 206f 626a  the inferred obj
+00002620: 6563 7473 2e0a 6967 6e6f 7265 2d6f 6e2d  ects..ignore-on-
+00002630: 6f70 6171 7565 2d69 6e66 6572 656e 6365  opaque-inference
+00002640: 3d79 6573 0a0a 2320 4c69 7374 206f 6620  =yes..# List of 
+00002650: 636c 6173 7320 6e61 6d65 7320 666f 7220  class names for 
+00002660: 7768 6963 6820 6d65 6d62 6572 2061 7474  which member att
+00002670: 7269 6275 7465 7320 7368 6f75 6c64 206e  ributes should n
+00002680: 6f74 2062 6520 6368 6563 6b65 6420 2875  ot be checked (u
+00002690: 7365 6675 6c0a 2320 666f 7220 636c 6173  seful.# for clas
+000026a0: 7365 7320 7769 7468 2064 796e 616d 6963  ses with dynamic
+000026b0: 616c 6c79 2073 6574 2061 7474 7269 6275  ally set attribu
+000026c0: 7465 7329 2e20 5468 6973 2073 7570 706f  tes). This suppo
+000026d0: 7274 7320 7468 6520 7573 6520 6f66 0a23  rts the use of.#
+000026e0: 2071 7561 6c69 6669 6564 206e 616d 6573   qualified names
+000026f0: 2e0a 6967 6e6f 7265 642d 636c 6173 7365  ..ignored-classe
+00002700: 733d 6f70 7470 6172 7365 2e56 616c 7565  s=optparse.Value
+00002710: 732c 7468 7265 6164 2e5f 6c6f 6361 6c2c  s,thread._local,
+00002720: 5f74 6872 6561 642e 5f6c 6f63 616c 0a0a  _thread._local..
+00002730: 2320 4c69 7374 206f 6620 6d6f 6475 6c65  # List of module
+00002740: 206e 616d 6573 2066 6f72 2077 6869 6368   names for which
+00002750: 206d 656d 6265 7220 6174 7472 6962 7574   member attribut
+00002760: 6573 2073 686f 756c 6420 6e6f 7420 6265  es should not be
+00002770: 2063 6865 636b 6564 0a23 2028 7573 6566   checked.# (usef
+00002780: 756c 2066 6f72 206d 6f64 756c 6573 2f70  ul for modules/p
+00002790: 726f 6a65 6374 7320 7768 6572 6520 6e61  rojects where na
+000027a0: 6d65 7370 6163 6573 2061 7265 206d 616e  mespaces are man
+000027b0: 6970 756c 6174 6564 2064 7572 696e 6720  ipulated during 
+000027c0: 7275 6e74 696d 650a 2320 616e 6420 7468  runtime.# and th
+000027d0: 7573 2065 7869 7374 696e 6720 6d65 6d62  us existing memb
+000027e0: 6572 2061 7474 7269 6275 7465 7320 6361  er attributes ca
+000027f0: 6e6e 6f74 2062 6520 6465 6475 6365 6420  nnot be deduced 
+00002800: 6279 2073 7461 7469 6320 616e 616c 7973  by static analys
+00002810: 6973 292e 2049 740a 2320 7375 7070 6f72  is). It.# suppor
+00002820: 7473 2071 7561 6c69 6669 6564 206d 6f64  ts qualified mod
+00002830: 756c 6520 6e61 6d65 732c 2061 7320 7765  ule names, as we
+00002840: 6c6c 2061 7320 556e 6978 2070 6174 7465  ll as Unix patte
+00002850: 726e 206d 6174 6368 696e 672e 0a69 676e  rn matching..ign
+00002860: 6f72 6564 2d6d 6f64 756c 6573 3d0a 0a23  ored-modules=..#
+00002870: 2053 686f 7720 6120 6869 6e74 2077 6974   Show a hint wit
+00002880: 6820 706f 7373 6962 6c65 206e 616d 6573  h possible names
+00002890: 2077 6865 6e20 6120 6d65 6d62 6572 206e   when a member n
+000028a0: 616d 6520 7761 7320 6e6f 7420 666f 756e  ame was not foun
+000028b0: 642e 2054 6865 2061 7370 6563 740a 2320  d. The aspect.# 
+000028c0: 6f66 2066 696e 6469 6e67 2074 6865 2068  of finding the h
+000028d0: 696e 7420 6973 2062 6173 6564 206f 6e20  int is based on 
+000028e0: 6564 6974 2064 6973 7461 6e63 652e 0a6d  edit distance..m
+000028f0: 6973 7369 6e67 2d6d 656d 6265 722d 6869  issing-member-hi
+00002900: 6e74 3d79 6573 0a0a 2320 5468 6520 6d69  nt=yes..# The mi
+00002910: 6e69 6d75 6d20 6564 6974 2064 6973 7461  nimum edit dista
+00002920: 6e63 6520 6120 6e61 6d65 2073 686f 756c  nce a name shoul
+00002930: 6420 6861 7665 2069 6e20 6f72 6465 7220  d have in order 
+00002940: 746f 2062 6520 636f 6e73 6964 6572 6564  to be considered
+00002950: 2061 0a23 2073 696d 696c 6172 206d 6174   a.# similar mat
+00002960: 6368 2066 6f72 2061 206d 6973 7369 6e67  ch for a missing
+00002970: 206d 656d 6265 7220 6e61 6d65 2e0a 6d69   member name..mi
+00002980: 7373 696e 672d 6d65 6d62 6572 2d68 696e  ssing-member-hin
+00002990: 742d 6469 7374 616e 6365 3d31 0a0a 2320  t-distance=1..# 
+000029a0: 5468 6520 746f 7461 6c20 6e75 6d62 6572  The total number
+000029b0: 206f 6620 7369 6d69 6c61 7220 6e61 6d65   of similar name
+000029c0: 7320 7468 6174 2073 686f 756c 6420 6265  s that should be
+000029d0: 2074 616b 656e 2069 6e20 636f 6e73 6964   taken in consid
+000029e0: 6572 6174 696f 6e20 7768 656e 0a23 2073  eration when.# s
+000029f0: 686f 7769 6e67 2061 2068 696e 7420 666f  howing a hint fo
+00002a00: 7220 6120 6d69 7373 696e 6720 6d65 6d62  r a missing memb
+00002a10: 6572 2e0a 6d69 7373 696e 672d 6d65 6d62  er..missing-memb
+00002a20: 6572 2d6d 6178 2d63 686f 6963 6573 3d31  er-max-choices=1
+00002a30: 0a0a 2320 4c69 7374 206f 6620 6465 636f  ..# List of deco
+00002a40: 7261 746f 7273 2074 6861 7420 6368 616e  rators that chan
+00002a50: 6765 2074 6865 2073 6967 6e61 7475 7265  ge the signature
+00002a60: 206f 6620 6120 6465 636f 7261 7465 6420   of a decorated 
+00002a70: 6675 6e63 7469 6f6e 2e0a 7369 676e 6174  function..signat
+00002a80: 7572 652d 6d75 7461 746f 7273 3d0a 0a0a  ure-mutators=...
+00002a90: 5b53 494d 494c 4152 4954 4945 535d 0a0a  [SIMILARITIES]..
+00002aa0: 2320 4967 6e6f 7265 2063 6f6d 6d65 6e74  # Ignore comment
+00002ab0: 7320 7768 656e 2063 6f6d 7075 7469 6e67  s when computing
+00002ac0: 2073 696d 696c 6172 6974 6965 732e 0a69   similarities..i
+00002ad0: 676e 6f72 652d 636f 6d6d 656e 7473 3d79  gnore-comments=y
+00002ae0: 6573 0a0a 2320 4967 6e6f 7265 2064 6f63  es..# Ignore doc
+00002af0: 7374 7269 6e67 7320 7768 656e 2063 6f6d  strings when com
+00002b00: 7075 7469 6e67 2073 696d 696c 6172 6974  puting similarit
+00002b10: 6965 732e 0a69 676e 6f72 652d 646f 6373  ies..ignore-docs
+00002b20: 7472 696e 6773 3d79 6573 0a0a 2320 4967  trings=yes..# Ig
+00002b30: 6e6f 7265 2069 6d70 6f72 7473 2077 6865  nore imports whe
+00002b40: 6e20 636f 6d70 7574 696e 6720 7369 6d69  n computing simi
+00002b50: 6c61 7269 7469 6573 2e0a 6967 6e6f 7265  larities..ignore
+00002b60: 2d69 6d70 6f72 7473 3d6e 6f0a 0a23 204d  -imports=no..# M
+00002b70: 696e 696d 756d 206c 696e 6573 206e 756d  inimum lines num
+00002b80: 6265 7220 6f66 2061 2073 696d 696c 6172  ber of a similar
+00002b90: 6974 792e 0a6d 696e 2d73 696d 696c 6172  ity..min-similar
+00002ba0: 6974 792d 6c69 6e65 733d 340a 0a0a 5b4d  ity-lines=4...[M
+00002bb0: 4953 4345 4c4c 414e 454f 5553 5d0a 0a23  ISCELLANEOUS]..#
+00002bc0: 204c 6973 7420 6f66 206e 6f74 6520 7461   List of note ta
+00002bd0: 6773 2074 6f20 7461 6b65 2069 6e20 636f  gs to take in co
+00002be0: 6e73 6964 6572 6174 696f 6e2c 2073 6570  nsideration, sep
+00002bf0: 6172 6174 6564 2062 7920 6120 636f 6d6d  arated by a comm
+00002c00: 612e 0a6e 6f74 6573 3d46 4958 4d45 2c0a  a..notes=FIXME,.
+00002c10: 2020 2020 2020 5858 582c 0a20 2020 2020        XXX,.     
+00002c20: 2054 4f44 4f0a 0a23 2052 6567 756c 6172   TODO..# Regular
+00002c30: 2065 7870 7265 7373 696f 6e20 6f66 206e   expression of n
+00002c40: 6f74 6520 7461 6773 2074 6f20 7461 6b65  ote tags to take
+00002c50: 2069 6e20 636f 6e73 6964 6572 6174 696f   in consideratio
+00002c60: 6e2e 0a23 6e6f 7465 732d 7267 783d 0a0a  n..#notes-rgx=..
+00002c70: 0a5b 4241 5349 435d 0a0a 2320 4e61 6d69  .[BASIC]..# Nami
+00002c80: 6e67 2073 7479 6c65 206d 6174 6368 696e  ng style matchin
+00002c90: 6720 636f 7272 6563 7420 6172 6775 6d65  g correct argume
+00002ca0: 6e74 206e 616d 6573 2e0a 6172 6775 6d65  nt names..argume
+00002cb0: 6e74 2d6e 616d 696e 672d 7374 796c 653d  nt-naming-style=
+00002cc0: 736e 616b 655f 6361 7365 0a0a 2320 5265  snake_case..# Re
+00002cd0: 6775 6c61 7220 6578 7072 6573 7369 6f6e  gular expression
+00002ce0: 206d 6174 6368 696e 6720 636f 7272 6563   matching correc
+00002cf0: 7420 6172 6775 6d65 6e74 206e 616d 6573  t argument names
+00002d00: 2e20 4f76 6572 7269 6465 7320 6172 6775  . Overrides argu
+00002d10: 6d65 6e74 2d0a 2320 6e61 6d69 6e67 2d73  ment-.# naming-s
+00002d20: 7479 6c65 2e0a 2361 7267 756d 656e 742d  tyle..#argument-
+00002d30: 7267 783d 0a0a 2320 4e61 6d69 6e67 2073  rgx=..# Naming s
+00002d40: 7479 6c65 206d 6174 6368 696e 6720 636f  tyle matching co
+00002d50: 7272 6563 7420 6174 7472 6962 7574 6520  rrect attribute 
+00002d60: 6e61 6d65 732e 0a61 7474 722d 6e61 6d69  names..attr-nami
+00002d70: 6e67 2d73 7479 6c65 3d73 6e61 6b65 5f63  ng-style=snake_c
+00002d80: 6173 650a 0a23 2052 6567 756c 6172 2065  ase..# Regular e
+00002d90: 7870 7265 7373 696f 6e20 6d61 7463 6869  xpression matchi
+00002da0: 6e67 2063 6f72 7265 6374 2061 7474 7269  ng correct attri
+00002db0: 6275 7465 206e 616d 6573 2e20 4f76 6572  bute names. Over
+00002dc0: 7269 6465 7320 6174 7472 2d6e 616d 696e  rides attr-namin
+00002dd0: 672d 0a23 2073 7479 6c65 2e0a 2361 7474  g-.# style..#att
+00002de0: 722d 7267 783d 0a0a 2320 4261 6420 7661  r-rgx=..# Bad va
+00002df0: 7269 6162 6c65 206e 616d 6573 2077 6869  riable names whi
+00002e00: 6368 2073 686f 756c 6420 616c 7761 7973  ch should always
+00002e10: 2062 6520 7265 6675 7365 642c 2073 6570   be refused, sep
+00002e20: 6172 6174 6564 2062 7920 6120 636f 6d6d  arated by a comm
+00002e30: 612e 0a62 6164 2d6e 616d 6573 3d66 6f6f  a..bad-names=foo
+00002e40: 2c0a 2020 2020 2020 2020 2020 6261 722c  ,.          bar,
+00002e50: 0a20 2020 2020 2020 2020 2062 617a 2c0a  .          baz,.
+00002e60: 2020 2020 2020 2020 2020 746f 746f 2c0a            toto,.
+00002e70: 2020 2020 2020 2020 2020 7475 7475 2c0a            tutu,.
+00002e80: 2020 2020 2020 2020 2020 7461 7461 0a0a            tata..
+00002e90: 2320 4261 6420 7661 7269 6162 6c65 206e  # Bad variable n
+00002ea0: 616d 6573 2072 6567 6578 6573 2c20 7365  ames regexes, se
+00002eb0: 7061 7261 7465 6420 6279 2061 2063 6f6d  parated by a com
+00002ec0: 6d61 2e20 4966 206e 616d 6573 206d 6174  ma. If names mat
+00002ed0: 6368 2061 6e79 2072 6567 6578 2c0a 2320  ch any regex,.# 
+00002ee0: 7468 6579 2077 696c 6c20 616c 7761 7973  they will always
+00002ef0: 2062 6520 7265 6675 7365 640a 6261 642d   be refused.bad-
+00002f00: 6e61 6d65 732d 7267 7873 3d0a 0a23 204e  names-rgxs=..# N
+00002f10: 616d 696e 6720 7374 796c 6520 6d61 7463  aming style matc
+00002f20: 6869 6e67 2063 6f72 7265 6374 2063 6c61  hing correct cla
+00002f30: 7373 2061 7474 7269 6275 7465 206e 616d  ss attribute nam
+00002f40: 6573 2e0a 636c 6173 732d 6174 7472 6962  es..class-attrib
+00002f50: 7574 652d 6e61 6d69 6e67 2d73 7479 6c65  ute-naming-style
+00002f60: 3d61 6e79 0a0a 2320 5265 6775 6c61 7220  =any..# Regular 
+00002f70: 6578 7072 6573 7369 6f6e 206d 6174 6368  expression match
+00002f80: 696e 6720 636f 7272 6563 7420 636c 6173  ing correct clas
+00002f90: 7320 6174 7472 6962 7574 6520 6e61 6d65  s attribute name
+00002fa0: 732e 204f 7665 7272 6964 6573 2063 6c61  s. Overrides cla
+00002fb0: 7373 2d0a 2320 6174 7472 6962 7574 652d  ss-.# attribute-
+00002fc0: 6e61 6d69 6e67 2d73 7479 6c65 2e0a 2363  naming-style..#c
+00002fd0: 6c61 7373 2d61 7474 7269 6275 7465 2d72  lass-attribute-r
+00002fe0: 6778 3d0a 0a23 204e 616d 696e 6720 7374  gx=..# Naming st
+00002ff0: 796c 6520 6d61 7463 6869 6e67 2063 6f72  yle matching cor
+00003000: 7265 6374 2063 6c61 7373 206e 616d 6573  rect class names
+00003010: 2e0a 636c 6173 732d 6e61 6d69 6e67 2d73  ..class-naming-s
+00003020: 7479 6c65 3d50 6173 6361 6c43 6173 650a  tyle=PascalCase.
+00003030: 0a23 2052 6567 756c 6172 2065 7870 7265  .# Regular expre
+00003040: 7373 696f 6e20 6d61 7463 6869 6e67 2063  ssion matching c
+00003050: 6f72 7265 6374 2063 6c61 7373 206e 616d  orrect class nam
+00003060: 6573 2e20 4f76 6572 7269 6465 7320 636c  es. Overrides cl
+00003070: 6173 732d 6e61 6d69 6e67 2d0a 2320 7374  ass-naming-.# st
+00003080: 796c 652e 0a23 636c 6173 732d 7267 783d  yle..#class-rgx=
+00003090: 0a0a 2320 4e61 6d69 6e67 2073 7479 6c65  ..# Naming style
+000030a0: 206d 6174 6368 696e 6720 636f 7272 6563   matching correc
+000030b0: 7420 636f 6e73 7461 6e74 206e 616d 6573  t constant names
+000030c0: 2e0a 636f 6e73 742d 6e61 6d69 6e67 2d73  ..const-naming-s
+000030d0: 7479 6c65 3d55 5050 4552 5f43 4153 450a  tyle=UPPER_CASE.
+000030e0: 0a23 2052 6567 756c 6172 2065 7870 7265  .# Regular expre
+000030f0: 7373 696f 6e20 6d61 7463 6869 6e67 2063  ssion matching c
+00003100: 6f72 7265 6374 2063 6f6e 7374 616e 7420  orrect constant 
+00003110: 6e61 6d65 732e 204f 7665 7272 6964 6573  names. Overrides
+00003120: 2063 6f6e 7374 2d6e 616d 696e 672d 0a23   const-naming-.#
+00003130: 2073 7479 6c65 2e0a 2363 6f6e 7374 2d72   style..#const-r
+00003140: 6778 3d0a 0a23 204d 696e 696d 756d 206c  gx=..# Minimum l
+00003150: 696e 6520 6c65 6e67 7468 2066 6f72 2066  ine length for f
+00003160: 756e 6374 696f 6e73 2f63 6c61 7373 6573  unctions/classes
+00003170: 2074 6861 7420 7265 7175 6972 6520 646f   that require do
+00003180: 6373 7472 696e 6773 2c20 7368 6f72 7465  cstrings, shorte
+00003190: 720a 2320 6f6e 6573 2061 7265 2065 7865  r.# ones are exe
+000031a0: 6d70 742e 0a64 6f63 7374 7269 6e67 2d6d  mpt..docstring-m
+000031b0: 696e 2d6c 656e 6774 683d 2d31 0a0a 2320  in-length=-1..# 
+000031c0: 4e61 6d69 6e67 2073 7479 6c65 206d 6174  Naming style mat
+000031d0: 6368 696e 6720 636f 7272 6563 7420 6675  ching correct fu
+000031e0: 6e63 7469 6f6e 206e 616d 6573 2e0a 6675  nction names..fu
+000031f0: 6e63 7469 6f6e 2d6e 616d 696e 672d 7374  nction-naming-st
+00003200: 796c 653d 736e 616b 655f 6361 7365 0a0a  yle=snake_case..
+00003210: 2320 5265 6775 6c61 7220 6578 7072 6573  # Regular expres
+00003220: 7369 6f6e 206d 6174 6368 696e 6720 636f  sion matching co
+00003230: 7272 6563 7420 6675 6e63 7469 6f6e 206e  rrect function n
+00003240: 616d 6573 2e20 4f76 6572 7269 6465 7320  ames. Overrides 
+00003250: 6675 6e63 7469 6f6e 2d0a 2320 6e61 6d69  function-.# nami
+00003260: 6e67 2d73 7479 6c65 2e0a 2366 756e 6374  ng-style..#funct
+00003270: 696f 6e2d 7267 783d 0a0a 2320 476f 6f64  ion-rgx=..# Good
+00003280: 2076 6172 6961 626c 6520 6e61 6d65 7320   variable names 
+00003290: 7768 6963 6820 7368 6f75 6c64 2061 6c77  which should alw
+000032a0: 6179 7320 6265 2061 6363 6570 7465 642c  ays be accepted,
+000032b0: 2073 6570 6172 6174 6564 2062 7920 6120   separated by a 
+000032c0: 636f 6d6d 612e 0a67 6f6f 642d 6e61 6d65  comma..good-name
+000032d0: 733d 692c 0a20 2020 2020 2020 2020 2020  s=i,.           
+000032e0: 6a2c 0a20 2020 2020 2020 2020 2020 6b2c  j,.           k,
+000032f0: 0a20 2020 2020 2020 2020 2020 6578 2c0a  .           ex,.
+00003300: 2020 2020 2020 2020 2020 2052 756e 2c0a             Run,.
+00003310: 2020 2020 2020 2020 2020 2062 792c 0a20             by,. 
+00003320: 2020 2020 2020 2020 2020 742c 0a20 2020            t,.   
+00003330: 2020 2020 2020 2020 5f2c 0a20 2020 2020          _,.     
+00003340: 2020 2020 2020 6466 2c0a 2020 2020 2020        df,.      
+00003350: 2020 2020 2064 730a 0a23 2047 6f6f 6420       ds..# Good 
+00003360: 7661 7269 6162 6c65 206e 616d 6573 2072  variable names r
+00003370: 6567 6578 6573 2c20 7365 7061 7261 7465  egexes, separate
+00003380: 6420 6279 2061 2063 6f6d 6d61 2e20 4966  d by a comma. If
+00003390: 206e 616d 6573 206d 6174 6368 2061 6e79   names match any
+000033a0: 2072 6567 6578 2c0a 2320 7468 6579 2077   regex,.# they w
+000033b0: 696c 6c20 616c 7761 7973 2062 6520 6163  ill always be ac
+000033c0: 6365 7074 6564 0a67 6f6f 642d 6e61 6d65  cepted.good-name
+000033d0: 732d 7267 7873 3d0a 0a23 2049 6e63 6c75  s-rgxs=..# Inclu
+000033e0: 6465 2061 2068 696e 7420 666f 7220 7468  de a hint for th
+000033f0: 6520 636f 7272 6563 7420 6e61 6d69 6e67  e correct naming
+00003400: 2066 6f72 6d61 7420 7769 7468 2069 6e76   format with inv
+00003410: 616c 6964 2d6e 616d 652e 0a69 6e63 6c75  alid-name..inclu
+00003420: 6465 2d6e 616d 696e 672d 6869 6e74 3d6e  de-naming-hint=n
+00003430: 6f0a 0a23 204e 616d 696e 6720 7374 796c  o..# Naming styl
+00003440: 6520 6d61 7463 6869 6e67 2063 6f72 7265  e matching corre
+00003450: 6374 2069 6e6c 696e 6520 6974 6572 6174  ct inline iterat
+00003460: 696f 6e20 6e61 6d65 732e 0a69 6e6c 696e  ion names..inlin
+00003470: 6576 6172 2d6e 616d 696e 672d 7374 796c  evar-naming-styl
+00003480: 653d 616e 790a 0a23 2052 6567 756c 6172  e=any..# Regular
+00003490: 2065 7870 7265 7373 696f 6e20 6d61 7463   expression matc
+000034a0: 6869 6e67 2063 6f72 7265 6374 2069 6e6c  hing correct inl
+000034b0: 696e 6520 6974 6572 6174 696f 6e20 6e61  ine iteration na
+000034c0: 6d65 732e 204f 7665 7272 6964 6573 0a23  mes. Overrides.#
+000034d0: 2069 6e6c 696e 6576 6172 2d6e 616d 696e   inlinevar-namin
+000034e0: 672d 7374 796c 652e 0a23 696e 6c69 6e65  g-style..#inline
+000034f0: 7661 722d 7267 783d 0a0a 2320 4e61 6d69  var-rgx=..# Nami
+00003500: 6e67 2073 7479 6c65 206d 6174 6368 696e  ng style matchin
+00003510: 6720 636f 7272 6563 7420 6d65 7468 6f64  g correct method
+00003520: 206e 616d 6573 2e0a 6d65 7468 6f64 2d6e   names..method-n
+00003530: 616d 696e 672d 7374 796c 653d 736e 616b  aming-style=snak
+00003540: 655f 6361 7365 0a0a 2320 5265 6775 6c61  e_case..# Regula
+00003550: 7220 6578 7072 6573 7369 6f6e 206d 6174  r expression mat
+00003560: 6368 696e 6720 636f 7272 6563 7420 6d65  ching correct me
+00003570: 7468 6f64 206e 616d 6573 2e20 4f76 6572  thod names. Over
+00003580: 7269 6465 7320 6d65 7468 6f64 2d6e 616d  rides method-nam
+00003590: 696e 672d 0a23 2073 7479 6c65 2e0a 236d  ing-.# style..#m
+000035a0: 6574 686f 642d 7267 783d 0a0a 2320 4e61  ethod-rgx=..# Na
+000035b0: 6d69 6e67 2073 7479 6c65 206d 6174 6368  ming style match
+000035c0: 696e 6720 636f 7272 6563 7420 6d6f 6475  ing correct modu
+000035d0: 6c65 206e 616d 6573 2e0a 6d6f 6475 6c65  le names..module
+000035e0: 2d6e 616d 696e 672d 7374 796c 653d 736e  -naming-style=sn
+000035f0: 616b 655f 6361 7365 0a0a 2320 5265 6775  ake_case..# Regu
+00003600: 6c61 7220 6578 7072 6573 7369 6f6e 206d  lar expression m
+00003610: 6174 6368 696e 6720 636f 7272 6563 7420  atching correct 
+00003620: 6d6f 6475 6c65 206e 616d 6573 2e20 4f76  module names. Ov
+00003630: 6572 7269 6465 7320 6d6f 6475 6c65 2d6e  errides module-n
+00003640: 616d 696e 672d 0a23 2073 7479 6c65 2e0a  aming-.# style..
+00003650: 236d 6f64 756c 652d 7267 783d 0a0a 2320  #module-rgx=..# 
+00003660: 436f 6c6f 6e2d 6465 6c69 6d69 7465 6420  Colon-delimited 
+00003670: 7365 7473 206f 6620 6e61 6d65 7320 7468  sets of names th
+00003680: 6174 2064 6574 6572 6d69 6e65 2065 6163  at determine eac
+00003690: 6820 6f74 6865 7227 7320 6e61 6d69 6e67  h other's naming
+000036a0: 2073 7479 6c65 2077 6865 6e0a 2320 7468   style when.# th
+000036b0: 6520 6e61 6d65 2072 6567 6578 6573 2061  e name regexes a
+000036c0: 6c6c 6f77 2073 6576 6572 616c 2073 7479  llow several sty
+000036d0: 6c65 732e 0a6e 616d 652d 6772 6f75 703d  les..name-group=
+000036e0: 0a0a 2320 5265 6775 6c61 7220 6578 7072  ..# Regular expr
+000036f0: 6573 7369 6f6e 2077 6869 6368 2073 686f  ession which sho
+00003700: 756c 6420 6f6e 6c79 206d 6174 6368 2066  uld only match f
+00003710: 756e 6374 696f 6e20 6f72 2063 6c61 7373  unction or class
+00003720: 206e 616d 6573 2074 6861 7420 646f 0a23   names that do.#
+00003730: 206e 6f74 2072 6571 7569 7265 2061 2064   not require a d
+00003740: 6f63 7374 7269 6e67 2e0a 6e6f 2d64 6f63  ocstring..no-doc
+00003750: 7374 7269 6e67 2d72 6778 3d5e 5f0a 0a23  string-rgx=^_..#
+00003760: 204c 6973 7420 6f66 2064 6563 6f72 6174   List of decorat
+00003770: 6f72 7320 7468 6174 2070 726f 6475 6365  ors that produce
+00003780: 2070 726f 7065 7274 6965 732c 2073 7563   properties, suc
+00003790: 6820 6173 2061 6263 2e61 6273 7472 6163  h as abc.abstrac
+000037a0: 7470 726f 7065 7274 792e 2041 6464 0a23  tproperty. Add.#
+000037b0: 2074 6f20 7468 6973 206c 6973 7420 746f   to this list to
+000037c0: 2072 6567 6973 7465 7220 6f74 6865 7220   register other 
+000037d0: 6465 636f 7261 746f 7273 2074 6861 7420  decorators that 
+000037e0: 7072 6f64 7563 6520 7661 6c69 6420 7072  produce valid pr
+000037f0: 6f70 6572 7469 6573 2e0a 2320 5468 6573  operties..# Thes
+00003800: 6520 6465 636f 7261 746f 7273 2061 7265  e decorators are
+00003810: 2074 616b 656e 2069 6e20 636f 6e73 6964   taken in consid
+00003820: 6572 6174 696f 6e20 6f6e 6c79 2066 6f72  eration only for
+00003830: 2069 6e76 616c 6964 2d6e 616d 652e 0a70   invalid-name..p
+00003840: 726f 7065 7274 792d 636c 6173 7365 733d  roperty-classes=
+00003850: 6162 632e 6162 7374 7261 6374 7072 6f70  abc.abstractprop
+00003860: 6572 7479 0a0a 2320 4e61 6d69 6e67 2073  erty..# Naming s
+00003870: 7479 6c65 206d 6174 6368 696e 6720 636f  tyle matching co
+00003880: 7272 6563 7420 7661 7269 6162 6c65 206e  rrect variable n
+00003890: 616d 6573 2e0a 7661 7269 6162 6c65 2d6e  ames..variable-n
+000038a0: 616d 696e 672d 7374 796c 653d 736e 616b  aming-style=snak
+000038b0: 655f 6361 7365 0a0a 2320 5265 6775 6c61  e_case..# Regula
+000038c0: 7220 6578 7072 6573 7369 6f6e 206d 6174  r expression mat
+000038d0: 6368 696e 6720 636f 7272 6563 7420 7661  ching correct va
+000038e0: 7269 6162 6c65 206e 616d 6573 2e20 4f76  riable names. Ov
+000038f0: 6572 7269 6465 7320 7661 7269 6162 6c65  errides variable
+00003900: 2d0a 2320 6e61 6d69 6e67 2d73 7479 6c65  -.# naming-style
+00003910: 2e0a 2376 6172 6961 626c 652d 7267 783d  ..#variable-rgx=
+00003920: 0a0a 0a5b 4c4f 4747 494e 475d 0a0a 2320  ...[LOGGING]..# 
+00003930: 5468 6520 7479 7065 206f 6620 7374 7269  The type of stri
+00003940: 6e67 2066 6f72 6d61 7474 696e 6720 7468  ng formatting th
+00003950: 6174 206c 6f67 6769 6e67 206d 6574 686f  at logging metho
+00003960: 6473 2064 6f2e 2060 6f6c 6460 206d 6561  ds do. `old` mea
+00003970: 6e73 2075 7369 6e67 2025 0a23 2066 6f72  ns using %.# for
+00003980: 6d61 7474 696e 672c 2060 6e65 7760 2069  matting, `new` i
+00003990: 7320 666f 7220 607b 7d60 2066 6f72 6d61  s for `{}` forma
+000039a0: 7474 696e 672e 0a6c 6f67 6769 6e67 2d66  tting..logging-f
+000039b0: 6f72 6d61 742d 7374 796c 653d 6f6c 640a  ormat-style=old.
+000039c0: 0a23 204c 6f67 6769 6e67 206d 6f64 756c  .# Logging modul
+000039d0: 6573 2074 6f20 6368 6563 6b20 7468 6174  es to check that
+000039e0: 2074 6865 2073 7472 696e 6720 666f 726d   the string form
+000039f0: 6174 2061 7267 756d 656e 7473 2061 7265  at arguments are
+00003a00: 2069 6e20 6c6f 6767 696e 670a 2320 6675   in logging.# fu
+00003a10: 6e63 7469 6f6e 2070 6172 616d 6574 6572  nction parameter
+00003a20: 2066 6f72 6d61 742e 0a6c 6f67 6769 6e67   format..logging
+00003a30: 2d6d 6f64 756c 6573 3d6c 6f67 6769 6e67  -modules=logging
+00003a40: 0a0a 0a5b 5354 5249 4e47 5d0a 0a23 2054  ...[STRING]..# T
+00003a50: 6869 7320 666c 6167 2063 6f6e 7472 6f6c  his flag control
+00003a60: 7320 7768 6574 6865 7220 696e 636f 6e73  s whether incons
+00003a70: 6973 7465 6e74 2d71 756f 7465 7320 6765  istent-quotes ge
+00003a80: 6e65 7261 7465 7320 6120 7761 726e 696e  nerates a warnin
+00003a90: 6720 7768 656e 2074 6865 0a23 2063 6861  g when the.# cha
+00003aa0: 7261 6374 6572 2075 7365 6420 6173 2061  racter used as a
+00003ab0: 2071 756f 7465 2064 656c 696d 6974 6572   quote delimiter
+00003ac0: 2069 7320 7573 6564 2069 6e63 6f6e 7369   is used inconsi
+00003ad0: 7374 656e 746c 7920 7769 7468 696e 2061  stently within a
+00003ae0: 206d 6f64 756c 652e 0a63 6865 636b 2d71   module..check-q
+00003af0: 756f 7465 2d63 6f6e 7369 7374 656e 6379  uote-consistency
+00003b00: 3d6e 6f0a 0a23 2054 6869 7320 666c 6167  =no..# This flag
+00003b10: 2063 6f6e 7472 6f6c 7320 7768 6574 6865   controls whethe
+00003b20: 7220 7468 6520 696d 706c 6963 6974 2d73  r the implicit-s
+00003b30: 7472 2d63 6f6e 6361 7420 7368 6f75 6c64  tr-concat should
+00003b40: 2067 656e 6572 6174 6520 6120 7761 726e   generate a warn
+00003b50: 696e 670a 2320 6f6e 2069 6d70 6c69 6369  ing.# on implici
+00003b60: 7420 7374 7269 6e67 2063 6f6e 6361 7465  t string concate
+00003b70: 6e61 7469 6f6e 2069 6e20 7365 7175 656e  nation in sequen
+00003b80: 6365 7320 6465 6669 6e65 6420 6f76 6572  ces defined over
+00003b90: 2073 6576 6572 616c 206c 696e 6573 2e0a   several lines..
+00003ba0: 6368 6563 6b2d 7374 722d 636f 6e63 6174  check-str-concat
+00003bb0: 2d6f 7665 722d 6c69 6e65 2d6a 756d 7073  -over-line-jumps
+00003bc0: 3d6e 6f0a 0a0a 5b44 4553 4947 4e5d 0a0a  =no...[DESIGN]..
+00003bd0: 2320 4d61 7869 6d75 6d20 6e75 6d62 6572  # Maximum number
+00003be0: 206f 6620 6172 6775 6d65 6e74 7320 666f   of arguments fo
+00003bf0: 7220 6675 6e63 7469 6f6e 202f 206d 6574  r function / met
+00003c00: 686f 642e 0a6d 6178 2d61 7267 733d 350a  hod..max-args=5.
+00003c10: 0a23 204d 6178 696d 756d 206e 756d 6265  .# Maximum numbe
+00003c20: 7220 6f66 2061 7474 7269 6275 7465 7320  r of attributes 
+00003c30: 666f 7220 6120 636c 6173 7320 2873 6565  for a class (see
+00003c40: 2052 3039 3032 292e 0a6d 6178 2d61 7474   R0902)..max-att
+00003c50: 7269 6275 7465 733d 370a 0a23 204d 6178  ributes=7..# Max
+00003c60: 696d 756d 206e 756d 6265 7220 6f66 2062  imum number of b
+00003c70: 6f6f 6c65 616e 2065 7870 7265 7373 696f  oolean expressio
+00003c80: 6e73 2069 6e20 616e 2069 6620 7374 6174  ns in an if stat
+00003c90: 656d 656e 7420 2873 6565 2052 3039 3136  ement (see R0916
+00003ca0: 292e 0a6d 6178 2d62 6f6f 6c2d 6578 7072  )..max-bool-expr
+00003cb0: 3d35 0a0a 2320 4d61 7869 6d75 6d20 6e75  =5..# Maximum nu
+00003cc0: 6d62 6572 206f 6620 6272 616e 6368 2066  mber of branch f
+00003cd0: 6f72 2066 756e 6374 696f 6e20 2f20 6d65  or function / me
+00003ce0: 7468 6f64 2062 6f64 792e 0a6d 6178 2d62  thod body..max-b
+00003cf0: 7261 6e63 6865 733d 3132 0a0a 2320 4d61  ranches=12..# Ma
+00003d00: 7869 6d75 6d20 6e75 6d62 6572 206f 6620  ximum number of 
+00003d10: 6c6f 6361 6c73 2066 6f72 2066 756e 6374  locals for funct
+00003d20: 696f 6e20 2f20 6d65 7468 6f64 2062 6f64  ion / method bod
+00003d30: 792e 0a6d 6178 2d6c 6f63 616c 733d 3135  y..max-locals=15
+00003d40: 0a0a 2320 4d61 7869 6d75 6d20 6e75 6d62  ..# Maximum numb
+00003d50: 6572 206f 6620 7061 7265 6e74 7320 666f  er of parents fo
+00003d60: 7220 6120 636c 6173 7320 2873 6565 2052  r a class (see R
+00003d70: 3039 3031 292e 0a6d 6178 2d70 6172 656e  0901)..max-paren
+00003d80: 7473 3d37 0a0a 2320 4d61 7869 6d75 6d20  ts=7..# Maximum 
+00003d90: 6e75 6d62 6572 206f 6620 7075 626c 6963  number of public
+00003da0: 206d 6574 686f 6473 2066 6f72 2061 2063   methods for a c
+00003db0: 6c61 7373 2028 7365 6520 5230 3930 3429  lass (see R0904)
+00003dc0: 2e0a 6d61 782d 7075 626c 6963 2d6d 6574  ..max-public-met
+00003dd0: 686f 6473 3d32 300a 0a23 204d 6178 696d  hods=20..# Maxim
+00003de0: 756d 206e 756d 6265 7220 6f66 2072 6574  um number of ret
+00003df0: 7572 6e20 2f20 7969 656c 6420 666f 7220  urn / yield for 
+00003e00: 6675 6e63 7469 6f6e 202f 206d 6574 686f  function / metho
+00003e10: 6420 626f 6479 2e0a 6d61 782d 7265 7475  d body..max-retu
+00003e20: 726e 733d 360a 0a23 204d 6178 696d 756d  rns=6..# Maximum
+00003e30: 206e 756d 6265 7220 6f66 2073 7461 7465   number of state
+00003e40: 6d65 6e74 7320 696e 2066 756e 6374 696f  ments in functio
+00003e50: 6e20 2f20 6d65 7468 6f64 2062 6f64 792e  n / method body.
+00003e60: 0a6d 6178 2d73 7461 7465 6d65 6e74 733d  .max-statements=
+00003e70: 3530 0a0a 2320 4d69 6e69 6d75 6d20 6e75  50..# Minimum nu
+00003e80: 6d62 6572 206f 6620 7075 626c 6963 206d  mber of public m
+00003e90: 6574 686f 6473 2066 6f72 2061 2063 6c61  ethods for a cla
+00003ea0: 7373 2028 7365 6520 5230 3930 3329 2e0a  ss (see R0903)..
+00003eb0: 6d69 6e2d 7075 626c 6963 2d6d 6574 686f  min-public-metho
+00003ec0: 6473 3d32 0a0a 0a5b 494d 504f 5254 535d  ds=2...[IMPORTS]
+00003ed0: 0a0a 2320 4c69 7374 206f 6620 6d6f 6475  ..# List of modu
+00003ee0: 6c65 7320 7468 6174 2063 616e 2062 6520  les that can be 
+00003ef0: 696d 706f 7274 6564 2061 7420 616e 7920  imported at any 
+00003f00: 6c65 7665 6c2c 206e 6f74 206a 7573 7420  level, not just 
+00003f10: 7468 6520 746f 7020 6c65 7665 6c0a 2320  the top level.# 
+00003f20: 6f6e 652e 0a61 6c6c 6f77 2d61 6e79 2d69  one..allow-any-i
+00003f30: 6d70 6f72 742d 6c65 7665 6c3d 0a0a 2320  mport-level=..# 
+00003f40: 416c 6c6f 7720 7769 6c64 6361 7264 2069  Allow wildcard i
+00003f50: 6d70 6f72 7473 2066 726f 6d20 6d6f 6475  mports from modu
+00003f60: 6c65 7320 7468 6174 2064 6566 696e 6520  les that define 
+00003f70: 5f5f 616c 6c5f 5f2e 0a61 6c6c 6f77 2d77  __all__..allow-w
+00003f80: 696c 6463 6172 642d 7769 7468 2d61 6c6c  ildcard-with-all
+00003f90: 3d6e 6f0a 0a23 2041 6e61 6c79 7365 2069  =no..# Analyse i
+00003fa0: 6d70 6f72 7420 6661 6c6c 6261 636b 2062  mport fallback b
+00003fb0: 6c6f 636b 732e 2054 6869 7320 6361 6e20  locks. This can 
+00003fc0: 6265 2075 7365 6420 746f 2073 7570 706f  be used to suppo
+00003fd0: 7274 2062 6f74 6820 5079 7468 6f6e 2032  rt both Python 2
+00003fe0: 2061 6e64 0a23 2033 2063 6f6d 7061 7469   and.# 3 compati
+00003ff0: 626c 6520 636f 6465 2c20 7768 6963 6820  ble code, which 
+00004000: 6d65 616e 7320 7468 6174 2074 6865 2062  means that the b
+00004010: 6c6f 636b 206d 6967 6874 2068 6176 6520  lock might have 
+00004020: 636f 6465 2074 6861 7420 6578 6973 7473  code that exists
+00004030: 0a23 206f 6e6c 7920 696e 206f 6e65 206f  .# only in one o
+00004040: 7220 616e 6f74 6865 7220 696e 7465 7270  r another interp
+00004050: 7265 7465 722c 206c 6561 6469 6e67 2074  reter, leading t
+00004060: 6f20 6661 6c73 6520 706f 7369 7469 7665  o false positive
+00004070: 7320 7768 656e 2061 6e61 6c79 7365 642e  s when analysed.
+00004080: 0a61 6e61 6c79 7365 2d66 616c 6c62 6163  .analyse-fallbac
+00004090: 6b2d 626c 6f63 6b73 3d6e 6f0a 0a23 2044  k-blocks=no..# D
+000040a0: 6570 7265 6361 7465 6420 6d6f 6475 6c65  eprecated module
+000040b0: 7320 7768 6963 6820 7368 6f75 6c64 206e  s which should n
+000040c0: 6f74 2062 6520 7573 6564 2c20 7365 7061  ot be used, sepa
+000040d0: 7261 7465 6420 6279 2061 2063 6f6d 6d61  rated by a comma
+000040e0: 2e0a 6465 7072 6563 6174 6564 2d6d 6f64  ..deprecated-mod
+000040f0: 756c 6573 3d6f 7074 7061 7273 652c 746b  ules=optparse,tk
+00004100: 696e 7465 722e 7469 780a 0a23 2043 7265  inter.tix..# Cre
+00004110: 6174 6520 6120 6772 6170 6820 6f66 2065  ate a graph of e
+00004120: 7874 6572 6e61 6c20 6465 7065 6e64 656e  xternal dependen
+00004130: 6369 6573 2069 6e20 7468 6520 6769 7665  cies in the give
+00004140: 6e20 6669 6c65 2028 7265 706f 7274 2052  n file (report R
+00004150: 5030 3430 3220 6d75 7374 0a23 206e 6f74  P0402 must.# not
+00004160: 2062 6520 6469 7361 626c 6564 292e 0a65   be disabled)..e
+00004170: 7874 2d69 6d70 6f72 742d 6772 6170 683d  xt-import-graph=
+00004180: 0a0a 2320 4372 6561 7465 2061 2067 7261  ..# Create a gra
+00004190: 7068 206f 6620 6576 6572 7920 2869 2e65  ph of every (i.e
+000041a0: 2e20 696e 7465 726e 616c 2061 6e64 2065  . internal and e
+000041b0: 7874 6572 6e61 6c29 2064 6570 656e 6465  xternal) depende
+000041c0: 6e63 6965 7320 696e 2074 6865 0a23 2067  ncies in the.# g
+000041d0: 6976 656e 2066 696c 6520 2872 6570 6f72  iven file (repor
+000041e0: 7420 5250 3034 3032 206d 7573 7420 6e6f  t RP0402 must no
+000041f0: 7420 6265 2064 6973 6162 6c65 6429 2e0a  t be disabled)..
+00004200: 696d 706f 7274 2d67 7261 7068 3d0a 0a23  import-graph=..#
+00004210: 2043 7265 6174 6520 6120 6772 6170 6820   Create a graph 
+00004220: 6f66 2069 6e74 6572 6e61 6c20 6465 7065  of internal depe
+00004230: 6e64 656e 6369 6573 2069 6e20 7468 6520  ndencies in the 
+00004240: 6769 7665 6e20 6669 6c65 2028 7265 706f  given file (repo
+00004250: 7274 2052 5030 3430 3220 6d75 7374 0a23  rt RP0402 must.#
+00004260: 206e 6f74 2062 6520 6469 7361 626c 6564   not be disabled
+00004270: 292e 0a69 6e74 2d69 6d70 6f72 742d 6772  )..int-import-gr
+00004280: 6170 683d 0a0a 2320 466f 7263 6520 696d  aph=..# Force im
+00004290: 706f 7274 206f 7264 6572 2074 6f20 7265  port order to re
+000042a0: 636f 676e 697a 6520 6120 6d6f 6475 6c65  cognize a module
+000042b0: 2061 7320 7061 7274 206f 6620 7468 6520   as part of the 
+000042c0: 7374 616e 6461 7264 0a23 2063 6f6d 7061  standard.# compa
+000042d0: 7469 6269 6c69 7479 206c 6962 7261 7269  tibility librari
+000042e0: 6573 2e0a 6b6e 6f77 6e2d 7374 616e 6461  es..known-standa
+000042f0: 7264 2d6c 6962 7261 7279 3d0a 0a23 2046  rd-library=..# F
+00004300: 6f72 6365 2069 6d70 6f72 7420 6f72 6465  orce import orde
+00004310: 7220 746f 2072 6563 6f67 6e69 7a65 2061  r to recognize a
+00004320: 206d 6f64 756c 6520 6173 2070 6172 7420   module as part 
+00004330: 6f66 2061 2074 6869 7264 2070 6172 7479  of a third party
+00004340: 206c 6962 7261 7279 2e0a 6b6e 6f77 6e2d   library..known-
+00004350: 7468 6972 642d 7061 7274 793d 656e 6368  third-party=ench
+00004360: 616e 740a 0a23 2043 6f75 706c 6573 206f  ant..# Couples o
+00004370: 6620 6d6f 6475 6c65 7320 616e 6420 7072  f modules and pr
+00004380: 6566 6572 7265 6420 6d6f 6475 6c65 732c  eferred modules,
+00004390: 2073 6570 6172 6174 6564 2062 7920 6120   separated by a 
+000043a0: 636f 6d6d 612e 0a70 7265 6665 7272 6564  comma..preferred
+000043b0: 2d6d 6f64 756c 6573 3d0a 0a0a 5b43 4c41  -modules=...[CLA
+000043c0: 5353 4553 5d0a 0a23 204c 6973 7420 6f66  SSES]..# List of
+000043d0: 206d 6574 686f 6420 6e61 6d65 7320 7573   method names us
+000043e0: 6564 2074 6f20 6465 636c 6172 6520 2869  ed to declare (i
+000043f0: 2e65 2e20 6173 7369 676e 2920 696e 7374  .e. assign) inst
+00004400: 616e 6365 2061 7474 7269 6275 7465 732e  ance attributes.
+00004410: 0a64 6566 696e 696e 672d 6174 7472 2d6d  .defining-attr-m
+00004420: 6574 686f 6473 3d5f 5f69 6e69 745f 5f2c  ethods=__init__,
+00004430: 0a20 2020 2020 2020 2020 2020 2020 2020  .               
+00004440: 2020 2020 2020 205f 5f6e 6577 5f5f 2c0a         __new__,.
+00004450: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004460: 2020 2020 2020 7365 7455 702c 0a20 2020        setUp,.   
+00004470: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004480: 2020 205f 5f70 6f73 745f 696e 6974 5f5f     __post_init__
+00004490: 0a0a 2320 4c69 7374 206f 6620 6d65 6d62  ..# List of memb
+000044a0: 6572 206e 616d 6573 2c20 7768 6963 6820  er names, which 
+000044b0: 7368 6f75 6c64 2062 6520 6578 636c 7564  should be exclud
+000044c0: 6564 2066 726f 6d20 7468 6520 7072 6f74  ed from the prot
+000044d0: 6563 7465 6420 6163 6365 7373 0a23 2077  ected access.# w
+000044e0: 6172 6e69 6e67 2e0a 6578 636c 7564 652d  arning..exclude-
+000044f0: 7072 6f74 6563 7465 643d 5f61 7364 6963  protected=_asdic
+00004500: 742c 0a20 2020 2020 2020 2020 2020 2020  t,.             
+00004510: 2020 2020 205f 6669 656c 6473 2c0a 2020       _fields,.  
+00004520: 2020 2020 2020 2020 2020 2020 2020 2020                  
+00004530: 5f72 6570 6c61 6365 2c0a 2020 2020 2020  _replace,.      
+00004540: 2020 2020 2020 2020 2020 2020 5f73 6f75              _sou
+00004550: 7263 652c 0a20 2020 2020 2020 2020 2020  rce,.           
+00004560: 2020 2020 2020 205f 6d61 6b65 0a0a 2320         _make..# 
+00004570: 4c69 7374 206f 6620 7661 6c69 6420 6e61  List of valid na
+00004580: 6d65 7320 666f 7220 7468 6520 6669 7273  mes for the firs
+00004590: 7420 6172 6775 6d65 6e74 2069 6e20 6120  t argument in a 
+000045a0: 636c 6173 7320 6d65 7468 6f64 2e0a 7661  class method..va
+000045b0: 6c69 642d 636c 6173 736d 6574 686f 642d  lid-classmethod-
+000045c0: 6669 7273 742d 6172 673d 636c 730a 0a23  first-arg=cls..#
+000045d0: 204c 6973 7420 6f66 2076 616c 6964 206e   List of valid n
+000045e0: 616d 6573 2066 6f72 2074 6865 2066 6972  ames for the fir
+000045f0: 7374 2061 7267 756d 656e 7420 696e 2061  st argument in a
+00004600: 206d 6574 6163 6c61 7373 2063 6c61 7373   metaclass class
+00004610: 206d 6574 686f 642e 0a76 616c 6964 2d6d   method..valid-m
+00004620: 6574 6163 6c61 7373 2d63 6c61 7373 6d65  etaclass-classme
+00004630: 7468 6f64 2d66 6972 7374 2d61 7267 3d63  thod-first-arg=c
+00004640: 6c73 0a0a 0a5b 4558 4345 5054 494f 4e53  ls...[EXCEPTIONS
+00004650: 5d0a 0a23 2045 7863 6570 7469 6f6e 7320  ]..# Exceptions 
+00004660: 7468 6174 2077 696c 6c20 656d 6974 2061  that will emit a
+00004670: 2077 6172 6e69 6e67 2077 6865 6e20 6265   warning when be
+00004680: 696e 6720 6361 7567 6874 2e20 4465 6661  ing caught. Defa
+00004690: 756c 7473 2074 6f0a 2320 2242 6173 6545  ults to.# "BaseE
+000046a0: 7863 6570 7469 6f6e 2c20 4578 6365 7074  xception, Except
+000046b0: 696f 6e22 2e0a 6f76 6572 6765 6e65 7261  ion"..overgenera
+000046c0: 6c2d 6578 6365 7074 696f 6e73 3d42 6173  l-exceptions=Bas
+000046d0: 6545 7863 6570 7469 6f6e 2c0a 2020 2020  eException,.    
+000046e0: 2020 2020 2020 2020 2020 2020 2020 2020                  
+000046f0: 2020 2045 7863 6570 7469 6f6e 0a            Exception.
```

### Comparing `rsatoolbox-0.1.3.dev51/CONTRIBUTING.md` & `rsatoolbox-0.1.3.dev56/CONTRIBUTING.md`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,75 +1,75 @@
-This is some guidance for RSAtoolbox contributors, but our approach evolves over time. Don't hesitate to contact @ilogue (Jasper) or @HeikoSchuett with any questions.
-
-
-Your cycle
-==========
-
-1. If you identify something that has to be fixed or done: create an issue. Discussions
-or questions about the theory or best practices should be filed as Github Discussions.
-2. If you want to start coding or documenting something, the first step is to check if anyone else is working on this in the list of Pull Requests. If not, create a branch on your local machine, commit something small such as a note or empty file, and push the commit to the same branch on GitHub. Then you can then open a Pull Request. This is for two reasons: you're communicating to the team that you're working on this (so we're not doing things twice), and it gives you and the others an easy way to track your progress.
-3. Commit regularly (typically every 10-30 minutes) and give your commits useful messages. "Changes to the data package" does not say anything about what you've done, "Added new feature model" does. If your commit is too large this makes it harder to write a short message.
-4. Write unit-tests to cover your new code. This is easier when you recently wrote the code. Tip: try writing tests before you implement the code. The test should assert at least the most important outcomes of the functionality (typically the value returned).
-5. Add Python type annotations where practical.
-6. When you're done with the feature, ask for reviews from two team members or ask the maintainers for help.
-
-
-How-to
-======
-
-1. `pip install -r requirements.txt` install rsatoolbox dependencies (and repeat when you make changes)
-2. `pip install -r tests/requirements.txt` install test dependencies (and repeat when you make changes)
-3. `pip uninstall rsatoolbox` uninstall rsatoolbox in your environment
-4. `rm dist/*` remove any previously built packages if necessary
-5. `python -m build` compile a new rsatoolbox package with your latest changes
-6. `pip install dist/*` install the new package
-7. `pytest` run the unit tests on the installed version of rsatoolbox
-8. run linting tools such as `flake8`, `vulture` to discover any style issues
-
-
-Rules
-=====
-
-1. Only through Pull Requests can you submit changes or additions to the code.
-2. Every Pull Request has to be reviewed by two team members.
-3. New code should have useful unit tests.
-4. Code should pass the `pylint` style check.
-5. Functions, classes, methods should have a `Google-style docstring`.
-6. Larger new features should come with narrative documentation and an example.
-7. When you're ready for your Pull Request to be reviewed, in the top right corner you can suggest two reviewers,
-or alternatively, ping @ilogue or @HeikoSchuett and we will assign reviewers.
-8. Consider how to handle NaNs in the user input. If your code can't handle them, you can throw an exception.
-
-
-Deployment
-==========
-
-
-- when a PR is merged into the branch main, it is build as a pre-release (or "development") package and uploaded to pypi. The latest pre-release version can be installed using `pip install --pre rsatoolbox`
-- when a release tag is added to the branch main, the package is instead marked as a released (or "stable") version.
-
-
-Naming scheme
-=============
-
-**Classes**
-
-- CamelCase
-- ends in noun
-
-*example: FancyModel*
-
-
-**Functions and methods**
-
-- lowercase with underscores
-- starts with verb
-
-*example: rdm.ranktransform(), transform_rank(rdm), calculate_gram_matrix, load_fmri_data*
-
-
-**Variables**
-
-- lowercase and underscore
-- typically nouns or concepts
-
-*example: contrast_matrix*
+This is some guidance for RSAtoolbox contributors, but our approach evolves over time. Don't hesitate to contact @ilogue (Jasper) or @HeikoSchuett with any questions.
+
+
+Your cycle
+==========
+
+1. If you identify something that has to be fixed or done: create an issue. Discussions
+or questions about the theory or best practices should be filed as Github Discussions.
+2. If you want to start coding or documenting something, the first step is to check if anyone else is working on this in the list of Pull Requests. If not, create a branch on your local machine, commit something small such as a note or empty file, and push the commit to the same branch on GitHub. Then you can then open a Pull Request. This is for two reasons: you're communicating to the team that you're working on this (so we're not doing things twice), and it gives you and the others an easy way to track your progress.
+3. Commit regularly (typically every 10-30 minutes) and give your commits useful messages. "Changes to the data package" does not say anything about what you've done, "Added new feature model" does. If your commit is too large this makes it harder to write a short message.
+4. Write unit-tests to cover your new code. This is easier when you recently wrote the code. Tip: try writing tests before you implement the code. The test should assert at least the most important outcomes of the functionality (typically the value returned).
+5. Add Python type annotations where practical.
+6. When you're done with the feature, ask for reviews from two team members or ask the maintainers for help.
+
+
+How-to
+======
+
+1. `pip install -r requirements.txt` install rsatoolbox dependencies (and repeat when you make changes)
+2. `pip install -r tests/requirements.txt` install test dependencies (and repeat when you make changes)
+3. `pip uninstall rsatoolbox` uninstall rsatoolbox in your environment
+4. `rm dist/*` remove any previously built packages if necessary
+5. `python -m build` compile a new rsatoolbox package with your latest changes
+6. `pip install dist/*` install the new package
+7. `pytest` run the unit tests on the installed version of rsatoolbox
+8. run linting tools such as `flake8`, `vulture` to discover any style issues
+
+
+Rules
+=====
+
+1. Only through Pull Requests can you submit changes or additions to the code.
+2. Every Pull Request has to be reviewed by two team members.
+3. New code should have useful unit tests.
+4. Code should pass the `pylint` style check.
+5. Functions, classes, methods should have a `Google-style docstring`.
+6. Larger new features should come with narrative documentation and an example.
+7. When you're ready for your Pull Request to be reviewed, in the top right corner you can suggest two reviewers,
+or alternatively, ping @ilogue or @HeikoSchuett and we will assign reviewers.
+8. Consider how to handle NaNs in the user input. If your code can't handle them, you can throw an exception.
+
+
+Deployment
+==========
+
+
+- when a PR is merged into the branch main, it is build as a pre-release (or "development") package and uploaded to pypi. The latest pre-release version can be installed using `pip install --pre rsatoolbox`
+- when a release tag is added to the branch main, the package is instead marked as a released (or "stable") version.
+
+
+Naming scheme
+=============
+
+**Classes**
+
+- CamelCase
+- ends in noun
+
+*example: FancyModel*
+
+
+**Functions and methods**
+
+- lowercase with underscores
+- starts with verb
+
+*example: rdm.ranktransform(), transform_rank(rdm), calculate_gram_matrix, load_fmri_data*
+
+
+**Variables**
+
+- lowercase and underscore
+- typically nouns or concepts
+
+*example: contrast_matrix*
```

### Comparing `rsatoolbox-0.1.3.dev51/PKG-INFO` & `rsatoolbox-0.1.3.dev56/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-Metadata-Version: 2.1
-Name: rsatoolbox
-Version: 0.1.3.dev51
-Summary: Representational Similarity Analysis (RSA) in Python
-Author: rsatoolbox authors
-License: MIT License
-        
-        Copyright (c) 2019 rsatoolbox authors
-        
-        Permission is hereby granted, free of charge, to any person obtaining a copy
-        of this software and associated documentation files (the "Software"), to deal
-        in the Software without restriction, including without limitation the rights
-        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-        copies of the Software, and to permit persons to whom the Software is
-        furnished to do so, subject to the following conditions:
-        
-        The above copyright notice and this permission notice shall be included in all
-        copies or substantial portions of the Software.
-        
-        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-        SOFTWARE.
-        
-Project-URL: homepage, https://github.com/rsagroup/rsatoolbox
-Project-URL: documentation, https://rsatoolbox.readthedocs.io/
-Keywords: neuroscience
-Classifier: Programming Language :: Python
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Classifier: Development Status :: 4 - Beta
-Classifier: Topic :: Scientific/Engineering
-Classifier: Intended Audience :: Science/Research
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Requires-Python: <3.12,>=3.7
-Description-Content-Type: text/x-rst
-License-File: LICENSE
-License-File: AUTHORS
-
-# Representational Similarity Analysis 3.0
-
-[![Documentation Status](https://readthedocs.org/projects/rsatoolbox/badge/?version=latest)](https://rsatoolbox.readthedocs.io/en/latest/?badge=latest)
-[![PyPI version](https://badge.fury.io/py/rsatoolbox.svg)](https://badge.fury.io/py/rsatoolbox)
-[![Codacy Badge](https://app.codacy.com/project/badge/Grade/626ca9ec9f75485a9f73783c02710b1f)](https://www.codacy.com/gh/rsagroup/rsatoolbox?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=rsagroup/rsatoolbox&amp;utm_campaign=Badge_Grade)
-[![CodeFactor](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox/badge)](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox)
-[![codecov](https://codecov.io/gh/rsagroup/rsatoolbox/branch/master/graph/badge.svg)](https://codecov.io/gh/rsagroup/rsatoolbox)
-
-
-Conceived during the RSA retreat 2019 in Blue Mountains.
-
-[Documentation](https://rsatoolbox.readthedocs.io/)
-
-
-#### Getting Started
-
-To install the latest stable version of rsatoolbox with pip:
-
-```sh
-pip install rsatoolbox
-```
-
-or with conda:
-
-```sh
-conda install -c conda-forge rsatoolbox
-```
-
-
-here is a simple code sample:
-
-```python
-import numpy, rsatoolbox
-data = rsatoolbox.data.Dataset(numpy.random.rand(10, 5))
-rdms = rsatoolbox.rdm.calc_rdm(data)
-rsatoolbox.vis.show_rdm(rdms)
-```
+Metadata-Version: 2.1
+Name: rsatoolbox
+Version: 0.1.3.dev56
+Summary: Representational Similarity Analysis (RSA) in Python
+Author: rsatoolbox authors
+License: MIT License
+        
+        Copyright (c) 2019 rsatoolbox authors
+        
+        Permission is hereby granted, free of charge, to any person obtaining a copy
+        of this software and associated documentation files (the "Software"), to deal
+        in the Software without restriction, including without limitation the rights
+        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+        copies of the Software, and to permit persons to whom the Software is
+        furnished to do so, subject to the following conditions:
+        
+        The above copyright notice and this permission notice shall be included in all
+        copies or substantial portions of the Software.
+        
+        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+        SOFTWARE.
+        
+Project-URL: homepage, https://github.com/rsagroup/rsatoolbox
+Project-URL: documentation, https://rsatoolbox.readthedocs.io/
+Keywords: neuroscience
+Classifier: Programming Language :: Python
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Classifier: Development Status :: 4 - Beta
+Classifier: Topic :: Scientific/Engineering
+Classifier: Intended Audience :: Science/Research
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Requires-Python: <3.12,>=3.7
+Description-Content-Type: text/x-rst
+License-File: LICENSE
+License-File: AUTHORS
+
+# Representational Similarity Analysis 3.0
+
+[![Documentation Status](https://readthedocs.org/projects/rsatoolbox/badge/?version=latest)](https://rsatoolbox.readthedocs.io/en/latest/?badge=latest)
+[![PyPI version](https://badge.fury.io/py/rsatoolbox.svg)](https://badge.fury.io/py/rsatoolbox)
+[![Codacy Badge](https://app.codacy.com/project/badge/Grade/626ca9ec9f75485a9f73783c02710b1f)](https://www.codacy.com/gh/rsagroup/rsatoolbox?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=rsagroup/rsatoolbox&amp;utm_campaign=Badge_Grade)
+[![CodeFactor](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox/badge)](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox)
+[![codecov](https://codecov.io/gh/rsagroup/rsatoolbox/branch/master/graph/badge.svg)](https://codecov.io/gh/rsagroup/rsatoolbox)
+
+
+Conceived during the RSA retreat 2019 in Blue Mountains.
+
+[Documentation](https://rsatoolbox.readthedocs.io/)
+
+
+#### Getting Started
+
+To install the latest stable version of rsatoolbox with pip:
+
+```sh
+pip install rsatoolbox
+```
+
+or with conda:
+
+```sh
+conda install -c conda-forge rsatoolbox
+```
+
+
+here is a simple code sample:
+
+```python
+import numpy, rsatoolbox
+data = rsatoolbox.data.Dataset(numpy.random.rand(10, 5))
+rdms = rsatoolbox.rdm.calc_rdm(data)
+rsatoolbox.vis.show_rdm(rdms)
+```
```

### Comparing `rsatoolbox-0.1.3.dev51/pyproject.toml` & `rsatoolbox-0.1.3.dev56/pyproject.toml`

 * *Ordering differences only*

 * *Files 22% similar despite different names*

```diff
@@ -1,61 +1,61 @@
-[build-system]
-requires = [
-    "setuptools~=65.3",
-    "setuptools-scm[toml]~=7.0.0",
-    "wheel",
-    "numpy>=1.21.2",
-    "scipy",
-    "cython~=3.0.0a11",
-    "twine~=4.0.1"
-]
-build-backend = "setuptools.build_meta"
-
-[project]
-name = "rsatoolbox"
-description = "Representational Similarity Analysis (RSA) in Python"
-requires-python = ">=3.7,<3.12"
-authors = [
-    {name="rsatoolbox authors"},
-]
-keywords = ["neuroscience"]
-license = {file = "LICENSE"}
-classifiers = [
-      'Programming Language :: Python',
-      'License :: OSI Approved :: MIT License',
-      'Operating System :: OS Independent',
-      'Development Status :: 4 - Beta',
-      'Topic :: Scientific/Engineering',
-      'Intended Audience :: Science/Research',
-      'Programming Language :: Python :: 3.7',
-      'Programming Language :: Python :: 3.8',
-      'Programming Language :: Python :: 3.9',
-      'Programming Language :: Python :: 3.10',
-      'Programming Language :: Python :: 3.11',
-]
-dynamic = ["readme", "dependencies", "version"]
-
-[project.urls]
-homepage = "https://github.com/rsagroup/rsatoolbox"
-documentation = "https://rsatoolbox.readthedocs.io/"
-
-[tool.setuptools_scm]
-local_scheme = "no-local-version"
-
-[tool.setuptools.packages.find]
-where = ["src"]
-
-[tool.setuptools.dynamic]
-readme = {file = "README.md"}
-dependencies = {file = "requirements.txt"}
-
-[tool.pytest.ini_options]
-testpaths = [
-    "tests"
-]
-python_files = "*.py"
-
-[tool.cibuildwheel]
-test-requires = "pytest"
-test-command = "pytest {project}/tests"
-before-test = "pip install -r tests/requirements.txt"
-skip = ["*-win32", "*-manylinux_i686", "*-musllinux_*", "pp*"]
+[build-system]
+requires = [
+    "setuptools~=65.3",
+    "setuptools-scm[toml]~=7.0.0",
+    "wheel",
+    "numpy>=1.21.2",
+    "scipy",
+    "cython~=3.0.0a11",
+    "twine~=4.0.1"
+]
+build-backend = "setuptools.build_meta"
+
+[project]
+name = "rsatoolbox"
+description = "Representational Similarity Analysis (RSA) in Python"
+requires-python = ">=3.7,<3.12"
+authors = [
+    {name="rsatoolbox authors"},
+]
+keywords = ["neuroscience"]
+license = {file = "LICENSE"}
+classifiers = [
+      'Programming Language :: Python',
+      'License :: OSI Approved :: MIT License',
+      'Operating System :: OS Independent',
+      'Development Status :: 4 - Beta',
+      'Topic :: Scientific/Engineering',
+      'Intended Audience :: Science/Research',
+      'Programming Language :: Python :: 3.7',
+      'Programming Language :: Python :: 3.8',
+      'Programming Language :: Python :: 3.9',
+      'Programming Language :: Python :: 3.10',
+      'Programming Language :: Python :: 3.11',
+]
+dynamic = ["readme", "dependencies", "version"]
+
+[project.urls]
+homepage = "https://github.com/rsagroup/rsatoolbox"
+documentation = "https://rsatoolbox.readthedocs.io/"
+
+[tool.setuptools_scm]
+local_scheme = "no-local-version"
+
+[tool.setuptools.packages.find]
+where = ["src"]
+
+[tool.setuptools.dynamic]
+readme = {file = "README.md"}
+dependencies = {file = "requirements.txt"}
+
+[tool.pytest.ini_options]
+testpaths = [
+    "tests"
+]
+python_files = "*.py"
+
+[tool.cibuildwheel]
+test-requires = "pytest"
+test-command = "pytest {project}/tests"
+before-test = "pip install -r tests/requirements.txt"
+skip = ["*-win32", "*-manylinux_i686", "*-musllinux_*", "pp*"]
```

### Comparing `rsatoolbox-0.1.3.dev51/setup.py` & `rsatoolbox-0.1.3.dev56/setup.py`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,17 +1,17 @@
-"""Setup.py now only remains as a build script for the cython extensions.
-Using setup.py for other things is now deprecated:
-    setup.py test -> pytest
-    setup.py develop -> pip install -e
-"""
-from setuptools import setup, Extension
-import setuptools_scm  # noqa # pylint: disable=unused-import
-from Cython.Build import build_ext
-
-
-setup(
-    ext_modules = [
-        Extension(
-            "rsatoolbox.cengine.similarity",
-            ["src/rsatoolbox/cengine/similarity.pyx"])],
-    cmdclass={'build_ext': build_ext}
-)
+"""Setup.py now only remains as a build script for the cython extensions.
+Using setup.py for other things is now deprecated:
+    setup.py test -> pytest
+    setup.py develop -> pip install -e
+"""
+from setuptools import setup, Extension
+import setuptools_scm  # noqa # pylint: disable=unused-import
+from Cython.Build import build_ext
+
+
+setup(
+    ext_modules = [
+        Extension(
+            "rsatoolbox.cengine.similarity",
+            ["src/rsatoolbox/cengine/similarity.pyx"])],
+    cmdclass={'build_ext': build_ext}
+)
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/cengine/similarity.pyx` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/cengine/similarity.pyx`

 * *Ordering differences only*

 * *Files 27% similar despite different names*

```diff
@@ -1,329 +1,329 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-
-import cython
-from cython.view cimport array as cvarray
-from libc.math cimport log, sqrt, isnan, NAN
-from cpython.mem cimport PyMem_Malloc, PyMem_Realloc, PyMem_Free
-cimport scipy.linalg.cython_blas as blas
-
-
-@cython.boundscheck(False)
-@cython.cdivision(True)
-cpdef double [:] calc(
-    double [:, :] data, long [:] desc,
-    long [:] cv_desc, int n,
-    int method_idx, double [:, :] noise=None,
-    double prior_lambda=1, double prior_weight=0.1,
-    int weighting=1, int crossval=0):
-    # calculates an RDM from a double array of data with integer descriptors
-    # There are no checks or saveguards in this function!
-    # All entries in desc should be in [0, n-1]. They will be used for indexing
-    # into the RDM running into segfaults if they are outside the range.
-    # Inputs:
-    # double [:, :] data : the data
-    # long [:] desc : defines the patterns
-    # long [:] cv_desc : rows with equal values are excluded from the computation
-    # int n : defines the RDM size, all desc should be < n
-    # int method_idx: which method to use:
-    #     1: method == 'euclidean'
-    #     2: method == 'correlation'
-    #     3: method in ['mahalanobis', 'crossnobis']
-    #     4: method in ['poisson', 'poisson_cv']
-    # double [:, :] noise = None: noise for Mahalanobis/Crossnobis
-    # double prior_lambda=1 : for poisson KL
-    # double prior_weight=0.1 : for poisson KL
-    # int weighting=1 : controls weighting of rows:
-    #     0: each row has equal weight
-    #     1: rows weighted by number of valid measurements
-    cdef:
-        double [:] vec_i
-        double [:] vec_j
-        double weight, sim
-        double [:] weights
-        double [:] values
-        int i, j, idx
-        int n_rdm = (n * (n-1)) / 2
-        int n_dim = data.shape[1]
-        double prior_lambda_l = prior_lambda * prior_weight
-        double prior_weight_l = 1 + prior_weight
-        double [:, :] log_data
-    if (method_idx > 4) or (method_idx < 1):
-        raise ValueError('dissimilarity method not recognized!')
-    # precompute stuff for poisson KL
-    if method_idx == 4:
-        data = data.copy()
-        log_data = data.copy()
-        for i in range(data.shape[0]):
-            for j in range(n_dim):
-                data[i, j] = (data[i, j] + prior_lambda_l) / prior_weight_l
-                log_data[i, j] = log(data[i, j])
-    weights = <double [:(n_rdm+n)]> PyMem_Malloc((n_rdm+n) * sizeof(double))
-    values = <double [:(n_rdm+n)]> PyMem_Malloc((n_rdm+n) * sizeof(double))
-    for idx in range(n_rdm + n):
-        weights[idx] = 0
-        values[idx] = 0
-    for i in range(data.shape[0]):
-        if not crossval:
-            if method_idx == 1: # method == 'euclidean':
-                sim, weight = euclid(data[i], data[i], n_dim)
-            elif method_idx == 2: # method == 'correlation':
-                sim, weight = correlation(data[i], data[i], n_dim)
-            elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
-                if noise is None:
-                    sim, weight = euclid(data[i], data[i], n_dim)
-                else:
-                    sim = mahalanobis(data[i], data[i], n_dim, noise)
-                    weight = <double> n_dim
-            elif method_idx == 4: # method in ['poisson', 'poisson_cv']:
-                sim, weight = poisson_cv(data[i], data[i], log_data[i], log_data[i], n_dim)
-            idx = desc[i]
-            if weighting == 1: #'number':
-                values[idx] += sim / 2
-                weights[idx] += weight / 2
-            elif weighting == 0: #'equal':
-                values[idx] += sim / weight / 2
-                weights[idx] += 1 / 2
-        for j in range(i + 1, data.shape[0]):
-            if not crossval or not cv_desc[i] == cv_desc[j]:
-                #vec_i = data[i]
-                #vec_j = data[j]
-                if method_idx == 1: # method == 'euclidean':
-                    sim, weight = euclid(data[i], data[j], n_dim)
-                elif method_idx == 2: # method == 'correlation':
-                    sim, weight = correlation(data[i], data[j], n_dim)
-                elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
-                    if noise is None:
-                        sim, weight = euclid(data[i], data[j], n_dim)
-                    else:
-                        sim = mahalanobis(data[i], data[j], n_dim, noise)
-                        weight = <double> n_dim
-                elif method_idx == 4: # method in ['poisson', 'poisson_cv']:
-                    sim, weight = poisson_cv(data[i], data[j], log_data[i], log_data[j], n_dim)
-                if weight > 0:
-                    if desc[i] == desc[j]:
-                        idx = desc[i]
-                    else:
-                        if desc[j] > desc[i]:
-                            idx = (n - 1) * desc[i] - (((desc[i] + 1) * desc[i]) / 2) + desc[j] - 1 + n
-                        else:
-                            idx = (n - 1) * desc[j] - (((desc[j] + 1) * desc[j]) / 2) + desc[i] - 1 + n
-                    if weighting == 1: #'number':
-                        values[idx] += sim
-                        weights[idx] += weight
-                    elif weighting == 0: #'equal':
-                        values[idx] += sim / weight
-                        weights[idx] += 1
-    for idx in range(n_rdm + n):
-        if weights[idx] > 0:
-            values[idx] = values[idx] / weights[idx]
-        else:
-            values[idx] = NAN
-    return values
-
-
-@cython.boundscheck(False)
-@cython.cdivision(True)
-cpdef (double, double) calc_one(
-    double [:, :] data_i, double [:, :] data_j,
-    long [:] cv_desc_i, long [:] cv_desc_j,
-    int n_i, int n_j,
-    int method_idx, double [:, :] noise=None,
-    double prior_lambda=1, double prior_weight=0.1,
-    int weighting=1):
-    cdef:
-        #double [:] values = np.zeros(n_i * n_j)
-        #double [:] weights = np.zeros(n_i * n_j)
-        double [:] vec_i
-        double [:] vec_j
-        double weight, sim, weight_sum, value
-        int i, j
-        int n_dim = data_i.shape[1]
-        double prior_lambda_l = prior_lambda * prior_weight
-        double prior_weight_l = 1 + prior_weight
-        double [:, :] log_data_i
-        double [:, :] log_data_j
-    if (method_idx > 4) or (method_idx < 1):
-        raise ValueError('dissimilarity method not recognized!')
-    # precompute stuff for poisson KL
-    if method_idx == 4:
-        data_i = data_i.copy()
-        log_data_i = data_i.copy()
-        for i in range(data_i.shape[0]):
-            for j in range(n_dim):
-                data_i[i, j] = (data_i[i, j] + prior_lambda_l) / prior_weight_l
-                log_data_i[i, j] = log(data_i[i, j])
-        data_j = data_j.copy()
-        log_data_j = data_j.copy()
-        for i in range(data_j.shape[0]):
-            for j in range(n_dim):
-                data_j[i, j] = (data_j[i, j] + prior_lambda_l) / prior_weight_l
-                log_data_j[i, j] = log(data_j[i, j])
-    weight_sum = 0
-    value = 0
-    for i in range(n_i):
-        for j in range(n_j):
-            if not (cv_desc_i[i] == cv_desc_j[j]):
-                if method_idx == 1: # method == 'euclidean':
-                    sim, weight = euclid(data_i[i], data_j[j], n_dim)
-                elif method_idx == 2: # method == 'correlation':
-                    sim, weight = correlation(data_i[i], data_j[j], n_dim)
-                elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
-                    if noise is None:
-                        sim, weight = euclid(data_i[i], data_j[j], n_dim)
-                    else:
-                        sim = mahalanobis(data_i[i], data_j[j], n_dim, noise)
-                        weight = <double> n_dim
-                elif method_idx == 4: # method in ['poisson', 'poisson_cv']:
-                    sim, weight = poisson_cv(data_i[i], data_j[j], log_data_i[i], log_data_j[j], n_dim)
-                if weight > 0:
-                    if weighting == 1: #'number':
-                        value += sim
-                        weight_sum += weight
-                    elif weighting == 0: #'equal':
-                        value += sim / weight
-                        weight_sum += 1
-    if weight_sum > 0:
-        value = value / weight_sum
-    else:
-        value = NAN
-    return value, weight_sum
-
-
-@cython.boundscheck(False)
-cpdef (double, double) similarity(double [:] vec_i, double [:] vec_j, int method_idx,
-                       int n_dim, double [:, :] noise):
-    """
-    double similarity(double [:] vec_i, double [:] vec_j, int method_idx,
-                      int n_dim, double [:, :] noise=None)
-
-    This is a single similarity computation in cython.
-    remember to call everything with continuous numpy arrays.
-    In particular, noise must be such an array for Mahalanobis distances!
-
-    Mahalanobis distances require full measurement vectors at the moment!
-    """
-    cdef double sim
-    cdef double weight
-    if method_idx == 1: # method == 'euclidean':
-        sim, weight = euclid(vec_i, vec_j, n_dim)
-    elif method_idx == 2: # method == 'correlation':
-        sim, weight = correlation(vec_i, vec_j, n_dim)
-    elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
-        if noise is None:
-            sim, weight = euclid(vec_i, vec_j, n_dim)
-        else:
-            sim = mahalanobis(vec_i, vec_j, n_dim, noise)
-            weight = <double> n_dim
-    return sim, weight
-
-
-@cython.boundscheck(False)
-cdef (double, double) euclid(double [:] vec_i, double [:] vec_j, int n_dim):
-    cdef:
-        double sim = 0
-        double weight = 0
-        int i
-    for i in range(n_dim):
-        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
-            sim += vec_i[i] * vec_j[i]
-            weight += 1
-    return sim, weight
-
-
-@cython.boundscheck(False)
-@cython.cdivision(True)
-cdef (double, double) poisson_cv(double [:] vec_i, double [:] vec_j,
-                                 double [:] log_vec_i, double [:] log_vec_j,
-                                 int n_dim):
-    cdef:
-        double sim = 0
-        double weight = 0
-        int i
-    for i in range(n_dim):
-        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
-            sim += (vec_j[i] - vec_i[i]) * (log_vec_i[i] - log_vec_j[i])
-            weight += 1
-    sim = sim / 2.0
-    return (sim, weight)
-
-
-@cython.boundscheck(False)
-cdef double mahalanobis(double [:] vec_i, double [:] vec_j, int n_dim,
-                        double [:, :] noise):
-    cdef:
-        double *vec1
-        double *vec2
-        int *finite
-        int zero = 0
-        int one = 1
-        double onef = 1.0
-        double zerof = 0.0
-        char trans = b'n'
-        double sim = 0.0
-        int i, j, k, l, n_finite
-        double [:, :] noise_small
-    finite = <int*> PyMem_Malloc(n_dim * sizeof(int))
-    # use finite as a bool to choose the non-nan values
-    n_finite = 0
-    for i in range(n_dim):
-        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
-            finite[i] = 1
-            n_finite += 1
-        else:
-            finite[i] = 0
-    vec1 = <double*> PyMem_Malloc(n_finite * sizeof(double))
-    vec2 = <double*> PyMem_Malloc(n_finite * sizeof(double))
-    vec3 = <double*> PyMem_Malloc(n_finite * sizeof(double))
-    #noise_small = <double [:n_finite, :n_finite]> PyMem_Malloc(n_finite * n_finite * sizeof(double))
-    noise_small = cvarray(shape=(n_finite, n_finite), itemsize=sizeof(double), format="d")
-    k = 0
-    for i in range(n_dim):
-        if finite[i]:
-            vec1[k] = vec_i[i]
-            vec2[k] = vec_j[i]
-            l = 0
-            for j in range(n_dim):
-                if finite[j]:
-                    noise_small[k, l] = noise[i, j]
-                    l += 1
-            k += 1
-    blas.dgemv(&trans, &n_finite, &n_finite, &onef, &noise_small[0, 0], &n_finite, vec2, &one, &zerof, vec3, &one)
-    for i in range(n_dim):
-        sim += vec1[i] * vec3[i]
-    PyMem_Free(vec1)
-    PyMem_Free(vec2)
-    PyMem_Free(vec3)
-    PyMem_Free(finite)
-    return sim
-
-
-@cython.boundscheck(False)
-@cython.cdivision(True)
-cdef (double, double) correlation(double [:] vec_i, double [:] vec_j, int n_dim):
-    cdef:
-        double si = 0.0
-        double sj = 0.0
-        double si2 = 0.0
-        double sj2 = 0.0
-        double sij = 0.0
-        double sim
-        double weight = 0
-        int i
-    for i in range(n_dim):
-        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
-            si += vec_i[i]
-            sj += vec_j[i]
-            si2 += vec_i[i] * vec_i[i]
-            sj2 += vec_j[i] * vec_j[i]
-            sij += vec_i[i] * vec_j[i]
-            weight += 1
-    if si2 > 0 and sj2 > 0:
-        # sim = (np.sum(vec_i * vec_j) / np.sqrt(norm_i) / np.sqrt(norm_j))
-        sim = sij - (si * sj / n_dim)
-        sim /= sqrt(si2 - (si * si / n_dim))
-        sim /= sqrt(sj2 - (sj * sj / n_dim))
-    else:
-        sim = 1
-    sim = sim * n_dim / 2
-    return sim, weight
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+
+import cython
+from cython.view cimport array as cvarray
+from libc.math cimport log, sqrt, isnan, NAN
+from cpython.mem cimport PyMem_Malloc, PyMem_Realloc, PyMem_Free
+cimport scipy.linalg.cython_blas as blas
+
+
+@cython.boundscheck(False)
+@cython.cdivision(True)
+cpdef double [:] calc(
+    double [:, :] data, long [:] desc,
+    long [:] cv_desc, int n,
+    int method_idx, double [:, :] noise=None,
+    double prior_lambda=1, double prior_weight=0.1,
+    int weighting=1, int crossval=0):
+    # calculates an RDM from a double array of data with integer descriptors
+    # There are no checks or saveguards in this function!
+    # All entries in desc should be in [0, n-1]. They will be used for indexing
+    # into the RDM running into segfaults if they are outside the range.
+    # Inputs:
+    # double [:, :] data : the data
+    # long [:] desc : defines the patterns
+    # long [:] cv_desc : rows with equal values are excluded from the computation
+    # int n : defines the RDM size, all desc should be < n
+    # int method_idx: which method to use:
+    #     1: method == 'euclidean'
+    #     2: method == 'correlation'
+    #     3: method in ['mahalanobis', 'crossnobis']
+    #     4: method in ['poisson', 'poisson_cv']
+    # double [:, :] noise = None: noise for Mahalanobis/Crossnobis
+    # double prior_lambda=1 : for poisson KL
+    # double prior_weight=0.1 : for poisson KL
+    # int weighting=1 : controls weighting of rows:
+    #     0: each row has equal weight
+    #     1: rows weighted by number of valid measurements
+    cdef:
+        double [:] vec_i
+        double [:] vec_j
+        double weight, sim
+        double [:] weights
+        double [:] values
+        int i, j, idx
+        int n_rdm = (n * (n-1)) / 2
+        int n_dim = data.shape[1]
+        double prior_lambda_l = prior_lambda * prior_weight
+        double prior_weight_l = 1 + prior_weight
+        double [:, :] log_data
+    if (method_idx > 4) or (method_idx < 1):
+        raise ValueError('dissimilarity method not recognized!')
+    # precompute stuff for poisson KL
+    if method_idx == 4:
+        data = data.copy()
+        log_data = data.copy()
+        for i in range(data.shape[0]):
+            for j in range(n_dim):
+                data[i, j] = (data[i, j] + prior_lambda_l) / prior_weight_l
+                log_data[i, j] = log(data[i, j])
+    weights = <double [:(n_rdm+n)]> PyMem_Malloc((n_rdm+n) * sizeof(double))
+    values = <double [:(n_rdm+n)]> PyMem_Malloc((n_rdm+n) * sizeof(double))
+    for idx in range(n_rdm + n):
+        weights[idx] = 0
+        values[idx] = 0
+    for i in range(data.shape[0]):
+        if not crossval:
+            if method_idx == 1: # method == 'euclidean':
+                sim, weight = euclid(data[i], data[i], n_dim)
+            elif method_idx == 2: # method == 'correlation':
+                sim, weight = correlation(data[i], data[i], n_dim)
+            elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
+                if noise is None:
+                    sim, weight = euclid(data[i], data[i], n_dim)
+                else:
+                    sim = mahalanobis(data[i], data[i], n_dim, noise)
+                    weight = <double> n_dim
+            elif method_idx == 4: # method in ['poisson', 'poisson_cv']:
+                sim, weight = poisson_cv(data[i], data[i], log_data[i], log_data[i], n_dim)
+            idx = desc[i]
+            if weighting == 1: #'number':
+                values[idx] += sim / 2
+                weights[idx] += weight / 2
+            elif weighting == 0: #'equal':
+                values[idx] += sim / weight / 2
+                weights[idx] += 1 / 2
+        for j in range(i + 1, data.shape[0]):
+            if not crossval or not cv_desc[i] == cv_desc[j]:
+                #vec_i = data[i]
+                #vec_j = data[j]
+                if method_idx == 1: # method == 'euclidean':
+                    sim, weight = euclid(data[i], data[j], n_dim)
+                elif method_idx == 2: # method == 'correlation':
+                    sim, weight = correlation(data[i], data[j], n_dim)
+                elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
+                    if noise is None:
+                        sim, weight = euclid(data[i], data[j], n_dim)
+                    else:
+                        sim = mahalanobis(data[i], data[j], n_dim, noise)
+                        weight = <double> n_dim
+                elif method_idx == 4: # method in ['poisson', 'poisson_cv']:
+                    sim, weight = poisson_cv(data[i], data[j], log_data[i], log_data[j], n_dim)
+                if weight > 0:
+                    if desc[i] == desc[j]:
+                        idx = desc[i]
+                    else:
+                        if desc[j] > desc[i]:
+                            idx = (n - 1) * desc[i] - (((desc[i] + 1) * desc[i]) / 2) + desc[j] - 1 + n
+                        else:
+                            idx = (n - 1) * desc[j] - (((desc[j] + 1) * desc[j]) / 2) + desc[i] - 1 + n
+                    if weighting == 1: #'number':
+                        values[idx] += sim
+                        weights[idx] += weight
+                    elif weighting == 0: #'equal':
+                        values[idx] += sim / weight
+                        weights[idx] += 1
+    for idx in range(n_rdm + n):
+        if weights[idx] > 0:
+            values[idx] = values[idx] / weights[idx]
+        else:
+            values[idx] = NAN
+    return values
+
+
+@cython.boundscheck(False)
+@cython.cdivision(True)
+cpdef (double, double) calc_one(
+    double [:, :] data_i, double [:, :] data_j,
+    long [:] cv_desc_i, long [:] cv_desc_j,
+    int n_i, int n_j,
+    int method_idx, double [:, :] noise=None,
+    double prior_lambda=1, double prior_weight=0.1,
+    int weighting=1):
+    cdef:
+        #double [:] values = np.zeros(n_i * n_j)
+        #double [:] weights = np.zeros(n_i * n_j)
+        double [:] vec_i
+        double [:] vec_j
+        double weight, sim, weight_sum, value
+        int i, j
+        int n_dim = data_i.shape[1]
+        double prior_lambda_l = prior_lambda * prior_weight
+        double prior_weight_l = 1 + prior_weight
+        double [:, :] log_data_i
+        double [:, :] log_data_j
+    if (method_idx > 4) or (method_idx < 1):
+        raise ValueError('dissimilarity method not recognized!')
+    # precompute stuff for poisson KL
+    if method_idx == 4:
+        data_i = data_i.copy()
+        log_data_i = data_i.copy()
+        for i in range(data_i.shape[0]):
+            for j in range(n_dim):
+                data_i[i, j] = (data_i[i, j] + prior_lambda_l) / prior_weight_l
+                log_data_i[i, j] = log(data_i[i, j])
+        data_j = data_j.copy()
+        log_data_j = data_j.copy()
+        for i in range(data_j.shape[0]):
+            for j in range(n_dim):
+                data_j[i, j] = (data_j[i, j] + prior_lambda_l) / prior_weight_l
+                log_data_j[i, j] = log(data_j[i, j])
+    weight_sum = 0
+    value = 0
+    for i in range(n_i):
+        for j in range(n_j):
+            if not (cv_desc_i[i] == cv_desc_j[j]):
+                if method_idx == 1: # method == 'euclidean':
+                    sim, weight = euclid(data_i[i], data_j[j], n_dim)
+                elif method_idx == 2: # method == 'correlation':
+                    sim, weight = correlation(data_i[i], data_j[j], n_dim)
+                elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
+                    if noise is None:
+                        sim, weight = euclid(data_i[i], data_j[j], n_dim)
+                    else:
+                        sim = mahalanobis(data_i[i], data_j[j], n_dim, noise)
+                        weight = <double> n_dim
+                elif method_idx == 4: # method in ['poisson', 'poisson_cv']:
+                    sim, weight = poisson_cv(data_i[i], data_j[j], log_data_i[i], log_data_j[j], n_dim)
+                if weight > 0:
+                    if weighting == 1: #'number':
+                        value += sim
+                        weight_sum += weight
+                    elif weighting == 0: #'equal':
+                        value += sim / weight
+                        weight_sum += 1
+    if weight_sum > 0:
+        value = value / weight_sum
+    else:
+        value = NAN
+    return value, weight_sum
+
+
+@cython.boundscheck(False)
+cpdef (double, double) similarity(double [:] vec_i, double [:] vec_j, int method_idx,
+                       int n_dim, double [:, :] noise):
+    """
+    double similarity(double [:] vec_i, double [:] vec_j, int method_idx,
+                      int n_dim, double [:, :] noise=None)
+
+    This is a single similarity computation in cython.
+    remember to call everything with continuous numpy arrays.
+    In particular, noise must be such an array for Mahalanobis distances!
+
+    Mahalanobis distances require full measurement vectors at the moment!
+    """
+    cdef double sim
+    cdef double weight
+    if method_idx == 1: # method == 'euclidean':
+        sim, weight = euclid(vec_i, vec_j, n_dim)
+    elif method_idx == 2: # method == 'correlation':
+        sim, weight = correlation(vec_i, vec_j, n_dim)
+    elif method_idx == 3: # method in ['mahalanobis', 'crossnobis']:
+        if noise is None:
+            sim, weight = euclid(vec_i, vec_j, n_dim)
+        else:
+            sim = mahalanobis(vec_i, vec_j, n_dim, noise)
+            weight = <double> n_dim
+    return sim, weight
+
+
+@cython.boundscheck(False)
+cdef (double, double) euclid(double [:] vec_i, double [:] vec_j, int n_dim):
+    cdef:
+        double sim = 0
+        double weight = 0
+        int i
+    for i in range(n_dim):
+        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
+            sim += vec_i[i] * vec_j[i]
+            weight += 1
+    return sim, weight
+
+
+@cython.boundscheck(False)
+@cython.cdivision(True)
+cdef (double, double) poisson_cv(double [:] vec_i, double [:] vec_j,
+                                 double [:] log_vec_i, double [:] log_vec_j,
+                                 int n_dim):
+    cdef:
+        double sim = 0
+        double weight = 0
+        int i
+    for i in range(n_dim):
+        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
+            sim += (vec_j[i] - vec_i[i]) * (log_vec_i[i] - log_vec_j[i])
+            weight += 1
+    sim = sim / 2.0
+    return (sim, weight)
+
+
+@cython.boundscheck(False)
+cdef double mahalanobis(double [:] vec_i, double [:] vec_j, int n_dim,
+                        double [:, :] noise):
+    cdef:
+        double *vec1
+        double *vec2
+        int *finite
+        int zero = 0
+        int one = 1
+        double onef = 1.0
+        double zerof = 0.0
+        char trans = b'n'
+        double sim = 0.0
+        int i, j, k, l, n_finite
+        double [:, :] noise_small
+    finite = <int*> PyMem_Malloc(n_dim * sizeof(int))
+    # use finite as a bool to choose the non-nan values
+    n_finite = 0
+    for i in range(n_dim):
+        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
+            finite[i] = 1
+            n_finite += 1
+        else:
+            finite[i] = 0
+    vec1 = <double*> PyMem_Malloc(n_finite * sizeof(double))
+    vec2 = <double*> PyMem_Malloc(n_finite * sizeof(double))
+    vec3 = <double*> PyMem_Malloc(n_finite * sizeof(double))
+    #noise_small = <double [:n_finite, :n_finite]> PyMem_Malloc(n_finite * n_finite * sizeof(double))
+    noise_small = cvarray(shape=(n_finite, n_finite), itemsize=sizeof(double), format="d")
+    k = 0
+    for i in range(n_dim):
+        if finite[i]:
+            vec1[k] = vec_i[i]
+            vec2[k] = vec_j[i]
+            l = 0
+            for j in range(n_dim):
+                if finite[j]:
+                    noise_small[k, l] = noise[i, j]
+                    l += 1
+            k += 1
+    blas.dgemv(&trans, &n_finite, &n_finite, &onef, &noise_small[0, 0], &n_finite, vec2, &one, &zerof, vec3, &one)
+    for i in range(n_dim):
+        sim += vec1[i] * vec3[i]
+    PyMem_Free(vec1)
+    PyMem_Free(vec2)
+    PyMem_Free(vec3)
+    PyMem_Free(finite)
+    return sim
+
+
+@cython.boundscheck(False)
+@cython.cdivision(True)
+cdef (double, double) correlation(double [:] vec_i, double [:] vec_j, int n_dim):
+    cdef:
+        double si = 0.0
+        double sj = 0.0
+        double si2 = 0.0
+        double sj2 = 0.0
+        double sij = 0.0
+        double sim
+        double weight = 0
+        int i
+    for i in range(n_dim):
+        if not isnan(vec_i[i]) and not isnan(vec_j[i]):
+            si += vec_i[i]
+            sj += vec_j[i]
+            si2 += vec_i[i] * vec_i[i]
+            sj2 += vec_j[i] * vec_j[i]
+            sij += vec_i[i] * vec_j[i]
+            weight += 1
+    if si2 > 0 and sj2 > 0:
+        # sim = (np.sum(vec_i * vec_j) / np.sqrt(norm_i) / np.sqrt(norm_j))
+        sim = sij - (si * sj / n_dim)
+        sim /= sqrt(si2 - (si * si / n_dim))
+        sim /= sqrt(sj2 - (sj * sj / n_dim))
+    else:
+        sim = 1
+    sim = sim * n_dim / 2
+    return sim, weight
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/base.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/base.py`

 * *Ordering differences only*

 * *Files 13% similar despite different names*

```diff
@@ -1,201 +1,201 @@
-"""Base class for Dataset
-"""
-from __future__ import annotations
-from rsatoolbox.util.descriptor_utils import check_descriptor_length_error
-from rsatoolbox.util.descriptor_utils import format_descriptor
-from rsatoolbox.util.descriptor_utils import parse_input_descriptor
-from rsatoolbox.io.hdf5 import write_dict_hdf5
-from rsatoolbox.io.pkl import write_dict_pkl
-from rsatoolbox.util.file_io import remove_file
-
-
-class DatasetBase:
-    """
-    Abstract dataset class.
-    Defines members that every class needs to have, but does not
-    implement any interesting behavior. Inherit from this class
-    to define specific dataset types
-
-    Args:
-        measurements (numpy.ndarray): n_obs x n_channel 2d-array,
-        descriptors (dict):           descriptors (metadata)
-        obs_descriptors (dict):       observation descriptors (all
-            are array-like with shape = (n_obs,...))
-        channel_descriptors (dict):   channel descriptors (all are
-            array-like with shape = (n_channel,...))
-
-    Returns:
-        dataset object
-    """
-
-    def __init__(self, measurements, descriptors=None,
-                 obs_descriptors=None, channel_descriptors=None,
-                 check_dims=True):
-        if measurements.ndim != 2:
-            raise AttributeError(
-                "measurements must be in dimension n_obs x n_channel")
-        self.measurements = measurements
-        self.n_obs, self.n_channel = self.measurements.shape
-        if check_dims:
-            check_descriptor_length_error(obs_descriptors,
-                                          "obs_descriptors",
-                                          self.n_obs
-                                          )
-            check_descriptor_length_error(channel_descriptors,
-                                          "channel_descriptors",
-                                          self.n_channel
-                                          )
-        self.descriptors = parse_input_descriptor(descriptors)
-        self.obs_descriptors = parse_input_descriptor(obs_descriptors)
-        self.channel_descriptors = parse_input_descriptor(channel_descriptors)
-
-    def __repr__(self):
-        """
-        defines string which is printed for the object
-        """
-        return (f'rsatoolbox.data.{self.__class__.__name__}(\n'
-                f'measurements = \n{self.measurements}\n'
-                f'descriptors = \n{self.descriptors}\n'
-                f'obs_descriptors = \n{self.obs_descriptors}\n'
-                f'channel_descriptors = \n{self.channel_descriptors}\n'
-                )
-
-    def __str__(self):
-        """
-        defines the output of print
-        """
-        string_desc = format_descriptor(self.descriptors)
-        string_obs_desc = format_descriptor(self.obs_descriptors)
-        string_channel_desc = format_descriptor(self.channel_descriptors)
-        if self.measurements.shape[0] > 5:
-            measurements = self.measurements[:5, :]
-        else:
-            measurements = self.measurements
-        return (f'rsatoolbox.data.{self.__class__.__name__}\n'
-                f'measurements = \n{measurements}\n...\n\n'
-                f'descriptors: \n{string_desc}\n\n'
-                f'obs_descriptors: \n{string_obs_desc}\n\n'
-                f'channel_descriptors: \n{string_channel_desc}\n'
-                )
-
-    def __eq__(self, other: DatasetBase) -> bool:
-        """Equality check, to be implemented in the specific
-        Dataset class
-
-        Args:
-            other (DatasetBase): The object to compare to.
-
-        Raises:
-            NotImplementedError: This is not valid if not implemented
-                by the specific Dataset class
-
-        Returns:
-            bool: Never returns
-        """
-        raise NotImplementedError()
-
-    def copy(self) -> DatasetBase:
-        """Copy Dataset
-        To be implemented in child class
-
-        Raises:
-            NotImplementedError: raised if not implemented
-
-        Returns:
-            DatasetBase: Never returns
-        """
-        raise NotImplementedError
-
-    def split_obs(self, by):
-        """ Returns a list Datasets split by obs
-
-        Args:
-            by(String): the descriptor by which the splitting is made
-
-        Returns:
-            list of Datasets, splitted by the selected obs_descriptor
-
-        """
-        raise NotImplementedError(
-            "split_obs function not implemented in used Dataset class!")
-
-    def split_channel(self, by):
-        """ Returns a list Datasets split by channels
-
-        Args:
-            by(String): the descriptor by which the splitting is made
-
-        Returns:
-            list of Datasets,  splitted by the selected channel_descriptor
-
-        """
-        raise NotImplementedError(
-            "split_channel function not implemented in used Dataset class!")
-
-    def subset_obs(self, by, value):
-        """ Returns a subsetted Dataset defined by certain obs value
-
-        Args:
-            by(String): the descriptor by which the subset selection is made
-                from obs dimension
-            value:      the value by which the subset selection is made
-                from obs dimension
-
-        Returns:
-            Dataset, with subset defined by the selected obs_descriptor
-
-        """
-        raise NotImplementedError(
-            "subset_obs function not implemented in used Dataset class!")
-
-    def subset_channel(self, by, value):
-        """ Returns a subsetted Dataset defined by certain channel value
-
-        Args:
-            by(String): the descriptor by which the subset selection is made
-                from channel dimension
-            value:      the value by which the subset selection is made
-                from channel dimension
-
-        Returns:
-            Dataset, with subset defined by the selected channel_descriptor
-
-        """
-        raise NotImplementedError(
-            "subset_channel function not implemented in used Dataset class!")
-
-    def save(self, filename, file_type='hdf5', overwrite=False):
-        """ Saves the dataset object to a file
-
-        Args:
-            filename(String): path to the file
-                [or opened file]
-            file_type(String): Type of file to create:
-                hdf5: hdf5 file
-                pkl: pickle file
-            overwrite(Boolean): overwrites file if it already exists
-
-        """
-        data_dict = self.to_dict()
-        if overwrite:
-            remove_file(filename)
-        if file_type == 'hdf5':
-            write_dict_hdf5(filename, data_dict)
-        elif file_type == 'pkl':
-            write_dict_pkl(filename, data_dict)
-
-    def to_dict(self):
-        """ Generates a dictionary which contains the information to
-        recreate the dataset object. Used for saving to disc
-
-        Returns:
-            data_dict(dict): dictionary with dataset information
-
-        """
-        data_dict = {}
-        data_dict['measurements'] = self.measurements
-        data_dict['descriptors'] = self.descriptors
-        data_dict['obs_descriptors'] = self.obs_descriptors
-        data_dict['channel_descriptors'] = self.channel_descriptors
-        data_dict['type'] = type(self).__name__
-        return data_dict
+"""Base class for Dataset
+"""
+from __future__ import annotations
+from rsatoolbox.util.descriptor_utils import check_descriptor_length_error
+from rsatoolbox.util.descriptor_utils import format_descriptor
+from rsatoolbox.util.descriptor_utils import parse_input_descriptor
+from rsatoolbox.io.hdf5 import write_dict_hdf5
+from rsatoolbox.io.pkl import write_dict_pkl
+from rsatoolbox.util.file_io import remove_file
+
+
+class DatasetBase:
+    """
+    Abstract dataset class.
+    Defines members that every class needs to have, but does not
+    implement any interesting behavior. Inherit from this class
+    to define specific dataset types
+
+    Args:
+        measurements (numpy.ndarray): n_obs x n_channel 2d-array,
+        descriptors (dict):           descriptors (metadata)
+        obs_descriptors (dict):       observation descriptors (all
+            are array-like with shape = (n_obs,...))
+        channel_descriptors (dict):   channel descriptors (all are
+            array-like with shape = (n_channel,...))
+
+    Returns:
+        dataset object
+    """
+
+    def __init__(self, measurements, descriptors=None,
+                 obs_descriptors=None, channel_descriptors=None,
+                 check_dims=True):
+        if measurements.ndim != 2:
+            raise AttributeError(
+                "measurements must be in dimension n_obs x n_channel")
+        self.measurements = measurements
+        self.n_obs, self.n_channel = self.measurements.shape
+        if check_dims:
+            check_descriptor_length_error(obs_descriptors,
+                                          "obs_descriptors",
+                                          self.n_obs
+                                          )
+            check_descriptor_length_error(channel_descriptors,
+                                          "channel_descriptors",
+                                          self.n_channel
+                                          )
+        self.descriptors = parse_input_descriptor(descriptors)
+        self.obs_descriptors = parse_input_descriptor(obs_descriptors)
+        self.channel_descriptors = parse_input_descriptor(channel_descriptors)
+
+    def __repr__(self):
+        """
+        defines string which is printed for the object
+        """
+        return (f'rsatoolbox.data.{self.__class__.__name__}(\n'
+                f'measurements = \n{self.measurements}\n'
+                f'descriptors = \n{self.descriptors}\n'
+                f'obs_descriptors = \n{self.obs_descriptors}\n'
+                f'channel_descriptors = \n{self.channel_descriptors}\n'
+                )
+
+    def __str__(self):
+        """
+        defines the output of print
+        """
+        string_desc = format_descriptor(self.descriptors)
+        string_obs_desc = format_descriptor(self.obs_descriptors)
+        string_channel_desc = format_descriptor(self.channel_descriptors)
+        if self.measurements.shape[0] > 5:
+            measurements = self.measurements[:5, :]
+        else:
+            measurements = self.measurements
+        return (f'rsatoolbox.data.{self.__class__.__name__}\n'
+                f'measurements = \n{measurements}\n...\n\n'
+                f'descriptors: \n{string_desc}\n\n'
+                f'obs_descriptors: \n{string_obs_desc}\n\n'
+                f'channel_descriptors: \n{string_channel_desc}\n'
+                )
+
+    def __eq__(self, other: DatasetBase) -> bool:
+        """Equality check, to be implemented in the specific
+        Dataset class
+
+        Args:
+            other (DatasetBase): The object to compare to.
+
+        Raises:
+            NotImplementedError: This is not valid if not implemented
+                by the specific Dataset class
+
+        Returns:
+            bool: Never returns
+        """
+        raise NotImplementedError()
+
+    def copy(self) -> DatasetBase:
+        """Copy Dataset
+        To be implemented in child class
+
+        Raises:
+            NotImplementedError: raised if not implemented
+
+        Returns:
+            DatasetBase: Never returns
+        """
+        raise NotImplementedError
+
+    def split_obs(self, by):
+        """ Returns a list Datasets split by obs
+
+        Args:
+            by(String): the descriptor by which the splitting is made
+
+        Returns:
+            list of Datasets, splitted by the selected obs_descriptor
+
+        """
+        raise NotImplementedError(
+            "split_obs function not implemented in used Dataset class!")
+
+    def split_channel(self, by):
+        """ Returns a list Datasets split by channels
+
+        Args:
+            by(String): the descriptor by which the splitting is made
+
+        Returns:
+            list of Datasets,  splitted by the selected channel_descriptor
+
+        """
+        raise NotImplementedError(
+            "split_channel function not implemented in used Dataset class!")
+
+    def subset_obs(self, by, value):
+        """ Returns a subsetted Dataset defined by certain obs value
+
+        Args:
+            by(String): the descriptor by which the subset selection is made
+                from obs dimension
+            value:      the value by which the subset selection is made
+                from obs dimension
+
+        Returns:
+            Dataset, with subset defined by the selected obs_descriptor
+
+        """
+        raise NotImplementedError(
+            "subset_obs function not implemented in used Dataset class!")
+
+    def subset_channel(self, by, value):
+        """ Returns a subsetted Dataset defined by certain channel value
+
+        Args:
+            by(String): the descriptor by which the subset selection is made
+                from channel dimension
+            value:      the value by which the subset selection is made
+                from channel dimension
+
+        Returns:
+            Dataset, with subset defined by the selected channel_descriptor
+
+        """
+        raise NotImplementedError(
+            "subset_channel function not implemented in used Dataset class!")
+
+    def save(self, filename, file_type='hdf5', overwrite=False):
+        """ Saves the dataset object to a file
+
+        Args:
+            filename(String): path to the file
+                [or opened file]
+            file_type(String): Type of file to create:
+                hdf5: hdf5 file
+                pkl: pickle file
+            overwrite(Boolean): overwrites file if it already exists
+
+        """
+        data_dict = self.to_dict()
+        if overwrite:
+            remove_file(filename)
+        if file_type == 'hdf5':
+            write_dict_hdf5(filename, data_dict)
+        elif file_type == 'pkl':
+            write_dict_pkl(filename, data_dict)
+
+    def to_dict(self):
+        """ Generates a dictionary which contains the information to
+        recreate the dataset object. Used for saving to disc
+
+        Returns:
+            data_dict(dict): dictionary with dataset information
+
+        """
+        data_dict = {}
+        data_dict['measurements'] = self.measurements
+        data_dict['descriptors'] = self.descriptors
+        data_dict['obs_descriptors'] = self.obs_descriptors
+        data_dict['channel_descriptors'] = self.channel_descriptors
+        data_dict['type'] = type(self).__name__
+        return data_dict
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/dataset.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/dataset.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,866 +1,866 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Definition of RSA Dataset class and TemporalDataset
-
-@author: baihan, jdiedrichsen, bpeters, adkipnis
-"""
-
-from __future__ import annotations
-from typing import List, Optional
-from copy import deepcopy
-import numpy as np
-from pandas import DataFrame
-from rsatoolbox.util.data_utils import get_unique_unsorted
-from rsatoolbox.util.data_utils import get_unique_inverse
-from rsatoolbox.util.descriptor_utils import check_descriptor_length_error
-from rsatoolbox.util.descriptor_utils import subset_descriptor
-from rsatoolbox.util.descriptor_utils import num_index
-from rsatoolbox.util.descriptor_utils import format_descriptor
-from rsatoolbox.util.descriptor_utils import parse_input_descriptor
-from rsatoolbox.util.descriptor_utils import append_obs_descriptors
-from rsatoolbox.util.descriptor_utils import desc_eq
-from rsatoolbox.io.hdf5 import read_dict_hdf5
-from rsatoolbox.io.pkl import read_dict_pkl
-from rsatoolbox.data.base import DatasetBase
-
-
-class Dataset(DatasetBase):
-    """
-    Dataset class is a standard version of DatasetBase.
-    It contains one data set - or multiple data sets with the same structure
-    """
-
-    def __eq__(self, other: Dataset) -> bool:
-        """Test for equality
-        This magic method gets called when you compare two
-        Datasets objects: `ds1 == ds2`.
-        True if the objects are of the same type, and
-        measurements and descriptors are equal.
-
-        Args:
-            other (Dataset): The second Dataset to compare to
-
-        Returns:
-            bool: True if the objects' properties are equal
-        """
-        return all([
-            isinstance(other, Dataset),
-            np.all(self.measurements == other.measurements),
-            self.descriptors == other.descriptors,
-            desc_eq(self.obs_descriptors, other.obs_descriptors),
-            desc_eq(self.channel_descriptors, other.channel_descriptors),
-        ])
-
-    def copy(self) -> Dataset:
-        """Return a copy of this object, with all properties
-        equal to the original's
-
-        Returns:
-            Dataset: Value copy
-        """
-        return Dataset(
-            measurements=self.measurements.copy(),
-            descriptors=deepcopy(self.descriptors),
-            obs_descriptors=deepcopy(self.obs_descriptors),
-            channel_descriptors=deepcopy(self.channel_descriptors)
-        )
-
-    def split_obs(self, by):
-        """ Returns a list Datasets splited by obs
-
-        Args:
-            by(String): the descriptor by which the splitting is made
-
-        Returns:
-            list of Datasets, split by the selected obs_descriptor
-        """
-        unique_values, inverse = get_unique_inverse(self.obs_descriptors[by])
-        dataset_list = []
-        for i_v, _ in enumerate(unique_values):
-            selection = np.where(inverse == i_v)[0]
-            measurements = self.measurements[selection, :]
-            descriptors = self.descriptors.copy()
-            obs_descriptors = subset_descriptor(
-                self.obs_descriptors, selection)
-            channel_descriptors = self.channel_descriptors
-            dataset = Dataset(measurements=measurements,
-                              descriptors=descriptors,
-                              obs_descriptors=obs_descriptors,
-                              channel_descriptors=channel_descriptors,
-                              check_dims=False)
-            dataset_list.append(dataset)
-        return dataset_list
-
-    def split_channel(self, by):
-        """ Returns a list Datasets splited by channels
-
-        Args:
-            by(String): the descriptor by which the split is done
-
-        Returns:
-            list of Datasets,  split by the selected channel_descriptor
-        """
-        unique_values, inverse = get_unique_inverse(self.channel_descriptors[by])
-        dataset_list = []
-        for i_v, v in enumerate(unique_values):
-            selection = np.where(inverse == i_v)[0]
-            measurements = self.measurements[:, selection]
-            descriptors = self.descriptors.copy()
-            descriptors[by] = v
-            obs_descriptors = self.obs_descriptors
-            channel_descriptors = subset_descriptor(
-                self.channel_descriptors, selection)
-            dataset = Dataset(measurements=measurements,
-                              descriptors=descriptors,
-                              obs_descriptors=obs_descriptors,
-                              channel_descriptors=channel_descriptors,
-                              check_dims=False)
-            dataset_list.append(dataset)
-        return dataset_list
-
-    def subset_obs(self, by, value):
-        """ Returns a subsetted Dataset defined by certain obs value
-
-        Args:
-            by(String): the descriptor by which the subset selection
-                is made from obs dimension
-            value:      the value by which the subset selection is made
-                from obs dimension
-
-        Returns:
-            Dataset, with subset defined by the selected obs_descriptor
-
-        """
-        selection = num_index(self.obs_descriptors[by], value)
-        measurements = self.measurements[selection, :]
-        descriptors = self.descriptors
-        obs_descriptors = subset_descriptor(
-            self.obs_descriptors, selection)
-        channel_descriptors = self.channel_descriptors
-        dataset = Dataset(measurements=measurements,
-                          descriptors=descriptors,
-                          obs_descriptors=obs_descriptors,
-                          channel_descriptors=channel_descriptors)
-        return dataset
-
-    def subset_channel(self, by, value):
-        """ Returns a subsetted Dataset defined by certain channel value
-
-        Args:
-            by(String): the descriptor by which the subset selection is
-                made from channel dimension
-            value:      the value by which the subset selection is made
-                from channel dimension
-
-        Returns:
-            Dataset, with subset defined by the selected channel_descriptor
-
-        """
-        selection = num_index(self.channel_descriptors[by], value)
-        measurements = self.measurements[:, selection]
-        descriptors = self.descriptors
-        obs_descriptors = self.obs_descriptors
-        channel_descriptors = subset_descriptor(
-            self.channel_descriptors, selection)
-        dataset = Dataset(measurements=measurements,
-                          descriptors=descriptors,
-                          obs_descriptors=obs_descriptors,
-                          channel_descriptors=channel_descriptors)
-        return dataset
-
-    def sort_by(self, by):
-        """ sorts the dataset by a given observation descriptor
-
-        Args:
-            by(String): the descriptor by which the dataset shall be sorted
-
-        Returns:
-            ---
-
-        """
-        desc = self.obs_descriptors[by]
-        order = np.argsort(desc)
-        self.measurements = self.measurements[order]
-        self.obs_descriptors = subset_descriptor(self.obs_descriptors, order)
-
-    def get_measurements(self):
-        "Getter function for measurements"
-        return self.measurements.copy()
-
-    def get_measurements_tensor(self, by):
-        """ Returns a tensor version of the measurements array, split by an
-        observation descriptor. This procedure will keep the order of
-        measurements the same as it is in the dataset.
-
-        Args:
-            by(String):
-                the descriptor by which the splitting is made
-
-        Returns:
-            measurements_tensor (numpy.ndarray):
-                n_obs_rest x n_channel x n_obs_by 3d-array, where n_obs_by is
-                are the unique values that the obs_descriptor "by" takes, and
-                n_obs_rest is the remaining number of observations per unique
-                instance of "by"
-
-        """
-        assert by in self.obs_descriptors.keys(), \
-            "third dimension not in obs_descriptors"
-        unique_values = get_unique_unsorted(self.obs_descriptors[by])
-        measurements_list = []
-        for v in unique_values:
-            selection = np.array([desc == v
-                                  for desc in self.obs_descriptors[by]])
-            measurements_subset = self.measurements[selection, :]
-            measurements_list.append(measurements_subset)
-        measurements_tensor = np.stack(measurements_list, axis=0)
-        measurements_tensor = np.swapaxes(measurements_tensor, 1, 2)
-        return measurements_tensor, unique_values
-
-    def odd_even_split(self, obs_desc):
-        """
-        Perform a simple odd-even split on an rsatoolbox dataset. It will be
-        partitioned into n different datasets, where n is the number of
-        distinct values on dataset.obs_descriptors[obs_desc].
-        The resulting list will be split into odd and even (index) subset.
-        The datasets contained in these subsets will then be merged.
-
-        Args:
-            obs_desc (str):
-                Observation descriptor, basis for partitioning (must contained
-                in keys of dataset.obs_descriptors)
-
-        Returns:
-            odd_split (Dataset):
-                subset of the Dataset with odd list-indices after partitioning
-                according to obs_desc
-            even_split (Dataset):
-                subset of the Dataset with even list-indices after partitioning
-                according to obs_desc
-        """
-        assert obs_desc in self.obs_descriptors.keys(), \
-            "obs_desc must be contained in keys of dataset.obs_descriptors"
-        ds_part = self.split_obs(obs_desc)
-        odd_list = ds_part[0::2]
-        even_list = ds_part[1::2]
-        odd_split = merge_subsets(odd_list)
-        even_split = merge_subsets(even_list)
-        return odd_split, even_split
-
-    def nested_odd_even_split(self, l1_obs_desc, l2_obs_desc):
-        """
-        Nested version of odd_even_split, where dataset is first partitioned
-        according to the l1_obs_desc and each partition is again partitioned
-        according to the l2_obs_desc (after which the actual oe-split occurs).
-
-        Useful for balancing, especially if the order of your measurements is
-        inconsistent, or if the two descriptors are not orthogonalized. It's
-        advised to apply .sort_by(l2_obs_desc) to the output of this function.
-
-        Args:
-            l1_obs_desc (str):
-                Observation descriptor, basis for level 1 partitioning
-                (must contained in keys of dataset.obs_descriptors)
-
-        Returns:
-            odd_split (Dataset):
-                subset of the Dataset with odd list-indices after partitioning
-                according to obs_desc
-            even_split (Dataset):
-                subset of the Dataset with even list-indices after partitioning
-                according to obs_desc
-
-        """
-        assert l1_obs_desc and l2_obs_desc in self.obs_descriptors.keys(), \
-            "observation descriptors must be contained in keys " \
-            + "of dataset.obs_descriptors"
-        ds_part = self.split_obs(l1_obs_desc)
-        odd_list = []
-        even_list = []
-        for partition in ds_part:
-            odd_split, even_split = partition.odd_even_split(l2_obs_desc)
-            odd_list.append(odd_split)
-            even_list.append(even_split)
-        odd_split = merge_subsets(odd_list)
-        even_split = merge_subsets(even_list)
-        return odd_split, even_split
-
-    @staticmethod
-    def from_df(df: DataFrame,
-                channels: Optional[List] = None,
-                channel_descriptor: Optional[str] = None) -> Dataset:
-        """Create a Dataset from a Pandas DataFrame
-
-        Float columns are interpreted as channels, and their names stored as a
-        channel descriptor "name".
-        Columns of any other datatype will be interpreted as observation
-        descriptors, unless they have the same value throughout,
-        in which case they will be interpreted as Dataset descriptor.
-
-        Args:
-            df (DataFrame): a long-format DataFrame
-            channels (list): list of column names to interpret as channels.
-                By default all float columns are considered channels.
-            channel_descriptor (str): Name of the channel descriptor to create
-                on the Dataset which contains the column names.
-                Default is "name".
-
-        Returns:
-            Dataset: Dataset representing the data from the DataFrame
-        """
-        if channels is None:
-            channels = [c for (c, t) in df.dtypes.items() if 'float' in str(t)]
-        if channel_descriptor is None:
-            channel_descriptor = 'name'
-        descriptors = set(df.columns).difference(channels)
-        ds_descriptors, obs_descriptors = dict(), dict()
-        for desc in descriptors:
-            if df[desc].unique().size == 1:
-                ds_descriptors[desc] = df[desc][0]
-            else:
-                obs_descriptors[desc] = list(df[desc])
-        return Dataset(
-            measurements=df[channels].values,
-            descriptors=ds_descriptors,
-            obs_descriptors=obs_descriptors,
-            channel_descriptors={channel_descriptor: channels}
-        )
-
-    def to_df(self, channel_descriptor: Optional[str] = None) -> DataFrame:
-        """returns a Pandas DataFrame representing this Dataset
-
-        Channels, observation descriptors and Dataset descriptors make up the
-        columns. Rows represent observations.
-
-        Note that channel descriptors beyond the one used for the column names
-        will not be represented.
-
-        Args:
-            channel_descriptor: Which channel descriptor to use to
-                label the data columns in the Dataframe. Defaults to the
-                first channel descriptor.
-
-        Returns:
-            DataFrame: A pandas DataFrame representing the Dataset
-        """
-        desc = channel_descriptor or list(self.channel_descriptors.keys())[0]
-        ch_names = self.channel_descriptors[desc]
-        df = DataFrame(self.measurements, columns=ch_names)
-        all_descriptors = {**self.obs_descriptors, **self.descriptors}
-        for dname, dval in all_descriptors.items():
-            df[dname] = dval
-        return df
-
-
-class TemporalDataset(Dataset):
-    """
-    TemporalDataset for spatio-temporal datasets
-
-    Args:
-        measurements (numpy.ndarray): n_obs x n_channel x time 3d-array,
-        descriptors (dict):           descriptors (metadata)
-        obs_descriptors (dict):       observation descriptors (all
-            are array-like with shape = (n_obs,...))
-        channel_descriptors (dict):   channel descriptors (all are
-            array-like with shape = (n_channel,...))
-        time_descriptors (dict):      time descriptors (alls are
-            array-like with shape= (n_time,...))
-
-            time_descriptors needs to contain one key 'time' that
-            specifies the time-coordinate. if None is provided, 'time' is
-            set as (0, 1, ..., n_time-1)
-
-    Returns:
-        dataset object
-    """
-
-    def __init__(self, measurements, descriptors=None,
-                 obs_descriptors=None, channel_descriptors=None,
-                 time_descriptors=None, check_dims=True):
-
-        if measurements.ndim != 3:
-            raise AttributeError(
-                "measurements must be in dimension n_obs x n_channel x time")
-
-        self.measurements = measurements
-        self.n_obs, self.n_channel, self.n_time = self.measurements.shape
-
-        if time_descriptors is None:
-            time_descriptors = {'time': np.arange(self.n_time)}
-        elif 'time' not in time_descriptors:
-            time_descriptors['time'] = np.arange(self.n_time)
-            raise Warning(
-                "there was no 'time' provided in dictionary time_descriptors"
-                "\n'time' will be set to (0, 1, ..., n_time-1)")
-
-        if check_dims:
-            check_descriptor_length_error(obs_descriptors,
-                                          "obs_descriptors",
-                                          self.n_obs
-                                          )
-            check_descriptor_length_error(channel_descriptors,
-                                          "channel_descriptors",
-                                          self.n_channel
-                                          )
-            check_descriptor_length_error(time_descriptors,
-                                          "time_descriptors",
-                                          self.n_time
-                                          )
-        self.descriptors = parse_input_descriptor(descriptors)
-        self.obs_descriptors = parse_input_descriptor(obs_descriptors)
-        self.channel_descriptors = parse_input_descriptor(channel_descriptors)
-        self.time_descriptors = parse_input_descriptor(time_descriptors)
-
-    def __eq__(self, other: TemporalDataset) -> bool:
-        return all([
-            isinstance(other, TemporalDataset),
-            np.all(self.measurements == other.measurements),
-            self.descriptors == other.descriptors,
-            desc_eq(self.obs_descriptors, other.obs_descriptors),
-            desc_eq(self.channel_descriptors, other.channel_descriptors),
-            desc_eq(self.time_descriptors, other.time_descriptors)
-        ])
-
-    def __str__(self):
-        """
-        defines the output of print
-        """
-        string_desc = format_descriptor(self.descriptors)
-        string_obs_desc = format_descriptor(self.obs_descriptors)
-        string_channel_desc = format_descriptor(self.channel_descriptors)
-        string_time_desc = format_descriptor(self.time_descriptors)
-        if self.measurements.shape[0] > 5:
-            measurements = self.measurements[:5, :, :]
-        else:
-            measurements = self.measurements
-        return (f'rsatoolbox.data.{self.__class__.__name__}\n'
-                f'measurements = \n{measurements}\n...\n\n'
-                f'descriptors: \n{string_desc}\n\n'
-                f'obs_descriptors: \n{string_obs_desc}\n\n'
-                f'channel_descriptors: \n{string_channel_desc}\n'
-                f'time_descriptors: \n{string_time_desc}\n'
-                )
-
-    def copy(self) -> TemporalDataset:
-        """Return a copy of this object, with all properties
-        equal to the original's
-
-        Returns:
-            Dataset: Value copy
-        """
-        return TemporalDataset(
-            measurements=self.measurements.copy(),
-            descriptors=deepcopy(self.descriptors),
-            obs_descriptors=deepcopy(self.obs_descriptors),
-            channel_descriptors=deepcopy(self.channel_descriptors),
-            time_descriptors=deepcopy(self.time_descriptors)
-        )
-
-    def split_obs(self, by):
-        """ Returns a list TemporalDataset splited by obs
-
-        Args:
-            by(String): the descriptor by which the splitting is made
-
-        Returns:
-            list of TemporalDataset, splitted by the selected obs_descriptor
-        """
-        unique_values, inverse = get_unique_inverse(self.obs_descriptors[by])
-        dataset_list = []
-        for i_v, _ in enumerate(unique_values):
-            selection = np.where(inverse == i_v)[0]
-            measurements = self.measurements[selection, :, :]
-            descriptors = self.descriptors
-            obs_descriptors = subset_descriptor(
-                self.obs_descriptors, selection)
-            channel_descriptors = self.channel_descriptors
-            time_descriptors = self.time_descriptors
-            dataset = TemporalDataset(
-                measurements=measurements,
-                descriptors=descriptors,
-                obs_descriptors=obs_descriptors,
-                channel_descriptors=channel_descriptors,
-                time_descriptors=time_descriptors,
-                check_dims=False)
-            dataset_list.append(dataset)
-        return dataset_list
-
-    def split_channel(self, by):
-        """ Returns a list of TemporalDataset split by channels
-
-        Args:
-            by(String): the descriptor by which the splitting is made
-
-        Returns:
-            list of TemporalDataset,
-                split by the selected channel_descriptor
-        """
-        unique_values, inverse = get_unique_inverse(self.channel_descriptors[by])
-        dataset_list = []
-        for i_v, v in enumerate(unique_values):
-            selection = np.where(inverse == i_v)[0]
-            measurements = self.measurements[:, selection, :]
-            descriptors = self.descriptors.copy()
-            descriptors[by] = v
-            obs_descriptors = self.obs_descriptors
-            channel_descriptors = subset_descriptor(
-                self.channel_descriptors, selection)
-            time_descriptors = self.time_descriptors
-            dataset = TemporalDataset(
-                measurements=measurements,
-                descriptors=descriptors,
-                obs_descriptors=obs_descriptors,
-                channel_descriptors=channel_descriptors,
-                time_descriptors=time_descriptors,
-                check_dims=False)
-            dataset_list.append(dataset)
-        return dataset_list
-
-    def split_time(self, by):
-        """ Returns a list TemporalDataset splited by time
-
-        Args:
-            by(String): the descriptor by which the splitting is made
-
-        Returns:
-            list of TemporalDataset,  splitted by the selected time_descriptor
-        """
-
-        time = get_unique_unsorted(self.time_descriptors[by])
-        dataset_list = []
-        for v in time:
-            selection = [i for i, val in enumerate(self.time_descriptors[by])
-                         if val == v]
-            measurements = self.measurements[:, :, selection]
-            descriptors = self.descriptors
-            obs_descriptors = self.obs_descriptors
-            channel_descriptors = self.channel_descriptors
-            time_descriptors = subset_descriptor(
-                self.time_descriptors, selection)
-            dataset = TemporalDataset(
-                measurements=measurements,
-                descriptors=descriptors,
-                obs_descriptors=obs_descriptors,
-                channel_descriptors=channel_descriptors,
-                time_descriptors=time_descriptors,
-                check_dims=False)
-            dataset_list.append(dataset)
-        return dataset_list
-
-    def bin_time(self, by, bins):
-        """ Returns an object TemporalDataset with time-binned data.
-
-        Args:
-            bins(array-like): list of bins, with bins[i] containing the vector
-                of time-points for the i-th bin
-
-        Returns:
-            a single TemporalDataset object
-                Data is averaged within time-bins.
-                'time' descriptor is set to the average of the
-                binned time-points.
-        """
-
-        time = self.time_descriptors[by]
-        n_bins = len(bins)
-
-        binned_measurements = np.zeros((self.n_obs, self.n_channel, n_bins))
-        binned_time = np.zeros(n_bins)
-
-        for t in range(n_bins):
-            t_idx = np.isin(time, bins[t])
-            binned_measurements[:, :, t] = np.mean(
-                self.measurements[:, :, t_idx], axis=2)
-            binned_time[t] = np.mean(time[t_idx])
-
-        time_descriptors = self.time_descriptors.copy()
-        time_descriptors[by] = binned_time
-
-        # adding the bins as an additional descriptor currently
-        # does not work because of check_descriptor_length which transforms
-        # it into a numpy.array.
-        # time_descriptors['bins'] = [x for x in bins]
-        time_descriptors['bins'] = [
-            np.array2string(x, precision=2, separator=',')
-            for x in bins]
-
-        dataset = TemporalDataset(
-            measurements=binned_measurements,
-            descriptors=self.descriptors,
-            obs_descriptors=self.obs_descriptors,
-            channel_descriptors=self.channel_descriptors,
-            time_descriptors=time_descriptors)
-        return dataset
-
-    def subset_obs(self, by, value):
-        """ Returns a subsetted TemporalDataset defined by certain obs value
-
-        Args:
-            by(String): the descriptor by which the subset selection
-                is made from obs dimension
-            value:      the value by which the subset selection is made
-                from obs dimension
-
-        Returns:
-            TemporalDataset, with subset defined by the selected obs_descriptor
-
-        """
-        selection = num_index(self.obs_descriptors[by], value)
-        measurements = self.measurements[selection, :, :]
-        descriptors = self.descriptors
-        obs_descriptors = subset_descriptor(
-            self.obs_descriptors, selection)
-        channel_descriptors = self.channel_descriptors
-        time_descriptors = self.time_descriptors
-        dataset = TemporalDataset(
-            measurements=measurements,
-            descriptors=descriptors,
-            obs_descriptors=obs_descriptors,
-            channel_descriptors=channel_descriptors,
-            time_descriptors=time_descriptors)
-        return dataset
-
-    def subset_channel(self, by, value):
-        """ Returns a subsetted TemporalDataset defined by
-        a certain channel descriptor value
-
-        Args:
-            by(String): the descriptor by which the subset selection is
-                made from channel dimension
-            value:      the value by which the subset selection is made
-                from channel dimension
-
-        Returns:
-            TemporalDataset,
-            with subset defined by the selected channel_descriptor
-
-        """
-        selection = num_index(self.channel_descriptors[by], value)
-        measurements = self.measurements[:, selection]
-        descriptors = self.descriptors
-        obs_descriptors = self.obs_descriptors
-        channel_descriptors = subset_descriptor(
-            self.channel_descriptors, selection)
-        time_descriptors = self.time_descriptors
-        dataset = TemporalDataset(
-            measurements=measurements,
-            descriptors=descriptors,
-            obs_descriptors=obs_descriptors,
-            channel_descriptors=channel_descriptors,
-            time_descriptors=time_descriptors)
-        return dataset
-
-    def subset_time(self, by, t_from, t_to):
-        """ Returns a subsetted TemporalDataset
-        with time between t_from and t_to
-
-        Args:
-            by(String): the descriptor by which the subset selection is
-                made from channel dimension
-            t_from: time-point from which onwards data should be subsetted
-            t_to: time-point until which data should be subsetted
-
-        Returns:
-            TemporalDataset
-                with subset defined by the selected time_descriptor
-
-        """
-
-        time = get_unique_unsorted(self.time_descriptors[by])
-        sel_time = [t for t in time if t_from <= t <= t_to]
-
-        selection = num_index(self.time_descriptors[by], sel_time)
-        measurements = self.measurements[:, :, selection]
-        descriptors = self.descriptors
-        obs_descriptors = self.obs_descriptors
-        channel_descriptors = self.channel_descriptors
-        time_descriptors = subset_descriptor(
-            self.time_descriptors, selection)
-        dataset = TemporalDataset(
-            measurements=measurements,
-            descriptors=descriptors,
-            obs_descriptors=obs_descriptors,
-            channel_descriptors=channel_descriptors,
-            time_descriptors=time_descriptors)
-        return dataset
-
-    def sort_by(self, by):
-        """ sorts the dataset by a given observation descriptor
-
-        Args:
-            by(String): the descriptor by which the dataset shall be sorted
-
-        Returns:
-            ---
-
-        """
-        desc = self.obs_descriptors[by]
-        order = np.argsort(desc)
-        self.measurements = self.measurements[order]
-        self.obs_descriptors = subset_descriptor(self.obs_descriptors, order)
-
-    def convert_to_dataset(self, by):
-        """ converts to Dataset long format.
-            time dimension is absorbed into observation dimension
-
-        Args:
-            by(String): the descriptor which indicates the time dimension in
-                the time_descriptor
-
-        Returns:
-            Dataset
-
-        """
-        time = get_unique_unsorted(self.time_descriptors[by])
-
-        descriptors = self.descriptors
-        channel_descriptors = self.channel_descriptors.copy()
-
-        measurements = np.empty([0, self.n_channel])
-        obs_descriptors = dict.fromkeys(self.obs_descriptors, [])
-
-        for key in self.time_descriptors:
-            obs_descriptors[key] = np.array([])
-
-        for v in time:
-            selection = [i for i, val in enumerate(self.time_descriptors[by])
-                         if val == v]
-
-            measurements = np.concatenate((
-                measurements, self.measurements[:, :, selection].squeeze()),
-                axis=0)
-
-            for key in self.obs_descriptors:
-                obs_descriptors[key] = np.concatenate((
-                    obs_descriptors[key], self.obs_descriptors[key].copy()),
-                    axis=0)
-
-            for key in self.time_descriptors:
-                obs_descriptors[key] = np.concatenate((
-                    obs_descriptors[key], np.repeat(
-                        [self.time_descriptors[key][s]
-                         for s in selection],
-                        self.n_obs)),
-                    axis=0)
-
-        dataset = Dataset(measurements=measurements,
-                          descriptors=descriptors,
-                          obs_descriptors=obs_descriptors,
-                          channel_descriptors=channel_descriptors)
-        return dataset
-
-    def to_dict(self):
-        """ Generates a dictionary which contains the information to
-        recreate the TemporalDataset object. Used for saving to disc
-
-        Returns:
-            data_dict(dict): dictionary with TemporalDataset information
-
-        """
-        data_dict = {}
-        data_dict['measurements'] = self.measurements
-        data_dict['descriptors'] = self.descriptors
-        data_dict['obs_descriptors'] = self.obs_descriptors
-        data_dict['channel_descriptors'] = self.channel_descriptors
-        data_dict['time_descriptors'] = self.channel_descriptors
-        data_dict['type'] = type(self).__name__
-        return data_dict
-
-
-def load_dataset(filename, file_type=None):
-    """ loads a Dataset object from disc
-
-    Args:
-        filename(String): path to file to load
-
-    """
-    if file_type is None:
-        if isinstance(filename, str):
-            if filename[-4:] == '.pkl':
-                file_type = 'pkl'
-            elif filename[-3:] == '.h5' or filename[-4:] == 'hdf5':
-                file_type = 'hdf5'
-    if file_type == 'hdf5':
-        data_dict = read_dict_hdf5(filename)
-    elif file_type == 'pkl':
-        data_dict = read_dict_pkl(filename)
-    else:
-        raise ValueError('filetype not understood')
-    return dataset_from_dict(data_dict)
-
-
-def dataset_from_dict(data_dict):
-    """ regenerates a Dataset object from the dictionary representation
-
-    Currently this function works for Dataset, DatasetBase,
-    and TemporalDataset objects
-
-    Args:
-        data_dict(dict): the dictionary representation
-
-    Returns:
-        data(Dataset): the regenerated Dataset
-
-    """
-    if data_dict['type'] == 'Dataset':
-        data = Dataset(
-            data_dict['measurements'],
-            descriptors=data_dict['descriptors'],
-            obs_descriptors=data_dict['obs_descriptors'],
-            channel_descriptors=data_dict['channel_descriptors'])
-    elif data_dict['type'] == 'DatasetBase':
-        data = DatasetBase(
-            data_dict['measurements'],
-            descriptors=data_dict['descriptors'],
-            obs_descriptors=data_dict['obs_descriptors'],
-            channel_descriptors=data_dict['channel_descriptors'])
-    elif data_dict['type'] == 'TemporalDataset':
-        data = TemporalDataset(
-            data_dict['measurements'],
-            descriptors=data_dict['descriptors'],
-            obs_descriptors=data_dict['obs_descriptors'],
-            channel_descriptors=data_dict['channel_descriptors'],
-            time_descriptors=data_dict['time_descriptors'])
-    else:
-        raise ValueError('type of Dataset not recognized')
-    return data
-
-
-def merge_subsets(dataset_list):
-    """
-    Generate a dataset object from a list of smaller dataset objects
-    (e.g., as generated by the subset_* methods). Assumes that descriptors,
-    channel descriptors and number of channels per observation match.
-
-    Args:
-        dataset_list (list):
-            List containing rsatoolbox datasets
-
-    Returns:
-        merged_dataset (Dataset):
-            rsatoolbox dataset created from all datasets in dataset_list
-    """
-    assert isinstance(dataset_list, list), "Provided object is not a list."
-    assert "Dataset" in str(type(dataset_list[0])), \
-        "Provided list does not only contain Dataset objects."
-    baseline_ds = dataset_list[0]
-    descriptors = baseline_ds.descriptors.copy()
-    channel_descriptors = baseline_ds.channel_descriptors.copy()
-    measurements = baseline_ds.measurements.copy()
-    obs_descriptors = baseline_ds.obs_descriptors.copy()
-
-    for ds in dataset_list[1:]:
-        assert "Dataset" in str(type(ds)), \
-            "Provided list does not only contain Dataset objects."
-        assert descriptors == ds.descriptors.copy(), \
-            "Dataset descriptors do not match."
-        measurements = np.append(measurements, ds.measurements, axis=0)
-        obs_descriptors = append_obs_descriptors(obs_descriptors,
-                                                 ds.obs_descriptors.copy())
-
-    merged_dataset = Dataset(measurements,
-                             descriptors=descriptors,
-                             obs_descriptors=obs_descriptors,
-                             channel_descriptors=channel_descriptors)
-    return merged_dataset
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Definition of RSA Dataset class and TemporalDataset
+
+@author: baihan, jdiedrichsen, bpeters, adkipnis
+"""
+
+from __future__ import annotations
+from typing import List, Optional
+from copy import deepcopy
+import numpy as np
+from pandas import DataFrame
+from rsatoolbox.util.data_utils import get_unique_unsorted
+from rsatoolbox.util.data_utils import get_unique_inverse
+from rsatoolbox.util.descriptor_utils import check_descriptor_length_error
+from rsatoolbox.util.descriptor_utils import subset_descriptor
+from rsatoolbox.util.descriptor_utils import num_index
+from rsatoolbox.util.descriptor_utils import format_descriptor
+from rsatoolbox.util.descriptor_utils import parse_input_descriptor
+from rsatoolbox.util.descriptor_utils import append_obs_descriptors
+from rsatoolbox.util.descriptor_utils import desc_eq
+from rsatoolbox.io.hdf5 import read_dict_hdf5
+from rsatoolbox.io.pkl import read_dict_pkl
+from rsatoolbox.data.base import DatasetBase
+
+
+class Dataset(DatasetBase):
+    """
+    Dataset class is a standard version of DatasetBase.
+    It contains one data set - or multiple data sets with the same structure
+    """
+
+    def __eq__(self, other: Dataset) -> bool:
+        """Test for equality
+        This magic method gets called when you compare two
+        Datasets objects: `ds1 == ds2`.
+        True if the objects are of the same type, and
+        measurements and descriptors are equal.
+
+        Args:
+            other (Dataset): The second Dataset to compare to
+
+        Returns:
+            bool: True if the objects' properties are equal
+        """
+        return all([
+            isinstance(other, Dataset),
+            np.all(self.measurements == other.measurements),
+            self.descriptors == other.descriptors,
+            desc_eq(self.obs_descriptors, other.obs_descriptors),
+            desc_eq(self.channel_descriptors, other.channel_descriptors),
+        ])
+
+    def copy(self) -> Dataset:
+        """Return a copy of this object, with all properties
+        equal to the original's
+
+        Returns:
+            Dataset: Value copy
+        """
+        return Dataset(
+            measurements=self.measurements.copy(),
+            descriptors=deepcopy(self.descriptors),
+            obs_descriptors=deepcopy(self.obs_descriptors),
+            channel_descriptors=deepcopy(self.channel_descriptors)
+        )
+
+    def split_obs(self, by):
+        """ Returns a list Datasets splited by obs
+
+        Args:
+            by(String): the descriptor by which the splitting is made
+
+        Returns:
+            list of Datasets, split by the selected obs_descriptor
+        """
+        unique_values, inverse = get_unique_inverse(self.obs_descriptors[by])
+        dataset_list = []
+        for i_v, _ in enumerate(unique_values):
+            selection = np.where(inverse == i_v)[0]
+            measurements = self.measurements[selection, :]
+            descriptors = self.descriptors.copy()
+            obs_descriptors = subset_descriptor(
+                self.obs_descriptors, selection)
+            channel_descriptors = self.channel_descriptors
+            dataset = Dataset(measurements=measurements,
+                              descriptors=descriptors,
+                              obs_descriptors=obs_descriptors,
+                              channel_descriptors=channel_descriptors,
+                              check_dims=False)
+            dataset_list.append(dataset)
+        return dataset_list
+
+    def split_channel(self, by):
+        """ Returns a list Datasets splited by channels
+
+        Args:
+            by(String): the descriptor by which the split is done
+
+        Returns:
+            list of Datasets,  split by the selected channel_descriptor
+        """
+        unique_values, inverse = get_unique_inverse(self.channel_descriptors[by])
+        dataset_list = []
+        for i_v, v in enumerate(unique_values):
+            selection = np.where(inverse == i_v)[0]
+            measurements = self.measurements[:, selection]
+            descriptors = self.descriptors.copy()
+            descriptors[by] = v
+            obs_descriptors = self.obs_descriptors
+            channel_descriptors = subset_descriptor(
+                self.channel_descriptors, selection)
+            dataset = Dataset(measurements=measurements,
+                              descriptors=descriptors,
+                              obs_descriptors=obs_descriptors,
+                              channel_descriptors=channel_descriptors,
+                              check_dims=False)
+            dataset_list.append(dataset)
+        return dataset_list
+
+    def subset_obs(self, by, value):
+        """ Returns a subsetted Dataset defined by certain obs value
+
+        Args:
+            by(String): the descriptor by which the subset selection
+                is made from obs dimension
+            value:      the value by which the subset selection is made
+                from obs dimension
+
+        Returns:
+            Dataset, with subset defined by the selected obs_descriptor
+
+        """
+        selection = num_index(self.obs_descriptors[by], value)
+        measurements = self.measurements[selection, :]
+        descriptors = self.descriptors
+        obs_descriptors = subset_descriptor(
+            self.obs_descriptors, selection)
+        channel_descriptors = self.channel_descriptors
+        dataset = Dataset(measurements=measurements,
+                          descriptors=descriptors,
+                          obs_descriptors=obs_descriptors,
+                          channel_descriptors=channel_descriptors)
+        return dataset
+
+    def subset_channel(self, by, value):
+        """ Returns a subsetted Dataset defined by certain channel value
+
+        Args:
+            by(String): the descriptor by which the subset selection is
+                made from channel dimension
+            value:      the value by which the subset selection is made
+                from channel dimension
+
+        Returns:
+            Dataset, with subset defined by the selected channel_descriptor
+
+        """
+        selection = num_index(self.channel_descriptors[by], value)
+        measurements = self.measurements[:, selection]
+        descriptors = self.descriptors
+        obs_descriptors = self.obs_descriptors
+        channel_descriptors = subset_descriptor(
+            self.channel_descriptors, selection)
+        dataset = Dataset(measurements=measurements,
+                          descriptors=descriptors,
+                          obs_descriptors=obs_descriptors,
+                          channel_descriptors=channel_descriptors)
+        return dataset
+
+    def sort_by(self, by):
+        """ sorts the dataset by a given observation descriptor
+
+        Args:
+            by(String): the descriptor by which the dataset shall be sorted
+
+        Returns:
+            ---
+
+        """
+        desc = self.obs_descriptors[by]
+        order = np.argsort(desc)
+        self.measurements = self.measurements[order]
+        self.obs_descriptors = subset_descriptor(self.obs_descriptors, order)
+
+    def get_measurements(self):
+        "Getter function for measurements"
+        return self.measurements.copy()
+
+    def get_measurements_tensor(self, by):
+        """ Returns a tensor version of the measurements array, split by an
+        observation descriptor. This procedure will keep the order of
+        measurements the same as it is in the dataset.
+
+        Args:
+            by(String):
+                the descriptor by which the splitting is made
+
+        Returns:
+            measurements_tensor (numpy.ndarray):
+                n_obs_rest x n_channel x n_obs_by 3d-array, where n_obs_by is
+                are the unique values that the obs_descriptor "by" takes, and
+                n_obs_rest is the remaining number of observations per unique
+                instance of "by"
+
+        """
+        assert by in self.obs_descriptors.keys(), \
+            "third dimension not in obs_descriptors"
+        unique_values = get_unique_unsorted(self.obs_descriptors[by])
+        measurements_list = []
+        for v in unique_values:
+            selection = np.array([desc == v
+                                  for desc in self.obs_descriptors[by]])
+            measurements_subset = self.measurements[selection, :]
+            measurements_list.append(measurements_subset)
+        measurements_tensor = np.stack(measurements_list, axis=0)
+        measurements_tensor = np.swapaxes(measurements_tensor, 1, 2)
+        return measurements_tensor, unique_values
+
+    def odd_even_split(self, obs_desc):
+        """
+        Perform a simple odd-even split on an rsatoolbox dataset. It will be
+        partitioned into n different datasets, where n is the number of
+        distinct values on dataset.obs_descriptors[obs_desc].
+        The resulting list will be split into odd and even (index) subset.
+        The datasets contained in these subsets will then be merged.
+
+        Args:
+            obs_desc (str):
+                Observation descriptor, basis for partitioning (must contained
+                in keys of dataset.obs_descriptors)
+
+        Returns:
+            odd_split (Dataset):
+                subset of the Dataset with odd list-indices after partitioning
+                according to obs_desc
+            even_split (Dataset):
+                subset of the Dataset with even list-indices after partitioning
+                according to obs_desc
+        """
+        assert obs_desc in self.obs_descriptors.keys(), \
+            "obs_desc must be contained in keys of dataset.obs_descriptors"
+        ds_part = self.split_obs(obs_desc)
+        odd_list = ds_part[0::2]
+        even_list = ds_part[1::2]
+        odd_split = merge_subsets(odd_list)
+        even_split = merge_subsets(even_list)
+        return odd_split, even_split
+
+    def nested_odd_even_split(self, l1_obs_desc, l2_obs_desc):
+        """
+        Nested version of odd_even_split, where dataset is first partitioned
+        according to the l1_obs_desc and each partition is again partitioned
+        according to the l2_obs_desc (after which the actual oe-split occurs).
+
+        Useful for balancing, especially if the order of your measurements is
+        inconsistent, or if the two descriptors are not orthogonalized. It's
+        advised to apply .sort_by(l2_obs_desc) to the output of this function.
+
+        Args:
+            l1_obs_desc (str):
+                Observation descriptor, basis for level 1 partitioning
+                (must contained in keys of dataset.obs_descriptors)
+
+        Returns:
+            odd_split (Dataset):
+                subset of the Dataset with odd list-indices after partitioning
+                according to obs_desc
+            even_split (Dataset):
+                subset of the Dataset with even list-indices after partitioning
+                according to obs_desc
+
+        """
+        assert l1_obs_desc and l2_obs_desc in self.obs_descriptors.keys(), \
+            "observation descriptors must be contained in keys " \
+            + "of dataset.obs_descriptors"
+        ds_part = self.split_obs(l1_obs_desc)
+        odd_list = []
+        even_list = []
+        for partition in ds_part:
+            odd_split, even_split = partition.odd_even_split(l2_obs_desc)
+            odd_list.append(odd_split)
+            even_list.append(even_split)
+        odd_split = merge_subsets(odd_list)
+        even_split = merge_subsets(even_list)
+        return odd_split, even_split
+
+    @staticmethod
+    def from_df(df: DataFrame,
+                channels: Optional[List] = None,
+                channel_descriptor: Optional[str] = None) -> Dataset:
+        """Create a Dataset from a Pandas DataFrame
+
+        Float columns are interpreted as channels, and their names stored as a
+        channel descriptor "name".
+        Columns of any other datatype will be interpreted as observation
+        descriptors, unless they have the same value throughout,
+        in which case they will be interpreted as Dataset descriptor.
+
+        Args:
+            df (DataFrame): a long-format DataFrame
+            channels (list): list of column names to interpret as channels.
+                By default all float columns are considered channels.
+            channel_descriptor (str): Name of the channel descriptor to create
+                on the Dataset which contains the column names.
+                Default is "name".
+
+        Returns:
+            Dataset: Dataset representing the data from the DataFrame
+        """
+        if channels is None:
+            channels = [c for (c, t) in df.dtypes.items() if 'float' in str(t)]
+        if channel_descriptor is None:
+            channel_descriptor = 'name'
+        descriptors = set(df.columns).difference(channels)
+        ds_descriptors, obs_descriptors = dict(), dict()
+        for desc in descriptors:
+            if df[desc].unique().size == 1:
+                ds_descriptors[desc] = df[desc][0]
+            else:
+                obs_descriptors[desc] = list(df[desc])
+        return Dataset(
+            measurements=df[channels].values,
+            descriptors=ds_descriptors,
+            obs_descriptors=obs_descriptors,
+            channel_descriptors={channel_descriptor: channels}
+        )
+
+    def to_df(self, channel_descriptor: Optional[str] = None) -> DataFrame:
+        """returns a Pandas DataFrame representing this Dataset
+
+        Channels, observation descriptors and Dataset descriptors make up the
+        columns. Rows represent observations.
+
+        Note that channel descriptors beyond the one used for the column names
+        will not be represented.
+
+        Args:
+            channel_descriptor: Which channel descriptor to use to
+                label the data columns in the Dataframe. Defaults to the
+                first channel descriptor.
+
+        Returns:
+            DataFrame: A pandas DataFrame representing the Dataset
+        """
+        desc = channel_descriptor or list(self.channel_descriptors.keys())[0]
+        ch_names = self.channel_descriptors[desc]
+        df = DataFrame(self.measurements, columns=ch_names)
+        all_descriptors = {**self.obs_descriptors, **self.descriptors}
+        for dname, dval in all_descriptors.items():
+            df[dname] = dval
+        return df
+
+
+class TemporalDataset(Dataset):
+    """
+    TemporalDataset for spatio-temporal datasets
+
+    Args:
+        measurements (numpy.ndarray): n_obs x n_channel x time 3d-array,
+        descriptors (dict):           descriptors (metadata)
+        obs_descriptors (dict):       observation descriptors (all
+            are array-like with shape = (n_obs,...))
+        channel_descriptors (dict):   channel descriptors (all are
+            array-like with shape = (n_channel,...))
+        time_descriptors (dict):      time descriptors (alls are
+            array-like with shape= (n_time,...))
+
+            time_descriptors needs to contain one key 'time' that
+            specifies the time-coordinate. if None is provided, 'time' is
+            set as (0, 1, ..., n_time-1)
+
+    Returns:
+        dataset object
+    """
+
+    def __init__(self, measurements, descriptors=None,
+                 obs_descriptors=None, channel_descriptors=None,
+                 time_descriptors=None, check_dims=True):
+
+        if measurements.ndim != 3:
+            raise AttributeError(
+                "measurements must be in dimension n_obs x n_channel x time")
+
+        self.measurements = measurements
+        self.n_obs, self.n_channel, self.n_time = self.measurements.shape
+
+        if time_descriptors is None:
+            time_descriptors = {'time': np.arange(self.n_time)}
+        elif 'time' not in time_descriptors:
+            time_descriptors['time'] = np.arange(self.n_time)
+            raise Warning(
+                "there was no 'time' provided in dictionary time_descriptors"
+                "\n'time' will be set to (0, 1, ..., n_time-1)")
+
+        if check_dims:
+            check_descriptor_length_error(obs_descriptors,
+                                          "obs_descriptors",
+                                          self.n_obs
+                                          )
+            check_descriptor_length_error(channel_descriptors,
+                                          "channel_descriptors",
+                                          self.n_channel
+                                          )
+            check_descriptor_length_error(time_descriptors,
+                                          "time_descriptors",
+                                          self.n_time
+                                          )
+        self.descriptors = parse_input_descriptor(descriptors)
+        self.obs_descriptors = parse_input_descriptor(obs_descriptors)
+        self.channel_descriptors = parse_input_descriptor(channel_descriptors)
+        self.time_descriptors = parse_input_descriptor(time_descriptors)
+
+    def __eq__(self, other: TemporalDataset) -> bool:
+        return all([
+            isinstance(other, TemporalDataset),
+            np.all(self.measurements == other.measurements),
+            self.descriptors == other.descriptors,
+            desc_eq(self.obs_descriptors, other.obs_descriptors),
+            desc_eq(self.channel_descriptors, other.channel_descriptors),
+            desc_eq(self.time_descriptors, other.time_descriptors)
+        ])
+
+    def __str__(self):
+        """
+        defines the output of print
+        """
+        string_desc = format_descriptor(self.descriptors)
+        string_obs_desc = format_descriptor(self.obs_descriptors)
+        string_channel_desc = format_descriptor(self.channel_descriptors)
+        string_time_desc = format_descriptor(self.time_descriptors)
+        if self.measurements.shape[0] > 5:
+            measurements = self.measurements[:5, :, :]
+        else:
+            measurements = self.measurements
+        return (f'rsatoolbox.data.{self.__class__.__name__}\n'
+                f'measurements = \n{measurements}\n...\n\n'
+                f'descriptors: \n{string_desc}\n\n'
+                f'obs_descriptors: \n{string_obs_desc}\n\n'
+                f'channel_descriptors: \n{string_channel_desc}\n'
+                f'time_descriptors: \n{string_time_desc}\n'
+                )
+
+    def copy(self) -> TemporalDataset:
+        """Return a copy of this object, with all properties
+        equal to the original's
+
+        Returns:
+            Dataset: Value copy
+        """
+        return TemporalDataset(
+            measurements=self.measurements.copy(),
+            descriptors=deepcopy(self.descriptors),
+            obs_descriptors=deepcopy(self.obs_descriptors),
+            channel_descriptors=deepcopy(self.channel_descriptors),
+            time_descriptors=deepcopy(self.time_descriptors)
+        )
+
+    def split_obs(self, by):
+        """ Returns a list TemporalDataset splited by obs
+
+        Args:
+            by(String): the descriptor by which the splitting is made
+
+        Returns:
+            list of TemporalDataset, splitted by the selected obs_descriptor
+        """
+        unique_values, inverse = get_unique_inverse(self.obs_descriptors[by])
+        dataset_list = []
+        for i_v, _ in enumerate(unique_values):
+            selection = np.where(inverse == i_v)[0]
+            measurements = self.measurements[selection, :, :]
+            descriptors = self.descriptors
+            obs_descriptors = subset_descriptor(
+                self.obs_descriptors, selection)
+            channel_descriptors = self.channel_descriptors
+            time_descriptors = self.time_descriptors
+            dataset = TemporalDataset(
+                measurements=measurements,
+                descriptors=descriptors,
+                obs_descriptors=obs_descriptors,
+                channel_descriptors=channel_descriptors,
+                time_descriptors=time_descriptors,
+                check_dims=False)
+            dataset_list.append(dataset)
+        return dataset_list
+
+    def split_channel(self, by):
+        """ Returns a list of TemporalDataset split by channels
+
+        Args:
+            by(String): the descriptor by which the splitting is made
+
+        Returns:
+            list of TemporalDataset,
+                split by the selected channel_descriptor
+        """
+        unique_values, inverse = get_unique_inverse(self.channel_descriptors[by])
+        dataset_list = []
+        for i_v, v in enumerate(unique_values):
+            selection = np.where(inverse == i_v)[0]
+            measurements = self.measurements[:, selection, :]
+            descriptors = self.descriptors.copy()
+            descriptors[by] = v
+            obs_descriptors = self.obs_descriptors
+            channel_descriptors = subset_descriptor(
+                self.channel_descriptors, selection)
+            time_descriptors = self.time_descriptors
+            dataset = TemporalDataset(
+                measurements=measurements,
+                descriptors=descriptors,
+                obs_descriptors=obs_descriptors,
+                channel_descriptors=channel_descriptors,
+                time_descriptors=time_descriptors,
+                check_dims=False)
+            dataset_list.append(dataset)
+        return dataset_list
+
+    def split_time(self, by):
+        """ Returns a list TemporalDataset splited by time
+
+        Args:
+            by(String): the descriptor by which the splitting is made
+
+        Returns:
+            list of TemporalDataset,  splitted by the selected time_descriptor
+        """
+
+        time = get_unique_unsorted(self.time_descriptors[by])
+        dataset_list = []
+        for v in time:
+            selection = [i for i, val in enumerate(self.time_descriptors[by])
+                         if val == v]
+            measurements = self.measurements[:, :, selection]
+            descriptors = self.descriptors
+            obs_descriptors = self.obs_descriptors
+            channel_descriptors = self.channel_descriptors
+            time_descriptors = subset_descriptor(
+                self.time_descriptors, selection)
+            dataset = TemporalDataset(
+                measurements=measurements,
+                descriptors=descriptors,
+                obs_descriptors=obs_descriptors,
+                channel_descriptors=channel_descriptors,
+                time_descriptors=time_descriptors,
+                check_dims=False)
+            dataset_list.append(dataset)
+        return dataset_list
+
+    def bin_time(self, by, bins):
+        """ Returns an object TemporalDataset with time-binned data.
+
+        Args:
+            bins(array-like): list of bins, with bins[i] containing the vector
+                of time-points for the i-th bin
+
+        Returns:
+            a single TemporalDataset object
+                Data is averaged within time-bins.
+                'time' descriptor is set to the average of the
+                binned time-points.
+        """
+
+        time = self.time_descriptors[by]
+        n_bins = len(bins)
+
+        binned_measurements = np.zeros((self.n_obs, self.n_channel, n_bins))
+        binned_time = np.zeros(n_bins)
+
+        for t in range(n_bins):
+            t_idx = np.isin(time, bins[t])
+            binned_measurements[:, :, t] = np.mean(
+                self.measurements[:, :, t_idx], axis=2)
+            binned_time[t] = np.mean(time[t_idx])
+
+        time_descriptors = self.time_descriptors.copy()
+        time_descriptors[by] = binned_time
+
+        # adding the bins as an additional descriptor currently
+        # does not work because of check_descriptor_length which transforms
+        # it into a numpy.array.
+        # time_descriptors['bins'] = [x for x in bins]
+        time_descriptors['bins'] = [
+            np.array2string(x, precision=2, separator=',')
+            for x in bins]
+
+        dataset = TemporalDataset(
+            measurements=binned_measurements,
+            descriptors=self.descriptors,
+            obs_descriptors=self.obs_descriptors,
+            channel_descriptors=self.channel_descriptors,
+            time_descriptors=time_descriptors)
+        return dataset
+
+    def subset_obs(self, by, value):
+        """ Returns a subsetted TemporalDataset defined by certain obs value
+
+        Args:
+            by(String): the descriptor by which the subset selection
+                is made from obs dimension
+            value:      the value by which the subset selection is made
+                from obs dimension
+
+        Returns:
+            TemporalDataset, with subset defined by the selected obs_descriptor
+
+        """
+        selection = num_index(self.obs_descriptors[by], value)
+        measurements = self.measurements[selection, :, :]
+        descriptors = self.descriptors
+        obs_descriptors = subset_descriptor(
+            self.obs_descriptors, selection)
+        channel_descriptors = self.channel_descriptors
+        time_descriptors = self.time_descriptors
+        dataset = TemporalDataset(
+            measurements=measurements,
+            descriptors=descriptors,
+            obs_descriptors=obs_descriptors,
+            channel_descriptors=channel_descriptors,
+            time_descriptors=time_descriptors)
+        return dataset
+
+    def subset_channel(self, by, value):
+        """ Returns a subsetted TemporalDataset defined by
+        a certain channel descriptor value
+
+        Args:
+            by(String): the descriptor by which the subset selection is
+                made from channel dimension
+            value:      the value by which the subset selection is made
+                from channel dimension
+
+        Returns:
+            TemporalDataset,
+            with subset defined by the selected channel_descriptor
+
+        """
+        selection = num_index(self.channel_descriptors[by], value)
+        measurements = self.measurements[:, selection]
+        descriptors = self.descriptors
+        obs_descriptors = self.obs_descriptors
+        channel_descriptors = subset_descriptor(
+            self.channel_descriptors, selection)
+        time_descriptors = self.time_descriptors
+        dataset = TemporalDataset(
+            measurements=measurements,
+            descriptors=descriptors,
+            obs_descriptors=obs_descriptors,
+            channel_descriptors=channel_descriptors,
+            time_descriptors=time_descriptors)
+        return dataset
+
+    def subset_time(self, by, t_from, t_to):
+        """ Returns a subsetted TemporalDataset
+        with time between t_from and t_to
+
+        Args:
+            by(String): the descriptor by which the subset selection is
+                made from channel dimension
+            t_from: time-point from which onwards data should be subsetted
+            t_to: time-point until which data should be subsetted
+
+        Returns:
+            TemporalDataset
+                with subset defined by the selected time_descriptor
+
+        """
+
+        time = get_unique_unsorted(self.time_descriptors[by])
+        sel_time = [t for t in time if t_from <= t <= t_to]
+
+        selection = num_index(self.time_descriptors[by], sel_time)
+        measurements = self.measurements[:, :, selection]
+        descriptors = self.descriptors
+        obs_descriptors = self.obs_descriptors
+        channel_descriptors = self.channel_descriptors
+        time_descriptors = subset_descriptor(
+            self.time_descriptors, selection)
+        dataset = TemporalDataset(
+            measurements=measurements,
+            descriptors=descriptors,
+            obs_descriptors=obs_descriptors,
+            channel_descriptors=channel_descriptors,
+            time_descriptors=time_descriptors)
+        return dataset
+
+    def sort_by(self, by):
+        """ sorts the dataset by a given observation descriptor
+
+        Args:
+            by(String): the descriptor by which the dataset shall be sorted
+
+        Returns:
+            ---
+
+        """
+        desc = self.obs_descriptors[by]
+        order = np.argsort(desc)
+        self.measurements = self.measurements[order]
+        self.obs_descriptors = subset_descriptor(self.obs_descriptors, order)
+
+    def convert_to_dataset(self, by):
+        """ converts to Dataset long format.
+            time dimension is absorbed into observation dimension
+
+        Args:
+            by(String): the descriptor which indicates the time dimension in
+                the time_descriptor
+
+        Returns:
+            Dataset
+
+        """
+        time = get_unique_unsorted(self.time_descriptors[by])
+
+        descriptors = self.descriptors
+        channel_descriptors = self.channel_descriptors.copy()
+
+        measurements = np.empty([0, self.n_channel])
+        obs_descriptors = dict.fromkeys(self.obs_descriptors, [])
+
+        for key in self.time_descriptors:
+            obs_descriptors[key] = np.array([])
+
+        for v in time:
+            selection = [i for i, val in enumerate(self.time_descriptors[by])
+                         if val == v]
+
+            measurements = np.concatenate((
+                measurements, self.measurements[:, :, selection].squeeze()),
+                axis=0)
+
+            for key in self.obs_descriptors:
+                obs_descriptors[key] = np.concatenate((
+                    obs_descriptors[key], self.obs_descriptors[key].copy()),
+                    axis=0)
+
+            for key in self.time_descriptors:
+                obs_descriptors[key] = np.concatenate((
+                    obs_descriptors[key], np.repeat(
+                        [self.time_descriptors[key][s]
+                         for s in selection],
+                        self.n_obs)),
+                    axis=0)
+
+        dataset = Dataset(measurements=measurements,
+                          descriptors=descriptors,
+                          obs_descriptors=obs_descriptors,
+                          channel_descriptors=channel_descriptors)
+        return dataset
+
+    def to_dict(self):
+        """ Generates a dictionary which contains the information to
+        recreate the TemporalDataset object. Used for saving to disc
+
+        Returns:
+            data_dict(dict): dictionary with TemporalDataset information
+
+        """
+        data_dict = {}
+        data_dict['measurements'] = self.measurements
+        data_dict['descriptors'] = self.descriptors
+        data_dict['obs_descriptors'] = self.obs_descriptors
+        data_dict['channel_descriptors'] = self.channel_descriptors
+        data_dict['time_descriptors'] = self.channel_descriptors
+        data_dict['type'] = type(self).__name__
+        return data_dict
+
+
+def load_dataset(filename, file_type=None):
+    """ loads a Dataset object from disc
+
+    Args:
+        filename(String): path to file to load
+
+    """
+    if file_type is None:
+        if isinstance(filename, str):
+            if filename[-4:] == '.pkl':
+                file_type = 'pkl'
+            elif filename[-3:] == '.h5' or filename[-4:] == 'hdf5':
+                file_type = 'hdf5'
+    if file_type == 'hdf5':
+        data_dict = read_dict_hdf5(filename)
+    elif file_type == 'pkl':
+        data_dict = read_dict_pkl(filename)
+    else:
+        raise ValueError('filetype not understood')
+    return dataset_from_dict(data_dict)
+
+
+def dataset_from_dict(data_dict):
+    """ regenerates a Dataset object from the dictionary representation
+
+    Currently this function works for Dataset, DatasetBase,
+    and TemporalDataset objects
+
+    Args:
+        data_dict(dict): the dictionary representation
+
+    Returns:
+        data(Dataset): the regenerated Dataset
+
+    """
+    if data_dict['type'] == 'Dataset':
+        data = Dataset(
+            data_dict['measurements'],
+            descriptors=data_dict['descriptors'],
+            obs_descriptors=data_dict['obs_descriptors'],
+            channel_descriptors=data_dict['channel_descriptors'])
+    elif data_dict['type'] == 'DatasetBase':
+        data = DatasetBase(
+            data_dict['measurements'],
+            descriptors=data_dict['descriptors'],
+            obs_descriptors=data_dict['obs_descriptors'],
+            channel_descriptors=data_dict['channel_descriptors'])
+    elif data_dict['type'] == 'TemporalDataset':
+        data = TemporalDataset(
+            data_dict['measurements'],
+            descriptors=data_dict['descriptors'],
+            obs_descriptors=data_dict['obs_descriptors'],
+            channel_descriptors=data_dict['channel_descriptors'],
+            time_descriptors=data_dict['time_descriptors'])
+    else:
+        raise ValueError('type of Dataset not recognized')
+    return data
+
+
+def merge_subsets(dataset_list):
+    """
+    Generate a dataset object from a list of smaller dataset objects
+    (e.g., as generated by the subset_* methods). Assumes that descriptors,
+    channel descriptors and number of channels per observation match.
+
+    Args:
+        dataset_list (list):
+            List containing rsatoolbox datasets
+
+    Returns:
+        merged_dataset (Dataset):
+            rsatoolbox dataset created from all datasets in dataset_list
+    """
+    assert isinstance(dataset_list, list), "Provided object is not a list."
+    assert "Dataset" in str(type(dataset_list[0])), \
+        "Provided list does not only contain Dataset objects."
+    baseline_ds = dataset_list[0]
+    descriptors = baseline_ds.descriptors.copy()
+    channel_descriptors = baseline_ds.channel_descriptors.copy()
+    measurements = baseline_ds.measurements.copy()
+    obs_descriptors = baseline_ds.obs_descriptors.copy()
+
+    for ds in dataset_list[1:]:
+        assert "Dataset" in str(type(ds)), \
+            "Provided list does not only contain Dataset objects."
+        assert descriptors == ds.descriptors.copy(), \
+            "Dataset descriptors do not match."
+        measurements = np.append(measurements, ds.measurements, axis=0)
+        obs_descriptors = append_obs_descriptors(obs_descriptors,
+                                                 ds.obs_descriptors.copy())
+
+    merged_dataset = Dataset(measurements,
+                             descriptors=descriptors,
+                             obs_descriptors=obs_descriptors,
+                             channel_descriptors=channel_descriptors)
+    return merged_dataset
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/data/noise.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/data/noise.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,436 +1,436 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Functions for estimating the precision matrix based on the covariance of
-either the residuals (temporal based precision matrix) or of the measurements
-(instance based precision matrix)
-"""
-
-from collections.abc import Iterable
-import numpy as np
-from rsatoolbox.data import average_dataset_by
-from rsatoolbox.util.data_utils import get_unique_inverse
-
-
-def _check_demean(matrix):
-    """
-    checks that an input has 2 or 3 dimensions and subtracts the mean.
-    returns a 2D matrix for covariance/precision computation and the
-    degrees of freedom
-
-    Args:
-        matrix (np.ndarray):
-            n_conditions x n_channels
-
-    Returns:
-        numpy.ndarray:
-            demeaned matrix
-
-    """
-    assert isinstance(matrix, np.ndarray), "input must be ndarray"
-    if matrix.ndim in [1, 2]:
-        matrix = matrix - np.mean(matrix, axis=0, keepdims=True)
-        dof = matrix.shape[0] - 1
-    elif matrix.ndim == 3:
-        matrix -= np.mean(matrix, axis=2, keepdims=True)
-        dof = (matrix.shape[0] - 1) * matrix.shape[2]
-        matrix = matrix.transpose(0, 2, 1).reshape(
-            matrix.shape[0] * matrix.shape[2], matrix.shape[1])
-    else:
-        raise ValueError('Matrix for covariance estimation has wrong # of dimensions!')
-    return matrix, dof
-
-
-def _estimate_covariance(matrix, dof, method):
-    """ calls the right covariance estimation function based on the ""method" argument
-
-    Args:
-        matrix (np.ndarray):
-            n_conditions x n_channels
-
-        dof (int):
-            degrees of freedom
-
-        method (string):
-            which estimator to use
-
-    Returns:
-        numpy.ndarray, numpy.ndarray:
-            cov_mat: n_channels x n_channels sample covariance matrix
-
-    """
-    matrix, dof_nat = _check_demean(matrix)
-    if dof is None:
-        dof = dof_nat
-    # calculate sample covariance matrix s
-    if method == 'shrinkage_eye':
-        cov_mat = _covariance_eye(matrix, dof)
-    elif method == 'shrinkage_diag':
-        cov_mat = _covariance_diag(matrix, dof)
-    elif method == 'diag':
-        cov_mat = _variance(matrix, dof)
-    elif method == 'full':
-        cov_mat = _covariance_full(matrix, dof)
-    return cov_mat
-
-
-def _variance(matrix, dof):
-    """
-    returns the vector of variances per measurement channel.
-    The formula used here implies that the mean was already removed.
-
-    Args:
-        matrix (np.ndarray):
-            n_conditions x n_channels
-
-    Returns:
-        numpy.ndarray:
-            variance vector
-
-    """
-    return np.diag(np.einsum('ij, ij-> j', matrix, matrix) / dof)
-
-
-def _covariance_full(matrix, dof):
-    """
-    computes the sample covariance matrix from a 2d-array.
-    matrix should be demeaned before!
-
-    Args:
-        matrix (np.ndarray):
-            n_conditions x n_channels
-
-    Returns:
-        numpy.ndarray, numpy.ndarray:
-            s_mean: n_channels x n_channels sample covariance matrix
-
-    """
-    return np.einsum('ij, ik-> jk', matrix, matrix, optimize=True) / dof
-
-
-def _covariance_eye(matrix, dof):
-    """
-    computes the sample covariance matrix from a 2d-array.
-    matrix should be demeaned before!
-
-    Computes an optimal shrinkage estimate of a sample covariance matrix
-    as described by the following publication:
-
-    Ledoit and Wolfe (2004): "A well-conditioned
-    estimator for large-dimensional covariance matrices"
-
-    Args:
-        matrix (np.ndarray):
-            n_conditions x n_channels
-
-    Returns:
-        numpy.ndarray, numpy.ndarray:
-            s_mean: n_channels x n_channels sample covariance matrix
-
-            xt_x:
-            Einstein summation form of the matrix product
-            of the 2d-array with itself
-
-    """
-    s_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
-    s2_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
-    for m_line in matrix:
-        xt_x = np.outer(m_line, m_line)
-        s_sum += xt_x
-        s2_sum += xt_x ** 2
-    s = s_sum / matrix.shape[0]
-    b2 = np.sum(s2_sum / matrix.shape[0] - s * s) / matrix.shape[0]
-    # calculate the scalar estimators to find the optimal shrinkage:
-    # m, d^2, b^2 as in Ledoit & Wolfe paper
-    m = np.sum(np.diag(s)) / s.shape[0]
-    d2 = np.sum((s - m * np.eye(s.shape[0])) ** 2)
-    b2 = min(d2, b2)
-    # shrink covariance matrix
-    s_shrink = b2 / d2 * m * np.eye(s.shape[0]) \
-        + (d2-b2) / d2 * s
-    # correction for degrees of freedom
-    s_shrink = s_shrink * matrix.shape[0] / dof
-    return s_shrink
-
-
-def _covariance_diag(matrix, dof, mem_threshold=(10**9)/8):
-    """
-    computes the sample covariance matrix from a 2d-array.
-    matrix should be demeaned before!
-
-    Computes an optimal shrinkage estimate of a sample covariance matrix
-    as described by the following publication:
-
-    Schfer, J., & Strimmer, K. (2005). "A Shrinkage Approach to Large-Scale
-    Covariance Matrix Estimation and Implications for Functional Genomics.""
-
-    Args:
-        matrix (np.ndarray):
-            n_conditions x n_channels
-
-    Returns:
-        numpy.ndarray, numpy.ndarray:
-            s_mean: n_channels x n_channels sample covariance matrix
-
-            xt_x:
-            Einstein summation form of the matrix product
-            of the 2d-array with itself
-
-    """
-    s_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
-    s2_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
-    for m_line in matrix:
-        xt_x = np.outer(m_line, m_line)
-        s_sum += xt_x
-        s2_sum += xt_x ** 2
-    s = s_sum / dof
-    var = np.diag(s)
-    std = np.sqrt(var)
-    s_mean = s_sum / np.expand_dims(std, 0) / np.expand_dims(std, 1) / (matrix.shape[0] - 1)
-    s2_mean = s2_sum / np.expand_dims(var, 0) / np.expand_dims(var, 1) / (matrix.shape[0] - 1)
-    var_hat = matrix.shape[0] / dof ** 2 \
-        * (s2_mean - s_mean ** 2)
-    mask = ~np.eye(s.shape[0], dtype=bool)
-    lamb = np.sum(var_hat[mask]) / np.sum(s_mean[mask] ** 2)
-    lamb = max(min(lamb, 1), 0)
-    scaling = np.eye(s.shape[0]) + (1-lamb) * mask
-    s_shrink = s * scaling
-    return s_shrink
-
-
-def cov_from_residuals(residuals, dof=None, method='shrinkage_diag'):
-    """
-    Estimates a covariance matrix from measurements. Allows for shrinkage estimates.
-    Use 'method' to choose which estimation method is used.
-
-    Args:
-        residuals(numpy.ndarray or list of these): n_residuals x n_channels
-            matrix of residuals
-        dof(int or list of int): degrees of freedom for covariance estimation
-            defaults to n_res - 1, should be corrected for the number
-            of regressors in a GLM if applicable.
-        method(str): which estimate to use:
-            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
-            'full': computes the sample covariance without shrinkage
-            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
-            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
-
-    Returns:
-        numpy.ndarray (or list): sigma_p: covariance matrix over channels
-
-    """
-    if not isinstance(residuals, np.ndarray) or len(residuals.shape) > 2:
-        cov_mat = []
-        for i, residual in enumerate(residuals):
-            if dof is None:
-                cov_mat.append(cov_from_residuals(
-                    residual, method=method))
-            elif isinstance(dof, Iterable):
-                cov_mat.append(cov_from_residuals(
-                    residuals, method=method, dof=dof[i]))
-            else:
-                cov_mat.append(cov_from_residuals(
-                    residual, method=method, dof=dof))
-    else:
-        cov_mat = _estimate_covariance(residuals, dof, method)
-    return cov_mat
-
-
-def prec_from_residuals(residuals, dof=None, method='shrinkage_diag'):
-    """
-    Estimates the covariance matrix from residuals and finds its multiplicative
-    inverse (= the precision matrix)
-    Use 'method' to choose which estimation method is used.
-
-    Args:
-        residuals(numpy.ndarray or list of these): n_residuals x n_channels
-            matrix of residuals
-        dof(int or list of int): degrees of freedom for covariance estimation
-            defaults to n_res - 1, should be corrected for the number
-            of regressors in a GLM if applicable.
-        method(str): which estimate to use:
-            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
-            'full': computes the sample covariance without shrinkage
-            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
-            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
-
-    Returns:
-        numpy.ndarray (or list): sigma_p: precision matrix over channels
-
-    """
-    cov = cov_from_residuals(residuals=residuals, dof=dof, method=method)
-    if not isinstance(cov, np.ndarray):
-        prec = [None] * len(cov)
-        for i, cov_i in enumerate(cov):
-            prec[i] = np.linalg.inv(cov_i)
-    elif len(cov.shape) > 2:
-        prec = np.zeros(cov.shape)
-        for i, cov_i in enumerate(cov):
-            prec[i] = np.linalg.inv(cov_i)
-    else:
-        prec = np.linalg.inv(cov)
-    return prec
-
-
-def cov_from_measurements(dataset, obs_desc, dof=None, method='shrinkage_diag'):
-    """
-    Estimates a covariance matrix from measurements. Allows for shrinkage estimates.
-    Use 'method' to choose which estimation method is used.
-
-    Args:
-        dataset(data.Dataset):
-            rsatoolbox Dataset object
-        dof(int or list of int): degrees of freedom for covariance estimation
-            defaults to n_res - 1, should be corrected for the number
-            of regressors in a GLM if applicable.
-        method(str): which estimate to use:
-            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
-            'full': computes the sample covariance without shrinkage
-            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
-            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
-
-    Returns:
-        numpy.ndarray (or list): sigma_p: covariance matrix over channels
-
-    """
-    if isinstance(dataset, Iterable):
-        cov_mat = []
-        for i, dat in enumerate(dataset):
-            if dof is None:
-                cov_mat.append(cov_from_unbalanced(
-                    dat, obs_desc=obs_desc, method=method))
-            elif isinstance(dof, Iterable):
-                cov_mat.append(cov_from_unbalanced(
-                    dat, obs_desc=obs_desc, method=method, dof=dof[i]))
-            else:
-                cov_mat.append(cov_from_unbalanced(
-                    dat, obs_desc=obs_desc, method=method, dof=dof))
-    else:
-        assert "Dataset" in str(type(dataset)), "Provided object is not a dataset"
-        assert obs_desc in dataset.obs_descriptors.keys(), \
-            "obs_desc not contained in the dataset's obs_descriptors"
-        tensor, _ = dataset.get_measurements_tensor(obs_desc)
-        # calculate sample covariance matrix s
-        cov_mat = _estimate_covariance(tensor, dof, method)
-    return cov_mat
-
-
-def prec_from_measurements(dataset, obs_desc, dof=None, method='shrinkage_diag'):
-    """
-    Estimates the covariance matrix from measurements and finds its multiplicative
-    inverse (= the precision matrix)
-    Use 'method' to choose which estimation method is used.
-
-    Args:
-        residuals(numpy.ndarray or list of these): n_residuals x n_channels
-            matrix of residuals
-        dof(int or list of int): degrees of freedom for covariance estimation
-            defaults to n_res - 1, should be corrected for the number
-            of regressors in a GLM if applicable.
-        method(str): which estimate to use:
-            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
-            'full': computes the sample covariance without shrinkage
-            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
-            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
-
-    Returns:
-        numpy.ndarray (or list): sigma_p: precision matrix over channels
-
-    """
-    cov = cov_from_measurements(dataset, obs_desc, dof=dof, method=method)
-    if not isinstance(cov, np.ndarray):
-        prec = [None] * len(cov)
-        for i, cov_i in enumerate(cov):
-            prec[i] = np.linalg.inv(cov_i)
-    elif len(cov.shape) > 2:
-        prec = np.zeros(cov.shape)
-        for i, cov_i in enumerate(cov):
-            prec[i] = np.linalg.inv(cov_i)
-    else:
-        prec = np.linalg.inv(cov)
-    return prec
-
-
-def cov_from_unbalanced(dataset, obs_desc, dof=None, method='shrinkage_diag'):
-    """
-    Estimates a covariance matrix from an unbalanced dataset, i.e. from a
-    dataset that contains different numbers of samples for different
-    stimuli.
-
-    Args:
-        dataset(data.Dataset):
-            rsatoolbox Dataset object
-        dof(int or list of int): degrees of freedom for covariance estimation
-            defaults to n_measurements - n_stimuli, should be corrected
-            if this is not the case
-        method(str): which estimate to use:
-            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
-            'full': computes the sample covariance without shrinkage
-            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
-            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
-
-    Returns:
-        numpy.ndarray (or list): sigma_p: covariance matrix over channels
-
-    """
-    if isinstance(dataset, Iterable):
-        cov_mat = []
-        for i, dat in enumerate(dataset):
-            if dof is None:
-                cov_mat.append(cov_from_unbalanced(
-                    dat, obs_desc=obs_desc, method=method))
-            elif isinstance(dof, Iterable):
-                cov_mat.append(cov_from_unbalanced(
-                    dat, obs_desc=obs_desc, method=method, dof=dof[i]))
-            else:
-                cov_mat.append(cov_from_unbalanced(
-                    dat, obs_desc=obs_desc, method=method, dof=dof))
-    else:
-        assert "Dataset" in str(type(dataset)), "Provided object is not a dataset"
-        assert obs_desc in dataset.obs_descriptors.keys(), \
-            "obs_desc not contained in the dataset's obs_descriptors"
-        matrix = dataset.measurements
-        means, values, _ = average_dataset_by(dataset, obs_desc)
-        values, inverse = get_unique_inverse(dataset.obs_descriptors[obs_desc])
-        matrix -= means[inverse]
-        # calculate sample covariance matrix s
-        if dof is None:
-            dof = matrix.shape[0] - len(values)
-        cov_mat = _estimate_covariance(matrix, dof, method)
-    return cov_mat
-
-
-def prec_from_unbalanced(dataset, obs_desc, dof=None, method='shrinkage_diag'):
-    """
-    Estimates the covariance matrix from measurements and finds its multiplicative
-    inverse (= the precision matrix)
-    Use 'method' to choose which estimation method is used.
-
-    Args:
-        residuals(numpy.ndarray or list of these): n_residuals x n_channels
-            matrix of residuals
-        dof(int or list of int): degrees of freedom for covariance estimation
-            defaults to n_res - 1, should be corrected for the number
-            of regressors in a GLM if applicable.
-        method(str): which estimate to use:
-            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
-            'full': computes the sample covariance without shrinkage
-            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
-            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
-
-    Returns:
-        numpy.ndarray (or list): sigma_p: precision matrix over channels
-
-    """
-    cov = cov_from_unbalanced(dataset, obs_desc, dof=dof, method=method)
-    if not isinstance(cov, np.ndarray):
-        prec = [None] * len(cov)
-        for i, cov_i in enumerate(cov):
-            prec[i] = np.linalg.inv(cov_i)
-    elif len(cov.shape) > 2:
-        prec = np.zeros(cov.shape)
-        for i, cov_i in enumerate(cov):
-            prec[i] = np.linalg.inv(cov_i)
-    else:
-        prec = np.linalg.inv(cov)
-    return prec
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Functions for estimating the precision matrix based on the covariance of
+either the residuals (temporal based precision matrix) or of the measurements
+(instance based precision matrix)
+"""
+
+from collections.abc import Iterable
+import numpy as np
+from rsatoolbox.data import average_dataset_by
+from rsatoolbox.util.data_utils import get_unique_inverse
+
+
+def _check_demean(matrix):
+    """
+    checks that an input has 2 or 3 dimensions and subtracts the mean.
+    returns a 2D matrix for covariance/precision computation and the
+    degrees of freedom
+
+    Args:
+        matrix (np.ndarray):
+            n_conditions x n_channels
+
+    Returns:
+        numpy.ndarray:
+            demeaned matrix
+
+    """
+    assert isinstance(matrix, np.ndarray), "input must be ndarray"
+    if matrix.ndim in [1, 2]:
+        matrix = matrix - np.mean(matrix, axis=0, keepdims=True)
+        dof = matrix.shape[0] - 1
+    elif matrix.ndim == 3:
+        matrix -= np.mean(matrix, axis=2, keepdims=True)
+        dof = (matrix.shape[0] - 1) * matrix.shape[2]
+        matrix = matrix.transpose(0, 2, 1).reshape(
+            matrix.shape[0] * matrix.shape[2], matrix.shape[1])
+    else:
+        raise ValueError('Matrix for covariance estimation has wrong # of dimensions!')
+    return matrix, dof
+
+
+def _estimate_covariance(matrix, dof, method):
+    """ calls the right covariance estimation function based on the ""method" argument
+
+    Args:
+        matrix (np.ndarray):
+            n_conditions x n_channels
+
+        dof (int):
+            degrees of freedom
+
+        method (string):
+            which estimator to use
+
+    Returns:
+        numpy.ndarray, numpy.ndarray:
+            cov_mat: n_channels x n_channels sample covariance matrix
+
+    """
+    matrix, dof_nat = _check_demean(matrix)
+    if dof is None:
+        dof = dof_nat
+    # calculate sample covariance matrix s
+    if method == 'shrinkage_eye':
+        cov_mat = _covariance_eye(matrix, dof)
+    elif method == 'shrinkage_diag':
+        cov_mat = _covariance_diag(matrix, dof)
+    elif method == 'diag':
+        cov_mat = _variance(matrix, dof)
+    elif method == 'full':
+        cov_mat = _covariance_full(matrix, dof)
+    return cov_mat
+
+
+def _variance(matrix, dof):
+    """
+    returns the vector of variances per measurement channel.
+    The formula used here implies that the mean was already removed.
+
+    Args:
+        matrix (np.ndarray):
+            n_conditions x n_channels
+
+    Returns:
+        numpy.ndarray:
+            variance vector
+
+    """
+    return np.diag(np.einsum('ij, ij-> j', matrix, matrix) / dof)
+
+
+def _covariance_full(matrix, dof):
+    """
+    computes the sample covariance matrix from a 2d-array.
+    matrix should be demeaned before!
+
+    Args:
+        matrix (np.ndarray):
+            n_conditions x n_channels
+
+    Returns:
+        numpy.ndarray, numpy.ndarray:
+            s_mean: n_channels x n_channels sample covariance matrix
+
+    """
+    return np.einsum('ij, ik-> jk', matrix, matrix, optimize=True) / dof
+
+
+def _covariance_eye(matrix, dof):
+    """
+    computes the sample covariance matrix from a 2d-array.
+    matrix should be demeaned before!
+
+    Computes an optimal shrinkage estimate of a sample covariance matrix
+    as described by the following publication:
+
+    Ledoit and Wolfe (2004): "A well-conditioned
+    estimator for large-dimensional covariance matrices"
+
+    Args:
+        matrix (np.ndarray):
+            n_conditions x n_channels
+
+    Returns:
+        numpy.ndarray, numpy.ndarray:
+            s_mean: n_channels x n_channels sample covariance matrix
+
+            xt_x:
+            Einstein summation form of the matrix product
+            of the 2d-array with itself
+
+    """
+    s_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
+    s2_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
+    for m_line in matrix:
+        xt_x = np.outer(m_line, m_line)
+        s_sum += xt_x
+        s2_sum += xt_x ** 2
+    s = s_sum / matrix.shape[0]
+    b2 = np.sum(s2_sum / matrix.shape[0] - s * s) / matrix.shape[0]
+    # calculate the scalar estimators to find the optimal shrinkage:
+    # m, d^2, b^2 as in Ledoit & Wolfe paper
+    m = np.sum(np.diag(s)) / s.shape[0]
+    d2 = np.sum((s - m * np.eye(s.shape[0])) ** 2)
+    b2 = min(d2, b2)
+    # shrink covariance matrix
+    s_shrink = b2 / d2 * m * np.eye(s.shape[0]) \
+        + (d2-b2) / d2 * s
+    # correction for degrees of freedom
+    s_shrink = s_shrink * matrix.shape[0] / dof
+    return s_shrink
+
+
+def _covariance_diag(matrix, dof, mem_threshold=(10**9)/8):
+    """
+    computes the sample covariance matrix from a 2d-array.
+    matrix should be demeaned before!
+
+    Computes an optimal shrinkage estimate of a sample covariance matrix
+    as described by the following publication:
+
+    Schfer, J., & Strimmer, K. (2005). "A Shrinkage Approach to Large-Scale
+    Covariance Matrix Estimation and Implications for Functional Genomics.""
+
+    Args:
+        matrix (np.ndarray):
+            n_conditions x n_channels
+
+    Returns:
+        numpy.ndarray, numpy.ndarray:
+            s_mean: n_channels x n_channels sample covariance matrix
+
+            xt_x:
+            Einstein summation form of the matrix product
+            of the 2d-array with itself
+
+    """
+    s_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
+    s2_sum = np.zeros((matrix.shape[1], matrix.shape[1]))
+    for m_line in matrix:
+        xt_x = np.outer(m_line, m_line)
+        s_sum += xt_x
+        s2_sum += xt_x ** 2
+    s = s_sum / dof
+    var = np.diag(s)
+    std = np.sqrt(var)
+    s_mean = s_sum / np.expand_dims(std, 0) / np.expand_dims(std, 1) / (matrix.shape[0] - 1)
+    s2_mean = s2_sum / np.expand_dims(var, 0) / np.expand_dims(var, 1) / (matrix.shape[0] - 1)
+    var_hat = matrix.shape[0] / dof ** 2 \
+        * (s2_mean - s_mean ** 2)
+    mask = ~np.eye(s.shape[0], dtype=bool)
+    lamb = np.sum(var_hat[mask]) / np.sum(s_mean[mask] ** 2)
+    lamb = max(min(lamb, 1), 0)
+    scaling = np.eye(s.shape[0]) + (1-lamb) * mask
+    s_shrink = s * scaling
+    return s_shrink
+
+
+def cov_from_residuals(residuals, dof=None, method='shrinkage_diag'):
+    """
+    Estimates a covariance matrix from measurements. Allows for shrinkage estimates.
+    Use 'method' to choose which estimation method is used.
+
+    Args:
+        residuals(numpy.ndarray or list of these): n_residuals x n_channels
+            matrix of residuals
+        dof(int or list of int): degrees of freedom for covariance estimation
+            defaults to n_res - 1, should be corrected for the number
+            of regressors in a GLM if applicable.
+        method(str): which estimate to use:
+            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
+            'full': computes the sample covariance without shrinkage
+            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
+            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
+
+    Returns:
+        numpy.ndarray (or list): sigma_p: covariance matrix over channels
+
+    """
+    if not isinstance(residuals, np.ndarray) or len(residuals.shape) > 2:
+        cov_mat = []
+        for i, residual in enumerate(residuals):
+            if dof is None:
+                cov_mat.append(cov_from_residuals(
+                    residual, method=method))
+            elif isinstance(dof, Iterable):
+                cov_mat.append(cov_from_residuals(
+                    residuals, method=method, dof=dof[i]))
+            else:
+                cov_mat.append(cov_from_residuals(
+                    residual, method=method, dof=dof))
+    else:
+        cov_mat = _estimate_covariance(residuals, dof, method)
+    return cov_mat
+
+
+def prec_from_residuals(residuals, dof=None, method='shrinkage_diag'):
+    """
+    Estimates the covariance matrix from residuals and finds its multiplicative
+    inverse (= the precision matrix)
+    Use 'method' to choose which estimation method is used.
+
+    Args:
+        residuals(numpy.ndarray or list of these): n_residuals x n_channels
+            matrix of residuals
+        dof(int or list of int): degrees of freedom for covariance estimation
+            defaults to n_res - 1, should be corrected for the number
+            of regressors in a GLM if applicable.
+        method(str): which estimate to use:
+            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
+            'full': computes the sample covariance without shrinkage
+            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
+            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
+
+    Returns:
+        numpy.ndarray (or list): sigma_p: precision matrix over channels
+
+    """
+    cov = cov_from_residuals(residuals=residuals, dof=dof, method=method)
+    if not isinstance(cov, np.ndarray):
+        prec = [None] * len(cov)
+        for i, cov_i in enumerate(cov):
+            prec[i] = np.linalg.inv(cov_i)
+    elif len(cov.shape) > 2:
+        prec = np.zeros(cov.shape)
+        for i, cov_i in enumerate(cov):
+            prec[i] = np.linalg.inv(cov_i)
+    else:
+        prec = np.linalg.inv(cov)
+    return prec
+
+
+def cov_from_measurements(dataset, obs_desc, dof=None, method='shrinkage_diag'):
+    """
+    Estimates a covariance matrix from measurements. Allows for shrinkage estimates.
+    Use 'method' to choose which estimation method is used.
+
+    Args:
+        dataset(data.Dataset):
+            rsatoolbox Dataset object
+        dof(int or list of int): degrees of freedom for covariance estimation
+            defaults to n_res - 1, should be corrected for the number
+            of regressors in a GLM if applicable.
+        method(str): which estimate to use:
+            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
+            'full': computes the sample covariance without shrinkage
+            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
+            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
+
+    Returns:
+        numpy.ndarray (or list): sigma_p: covariance matrix over channels
+
+    """
+    if isinstance(dataset, Iterable):
+        cov_mat = []
+        for i, dat in enumerate(dataset):
+            if dof is None:
+                cov_mat.append(cov_from_unbalanced(
+                    dat, obs_desc=obs_desc, method=method))
+            elif isinstance(dof, Iterable):
+                cov_mat.append(cov_from_unbalanced(
+                    dat, obs_desc=obs_desc, method=method, dof=dof[i]))
+            else:
+                cov_mat.append(cov_from_unbalanced(
+                    dat, obs_desc=obs_desc, method=method, dof=dof))
+    else:
+        assert "Dataset" in str(type(dataset)), "Provided object is not a dataset"
+        assert obs_desc in dataset.obs_descriptors.keys(), \
+            "obs_desc not contained in the dataset's obs_descriptors"
+        tensor, _ = dataset.get_measurements_tensor(obs_desc)
+        # calculate sample covariance matrix s
+        cov_mat = _estimate_covariance(tensor, dof, method)
+    return cov_mat
+
+
+def prec_from_measurements(dataset, obs_desc, dof=None, method='shrinkage_diag'):
+    """
+    Estimates the covariance matrix from measurements and finds its multiplicative
+    inverse (= the precision matrix)
+    Use 'method' to choose which estimation method is used.
+
+    Args:
+        residuals(numpy.ndarray or list of these): n_residuals x n_channels
+            matrix of residuals
+        dof(int or list of int): degrees of freedom for covariance estimation
+            defaults to n_res - 1, should be corrected for the number
+            of regressors in a GLM if applicable.
+        method(str): which estimate to use:
+            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
+            'full': computes the sample covariance without shrinkage
+            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
+            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
+
+    Returns:
+        numpy.ndarray (or list): sigma_p: precision matrix over channels
+
+    """
+    cov = cov_from_measurements(dataset, obs_desc, dof=dof, method=method)
+    if not isinstance(cov, np.ndarray):
+        prec = [None] * len(cov)
+        for i, cov_i in enumerate(cov):
+            prec[i] = np.linalg.inv(cov_i)
+    elif len(cov.shape) > 2:
+        prec = np.zeros(cov.shape)
+        for i, cov_i in enumerate(cov):
+            prec[i] = np.linalg.inv(cov_i)
+    else:
+        prec = np.linalg.inv(cov)
+    return prec
+
+
+def cov_from_unbalanced(dataset, obs_desc, dof=None, method='shrinkage_diag'):
+    """
+    Estimates a covariance matrix from an unbalanced dataset, i.e. from a
+    dataset that contains different numbers of samples for different
+    stimuli.
+
+    Args:
+        dataset(data.Dataset):
+            rsatoolbox Dataset object
+        dof(int or list of int): degrees of freedom for covariance estimation
+            defaults to n_measurements - n_stimuli, should be corrected
+            if this is not the case
+        method(str): which estimate to use:
+            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
+            'full': computes the sample covariance without shrinkage
+            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
+            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
+
+    Returns:
+        numpy.ndarray (or list): sigma_p: covariance matrix over channels
+
+    """
+    if isinstance(dataset, Iterable):
+        cov_mat = []
+        for i, dat in enumerate(dataset):
+            if dof is None:
+                cov_mat.append(cov_from_unbalanced(
+                    dat, obs_desc=obs_desc, method=method))
+            elif isinstance(dof, Iterable):
+                cov_mat.append(cov_from_unbalanced(
+                    dat, obs_desc=obs_desc, method=method, dof=dof[i]))
+            else:
+                cov_mat.append(cov_from_unbalanced(
+                    dat, obs_desc=obs_desc, method=method, dof=dof))
+    else:
+        assert "Dataset" in str(type(dataset)), "Provided object is not a dataset"
+        assert obs_desc in dataset.obs_descriptors.keys(), \
+            "obs_desc not contained in the dataset's obs_descriptors"
+        matrix = dataset.measurements
+        means, values, _ = average_dataset_by(dataset, obs_desc)
+        values, inverse = get_unique_inverse(dataset.obs_descriptors[obs_desc])
+        matrix -= means[inverse]
+        # calculate sample covariance matrix s
+        if dof is None:
+            dof = matrix.shape[0] - len(values)
+        cov_mat = _estimate_covariance(matrix, dof, method)
+    return cov_mat
+
+
+def prec_from_unbalanced(dataset, obs_desc, dof=None, method='shrinkage_diag'):
+    """
+    Estimates the covariance matrix from measurements and finds its multiplicative
+    inverse (= the precision matrix)
+    Use 'method' to choose which estimation method is used.
+
+    Args:
+        residuals(numpy.ndarray or list of these): n_residuals x n_channels
+            matrix of residuals
+        dof(int or list of int): degrees of freedom for covariance estimation
+            defaults to n_res - 1, should be corrected for the number
+            of regressors in a GLM if applicable.
+        method(str): which estimate to use:
+            'diag': provides a diagonal matrix, i.e. univariate noise normalizer
+            'full': computes the sample covariance without shrinkage
+            'shrinkage_eye': shrinks the data covariance towards a multiple of the identity.
+            'shrinkage_diag': shrinks the covariance matrix towards the diagonal covariance matrix.
+
+    Returns:
+        numpy.ndarray (or list): sigma_p: precision matrix over channels
+
+    """
+    cov = cov_from_unbalanced(dataset, obs_desc, dof=dof, method=method)
+    if not isinstance(cov, np.ndarray):
+        prec = [None] * len(cov)
+        for i, cov_i in enumerate(cov):
+            prec[i] = np.linalg.inv(cov_i)
+    elif len(cov.shape) > 2:
+        prec = np.zeros(cov.shape)
+        for i, cov_i in enumerate(cov):
+            prec[i] = np.linalg.inv(cov_i)
+    else:
+        prec = np.linalg.inv(cov)
+    return prec
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/__init__.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/__init__.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-from .bootstrap import bootstrap_sample
-from .bootstrap import bootstrap_sample_rdm
-from .bootstrap import bootstrap_sample_pattern
-from .evaluate import eval_fixed
-from .evaluate import eval_bootstrap
-from .evaluate import eval_bootstrap_rdm
-from .evaluate import eval_bootstrap_pattern
-from .evaluate import eval_dual_bootstrap
-from .evaluate import bootstrap_crossval
-from .evaluate import eval_dual_bootstrap_random
-from .evaluate import crossval
-from .boot_testset import bootstrap_testset
-from .boot_testset import bootstrap_testset_pattern
-from .boot_testset import bootstrap_testset_rdm
-from .crossvalsets import sets_leave_one_out_pattern
-from .crossvalsets import sets_leave_one_out_rdm
-from .crossvalsets import sets_k_fold
-from .crossvalsets import sets_k_fold_pattern
-from .crossvalsets import sets_k_fold_rdm
-from .crossvalsets import sets_of_k_pattern
-from .noise_ceiling import cv_noise_ceiling
-from .noise_ceiling import boot_noise_ceiling
-from .result import load_results
-from .result import Result
-from .result import result_from_dict
+from .bootstrap import bootstrap_sample
+from .bootstrap import bootstrap_sample_rdm
+from .bootstrap import bootstrap_sample_pattern
+from .evaluate import eval_fixed
+from .evaluate import eval_bootstrap
+from .evaluate import eval_bootstrap_rdm
+from .evaluate import eval_bootstrap_pattern
+from .evaluate import eval_dual_bootstrap
+from .evaluate import bootstrap_crossval
+from .evaluate import eval_dual_bootstrap_random
+from .evaluate import crossval
+from .boot_testset import bootstrap_testset
+from .boot_testset import bootstrap_testset_pattern
+from .boot_testset import bootstrap_testset_rdm
+from .crossvalsets import sets_leave_one_out_pattern
+from .crossvalsets import sets_leave_one_out_rdm
+from .crossvalsets import sets_k_fold
+from .crossvalsets import sets_k_fold_pattern
+from .crossvalsets import sets_k_fold_rdm
+from .crossvalsets import sets_of_k_pattern
+from .noise_ceiling import cv_noise_ceiling
+from .noise_ceiling import boot_noise_ceiling
+from .result import load_results
+from .result import Result
+from .result import result_from_dict
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/boot_testset.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/boot_testset.py`

 * *Ordering differences only*

 * *Files 11% similar despite different names*

```diff
@@ -1,160 +1,160 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-boostrap-testset evaluation methods
-variants of taking a bootstrap sample and taking the unsampled patterns as
-a testset
-"""
-
-import numpy as np
-from rsatoolbox.util.inference_util import input_check_model
-from .bootstrap import bootstrap_sample
-from .bootstrap import bootstrap_sample_rdm
-from .bootstrap import bootstrap_sample_pattern
-from .evaluate import crossval
-
-
-def bootstrap_testset(models, data, method='cosine', fitter=None, N=1000,
-                      pattern_descriptor=None, rdm_descriptor=None):
-    """takes a bootstrap sample and evaluates on the rdms and patterns not
-    sampled
-    also returns the size of each test_set to allow later weighting
-    or selection if this is desired.
-
-    Args:
-        models(rsatoolbox.model.Model): Models to be evaluated
-        data(rsatoolbox.rdm.RDMs): RDM data to use
-        method(string): comparison method to use
-        fitter(function): fitting function
-        pattern_descriptor(string): descriptor to group patterns
-        rdm_descriptor(string): descriptor to group rdms
-
-    Returns:
-        numpy.ndarray: vector of evaluations of length N
-        numpy.ndarray: n_rdm for each test_set
-        numpy.ndarray: n_pattern for each test_set
-
-    """
-    models, evaluations, _, fitter = input_check_model(models, None, fitter, N)
-    n_rdm = np.zeros(N, dtype=int)
-    n_pattern = np.zeros(N, dtype=int)
-    if pattern_descriptor is None:
-        data.pattern_descriptors['index'] = np.arange(data.n_cond)
-        pattern_descriptor = 'index'
-    if rdm_descriptor is None:
-        data.rdm_descriptors['index'] = np.arange(data.n_rdm)
-        rdm_descriptor = 'index'
-    for i_sample in range(N):
-        sample, rdm_idx, pattern_idx = bootstrap_sample(
-            data,
-            rdm_descriptor=rdm_descriptor,
-            pattern_descriptor=pattern_descriptor)
-        train_set = [[sample, pattern_idx]]
-        rdm_idx_test = data.rdm_descriptors[rdm_descriptor]
-        rdm_idx_test = np.setdiff1d(rdm_idx_test, rdm_idx)
-        pattern_idx_test = data.pattern_descriptors[pattern_descriptor]
-        pattern_idx_test = np.setdiff1d(pattern_idx_test, pattern_idx)
-        if len(pattern_idx_test) >= 3 and len(rdm_idx_test) >= 1:
-            rdms_test = data.subsample_pattern(pattern_descriptor,
-                                               pattern_idx_test)
-            rdms_test = rdms_test.subsample(rdm_descriptor, rdm_idx_test)
-            test_set = [[rdms_test, pattern_idx_test]]
-            evaluations[i_sample] = crossval(
-                models, data, train_set, test_set,
-                method=method, fitter=fitter,
-                pattern_descriptor=pattern_descriptor).evaluations[:, 0]
-        else:
-            evaluations[i_sample] = np.nan
-        n_rdm[i_sample] = len(rdm_idx_test)
-        n_pattern[i_sample] = len(pattern_idx_test)
-    return evaluations, n_rdm, n_pattern
-
-
-def bootstrap_testset_pattern(models, data, method='cosine', fitter=None,
-                              N=1000, pattern_descriptor=None):
-    """takes a bootstrap sample and evaluates on the patterns not
-    sampled
-    also returns the size of each test_set to allow later weighting
-    or selection if this is desired.
-
-    Args:
-        models(rsatoolbox.model.Model): Model to be evaluated
-        datat(rsatoolbox.rdm.RDMs): RDM data to use
-        method(string): comparison method to use
-        fitter(function): fitting function for the model
-        pattern_descriptor(string): descriptor to group patterns
-
-    Returns:
-        numpy.ndarray: vector of evaluations of length
-        numpy.ndarray: n_pattern for each test_set
-
-    """
-    models, evaluations, _, fitter = input_check_model(models, None, fitter, N)
-    n_pattern = np.zeros(N, dtype=int)
-    if pattern_descriptor is None:
-        data.pattern_descriptors['index'] = np.arange(data.n_cond)
-        pattern_descriptor = 'index'
-    for i_sample in range(N):
-        sample, pattern_idx = bootstrap_sample_pattern(
-            data, pattern_descriptor=pattern_descriptor)
-        train_set = [[sample, pattern_idx]]
-        pattern_idx_test = data.pattern_descriptors[pattern_descriptor]
-        pattern_idx_test = np.setdiff1d(pattern_idx_test, pattern_idx)
-        if len(pattern_idx_test) >= 3:
-            rdms_test = data.subsample_pattern(pattern_descriptor,
-                                               pattern_idx_test)
-            test_set = [[rdms_test, pattern_idx_test]]
-            evaluations[i_sample] = crossval(
-                models, data, train_set, test_set,
-                method=method, fitter=fitter,
-                pattern_descriptor=pattern_descriptor).evaluations[:, 0]
-        else:
-            evaluations[i_sample] = np.nan
-        n_pattern[i_sample] = len(pattern_idx_test)
-    return evaluations, n_pattern
-
-
-def bootstrap_testset_rdm(models, data, method='cosine', fitter=None, N=1000,
-                          rdm_descriptor=None):
-    """takes a bootstrap sample and evaluates on the patterns not
-    sampled
-    also returns the size of each test_set to allow later weighting
-    or selection if this is desired.
-
-    Args:
-        model(rsatoolbox.model.Model): Model to be evaluated
-        datat(rsatoolbox.rdm.RDMs): RDM data to use
-        method(string): comparison method to use
-        fitter(function): fitting function for the model
-        pattern_descriptor(string): descriptor to group patterns
-
-    Returns:
-        numpy.ndarray: vector of evaluations of length
-        numpy.ndarray: n_pattern for each test_set
-
-    """
-    models, evaluations, _, fitter = input_check_model(models, None, fitter, N)
-    n_rdm = np.zeros(N, dtype=int)
-    if rdm_descriptor is None:
-        data.rdm_descriptors['index'] = np.arange(data.n_rdm)
-        rdm_descriptor = 'index'
-    data.pattern_descriptors['index'] = np.arange(data.n_cond)
-    pattern_descriptor = 'index'
-    for i_sample in range(N):
-        sample, rdm_idx = bootstrap_sample_rdm(
-            data, rdm_descriptor=rdm_descriptor)
-        pattern_idx = np.arange(data.n_cond)
-        train_set = [[sample, pattern_idx]]
-        rdm_idx_test = data.rdm_descriptors[rdm_descriptor]
-        rdm_idx_test = np.setdiff1d(rdm_idx_test, rdm_idx)
-        if len(rdm_idx_test) >= 1:
-            rdms_test = data.subsample(rdm_descriptor, rdm_idx_test)
-            test_set = [[rdms_test, pattern_idx]]
-            evaluations[i_sample] = crossval(
-                models, data, train_set, test_set,
-                method=method, fitter=fitter,
-                pattern_descriptor=pattern_descriptor).evaluations[:, 0]
-        else:
-            evaluations[i_sample] = np.nan
-        n_rdm[i_sample] = len(rdm_idx_test)
-    return evaluations, n_rdm
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+boostrap-testset evaluation methods
+variants of taking a bootstrap sample and taking the unsampled patterns as
+a testset
+"""
+
+import numpy as np
+from rsatoolbox.util.inference_util import input_check_model
+from .bootstrap import bootstrap_sample
+from .bootstrap import bootstrap_sample_rdm
+from .bootstrap import bootstrap_sample_pattern
+from .evaluate import crossval
+
+
+def bootstrap_testset(models, data, method='cosine', fitter=None, N=1000,
+                      pattern_descriptor=None, rdm_descriptor=None):
+    """takes a bootstrap sample and evaluates on the rdms and patterns not
+    sampled
+    also returns the size of each test_set to allow later weighting
+    or selection if this is desired.
+
+    Args:
+        models(rsatoolbox.model.Model): Models to be evaluated
+        data(rsatoolbox.rdm.RDMs): RDM data to use
+        method(string): comparison method to use
+        fitter(function): fitting function
+        pattern_descriptor(string): descriptor to group patterns
+        rdm_descriptor(string): descriptor to group rdms
+
+    Returns:
+        numpy.ndarray: vector of evaluations of length N
+        numpy.ndarray: n_rdm for each test_set
+        numpy.ndarray: n_pattern for each test_set
+
+    """
+    models, evaluations, _, fitter = input_check_model(models, None, fitter, N)
+    n_rdm = np.zeros(N, dtype=int)
+    n_pattern = np.zeros(N, dtype=int)
+    if pattern_descriptor is None:
+        data.pattern_descriptors['index'] = np.arange(data.n_cond)
+        pattern_descriptor = 'index'
+    if rdm_descriptor is None:
+        data.rdm_descriptors['index'] = np.arange(data.n_rdm)
+        rdm_descriptor = 'index'
+    for i_sample in range(N):
+        sample, rdm_idx, pattern_idx = bootstrap_sample(
+            data,
+            rdm_descriptor=rdm_descriptor,
+            pattern_descriptor=pattern_descriptor)
+        train_set = [[sample, pattern_idx]]
+        rdm_idx_test = data.rdm_descriptors[rdm_descriptor]
+        rdm_idx_test = np.setdiff1d(rdm_idx_test, rdm_idx)
+        pattern_idx_test = data.pattern_descriptors[pattern_descriptor]
+        pattern_idx_test = np.setdiff1d(pattern_idx_test, pattern_idx)
+        if len(pattern_idx_test) >= 3 and len(rdm_idx_test) >= 1:
+            rdms_test = data.subsample_pattern(pattern_descriptor,
+                                               pattern_idx_test)
+            rdms_test = rdms_test.subsample(rdm_descriptor, rdm_idx_test)
+            test_set = [[rdms_test, pattern_idx_test]]
+            evaluations[i_sample] = crossval(
+                models, data, train_set, test_set,
+                method=method, fitter=fitter,
+                pattern_descriptor=pattern_descriptor).evaluations[:, 0]
+        else:
+            evaluations[i_sample] = np.nan
+        n_rdm[i_sample] = len(rdm_idx_test)
+        n_pattern[i_sample] = len(pattern_idx_test)
+    return evaluations, n_rdm, n_pattern
+
+
+def bootstrap_testset_pattern(models, data, method='cosine', fitter=None,
+                              N=1000, pattern_descriptor=None):
+    """takes a bootstrap sample and evaluates on the patterns not
+    sampled
+    also returns the size of each test_set to allow later weighting
+    or selection if this is desired.
+
+    Args:
+        models(rsatoolbox.model.Model): Model to be evaluated
+        datat(rsatoolbox.rdm.RDMs): RDM data to use
+        method(string): comparison method to use
+        fitter(function): fitting function for the model
+        pattern_descriptor(string): descriptor to group patterns
+
+    Returns:
+        numpy.ndarray: vector of evaluations of length
+        numpy.ndarray: n_pattern for each test_set
+
+    """
+    models, evaluations, _, fitter = input_check_model(models, None, fitter, N)
+    n_pattern = np.zeros(N, dtype=int)
+    if pattern_descriptor is None:
+        data.pattern_descriptors['index'] = np.arange(data.n_cond)
+        pattern_descriptor = 'index'
+    for i_sample in range(N):
+        sample, pattern_idx = bootstrap_sample_pattern(
+            data, pattern_descriptor=pattern_descriptor)
+        train_set = [[sample, pattern_idx]]
+        pattern_idx_test = data.pattern_descriptors[pattern_descriptor]
+        pattern_idx_test = np.setdiff1d(pattern_idx_test, pattern_idx)
+        if len(pattern_idx_test) >= 3:
+            rdms_test = data.subsample_pattern(pattern_descriptor,
+                                               pattern_idx_test)
+            test_set = [[rdms_test, pattern_idx_test]]
+            evaluations[i_sample] = crossval(
+                models, data, train_set, test_set,
+                method=method, fitter=fitter,
+                pattern_descriptor=pattern_descriptor).evaluations[:, 0]
+        else:
+            evaluations[i_sample] = np.nan
+        n_pattern[i_sample] = len(pattern_idx_test)
+    return evaluations, n_pattern
+
+
+def bootstrap_testset_rdm(models, data, method='cosine', fitter=None, N=1000,
+                          rdm_descriptor=None):
+    """takes a bootstrap sample and evaluates on the patterns not
+    sampled
+    also returns the size of each test_set to allow later weighting
+    or selection if this is desired.
+
+    Args:
+        model(rsatoolbox.model.Model): Model to be evaluated
+        datat(rsatoolbox.rdm.RDMs): RDM data to use
+        method(string): comparison method to use
+        fitter(function): fitting function for the model
+        pattern_descriptor(string): descriptor to group patterns
+
+    Returns:
+        numpy.ndarray: vector of evaluations of length
+        numpy.ndarray: n_pattern for each test_set
+
+    """
+    models, evaluations, _, fitter = input_check_model(models, None, fitter, N)
+    n_rdm = np.zeros(N, dtype=int)
+    if rdm_descriptor is None:
+        data.rdm_descriptors['index'] = np.arange(data.n_rdm)
+        rdm_descriptor = 'index'
+    data.pattern_descriptors['index'] = np.arange(data.n_cond)
+    pattern_descriptor = 'index'
+    for i_sample in range(N):
+        sample, rdm_idx = bootstrap_sample_rdm(
+            data, rdm_descriptor=rdm_descriptor)
+        pattern_idx = np.arange(data.n_cond)
+        train_set = [[sample, pattern_idx]]
+        rdm_idx_test = data.rdm_descriptors[rdm_descriptor]
+        rdm_idx_test = np.setdiff1d(rdm_idx_test, rdm_idx)
+        if len(rdm_idx_test) >= 1:
+            rdms_test = data.subsample(rdm_descriptor, rdm_idx_test)
+            test_set = [[rdms_test, pattern_idx]]
+            evaluations[i_sample] = crossval(
+                models, data, train_set, test_set,
+                method=method, fitter=fitter,
+                pattern_descriptor=pattern_descriptor).evaluations[:, 0]
+        else:
+            evaluations[i_sample] = np.nan
+        n_rdm[i_sample] = len(rdm_idx_test)
+    return evaluations, n_rdm
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/crossvalsets.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/crossvalsets.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,386 +1,386 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-generation of crossvalidation splits
-"""
-
-from copy import deepcopy
-import numpy as np
-from rsatoolbox.util.rdm_utils import add_pattern_index
-from rsatoolbox.util.inference_util import default_k_pattern, default_k_rdm
-
-
-def sets_leave_one_out_pattern(rdms, pattern_descriptor):
-    """ generates training and test set combinations by leaving one level
-    of pattern_descriptor out as a test set.
-    This is only sensible if pattern_descriptor already defines larger groups!
-
-    the ceil_train_set contains the rdms for the test-patterns from the
-    training-rdms. This is required for computing the noise-ceiling
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        pattern_descriptor(String): descriptor to select groups
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-        ceil_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    pattern_descriptor, pattern_select = \
-        add_pattern_index(rdms, pattern_descriptor)
-    train_set = []
-    test_set = []
-    ceil_set = []
-    for i_pattern in pattern_select:
-        pattern_idx_train = np.setdiff1d(pattern_select, i_pattern)
-        rdms_train = rdms.subset_pattern(pattern_descriptor,
-                                         pattern_idx_train)
-        pattern_idx_test = [i_pattern]
-        rdms_test = rdms.subset_pattern(pattern_descriptor,
-                                        pattern_idx_test)
-        rdms_ceil = rdms.subset_pattern(pattern_descriptor,
-                                        pattern_idx_test)
-        train_set.append((rdms_train, pattern_idx_train))
-        test_set.append((rdms_test, pattern_idx_test))
-        ceil_set.append((rdms_ceil, pattern_idx_test))
-    return train_set, test_set, ceil_set
-
-
-def sets_leave_one_out_rdm(rdms, rdm_descriptor='index'):
-    """ generates training and test set combinations by leaving one level
-    of rdm_descriptor out as a test set.\
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        rdm_descriptor(String): descriptor to select groups
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-        ceil_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
-    rdm_select = np.unique(rdm_select)
-    if len(rdm_select) > 1:
-        train_set = []
-        test_set = []
-        for i_pattern in rdm_select:
-            rdm_idx_train = np.setdiff1d(rdm_select, i_pattern)
-            rdms_train = rdms.subset(rdm_descriptor,
-                                     rdm_idx_train)
-            rdm_idx_test = [i_pattern]
-            rdms_test = rdms.subset(rdm_descriptor,
-                                    rdm_idx_test)
-            train_set.append((rdms_train, np.arange(rdms.n_cond)))
-            test_set.append((rdms_test, np.arange(rdms.n_cond)))
-        ceil_set = train_set
-    else:
-        Warning('leave one out called with only one group')
-        train_set = [(rdms, np.arange(rdms.n_cond))]
-        test_set = [(rdms, np.arange(rdms.n_cond))]
-        ceil_set = [(rdms, np.arange(rdms.n_cond))]
-    return train_set, test_set, ceil_set
-
-
-def sets_k_fold(rdms, k_rdm=None, k_pattern=None, random=True,
-                pattern_descriptor='index', rdm_descriptor='index'):
-    """ generates training and test set combinations by splitting into k
-    similar sized groups. This version splits both over rdms and over patterns
-    resulting in k_rdm * k_pattern (training, test) pairs.
-
-    If a k is set to 1 the corresponding dimension is not crossvalidated.
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        pattern_descriptor(String): descriptor to select pattern groups
-        rdm_descriptor(String): descriptor to select rdm groups
-        k_rdm(int): number of rdm groups
-        k_pattern(int): number of pattern groups
-        random(bool): whether the assignment shall be randomized
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-        ceil_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
-    rdm_select = np.unique(rdm_select)
-    if k_rdm is None:
-        k_rdm = default_k_rdm(len(rdm_select))
-    pattern_descriptor, pattern_select = \
-        add_pattern_index(rdms, pattern_descriptor)
-    if k_pattern is None:
-        k_pattern = default_k_pattern(len(pattern_select))
-    assert k_rdm <= len(rdm_select), \
-        'Can make at most as many groups as rdms'
-    if random:
-        np.random.shuffle(rdm_select)
-    group_size_rdm = np.floor(len(rdm_select) / k_rdm)
-    additional_rdms = len(rdm_select) % k_rdm
-    train_set = []
-    test_set = []
-    ceil_set = []
-    for i_group in range(k_rdm):
-        test_idx = np.arange(i_group * group_size_rdm,
-                             (i_group + 1) * group_size_rdm)
-        if i_group < additional_rdms:
-            test_idx = np.concatenate((test_idx, [len(rdm_select)-(i_group+1)]))
-        if k_rdm <= 1:
-            train_idx = test_idx
-        else:
-            train_idx = np.setdiff1d(np.arange(len(rdm_select)),
-                                     test_idx)
-        rdm_idx_test = [rdm_select[int(idx)] for idx in test_idx]
-        rdm_idx_train = [rdm_select[int(idx)] for idx in train_idx]
-        rdms_test = rdms.subsample(rdm_descriptor,
-                                   rdm_idx_test)
-        rdms_train = rdms.subsample(rdm_descriptor,
-                                    rdm_idx_train)
-        train_new, test_new, _ = sets_k_fold_pattern(
-            rdms_train, k=k_pattern,
-            pattern_descriptor=pattern_descriptor, random=random)
-        ceil_new = deepcopy(test_new)
-        for i_pattern in range(k_pattern):
-            test_new[i_pattern][0] = rdms_test.subset_pattern(
-                by=pattern_descriptor,
-                value=test_new[i_pattern][1])
-        train_set += train_new
-        test_set += test_new
-        ceil_set += ceil_new
-    return train_set, test_set, ceil_set
-
-
-def sets_k_fold_rdm(rdms, k_rdm=None, random=True, rdm_descriptor='index'):
-    """ generates training and test set combinations by splitting into k
-    similar sized groups. This version splits both over rdms and over patterns
-    resulting in k_rdm * k_pattern (training, test) pairs.
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        rdm_descriptor(String): descriptor to select rdm groups
-        k_rdm(int): number of rdm groups
-        random(bool): whether the assignment shall be randomized
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
-    rdm_select = np.unique(rdm_select)
-    if k_rdm is None:
-        k_rdm = default_k_rdm(len(rdm_select))
-    assert k_rdm <= len(rdm_select), \
-        'Can make at most as many groups as rdms'
-    if random:
-        np.random.shuffle(rdm_select)
-    group_size_rdm = np.floor(len(rdm_select) / k_rdm)
-    additional_rdms = len(rdm_select) % k_rdm
-    train_set = []
-    test_set = []
-    for i_group in range(k_rdm):
-        test_idx = np.arange(i_group * group_size_rdm,
-                             (i_group + 1) * group_size_rdm)
-        if i_group < additional_rdms:
-            test_idx = np.concatenate((test_idx, [len(rdm_select)-(i_group+1)]))
-        train_idx = np.setdiff1d(np.arange(len(rdm_select)),
-                                 test_idx)
-        rdm_idx_test = [rdm_select[int(idx)] for idx in test_idx]
-        rdm_idx_train = [rdm_select[int(idx)] for idx in train_idx]
-        rdms_test = rdms.subsample(rdm_descriptor,
-                                   rdm_idx_test)
-        rdms_train = rdms.subsample(rdm_descriptor,
-                                    rdm_idx_train)
-        train_set.append([rdms_train, np.arange(rdms_train.n_cond)])
-        test_set.append([rdms_test, np.arange(rdms_train.n_cond)])
-    ceil_set = train_set
-    return train_set, test_set, ceil_set
-
-
-def sets_k_fold_pattern(rdms, pattern_descriptor='index',
-                        k=None, random=False):
-    """ generates training and test set combinations by splitting into k
-    similar sized groups. This version splits in the given order or
-    randomizes the order. For k=1 training and test_set are whole dataset,
-    i.e. no crossvalidation is performed.
-
-    For only crossvalidating over patterns there is no independent training
-    set for calculating a noise ceiling for the patterns.
-    To express this we set ceil_set to None, which makes the crossvalidation
-    function calculate a leave one rdm out noise ceiling for the right
-    patterns instead.
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        pattern_descriptor(String): descriptor to select groups
-        k(int): number of groups
-        random(bool): whether the assignment shall be randomized
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-        ceil_set = None
-
-    """
-    pattern_descriptor, pattern_select = \
-        add_pattern_index(rdms, pattern_descriptor)
-    if k is None:
-        k = default_k_pattern(len(pattern_select))
-    assert k <= len(pattern_select), \
-        'Can make at most as many groups as conditions'
-    if random:
-        np.random.shuffle(pattern_select)
-    group_size = np.floor(len(pattern_select) / k)
-    additional_patterns = len(pattern_select) % k
-    train_set = []
-    test_set = []
-    for i_group in range(k):
-        test_idx = np.arange(i_group * group_size,
-                             (i_group + 1) * group_size)
-        if i_group < additional_patterns:
-            test_idx = np.concatenate((test_idx, [len(pattern_select)-(i_group+1)]))
-        if k <= 1:
-            train_idx = test_idx
-        else:
-            train_idx = np.setdiff1d(np.arange(len(pattern_select)),
-                                     test_idx)
-        pattern_idx_test = [pattern_select[int(idx)] for idx in test_idx]
-        pattern_idx_train = [pattern_select[int(idx)] for idx in train_idx]
-        rdms_test = rdms.subset_pattern(pattern_descriptor,
-                                        pattern_idx_test)
-        rdms_train = rdms.subset_pattern(pattern_descriptor,
-                                         pattern_idx_train)
-        test_set.append([rdms_test, pattern_idx_test])
-        train_set.append([rdms_train, pattern_idx_train])
-    ceil_set = None
-    return train_set, test_set, ceil_set
-
-
-def sets_of_k_rdm(rdms, rdm_descriptor='index', k=5, random=False):
-    """ generates training and test set combinations by splitting into
-    groups of k. This version splits in the given order or
-    randomizes the order. If the number of patterns is not divisible by k
-    patterns are added to the first groups such that those have k+1 patterns
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        pattern_descriptor(String): descriptor to select groups
-        k(int): number of groups
-        random(bool): whether the assignment shall be randomized
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-        ceil_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
-    rdm_select = np.unique(rdm_select)
-    assert k <= len(rdm_select) / 2, \
-        'to form groups we can use at most half the patterns per group'
-    n_groups = int(len(rdm_select) / k)
-    return sets_k_fold_rdm(rdms, rdm_descriptor=rdm_descriptor,
-                           k=n_groups, random=random)
-
-
-def sets_of_k_pattern(rdms, pattern_descriptor=None, k=5, random=False):
-    """ generates training and test set combinations by splitting into
-    groups of k. This version splits in the given order or
-    randomizes the order. If the number of patterns is not divisible by k
-    patterns are added to the first groups such that those have k+1 patterns
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to use
-        pattern_descriptor(String): descriptor to select groups
-        k(int): number of groups
-        random(bool): whether the assignment shall be randomized
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    pattern_descriptor, pattern_select = \
-        add_pattern_index(rdms, pattern_descriptor)
-    assert k <= len(pattern_select) / 2, \
-        'to form groups we can use at most half the patterns per group'
-    n_groups = int(len(pattern_select) / k)
-    return sets_k_fold_pattern(rdms, pattern_descriptor=pattern_descriptor,
-                               k=n_groups, random=random)
-
-
-def sets_random(rdms, n_rdm=None, n_pattern=None, n_cv=2,
-                pattern_descriptor='index', rdm_descriptor='index'):
-    """ generates training and test set combinations by selecting random
-    test sets of n_rdm RDMs and n_pattern patterns and using the rest of
-    the data as the training set.
-
-    If a n is set to 0 the corresponding dimension is not crossvalidated.
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): rdms to split
-        pattern_descriptor(String): descriptor to select pattern groups
-        rdm_descriptor(String): descriptor to select rdm groups
-        n_rdm(int): number of rdms per test set
-        n_pattern(int): number of patterns per test set
-
-    Returns:
-        train_set(list): list of tuples (rdms, pattern_idx)
-        test_set(list): list of tuples (rdms, pattern_idx)
-        ceil_set(list): list of tuples (rdms, pattern_idx)
-
-    """
-    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
-    rdm_select = np.unique(rdm_select)
-    if n_rdm is None:
-        k_rdm = default_k_rdm(len(rdm_select))
-        n_rdm = int(np.floor(len(rdm_select) / k_rdm))
-    pattern_descriptor, pattern_select = \
-        add_pattern_index(rdms, pattern_descriptor)
-    if n_pattern is None:
-        k_pattern = default_k_pattern(len(pattern_select))
-        n_pattern = int(np.floor(len(pattern_select) / k_pattern))
-    train_set = []
-    test_set = []
-    ceil_set = []
-    for _i_group in range(n_cv):
-        # shuffle
-        np.random.shuffle(rdm_select)
-        np.random.shuffle(pattern_select)
-        # choose indices based on n_rdm
-        if n_rdm == 0:
-            train_idx = np.arange(len(rdm_select))
-            test_idx = np.arange(len(rdm_select))
-        else:
-            test_idx = np.arange(n_rdm)
-            train_idx = np.arange(n_rdm, len(rdm_select))
-        # take subset of rdms
-        rdm_idx_test = [rdm_select[int(idx)] for idx in test_idx]
-        rdm_idx_train = [rdm_select[int(idx)] for idx in train_idx]
-        rdms_test = rdms.subsample(rdm_descriptor,
-                                   rdm_idx_test)
-        rdms_train = rdms.subsample(rdm_descriptor,
-                                    rdm_idx_train)
-        # choose indices based on n_pattern
-        if n_pattern == 0:
-            train_idx = np.arange(len(pattern_select))
-            test_idx = np.arange(len(pattern_select))
-        else:
-            test_idx = np.arange(n_pattern)
-            train_idx = np.arange(n_pattern, len(pattern_select))
-        pattern_idx_test = [pattern_select[int(idx)] for idx in test_idx]
-        pattern_idx_train = [pattern_select[int(idx)] for idx in train_idx]
-        rdms_test = rdms_test.subset_pattern(pattern_descriptor,
-                                             pattern_idx_test)
-        rdms_ceil = rdms_train.subset_pattern(pattern_descriptor,
-                                              pattern_idx_test)
-        rdms_train = rdms_train.subset_pattern(pattern_descriptor,
-                                               pattern_idx_train)
-        test_set.append([rdms_test, pattern_idx_test])
-        train_set.append([rdms_train, pattern_idx_train])
-        ceil_set.append([rdms_ceil, pattern_idx_test])
-    return train_set, test_set, ceil_set
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+generation of crossvalidation splits
+"""
+
+from copy import deepcopy
+import numpy as np
+from rsatoolbox.util.rdm_utils import add_pattern_index
+from rsatoolbox.util.inference_util import default_k_pattern, default_k_rdm
+
+
+def sets_leave_one_out_pattern(rdms, pattern_descriptor):
+    """ generates training and test set combinations by leaving one level
+    of pattern_descriptor out as a test set.
+    This is only sensible if pattern_descriptor already defines larger groups!
+
+    the ceil_train_set contains the rdms for the test-patterns from the
+    training-rdms. This is required for computing the noise-ceiling
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        pattern_descriptor(String): descriptor to select groups
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+        ceil_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    pattern_descriptor, pattern_select = \
+        add_pattern_index(rdms, pattern_descriptor)
+    train_set = []
+    test_set = []
+    ceil_set = []
+    for i_pattern in pattern_select:
+        pattern_idx_train = np.setdiff1d(pattern_select, i_pattern)
+        rdms_train = rdms.subset_pattern(pattern_descriptor,
+                                         pattern_idx_train)
+        pattern_idx_test = [i_pattern]
+        rdms_test = rdms.subset_pattern(pattern_descriptor,
+                                        pattern_idx_test)
+        rdms_ceil = rdms.subset_pattern(pattern_descriptor,
+                                        pattern_idx_test)
+        train_set.append((rdms_train, pattern_idx_train))
+        test_set.append((rdms_test, pattern_idx_test))
+        ceil_set.append((rdms_ceil, pattern_idx_test))
+    return train_set, test_set, ceil_set
+
+
+def sets_leave_one_out_rdm(rdms, rdm_descriptor='index'):
+    """ generates training and test set combinations by leaving one level
+    of rdm_descriptor out as a test set.\
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        rdm_descriptor(String): descriptor to select groups
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+        ceil_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
+    rdm_select = np.unique(rdm_select)
+    if len(rdm_select) > 1:
+        train_set = []
+        test_set = []
+        for i_pattern in rdm_select:
+            rdm_idx_train = np.setdiff1d(rdm_select, i_pattern)
+            rdms_train = rdms.subset(rdm_descriptor,
+                                     rdm_idx_train)
+            rdm_idx_test = [i_pattern]
+            rdms_test = rdms.subset(rdm_descriptor,
+                                    rdm_idx_test)
+            train_set.append((rdms_train, np.arange(rdms.n_cond)))
+            test_set.append((rdms_test, np.arange(rdms.n_cond)))
+        ceil_set = train_set
+    else:
+        Warning('leave one out called with only one group')
+        train_set = [(rdms, np.arange(rdms.n_cond))]
+        test_set = [(rdms, np.arange(rdms.n_cond))]
+        ceil_set = [(rdms, np.arange(rdms.n_cond))]
+    return train_set, test_set, ceil_set
+
+
+def sets_k_fold(rdms, k_rdm=None, k_pattern=None, random=True,
+                pattern_descriptor='index', rdm_descriptor='index'):
+    """ generates training and test set combinations by splitting into k
+    similar sized groups. This version splits both over rdms and over patterns
+    resulting in k_rdm * k_pattern (training, test) pairs.
+
+    If a k is set to 1 the corresponding dimension is not crossvalidated.
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        pattern_descriptor(String): descriptor to select pattern groups
+        rdm_descriptor(String): descriptor to select rdm groups
+        k_rdm(int): number of rdm groups
+        k_pattern(int): number of pattern groups
+        random(bool): whether the assignment shall be randomized
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+        ceil_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
+    rdm_select = np.unique(rdm_select)
+    if k_rdm is None:
+        k_rdm = default_k_rdm(len(rdm_select))
+    pattern_descriptor, pattern_select = \
+        add_pattern_index(rdms, pattern_descriptor)
+    if k_pattern is None:
+        k_pattern = default_k_pattern(len(pattern_select))
+    assert k_rdm <= len(rdm_select), \
+        'Can make at most as many groups as rdms'
+    if random:
+        np.random.shuffle(rdm_select)
+    group_size_rdm = np.floor(len(rdm_select) / k_rdm)
+    additional_rdms = len(rdm_select) % k_rdm
+    train_set = []
+    test_set = []
+    ceil_set = []
+    for i_group in range(k_rdm):
+        test_idx = np.arange(i_group * group_size_rdm,
+                             (i_group + 1) * group_size_rdm)
+        if i_group < additional_rdms:
+            test_idx = np.concatenate((test_idx, [len(rdm_select)-(i_group+1)]))
+        if k_rdm <= 1:
+            train_idx = test_idx
+        else:
+            train_idx = np.setdiff1d(np.arange(len(rdm_select)),
+                                     test_idx)
+        rdm_idx_test = [rdm_select[int(idx)] for idx in test_idx]
+        rdm_idx_train = [rdm_select[int(idx)] for idx in train_idx]
+        rdms_test = rdms.subsample(rdm_descriptor,
+                                   rdm_idx_test)
+        rdms_train = rdms.subsample(rdm_descriptor,
+                                    rdm_idx_train)
+        train_new, test_new, _ = sets_k_fold_pattern(
+            rdms_train, k=k_pattern,
+            pattern_descriptor=pattern_descriptor, random=random)
+        ceil_new = deepcopy(test_new)
+        for i_pattern in range(k_pattern):
+            test_new[i_pattern][0] = rdms_test.subset_pattern(
+                by=pattern_descriptor,
+                value=test_new[i_pattern][1])
+        train_set += train_new
+        test_set += test_new
+        ceil_set += ceil_new
+    return train_set, test_set, ceil_set
+
+
+def sets_k_fold_rdm(rdms, k_rdm=None, random=True, rdm_descriptor='index'):
+    """ generates training and test set combinations by splitting into k
+    similar sized groups. This version splits both over rdms and over patterns
+    resulting in k_rdm * k_pattern (training, test) pairs.
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        rdm_descriptor(String): descriptor to select rdm groups
+        k_rdm(int): number of rdm groups
+        random(bool): whether the assignment shall be randomized
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
+    rdm_select = np.unique(rdm_select)
+    if k_rdm is None:
+        k_rdm = default_k_rdm(len(rdm_select))
+    assert k_rdm <= len(rdm_select), \
+        'Can make at most as many groups as rdms'
+    if random:
+        np.random.shuffle(rdm_select)
+    group_size_rdm = np.floor(len(rdm_select) / k_rdm)
+    additional_rdms = len(rdm_select) % k_rdm
+    train_set = []
+    test_set = []
+    for i_group in range(k_rdm):
+        test_idx = np.arange(i_group * group_size_rdm,
+                             (i_group + 1) * group_size_rdm)
+        if i_group < additional_rdms:
+            test_idx = np.concatenate((test_idx, [len(rdm_select)-(i_group+1)]))
+        train_idx = np.setdiff1d(np.arange(len(rdm_select)),
+                                 test_idx)
+        rdm_idx_test = [rdm_select[int(idx)] for idx in test_idx]
+        rdm_idx_train = [rdm_select[int(idx)] for idx in train_idx]
+        rdms_test = rdms.subsample(rdm_descriptor,
+                                   rdm_idx_test)
+        rdms_train = rdms.subsample(rdm_descriptor,
+                                    rdm_idx_train)
+        train_set.append([rdms_train, np.arange(rdms_train.n_cond)])
+        test_set.append([rdms_test, np.arange(rdms_train.n_cond)])
+    ceil_set = train_set
+    return train_set, test_set, ceil_set
+
+
+def sets_k_fold_pattern(rdms, pattern_descriptor='index',
+                        k=None, random=False):
+    """ generates training and test set combinations by splitting into k
+    similar sized groups. This version splits in the given order or
+    randomizes the order. For k=1 training and test_set are whole dataset,
+    i.e. no crossvalidation is performed.
+
+    For only crossvalidating over patterns there is no independent training
+    set for calculating a noise ceiling for the patterns.
+    To express this we set ceil_set to None, which makes the crossvalidation
+    function calculate a leave one rdm out noise ceiling for the right
+    patterns instead.
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        pattern_descriptor(String): descriptor to select groups
+        k(int): number of groups
+        random(bool): whether the assignment shall be randomized
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+        ceil_set = None
+
+    """
+    pattern_descriptor, pattern_select = \
+        add_pattern_index(rdms, pattern_descriptor)
+    if k is None:
+        k = default_k_pattern(len(pattern_select))
+    assert k <= len(pattern_select), \
+        'Can make at most as many groups as conditions'
+    if random:
+        np.random.shuffle(pattern_select)
+    group_size = np.floor(len(pattern_select) / k)
+    additional_patterns = len(pattern_select) % k
+    train_set = []
+    test_set = []
+    for i_group in range(k):
+        test_idx = np.arange(i_group * group_size,
+                             (i_group + 1) * group_size)
+        if i_group < additional_patterns:
+            test_idx = np.concatenate((test_idx, [len(pattern_select)-(i_group+1)]))
+        if k <= 1:
+            train_idx = test_idx
+        else:
+            train_idx = np.setdiff1d(np.arange(len(pattern_select)),
+                                     test_idx)
+        pattern_idx_test = [pattern_select[int(idx)] for idx in test_idx]
+        pattern_idx_train = [pattern_select[int(idx)] for idx in train_idx]
+        rdms_test = rdms.subset_pattern(pattern_descriptor,
+                                        pattern_idx_test)
+        rdms_train = rdms.subset_pattern(pattern_descriptor,
+                                         pattern_idx_train)
+        test_set.append([rdms_test, pattern_idx_test])
+        train_set.append([rdms_train, pattern_idx_train])
+    ceil_set = None
+    return train_set, test_set, ceil_set
+
+
+def sets_of_k_rdm(rdms, rdm_descriptor='index', k=5, random=False):
+    """ generates training and test set combinations by splitting into
+    groups of k. This version splits in the given order or
+    randomizes the order. If the number of patterns is not divisible by k
+    patterns are added to the first groups such that those have k+1 patterns
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        pattern_descriptor(String): descriptor to select groups
+        k(int): number of groups
+        random(bool): whether the assignment shall be randomized
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+        ceil_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
+    rdm_select = np.unique(rdm_select)
+    assert k <= len(rdm_select) / 2, \
+        'to form groups we can use at most half the patterns per group'
+    n_groups = int(len(rdm_select) / k)
+    return sets_k_fold_rdm(rdms, rdm_descriptor=rdm_descriptor,
+                           k=n_groups, random=random)
+
+
+def sets_of_k_pattern(rdms, pattern_descriptor=None, k=5, random=False):
+    """ generates training and test set combinations by splitting into
+    groups of k. This version splits in the given order or
+    randomizes the order. If the number of patterns is not divisible by k
+    patterns are added to the first groups such that those have k+1 patterns
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to use
+        pattern_descriptor(String): descriptor to select groups
+        k(int): number of groups
+        random(bool): whether the assignment shall be randomized
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    pattern_descriptor, pattern_select = \
+        add_pattern_index(rdms, pattern_descriptor)
+    assert k <= len(pattern_select) / 2, \
+        'to form groups we can use at most half the patterns per group'
+    n_groups = int(len(pattern_select) / k)
+    return sets_k_fold_pattern(rdms, pattern_descriptor=pattern_descriptor,
+                               k=n_groups, random=random)
+
+
+def sets_random(rdms, n_rdm=None, n_pattern=None, n_cv=2,
+                pattern_descriptor='index', rdm_descriptor='index'):
+    """ generates training and test set combinations by selecting random
+    test sets of n_rdm RDMs and n_pattern patterns and using the rest of
+    the data as the training set.
+
+    If a n is set to 0 the corresponding dimension is not crossvalidated.
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): rdms to split
+        pattern_descriptor(String): descriptor to select pattern groups
+        rdm_descriptor(String): descriptor to select rdm groups
+        n_rdm(int): number of rdms per test set
+        n_pattern(int): number of patterns per test set
+
+    Returns:
+        train_set(list): list of tuples (rdms, pattern_idx)
+        test_set(list): list of tuples (rdms, pattern_idx)
+        ceil_set(list): list of tuples (rdms, pattern_idx)
+
+    """
+    rdm_select = rdms.rdm_descriptors[rdm_descriptor]
+    rdm_select = np.unique(rdm_select)
+    if n_rdm is None:
+        k_rdm = default_k_rdm(len(rdm_select))
+        n_rdm = int(np.floor(len(rdm_select) / k_rdm))
+    pattern_descriptor, pattern_select = \
+        add_pattern_index(rdms, pattern_descriptor)
+    if n_pattern is None:
+        k_pattern = default_k_pattern(len(pattern_select))
+        n_pattern = int(np.floor(len(pattern_select) / k_pattern))
+    train_set = []
+    test_set = []
+    ceil_set = []
+    for _i_group in range(n_cv):
+        # shuffle
+        np.random.shuffle(rdm_select)
+        np.random.shuffle(pattern_select)
+        # choose indices based on n_rdm
+        if n_rdm == 0:
+            train_idx = np.arange(len(rdm_select))
+            test_idx = np.arange(len(rdm_select))
+        else:
+            test_idx = np.arange(n_rdm)
+            train_idx = np.arange(n_rdm, len(rdm_select))
+        # take subset of rdms
+        rdm_idx_test = [rdm_select[int(idx)] for idx in test_idx]
+        rdm_idx_train = [rdm_select[int(idx)] for idx in train_idx]
+        rdms_test = rdms.subsample(rdm_descriptor,
+                                   rdm_idx_test)
+        rdms_train = rdms.subsample(rdm_descriptor,
+                                    rdm_idx_train)
+        # choose indices based on n_pattern
+        if n_pattern == 0:
+            train_idx = np.arange(len(pattern_select))
+            test_idx = np.arange(len(pattern_select))
+        else:
+            test_idx = np.arange(n_pattern)
+            train_idx = np.arange(n_pattern, len(pattern_select))
+        pattern_idx_test = [pattern_select[int(idx)] for idx in test_idx]
+        pattern_idx_train = [pattern_select[int(idx)] for idx in train_idx]
+        rdms_test = rdms_test.subset_pattern(pattern_descriptor,
+                                             pattern_idx_test)
+        rdms_ceil = rdms_train.subset_pattern(pattern_descriptor,
+                                              pattern_idx_test)
+        rdms_train = rdms_train.subset_pattern(pattern_descriptor,
+                                               pattern_idx_train)
+        test_set.append([rdms_test, pattern_idx_test])
+        train_set.append([rdms_train, pattern_idx_train])
+        ceil_set.append([rdms_ceil, pattern_idx_test])
+    return train_set, test_set, ceil_set
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/evaluate.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/evaluate.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,781 +1,791 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-evaluate model performance
-"""
-
-import numpy as np
-import tqdm
-from rsatoolbox.rdm import compare
-from rsatoolbox.inference import bootstrap_sample
-from rsatoolbox.inference import bootstrap_sample_rdm
-from rsatoolbox.inference import bootstrap_sample_pattern
-from rsatoolbox.model import Model
-from rsatoolbox.util.inference_util import input_check_model
-from rsatoolbox.util.inference_util import default_k_pattern, default_k_rdm
-from .result import Result
-from .crossvalsets import sets_k_fold, sets_random
-from .noise_ceiling import boot_noise_ceiling
-from .noise_ceiling import cv_noise_ceiling
-
-
-def eval_dual_bootstrap(
-        models, data, method='cosine', fitter=None,
-        k_pattern=1, k_rdm=1, N=1000, n_cv=2,
-        pattern_descriptor='index', rdm_descriptor='index',
-        use_correction=True):
-    """dual bootstrap evaluation of models
-    i.e. models are evaluated in a bootstrap over rdms, one over patterns
-    and a bootstrap over both using the same bootstrap samples for each.
-    The variance estimates from these bootstraps are then combined into
-    a better overall estimate for the variance.
-
-    This method allows the incorporation of crossvalidation inside the
-    bootstrap to handle fitted models.
-    To activate this set k_rdm and k_pattern as described below.
-
-    Crossvalidation creates variance in the results for a single bootstrap
-    sample, because different assginments to the training and test group
-    lead to different results. To correct for this, we apply a formula
-    which estimates the variance we expect if we evaluated all possible
-    crossvalidation assignments from n_cv different assignments per bootstrap
-    sample.
-    In our statistical evaluations we saw that many bootstrap samples and
-    few different crossvalidation assignments are optimal to minimize the
-    variance of the variance estimate. Thus, this function by default
-    applies this correction formula and sets n_cv=2, i.e. performs only two
-    different assignments per fold.
-    This function nonetheless performs full crossvalidation schemes, i.e.
-    in every bootstrap sample all crossvalidation folds are evaluated such
-    that each RDM and each condition is in the test set n_cv times.
-
-    The k_[] parameters control the cross-validation per sample. They give
-    the number of crossvalidation folds to be created along this dimension.
-    If a k is set to 1 no crossvalidation is performed over the
-    corresponding dimension.
-    by default ks are set by rsatoolbox.util.inference_util.default_k_pattern
-    and rsatoolbox.util.inference_util.default_k_rdm based on the number of
-    rdms and patterns provided. the ks are then in the range 2-5.
-
-    Using the []_descriptor inputs you may make the crossvalidation and
-    bootstrap aware of groups of rdms or conditions to be handled en block.
-    Conditions with the same entry will be sampled in or out of the bootstrap
-    together and will be assigned to cross-calidation folds together.
-
-    models should be a list of models. data the RDMs object to evaluate against
-    method the method for comparing the predictions and the data. fitter may
-    provide a non-default funcion or list of functions to fit the models.
-
-    Args:
-        models(rsatoolbox.model.Model): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): RDM data to use
-        method(string): comparison method to use
-        fitter(function): fitting method for models
-        k_pattern(int): #folds over patterns
-        k_rdm(int): #folds over rdms
-        N(int): number of bootstrap samples (default: 1000)
-        n_cv(int) : number of crossvalidation runs per sample (default: 1)
-        pattern_descriptor(string): descriptor to group patterns
-        rdm_descriptor(string): descriptor to group rdms
-        random(bool): randomize group assignments (default: True)
-        boot_type(String): which dimension to bootstrap over (default: 'both')
-            alternatives: 'rdm', 'pattern'
-        use_correction(bool): switch for the correction for the
-            variance caused by crossvalidation (default: True)
-
-    Returns:
-        numpy.ndarray: matrix of evaluations (N x k)
-
-    """
-    if k_pattern is None:
-        n_pattern = len(np.unique(data.pattern_descriptors[
-            pattern_descriptor]))
-        k_pattern = default_k_pattern((1 - 1 / np.exp(1)) * n_pattern)
-    if k_rdm is None:
-        n_rdm = len(np.unique(data.rdm_descriptors[
-            rdm_descriptor]))
-        k_rdm = default_k_rdm((1 - 1 / np.exp(1)) * n_rdm)
-    if k_rdm == 1 and k_pattern == 1:
-        n_cv = 1
-        use_correction = False
-    if isinstance(models, Model):
-        models = [models]
-    evaluations = np.zeros((N, len(models), k_pattern * k_rdm, n_cv, 3))
-    noise_ceil = np.zeros((2, N, n_cv, 3))
-    for i_sample in tqdm.trange(N):
-        sample, rdm_idx, pattern_idx = bootstrap_sample(
-            data,
-            rdm_descriptor=rdm_descriptor,
-            pattern_descriptor=pattern_descriptor)
-        sample_rdm = data.subsample(rdm_descriptor, rdm_idx)
-        sample_pattern = data.subsample_pattern(
-            pattern_descriptor, pattern_idx)
-        if len(np.unique(rdm_idx)) >= k_rdm \
-           and len(np.unique(pattern_idx)) >= 3 * k_pattern:
-            for i_rep in range(n_cv):
-                evals, cv_nc = _internal_cv(
-                    models, sample,
-                    pattern_descriptor, rdm_descriptor, pattern_idx,
-                    k_pattern, k_rdm,
-                    method, fitter)
-                noise_ceil[:, i_sample, i_rep, 0] = cv_nc
-                evaluations[i_sample, :, :, i_rep, 0] = evals[0]
-                evals, cv_nc = _internal_cv(
-                    models, sample_rdm,
-                    pattern_descriptor, rdm_descriptor,
-                    np.unique(data.pattern_descriptors[pattern_descriptor]),
-                    k_pattern, k_rdm,
-                    method, fitter)
-                noise_ceil[:, i_sample, i_rep, 1] = cv_nc
-                evaluations[i_sample, :, :, i_rep, 1] = evals[0]
-                evals, cv_nc = _internal_cv(
-                    models, sample_pattern,
-                    pattern_descriptor, rdm_descriptor, pattern_idx,
-                    k_pattern, k_rdm,
-                    method, fitter)
-                noise_ceil[:, i_sample, i_rep, 2] = cv_nc
-                evaluations[i_sample, :, :, i_rep, 2] = evals[0]
-        else:  # sample does not allow desired crossvalidation
-            evaluations[i_sample, :, :, :, :] = np.nan
-            noise_ceil[:, i_sample, :, :] = np.nan
-    cv_method = 'dual_bootstrap'
-    dof = min(data.n_rdm, data.n_cond) - 1
-    eval_ok = ~np.isnan(evaluations[:, 0, 0, 0, 0])
-    if use_correction and n_cv > 1:
-        # we essentially project from the two points for 1 repetition and
-        # for n_cv repetitions to infinitely many cv repetitions
-        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -2), -2)
-        evals_1 = np.mean(evaluations[eval_ok], -3)
-        noise_ceil_nonan = np.mean(
-            noise_ceil[:, eval_ok], -2).transpose([1, 0, 2])
-        noise_ceil_1 = noise_ceil[:, eval_ok].transpose([1, 0, 2, 3])
-        matrix = np.concatenate([evals_nonan, noise_ceil_nonan], 1)
-        matrix -= np.mean(matrix, 0, keepdims=True)
-        var_mean = np.einsum('ijk,ilk->kjl', matrix, matrix) \
-            / (matrix.shape[0] - 1)
-        matrix_1 = np.concatenate([evals_1, noise_ceil_1], 1)
-        matrix_1 -= np.mean(matrix_1, 0, keepdims=True)
-        var_1 = np.einsum('ijmk,ilmk->kjl', matrix_1, matrix_1) \
-            / (matrix_1.shape[0] - 1) / matrix_1.shape[2]
-        # this is the main formula for the correction:
-        variances = (n_cv * var_mean - var_1) / (n_cv - 1)
-    else:
-        if use_correction:
-            raise Warning('correction requested, but only one cv run'
-                          + ' per sample requested. This is invalid!'
-                          + ' We do not use the correction for now.')
-        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -2), -2)
-        noise_ceil_nonan = np.mean(
-            noise_ceil[:, eval_ok], -2).transpose([1, 0, 2])
-        matrix = np.concatenate([evals_nonan, noise_ceil_nonan], 1)
-        matrix -= np.mean(matrix, 0, keepdims=True)
-        variances = np.einsum('ijk,ilk->kjl', matrix, matrix) \
-            / (matrix.shape[0] - 1)
-    result = Result(models, evaluations, method=method,
-                    cv_method=cv_method, noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def eval_fixed(models, data, theta=None, method='cosine'):
-    """evaluates models on data, without any bootstrapping or
-    cross-validation
-
-    Args:
-        models(list of rsatoolbox.model.Model or list): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): data to evaluate on
-        theta(numpy.ndarray): parameter vector for the models
-        method(string): comparison method to use
-
-    Returns:
-        float: evaluation
-
-    """
-    models, evaluations, theta, _ = input_check_model(models, theta, None, 1)
-    evaluations = np.repeat(np.expand_dims(evaluations, -1),
-                            data.n_rdm, -1)
-    for k, model in enumerate(models):
-        rdm_pred = model.predict_rdm(theta=theta[k])
-        evaluations[k] = compare(rdm_pred, data, method)
-    evaluations = evaluations.reshape((1, len(models), data.n_rdm))
-    noise_ceil = boot_noise_ceiling(
-        data, method=method, rdm_descriptor='index')
-    if data.n_rdm > 1:
-        variances = np.cov(evaluations[0], ddof=1) \
-            / evaluations.shape[-1]
-        dof = evaluations.shape[-1] - 1
-    else:
-        variances = None
-        dof = 0
-    result = Result(models, evaluations, method=method,
-                    cv_method='fixed', noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def eval_bootstrap(models, data, theta=None, method='cosine', N=1000,
-                   pattern_descriptor='index', rdm_descriptor='index',
-                   boot_noise_ceil=True):
-    """evaluates models on data
-    performs bootstrapping to get a sampling distribution
-
-    Args:
-        models(rsatoolbox.model.Model or list): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): data to evaluate on
-        theta(numpy.ndarray): parameter vector for the models
-        method(string): comparison method to use
-        N(int): number of samples
-        pattern_descriptor(string): descriptor to group patterns for bootstrap
-        rdm_descriptor(string): descriptor to group rdms for bootstrap
-
-    Returns:
-        numpy.ndarray: vector of evaluations
-
-    """
-    models, evaluations, theta, _ = \
-        input_check_model(models, theta, None, N)
-    noise_min = []
-    noise_max = []
-    for i in tqdm.trange(N):
-        sample, _, pattern_idx = \
-            bootstrap_sample(data, rdm_descriptor=rdm_descriptor,
-                             pattern_descriptor=pattern_descriptor)
-        if len(np.unique(pattern_idx)) >= 3:
-            for j, mod in enumerate(models):
-                rdm_pred = mod.predict_rdm(theta=theta[j])
-                rdm_pred = rdm_pred.subsample_pattern(pattern_descriptor,
-                                                      pattern_idx)
-                evaluations[i, j] = np.mean(compare(rdm_pred, sample,
-                                                    method))
-            if boot_noise_ceil:
-                noise_min_sample, noise_max_sample = boot_noise_ceiling(
-                    sample, method=method, rdm_descriptor=rdm_descriptor)
-                noise_min.append(noise_min_sample)
-                noise_max.append(noise_max_sample)
-        else:
-            evaluations[i, :] = np.nan
-            noise_min.append(np.nan)
-            noise_max.append(np.nan)
-    if boot_noise_ceil:
-        eval_ok = np.isfinite(evaluations[:, 0])
-        noise_ceil = np.array([noise_min, noise_max])
-        variances = np.cov(np.concatenate([evaluations[eval_ok, :].T,
-                                           noise_ceil[:, eval_ok]]))
-    else:
-        eval_ok = np.isfinite(evaluations[:, 0])
-        noise_ceil = np.array(boot_noise_ceiling(
-            data, method=method, rdm_descriptor=rdm_descriptor))
-        variances = np.cov(evaluations[eval_ok, :].T)
-    dof = min(data.n_rdm, data.n_cond) - 1
-    result = Result(models, evaluations, method=method,
-                    cv_method='bootstrap', noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def eval_bootstrap_pattern(models, data, theta=None, method='cosine', N=1000,
-                           pattern_descriptor='index', rdm_descriptor='index',
-                           boot_noise_ceil=True):
-    """evaluates a models on data
-    performs bootstrapping over patterns to get a sampling distribution
-
-    Args:
-        models(rsatoolbox.model.Model or list): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): data to evaluate on
-        theta(numpy.ndarray): parameter vector for the models
-        method(string): comparison method to use
-        N(int): number of samples
-        pattern_descriptor(string): descriptor to group patterns for bootstrap
-        rdm_descriptor(string): descriptor to group patterns for noise
-            ceiling calculation
-
-    Returns:
-        numpy.ndarray: vector of evaluations
-
-    """
-    models, evaluations, theta, _ = \
-        input_check_model(models, theta, None, N)
-    noise_min = []
-    noise_max = []
-    for i in tqdm.trange(N):
-        sample, pattern_idx = \
-            bootstrap_sample_pattern(data, pattern_descriptor)
-        if len(np.unique(pattern_idx)) >= 3:
-            for j, mod in enumerate(models):
-                rdm_pred = mod.predict_rdm(theta=theta[j])
-                rdm_pred = rdm_pred.subsample_pattern(pattern_descriptor,
-                                                      pattern_idx)
-                evaluations[i, j] = np.mean(compare(rdm_pred, sample,
-                                                    method))
-            if boot_noise_ceil:
-                noise_min_sample, noise_max_sample = boot_noise_ceiling(
-                    sample, method=method, rdm_descriptor=rdm_descriptor)
-                noise_min.append(noise_min_sample)
-                noise_max.append(noise_max_sample)
-        else:
-            evaluations[i, :] = np.nan
-            noise_min.append(np.nan)
-            noise_max.append(np.nan)
-    if boot_noise_ceil:
-        eval_ok = np.isfinite(evaluations[:, 0])
-        noise_ceil = np.array([noise_min, noise_max])
-        variances = np.cov(np.concatenate([evaluations[eval_ok, :].T,
-                                           noise_ceil[:, eval_ok]]))
-    else:
-        eval_ok = np.isfinite(evaluations[:, 0])
-        noise_ceil = np.array(boot_noise_ceiling(
-            data, method=method, rdm_descriptor=rdm_descriptor))
-        variances = np.cov(evaluations[eval_ok, :].T)
-    dof = data.n_cond - 1
-    result = Result(models, evaluations, method=method,
-                    cv_method='bootstrap_pattern', noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def eval_bootstrap_rdm(models, data, theta=None, method='cosine', N=1000,
-                       rdm_descriptor='index', boot_noise_ceil=True):
-    """evaluates models on data
-    performs bootstrapping to get a sampling distribution
-
-    Args:
-        models(rsatoolbox.model.Model or list of these): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): data to evaluate on
-        theta(numpy.ndarray): parameter vector for the models
-        method(string): comparison method to use
-        N(int): number of samples
-        rdm_descriptor(string): rdm_descriptor to group rdms for bootstrap
-
-    Returns:
-        numpy.ndarray: vector of evaluations
-
-    """
-    models, evaluations, theta, _ = input_check_model(models, theta, None, N)
-    noise_min = []
-    noise_max = []
-    for i in tqdm.trange(N):
-        sample, _ = bootstrap_sample_rdm(data, rdm_descriptor)
-        for j, mod in enumerate(models):
-            rdm_pred = mod.predict_rdm(theta=theta[j])
-            evaluations[i, j] = np.mean(compare(rdm_pred, sample,
-                                                method))
-        if boot_noise_ceil:
-            noise_min_sample, noise_max_sample = boot_noise_ceiling(
-                sample, method=method, rdm_descriptor=rdm_descriptor)
-            noise_min.append(noise_min_sample)
-            noise_max.append(noise_max_sample)
-    if boot_noise_ceil:
-        eval_ok = np.isfinite(evaluations[:, 0])
-        noise_ceil = np.array([noise_min, noise_max])
-        variances = np.cov(np.concatenate([evaluations[eval_ok, :].T,
-                                           noise_ceil[:, eval_ok]]))
-    else:
-        eval_ok = np.isfinite(evaluations[:, 0])
-        noise_ceil = np.array(boot_noise_ceiling(
-            data, method=method, rdm_descriptor=rdm_descriptor))
-        variances = np.cov(evaluations[eval_ok, :].T)
-    dof = data.n_rdm - 1
-    variances = np.cov(evaluations.T)
-    result = Result(models, evaluations, method=method,
-                    cv_method='bootstrap_rdm', noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def crossval(models, rdms, train_set, test_set, ceil_set=None, method='cosine',
-             fitter=None, pattern_descriptor='index', calc_noise_ceil=True):
-    """evaluates models on cross-validation sets
-
-    Args:
-        models(rsatoolbox.model.Model): models to be evaluated
-        rdms(rsatoolbox.rdm.RDMs): full dataset
-        train_set(list): a list of the training RDMs with 2-tuple entries:
-            (RDMs, pattern_idx)
-        test_set(list): a list of the test RDMs with 2-tuple entries:
-            (RDMs, pattern_idx)
-        method(string): comparison method to use
-        pattern_descriptor(string): descriptor to group patterns
-
-    Returns:
-        numpy.ndarray: vector of evaluations
-
-    """
-    assert len(train_set) == len(test_set), \
-        'train_set and test_set must have the same length'
-    if ceil_set is not None:
-        assert len(ceil_set) == len(test_set), \
-            'ceil_set and test_set must have the same length'
-    if isinstance(models, Model):
-        models = [models]
-    evaluations = []
-    noise_ceil = []
-    for i, train in enumerate(train_set):
-        test = test_set[i]
-        if (train[0].n_rdm == 0 or test[0].n_rdm == 0 or
-                train[0].n_cond <= 2 or test[0].n_cond <= 2):
-            evals = np.empty(len(models)) * np.nan
-        else:
-            models, evals, _, fitter = \
-                input_check_model(models, None, fitter)
-            for j, model in enumerate(models):
-                theta = fitter[j](model, train[0], method=method,
-                                  pattern_idx=train[1],
-                                  pattern_descriptor=pattern_descriptor)
-                pred = model.predict_rdm(theta)
-                pred = pred.subsample_pattern(by=pattern_descriptor,
-                                              value=test[1])
-                evals[j] = np.mean(compare(pred, test[0], method))
-            if ceil_set is None and calc_noise_ceil:
-                noise_ceil.append(boot_noise_ceiling(
-                    rdms.subsample_pattern(by=pattern_descriptor,
-                                           value=test[1]),
-                    method=method))
-        evaluations.append(evals)
-    evaluations = np.array(evaluations).T  # .T to switch models/set order
-    evaluations = evaluations.reshape((1, len(models), len(train_set)))
-    if ceil_set is not None and calc_noise_ceil:
-        noise_ceil = cv_noise_ceiling(rdms, ceil_set, test_set, method=method,
-                                      pattern_descriptor=pattern_descriptor)
-    elif calc_noise_ceil:
-        noise_ceil = np.array(noise_ceil).T
-    else:
-        noise_ceil = np.array([np.nan, np.nan])
-
-    result = Result(models, evaluations, method=method,
-                    cv_method='crossvalidation', noise_ceiling=noise_ceil)
-    return result
-
-
-def bootstrap_crossval(models, data, method='cosine', fitter=None,
-                       k_pattern=None, k_rdm=None, N=1000, n_cv=2,
-                       pattern_descriptor='index', rdm_descriptor='index',
-                       boot_type='both', use_correction=True):
-    """evaluates a set of models by k-fold crossvalidation within a bootstrap
-
-    Crossvalidation creates variance in the results for a single bootstrap
-    sample, because different assginments to the training and test group
-    lead to different results. To correct for this, we apply a formula
-    which estimates the variance we expect if we evaluated all possible
-    crossvalidation assignments from n_cv different assignments per bootstrap
-    sample.
-    In our statistical evaluations we saw that many bootstrap samples and
-    few different crossvalidation assignments are optimal to minimize the
-    variance of the variance estimate. Thus, this function by default
-    applies this correction formula and sets n_cv=2, i.e. performs only two
-    different assignments per fold.
-    This function nonetheless performs full crossvalidation schemes, i.e.
-    in every bootstrap sample all crossvalidation folds are evaluated such
-    that each RDM and each condition is in the test set n_cv times. For the
-    even more optimized version which computes only two randomly chosen test
-    sets see bootstrap_cv_random.
-
-    The k_[] parameters control the cross-validation per sample. They give
-    the number of crossvalidation folds to be created along this dimension.
-    If a k is set to 1 no crossvalidation is performed over the
-    corresponding dimension.
-    by default ks are set by rsatoolbox.util.inference_util.default_k_pattern
-    and rsatoolbox.util.inference_util.default_k_rdm based on the number of
-    rdms and patterns provided. the ks are then in the range 2-5.
-
-    Using the []_descriptor inputs you may make the crossvalidation and
-    bootstrap aware of groups of rdms or conditions to be handled en block.
-    Conditions with the same entry will be sampled in or out of the bootstrap
-    together and will be assigned to cross-calidation folds together.
-
-    Using the boot_type argument you may choose the dimension to bootstrap.
-    By default both conditions and RDMs are resampled. You may alternatively
-    choose to resample only one of them by passing 'rdm' or 'pattern'.
-
-    models should be a list of models. data the RDMs object to evaluate against
-    method the method for comparing the predictions and the data. fitter may
-    provide a non-default funcion or list of functions to fit the models.
-
-    Args:
-        models(rsatoolbox.model.Model): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): RDM data to use
-        method(string): comparison method to use
-        fitter(function): fitting method for models
-        k_pattern(int): #folds over patterns
-        k_rdm(int): #folds over rdms
-        N(int): number of bootstrap samples (default: 1000)
-        n_cv(int) : number of crossvalidation runs per sample (default: 1)
-        pattern_descriptor(string): descriptor to group patterns
-        rdm_descriptor(string): descriptor to group rdms
-        boot_type(String): which dimension to bootstrap over (default: 'both')
-            alternatives: 'rdm', 'pattern'
-        use_correction(bool): switch for the correction for the
-            variance caused by crossvalidation (default: True)
-
-    Returns:
-        numpy.ndarray: matrix of evaluations (N x k)
-
-    """
-    if k_pattern is None:
-        n_pattern = len(np.unique(data.pattern_descriptors[
-            pattern_descriptor]))
-        k_pattern = default_k_pattern((1 - 1 / np.exp(1)) * n_pattern)
-    if k_rdm is None:
-        n_rdm = len(np.unique(data.rdm_descriptors[
-            rdm_descriptor]))
-        k_rdm = default_k_rdm((1 - 1 / np.exp(1)) * n_rdm)
-    if isinstance(models, Model):
-        models = [models]
-    evaluations = np.empty((N, len(models), k_pattern * k_rdm, n_cv))
-    noise_ceil = np.empty((2, N, n_cv))
-    for i_sample in tqdm.trange(N):
-        if boot_type == 'both':
-            sample, rdm_idx, pattern_idx = bootstrap_sample(
-                data,
-                rdm_descriptor=rdm_descriptor,
-                pattern_descriptor=pattern_descriptor)
-        elif boot_type == 'pattern':
-            sample, pattern_idx = bootstrap_sample_pattern(
-                data,
-                pattern_descriptor=pattern_descriptor)
-            rdm_idx = np.unique(data.rdm_descriptors[rdm_descriptor])
-        elif boot_type == 'rdm':
-            sample, rdm_idx = bootstrap_sample_rdm(
-                data,
-                rdm_descriptor=rdm_descriptor)
-            pattern_idx = np.unique(
-                data.pattern_descriptors[pattern_descriptor])
-        else:
-            raise ValueError('boot_type not understood')
-        if len(np.unique(rdm_idx)) >= k_rdm \
-           and len(np.unique(pattern_idx)) >= 3 * k_pattern:
-            for i_rep in range(n_cv):
-                evals, cv_nc = _internal_cv(
-                    models, sample,
-                    pattern_descriptor, rdm_descriptor, pattern_idx,
-                    k_pattern, k_rdm,
-                    method, fitter)
-                noise_ceil[:, i_sample, i_rep] = cv_nc
-                evaluations[i_sample, :, :, i_rep] = evals[0]
-        else:  # sample does not allow desired crossvalidation
-            evaluations[i_sample, :, :] = np.nan
-            noise_ceil[:, i_sample] = np.nan
-    if boot_type == 'both':
-        cv_method = 'bootstrap_crossval'
-        dof = min(data.n_rdm, data.n_cond) - 1
-    elif boot_type == 'pattern':
-        cv_method = 'bootstrap_crossval_pattern'
-        dof = data.n_cond - 1
-    elif boot_type == 'rdm':
-        cv_method = 'bootstrap_crossval_rdm'
-        dof = data.n_rdm - 1
-    eval_ok = ~np.isnan(evaluations[:, 0, 0, 0])
-    if use_correction and n_cv > 1:
-        # we essentially project from the two points for 1 repetition and
-        # for n_cv repetitions to infinitely many cv repetitions
-        evals_mean = np.mean(np.mean(evaluations[eval_ok], -1), -1)
-        evals_1 = np.mean(evaluations[eval_ok], -2)
-        noise_ceil_mean = np.mean(noise_ceil[:, eval_ok], -1)
-        noise_ceil_1 = noise_ceil[:, eval_ok]
-        var_mean = np.cov(
-            np.concatenate([evals_mean.T, noise_ceil_mean]))
-        var_1 = []
-        for i in range(n_cv):
-            var_1.append(np.cov(np.concatenate([
-                evals_1[:, :, i].T, noise_ceil_1[:, :, i]])))
-        var_1 = np.mean(np.array(var_1), axis=0)
-        # this is the main formula for the correction:
-        variances = (n_cv * var_mean - var_1) / (n_cv - 1)
-    else:
-        if use_correction:
-            raise Warning('correction requested, but only one cv run'
-                          + ' per sample requested. This is invalid!'
-                          + ' We do not use the correction for now.')
-        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -1), -1)
-        noise_ceil_nonan = np.mean(noise_ceil[:, eval_ok], -1)
-        variances = np.cov(np.concatenate([evals_nonan.T, noise_ceil_nonan]))
-    result = Result(models, evaluations, method=method,
-                    cv_method=cv_method, noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def eval_dual_bootstrap_random(
-        models, data, method='cosine', fitter=None,
-        n_pattern=None, n_rdm=None, N=1000, n_cv=2,
-        pattern_descriptor='index', rdm_descriptor='index',
-        boot_type='both', use_correction=True):
-    """evaluates a set of models by a evaluating a few random crossvalidation
-    folds per bootstrap.
-
-    If a k is set to 1 no crossvalidation is performed over the
-    corresponding dimension.
-
-    As especially crossvalidation over patterns/conditions creates
-    variance in the cv result for a single variance the default setting
-    of n_cv=1 inflates the estimated variance. Setting this value
-    higher will decrease this effect at the cost of more computation time.
-
-    by default ks are set by rsatoolbox.util.inference_util.default_k_pattern
-    and rsatoolbox.util.inference_util.default_k_rdm based on the number of
-    rdms and patterns provided. the ks are then in the range 2-5.
-
-    Args:
-        models(rsatoolbox.model.Model): models to be evaluated
-        data(rsatoolbox.rdm.RDMs): RDM data to use
-        method(string): comparison method to use
-        fitter(function): fitting method for models
-        k_pattern(int): #folds over patterns
-        k_rdm(int): #folds over rdms
-        N(int): number of bootstrap samples (default: 1000)
-        n_cv(int) : number of crossvalidation runs per sample (default: 1)
-        pattern_descriptor(string): descriptor to group patterns
-        rdm_descriptor(string): descriptor to group rdms
-        boot_type(String): which dimension to bootstrap over (default: 'both')
-            alternatives: 'rdm', 'pattern'
-        use_correction(bool): switch for the correction for the
-            variance caused by crossvalidation (default: True)
-
-    Returns:
-        numpy.ndarray: matrix of evaluations (N x k)
-
-    """
-    if n_pattern is None:
-        n_pattern_all = len(np.unique(data.pattern_descriptors[
-            pattern_descriptor]))
-        k_pattern = default_k_pattern((1 - 1 / np.exp(1)) * n_pattern_all)
-        n_pattern = int(np.floor(n_pattern_all / k_pattern))
-    if n_rdm is None:
-        n_rdm_all = len(np.unique(data.rdm_descriptors[
-            rdm_descriptor]))
-        k_rdm = default_k_rdm((1 - 1 / np.exp(1)) * n_rdm_all)
-        n_rdm = int(np.floor(n_rdm_all / k_rdm))
-    if isinstance(models, Model):
-        models = [models]
-    evaluations = np.zeros((N, len(models), n_cv))
-    noise_ceil = np.zeros((2, N, n_cv))
-    for i_sample in tqdm.trange(N):
-        if boot_type == 'both':
-            sample, rdm_idx, pattern_idx = bootstrap_sample(
-                data,
-                rdm_descriptor=rdm_descriptor,
-                pattern_descriptor=pattern_descriptor)
-        elif boot_type == 'pattern':
-            sample, pattern_idx = bootstrap_sample_pattern(
-                data,
-                pattern_descriptor=pattern_descriptor)
-            rdm_idx = np.unique(data.rdm_descriptors[rdm_descriptor])
-        elif boot_type == 'rdm':
-            sample, rdm_idx = bootstrap_sample_rdm(
-                data,
-                rdm_descriptor=rdm_descriptor)
-            pattern_idx = np.unique(
-                data.pattern_descriptors[pattern_descriptor])
-        else:
-            raise ValueError('boot_type not understood')
-        if len(np.unique(rdm_idx)) > n_rdm \
-           and len(np.unique(pattern_idx)) >= 3 + n_pattern:
-            train_set, test_set, ceil_set = sets_random(
-                sample,
-                pattern_descriptor=pattern_descriptor,
-                rdm_descriptor=rdm_descriptor,
-                n_pattern=n_pattern, n_rdm=n_rdm, n_cv=n_cv)
-            if n_rdm > 0 or n_pattern > 0:
-                nc = cv_noise_ceiling(
-                    sample, ceil_set, test_set,
-                    method=method,
-                    pattern_descriptor=pattern_descriptor)
-            else:
-                nc = boot_noise_ceiling(
-                    sample,
-                    method=method,
-                    rdm_descriptor=rdm_descriptor)
-            noise_ceil[:, i_sample] = nc
-            for test_s in test_set:
-                test_s[1] = _concat_sampling(pattern_idx, test_s[1])
-            for train_s in train_set:
-                train_s[1] = _concat_sampling(pattern_idx, train_s[1])
-            cv_result = crossval(
-                models, sample,
-                train_set, test_set,
-                method=method, fitter=fitter,
-                pattern_descriptor=pattern_descriptor,
-                calc_noise_ceil=False)
-            evaluations[i_sample, :, :] = cv_result.evaluations[0]
-        else:  # sample does not allow desired crossvalidation
-            evaluations[i_sample, :, :] = np.nan
-            noise_ceil[:, i_sample] = np.nan
-    if boot_type == 'both':
-        cv_method = 'bootstrap_crossval'
-        dof = min(data.n_rdm, data.n_cond) - 1
-    elif boot_type == 'pattern':
-        cv_method = 'bootstrap_crossval_pattern'
-        dof = data.n_cond - 1
-    elif boot_type == 'rdm':
-        cv_method = 'bootstrap_crossval_rdm'
-        dof = data.n_rdm - 1
-    eval_ok = ~np.isnan(evaluations[:, 0, 0])
-    if use_correction and n_cv > 1:
-        # we essentially project from the two points for 1 repetition and
-        # for n_cv repetitions to infinitely many cv repetitions
-        evals_mean = np.mean(evaluations[eval_ok], -1)
-        evals_1 = evaluations[eval_ok]
-        noise_ceil_mean = np.mean(noise_ceil[:, eval_ok], -1)
-        noise_ceil_1 = noise_ceil[:, eval_ok]
-        var_mean = np.cov(
-            np.concatenate([evals_mean.T, noise_ceil_mean]))
-        var_1 = []
-        for i in range(n_cv):
-            var_1.append(np.cov(np.concatenate([
-                evals_1[:, :, i].T, noise_ceil_1[:, :, i]])))
-        var_1 = np.mean(np.array(var_1), axis=0)
-        # this is the main formula for the correction:
-        variances = (n_cv * var_mean - var_1) / (n_cv - 1)
-    else:
-        if use_correction:
-            raise Warning('correction requested, but only one cv run'
-                          + ' per sample requested. This is invalid!'
-                          + ' We do not use the correction for now.')
-        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -1), -1)
-        noise_ceil_nonan = np.mean(noise_ceil[:, eval_ok], -1)
-        variances = np.cov(np.concatenate([evals_nonan.T, noise_ceil_nonan]))
-    result = Result(models, evaluations, method=method,
-                    cv_method=cv_method, noise_ceiling=noise_ceil,
-                    variances=variances, dof=dof)
-    return result
-
-
-def _concat_sampling(sample1, sample2):
-    """ computes an index vector for the sequential sampling with sample1
-    and sample2
-    """
-    sample_out = [[i_samp1 for i_samp1 in sample1 if i_samp1 == i_samp2]
-                  for i_samp2 in sample2]
-    return sum(sample_out, [])
-
-
-def _internal_cv(models, sample,
-                 pattern_descriptor, rdm_descriptor, pattern_idx,
-                 k_pattern, k_rdm,
-                 method, fitter):
-    """ runs a crossvalidation for use in bootstrap"""
-    train_set, test_set, ceil_set = sets_k_fold(
-        sample,
-        pattern_descriptor=pattern_descriptor,
-        rdm_descriptor=rdm_descriptor,
-        k_pattern=k_pattern, k_rdm=k_rdm, random=True)
-    if k_rdm > 1 or k_pattern > 1:
-        nc = cv_noise_ceiling(
-            sample, ceil_set, test_set,
-            method=method,
-            pattern_descriptor=pattern_descriptor)
-    else:
-        nc = boot_noise_ceiling(
-            sample,
-            method=method,
-            rdm_descriptor=rdm_descriptor)
-    for test_s in test_set:
-        test_s[1] = _concat_sampling(pattern_idx, test_s[1])
-    for train_s in train_set:
-        train_s[1] = _concat_sampling(pattern_idx, train_s[1])
-    cv_result = crossval(
-        models, sample,
-        train_set, test_set,
-        method=method, fitter=fitter,
-        pattern_descriptor=pattern_descriptor,
-        calc_noise_ceil=False)
-    return cv_result.evaluations, nc
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+evaluate model performance
+"""
+
+import numpy as np
+import tqdm
+from rsatoolbox.rdm import compare
+from rsatoolbox.inference import bootstrap_sample
+from rsatoolbox.inference import bootstrap_sample_rdm
+from rsatoolbox.inference import bootstrap_sample_pattern
+from rsatoolbox.model import Model
+from rsatoolbox.util.inference_util import input_check_model
+from rsatoolbox.util.inference_util import default_k_pattern, default_k_rdm
+from .result import Result
+from .crossvalsets import sets_k_fold, sets_random
+from .noise_ceiling import boot_noise_ceiling
+from .noise_ceiling import cv_noise_ceiling
+
+
+def eval_dual_bootstrap(
+        models, data, method='cosine', fitter=None,
+        k_pattern=1, k_rdm=1, N=1000, n_cv=2,
+        pattern_descriptor='index', rdm_descriptor='index',
+        use_correction=True):
+    """dual bootstrap evaluation of models
+    i.e. models are evaluated in a bootstrap over rdms, one over patterns
+    and a bootstrap over both using the same bootstrap samples for each.
+    The variance estimates from these bootstraps are then combined into
+    a better overall estimate for the variance.
+
+    This method allows the incorporation of crossvalidation inside the
+    bootstrap to handle fitted models.
+    To activate this set k_rdm and k_pattern as described below.
+
+    Crossvalidation creates variance in the results for a single bootstrap
+    sample, because different assginments to the training and test group
+    lead to different results. To correct for this, we apply a formula
+    which estimates the variance we expect if we evaluated all possible
+    crossvalidation assignments from n_cv different assignments per bootstrap
+    sample.
+    In our statistical evaluations we saw that many bootstrap samples and
+    few different crossvalidation assignments are optimal to minimize the
+    variance of the variance estimate. Thus, this function by default
+    applies this correction formula and sets n_cv=2, i.e. performs only two
+    different assignments per fold.
+    This function nonetheless performs full crossvalidation schemes, i.e.
+    in every bootstrap sample all crossvalidation folds are evaluated such
+    that each RDM and each condition is in the test set n_cv times.
+
+    The k_[] parameters control the cross-validation per sample. They give
+    the number of crossvalidation folds to be created along this dimension.
+    If a k is set to 1 no crossvalidation is performed over the
+    corresponding dimension.
+    by default ks are set by rsatoolbox.util.inference_util.default_k_pattern
+    and rsatoolbox.util.inference_util.default_k_rdm based on the number of
+    rdms and patterns provided. the ks are then in the range 2-5.
+
+    Using the []_descriptor inputs you may make the crossvalidation and
+    bootstrap aware of groups of rdms or conditions to be handled en block.
+    Conditions with the same entry will be sampled in or out of the bootstrap
+    together and will be assigned to cross-calidation folds together.
+
+    models should be a list of models. data the RDMs object to evaluate against
+    method the method for comparing the predictions and the data. fitter may
+    provide a non-default funcion or list of functions to fit the models.
+
+    Args:
+        models(rsatoolbox.model.Model): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): RDM data to use
+        method(string): comparison method to use
+        fitter(function): fitting method for models
+        k_pattern(int): #folds over patterns
+        k_rdm(int): #folds over rdms
+        N(int): number of bootstrap samples (default: 1000)
+        n_cv(int) : number of crossvalidation runs per sample (default: 1)
+        pattern_descriptor(string): descriptor to group patterns
+        rdm_descriptor(string): descriptor to group rdms
+        random(bool): randomize group assignments (default: True)
+        boot_type(String): which dimension to bootstrap over (default: 'both')
+            alternatives: 'rdm', 'pattern'
+        use_correction(bool): switch for the correction for the
+            variance caused by crossvalidation (default: True)
+
+    Returns:
+        numpy.ndarray: matrix of evaluations (N x k)
+
+    """
+    if k_pattern is None:
+        n_pattern = len(np.unique(data.pattern_descriptors[
+            pattern_descriptor]))
+        k_pattern = default_k_pattern((1 - 1 / np.exp(1)) * n_pattern)
+    if k_rdm is None:
+        n_rdm = len(np.unique(data.rdm_descriptors[
+            rdm_descriptor]))
+        k_rdm = default_k_rdm((1 - 1 / np.exp(1)) * n_rdm)
+    if k_rdm == 1 and k_pattern == 1:
+        n_cv = 1
+        use_correction = False
+    if isinstance(models, Model):
+        models = [models]
+    evaluations = np.zeros((N, len(models), k_pattern * k_rdm, n_cv, 3))
+    noise_ceil = np.zeros((2, N, n_cv, 3))
+    for i_sample in tqdm.trange(N):
+        sample, rdm_idx, pattern_idx = bootstrap_sample(
+            data,
+            rdm_descriptor=rdm_descriptor,
+            pattern_descriptor=pattern_descriptor)
+        sample_rdm = data.subsample(rdm_descriptor, rdm_idx)
+        sample_pattern = data.subsample_pattern(
+            pattern_descriptor, pattern_idx)
+        if len(np.unique(rdm_idx)) >= k_rdm \
+           and len(np.unique(pattern_idx)) >= 3 * k_pattern:
+            for i_rep in range(n_cv):
+                evals, cv_nc = _internal_cv(
+                    models, sample,
+                    pattern_descriptor, rdm_descriptor, pattern_idx,
+                    k_pattern, k_rdm,
+                    method, fitter)
+                noise_ceil[:, i_sample, i_rep, 0] = cv_nc
+                evaluations[i_sample, :, :, i_rep, 0] = evals[0]
+                evals, cv_nc = _internal_cv(
+                    models, sample_rdm,
+                    pattern_descriptor, rdm_descriptor,
+                    np.unique(data.pattern_descriptors[pattern_descriptor]),
+                    k_pattern, k_rdm,
+                    method, fitter)
+                noise_ceil[:, i_sample, i_rep, 1] = cv_nc
+                evaluations[i_sample, :, :, i_rep, 1] = evals[0]
+                evals, cv_nc = _internal_cv(
+                    models, sample_pattern,
+                    pattern_descriptor, rdm_descriptor, pattern_idx,
+                    k_pattern, k_rdm,
+                    method, fitter)
+                noise_ceil[:, i_sample, i_rep, 2] = cv_nc
+                evaluations[i_sample, :, :, i_rep, 2] = evals[0]
+        else:  # sample does not allow desired crossvalidation
+            evaluations[i_sample, :, :, :, :] = np.nan
+            noise_ceil[:, i_sample, :, :] = np.nan
+    cv_method = 'dual_bootstrap'
+    dof = min(data.n_rdm, data.n_cond) - 1
+    eval_ok = ~np.isnan(evaluations[:, 0, 0, 0, 0])
+    if use_correction and n_cv > 1:
+        # we essentially project from the two points for 1 repetition and
+        # for n_cv repetitions to infinitely many cv repetitions
+        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -2), -2)
+        evals_1 = np.mean(evaluations[eval_ok], -3)
+        noise_ceil_nonan = np.mean(
+            noise_ceil[:, eval_ok], -2).transpose([1, 0, 2])
+        noise_ceil_1 = noise_ceil[:, eval_ok].transpose([1, 0, 2, 3])
+        matrix = np.concatenate([evals_nonan, noise_ceil_nonan], 1)
+        matrix -= np.mean(matrix, 0, keepdims=True)
+        var_mean = np.einsum('ijk,ilk->kjl', matrix, matrix) \
+            / (matrix.shape[0] - 1)
+        matrix_1 = np.concatenate([evals_1, noise_ceil_1], 1)
+        matrix_1 -= np.mean(matrix_1, 0, keepdims=True)
+        var_1 = np.einsum('ijmk,ilmk->kjl', matrix_1, matrix_1) \
+            / (matrix_1.shape[0] - 1) / matrix_1.shape[2]
+        # this is the main formula for the correction:
+        variances = (n_cv * var_mean - var_1) / (n_cv - 1)
+    else:
+        if use_correction:
+            raise Warning('correction requested, but only one cv run'
+                          + ' per sample requested. This is invalid!'
+                          + ' We do not use the correction for now.')
+        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -2), -2)
+        noise_ceil_nonan = np.mean(
+            noise_ceil[:, eval_ok], -2).transpose([1, 0, 2])
+        matrix = np.concatenate([evals_nonan, noise_ceil_nonan], 1)
+        matrix -= np.mean(matrix, 0, keepdims=True)
+        variances = np.einsum('ijk,ilk->kjl', matrix, matrix) \
+            / (matrix.shape[0] - 1)
+    result = Result(models, evaluations, method=method,
+                    cv_method=cv_method, noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=data.n_rdm,
+                    n_pattern=data.n_cond)
+    return result
+
+
+def eval_fixed(models, data, theta=None, method='cosine'):
+    """evaluates models on data, without any bootstrapping or
+    cross-validation
+
+    Args:
+        models(list of rsatoolbox.model.Model or list): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): data to evaluate on
+        theta(numpy.ndarray): parameter vector for the models
+        method(string): comparison method to use
+
+    Returns:
+        float: evaluation
+
+    """
+    models, evaluations, theta, _ = input_check_model(models, theta, None, 1)
+    evaluations = np.repeat(np.expand_dims(evaluations, -1),
+                            data.n_rdm, -1)
+    for k, model in enumerate(models):
+        rdm_pred = model.predict_rdm(theta=theta[k])
+        evaluations[k] = compare(rdm_pred, data, method)
+    evaluations = evaluations.reshape((1, len(models), data.n_rdm))
+    noise_ceil = boot_noise_ceiling(
+        data, method=method, rdm_descriptor='index')
+    if data.n_rdm > 1:
+        variances = np.cov(evaluations[0], ddof=0) \
+            / evaluations.shape[-1]
+        dof = evaluations.shape[-1] - 1
+    else:
+        variances = None
+        dof = 0
+    result = Result(models, evaluations, method=method,
+                    cv_method='fixed', noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=data.n_rdm,
+                    n_pattern=None)
+    result.n_pattern = data.n_cond
+    return result
+
+
+def eval_bootstrap(models, data, theta=None, method='cosine', N=1000,
+                   pattern_descriptor='index', rdm_descriptor='index',
+                   boot_noise_ceil=True):
+    """evaluates models on data
+    performs bootstrapping to get a sampling distribution
+
+    Args:
+        models(rsatoolbox.model.Model or list): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): data to evaluate on
+        theta(numpy.ndarray): parameter vector for the models
+        method(string): comparison method to use
+        N(int): number of samples
+        pattern_descriptor(string): descriptor to group patterns for bootstrap
+        rdm_descriptor(string): descriptor to group rdms for bootstrap
+
+    Returns:
+        numpy.ndarray: vector of evaluations
+
+    """
+    models, evaluations, theta, _ = \
+        input_check_model(models, theta, None, N)
+    noise_min = []
+    noise_max = []
+    for i in tqdm.trange(N):
+        sample, _, pattern_idx = \
+            bootstrap_sample(data, rdm_descriptor=rdm_descriptor,
+                             pattern_descriptor=pattern_descriptor)
+        if len(np.unique(pattern_idx)) >= 3:
+            for j, mod in enumerate(models):
+                rdm_pred = mod.predict_rdm(theta=theta[j])
+                rdm_pred = rdm_pred.subsample_pattern(pattern_descriptor,
+                                                      pattern_idx)
+                evaluations[i, j] = np.mean(compare(rdm_pred, sample,
+                                                    method))
+            if boot_noise_ceil:
+                noise_min_sample, noise_max_sample = boot_noise_ceiling(
+                    sample, method=method, rdm_descriptor=rdm_descriptor)
+                noise_min.append(noise_min_sample)
+                noise_max.append(noise_max_sample)
+        else:
+            evaluations[i, :] = np.nan
+            noise_min.append(np.nan)
+            noise_max.append(np.nan)
+    if boot_noise_ceil:
+        eval_ok = np.isfinite(evaluations[:, 0])
+        noise_ceil = np.array([noise_min, noise_max])
+        variances = np.cov(np.concatenate([evaluations[eval_ok, :].T,
+                                           noise_ceil[:, eval_ok]]))
+    else:
+        eval_ok = np.isfinite(evaluations[:, 0])
+        noise_ceil = np.array(boot_noise_ceiling(
+            data, method=method, rdm_descriptor=rdm_descriptor))
+        variances = np.cov(evaluations[eval_ok, :].T)
+    dof = min(data.n_rdm, data.n_cond) - 1
+    result = Result(models, evaluations, method=method,
+                    cv_method='bootstrap', noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=data.n_rdm,
+                    n_pattern=data.n_cond)
+    return result
+
+
+def eval_bootstrap_pattern(models, data, theta=None, method='cosine', N=1000,
+                           pattern_descriptor='index', rdm_descriptor='index',
+                           boot_noise_ceil=True):
+    """evaluates a models on data
+    performs bootstrapping over patterns to get a sampling distribution
+
+    Args:
+        models(rsatoolbox.model.Model or list): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): data to evaluate on
+        theta(numpy.ndarray): parameter vector for the models
+        method(string): comparison method to use
+        N(int): number of samples
+        pattern_descriptor(string): descriptor to group patterns for bootstrap
+        rdm_descriptor(string): descriptor to group patterns for noise
+            ceiling calculation
+
+    Returns:
+        numpy.ndarray: vector of evaluations
+
+    """
+    models, evaluations, theta, _ = \
+        input_check_model(models, theta, None, N)
+    noise_min = []
+    noise_max = []
+    for i in tqdm.trange(N):
+        sample, pattern_idx = \
+            bootstrap_sample_pattern(data, pattern_descriptor)
+        if len(np.unique(pattern_idx)) >= 3:
+            for j, mod in enumerate(models):
+                rdm_pred = mod.predict_rdm(theta=theta[j])
+                rdm_pred = rdm_pred.subsample_pattern(pattern_descriptor,
+                                                      pattern_idx)
+                evaluations[i, j] = np.mean(compare(rdm_pred, sample,
+                                                    method))
+            if boot_noise_ceil:
+                noise_min_sample, noise_max_sample = boot_noise_ceiling(
+                    sample, method=method, rdm_descriptor=rdm_descriptor)
+                noise_min.append(noise_min_sample)
+                noise_max.append(noise_max_sample)
+        else:
+            evaluations[i, :] = np.nan
+            noise_min.append(np.nan)
+            noise_max.append(np.nan)
+    if boot_noise_ceil:
+        eval_ok = np.isfinite(evaluations[:, 0])
+        noise_ceil = np.array([noise_min, noise_max])
+        variances = np.cov(np.concatenate([evaluations[eval_ok, :].T,
+                                           noise_ceil[:, eval_ok]]))
+    else:
+        eval_ok = np.isfinite(evaluations[:, 0])
+        noise_ceil = np.array(boot_noise_ceiling(
+            data, method=method, rdm_descriptor=rdm_descriptor))
+        variances = np.cov(evaluations[eval_ok, :].T)
+    dof = data.n_cond - 1
+    result = Result(models, evaluations, method=method,
+                    cv_method='bootstrap_pattern', noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=None,
+                    n_pattern=data.n_cond)
+    result.n_rdm = data.n_rdm
+    return result
+
+
+def eval_bootstrap_rdm(models, data, theta=None, method='cosine', N=1000,
+                       rdm_descriptor='index', boot_noise_ceil=True):
+    """evaluates models on data
+    performs bootstrapping to get a sampling distribution
+
+    Args:
+        models(rsatoolbox.model.Model or list of these): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): data to evaluate on
+        theta(numpy.ndarray): parameter vector for the models
+        method(string): comparison method to use
+        N(int): number of samples
+        rdm_descriptor(string): rdm_descriptor to group rdms for bootstrap
+
+    Returns:
+        numpy.ndarray: vector of evaluations
+
+    """
+    models, evaluations, theta, _ = input_check_model(models, theta, None, N)
+    noise_min = []
+    noise_max = []
+    for i in tqdm.trange(N):
+        sample, _ = bootstrap_sample_rdm(data, rdm_descriptor)
+        for j, mod in enumerate(models):
+            rdm_pred = mod.predict_rdm(theta=theta[j])
+            evaluations[i, j] = np.mean(compare(rdm_pred, sample,
+                                                method))
+        if boot_noise_ceil:
+            noise_min_sample, noise_max_sample = boot_noise_ceiling(
+                sample, method=method, rdm_descriptor=rdm_descriptor)
+            noise_min.append(noise_min_sample)
+            noise_max.append(noise_max_sample)
+    if boot_noise_ceil:
+        eval_ok = np.isfinite(evaluations[:, 0])
+        noise_ceil = np.array([noise_min, noise_max])
+        variances = np.cov(np.concatenate([evaluations[eval_ok, :].T,
+                                           noise_ceil[:, eval_ok]]))
+    else:
+        eval_ok = np.isfinite(evaluations[:, 0])
+        noise_ceil = np.array(boot_noise_ceiling(
+            data, method=method, rdm_descriptor=rdm_descriptor))
+        variances = np.cov(evaluations[eval_ok, :].T)
+    dof = data.n_rdm - 1
+    variances = np.cov(evaluations.T)
+    result = Result(models, evaluations, method=method,
+                    cv_method='bootstrap_rdm', noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=data.n_rdm,
+                    n_pattern=None)
+    result.n_pattern = data.n_cond
+    return result
+
+
+def crossval(models, rdms, train_set, test_set, ceil_set=None, method='cosine',
+             fitter=None, pattern_descriptor='index', calc_noise_ceil=True):
+    """evaluates models on cross-validation sets
+
+    Args:
+        models(rsatoolbox.model.Model): models to be evaluated
+        rdms(rsatoolbox.rdm.RDMs): full dataset
+        train_set(list): a list of the training RDMs with 2-tuple entries:
+            (RDMs, pattern_idx)
+        test_set(list): a list of the test RDMs with 2-tuple entries:
+            (RDMs, pattern_idx)
+        method(string): comparison method to use
+        pattern_descriptor(string): descriptor to group patterns
+
+    Returns:
+        numpy.ndarray: vector of evaluations
+
+    """
+    assert len(train_set) == len(test_set), \
+        'train_set and test_set must have the same length'
+    if ceil_set is not None:
+        assert len(ceil_set) == len(test_set), \
+            'ceil_set and test_set must have the same length'
+    if isinstance(models, Model):
+        models = [models]
+    evaluations = []
+    noise_ceil = []
+    for i, train in enumerate(train_set):
+        test = test_set[i]
+        if (train[0].n_rdm == 0 or test[0].n_rdm == 0 or
+                train[0].n_cond <= 2 or test[0].n_cond <= 2):
+            evals = np.empty(len(models)) * np.nan
+        else:
+            models, evals, _, fitter = \
+                input_check_model(models, None, fitter)
+            for j, model in enumerate(models):
+                theta = fitter[j](model, train[0], method=method,
+                                  pattern_idx=train[1],
+                                  pattern_descriptor=pattern_descriptor)
+                pred = model.predict_rdm(theta)
+                pred = pred.subsample_pattern(by=pattern_descriptor,
+                                              value=test[1])
+                evals[j] = np.mean(compare(pred, test[0], method))
+            if ceil_set is None and calc_noise_ceil:
+                noise_ceil.append(boot_noise_ceiling(
+                    rdms.subsample_pattern(by=pattern_descriptor,
+                                           value=test[1]),
+                    method=method))
+        evaluations.append(evals)
+    evaluations = np.array(evaluations).T  # .T to switch models/set order
+    evaluations = evaluations.reshape((1, len(models), len(train_set)))
+    if ceil_set is not None and calc_noise_ceil:
+        noise_ceil = cv_noise_ceiling(rdms, ceil_set, test_set, method=method,
+                                      pattern_descriptor=pattern_descriptor)
+    elif calc_noise_ceil:
+        noise_ceil = np.array(noise_ceil).T
+    else:
+        noise_ceil = np.array([np.nan, np.nan])
+
+    result = Result(models, evaluations, method=method,
+                    cv_method='crossvalidation', noise_ceiling=noise_ceil)
+    return result
+
+
+def bootstrap_crossval(models, data, method='cosine', fitter=None,
+                       k_pattern=None, k_rdm=None, N=1000, n_cv=2,
+                       pattern_descriptor='index', rdm_descriptor='index',
+                       boot_type='both', use_correction=True):
+    """evaluates a set of models by k-fold crossvalidation within a bootstrap
+
+    Crossvalidation creates variance in the results for a single bootstrap
+    sample, because different assginments to the training and test group
+    lead to different results. To correct for this, we apply a formula
+    which estimates the variance we expect if we evaluated all possible
+    crossvalidation assignments from n_cv different assignments per bootstrap
+    sample.
+    In our statistical evaluations we saw that many bootstrap samples and
+    few different crossvalidation assignments are optimal to minimize the
+    variance of the variance estimate. Thus, this function by default
+    applies this correction formula and sets n_cv=2, i.e. performs only two
+    different assignments per fold.
+    This function nonetheless performs full crossvalidation schemes, i.e.
+    in every bootstrap sample all crossvalidation folds are evaluated such
+    that each RDM and each condition is in the test set n_cv times. For the
+    even more optimized version which computes only two randomly chosen test
+    sets see bootstrap_cv_random.
+
+    The k_[] parameters control the cross-validation per sample. They give
+    the number of crossvalidation folds to be created along this dimension.
+    If a k is set to 1 no crossvalidation is performed over the
+    corresponding dimension.
+    by default ks are set by rsatoolbox.util.inference_util.default_k_pattern
+    and rsatoolbox.util.inference_util.default_k_rdm based on the number of
+    rdms and patterns provided. the ks are then in the range 2-5.
+
+    Using the []_descriptor inputs you may make the crossvalidation and
+    bootstrap aware of groups of rdms or conditions to be handled en block.
+    Conditions with the same entry will be sampled in or out of the bootstrap
+    together and will be assigned to cross-calidation folds together.
+
+    Using the boot_type argument you may choose the dimension to bootstrap.
+    By default both conditions and RDMs are resampled. You may alternatively
+    choose to resample only one of them by passing 'rdm' or 'pattern'.
+
+    models should be a list of models. data the RDMs object to evaluate against
+    method the method for comparing the predictions and the data. fitter may
+    provide a non-default funcion or list of functions to fit the models.
+
+    Args:
+        models(rsatoolbox.model.Model): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): RDM data to use
+        method(string): comparison method to use
+        fitter(function): fitting method for models
+        k_pattern(int): #folds over patterns
+        k_rdm(int): #folds over rdms
+        N(int): number of bootstrap samples (default: 1000)
+        n_cv(int) : number of crossvalidation runs per sample (default: 1)
+        pattern_descriptor(string): descriptor to group patterns
+        rdm_descriptor(string): descriptor to group rdms
+        boot_type(String): which dimension to bootstrap over (default: 'both')
+            alternatives: 'rdm', 'pattern'
+        use_correction(bool): switch for the correction for the
+            variance caused by crossvalidation (default: True)
+
+    Returns:
+        numpy.ndarray: matrix of evaluations (N x k)
+
+    """
+    if k_pattern is None:
+        n_pattern = len(np.unique(data.pattern_descriptors[
+            pattern_descriptor]))
+        k_pattern = default_k_pattern((1 - 1 / np.exp(1)) * n_pattern)
+    if k_rdm is None:
+        n_rdm = len(np.unique(data.rdm_descriptors[
+            rdm_descriptor]))
+        k_rdm = default_k_rdm((1 - 1 / np.exp(1)) * n_rdm)
+    if isinstance(models, Model):
+        models = [models]
+    evaluations = np.empty((N, len(models), k_pattern * k_rdm, n_cv))
+    noise_ceil = np.empty((2, N, n_cv))
+    for i_sample in tqdm.trange(N):
+        if boot_type == 'both':
+            sample, rdm_idx, pattern_idx = bootstrap_sample(
+                data,
+                rdm_descriptor=rdm_descriptor,
+                pattern_descriptor=pattern_descriptor)
+        elif boot_type == 'pattern':
+            sample, pattern_idx = bootstrap_sample_pattern(
+                data,
+                pattern_descriptor=pattern_descriptor)
+            rdm_idx = np.unique(data.rdm_descriptors[rdm_descriptor])
+        elif boot_type == 'rdm':
+            sample, rdm_idx = bootstrap_sample_rdm(
+                data,
+                rdm_descriptor=rdm_descriptor)
+            pattern_idx = np.unique(
+                data.pattern_descriptors[pattern_descriptor])
+        else:
+            raise ValueError('boot_type not understood')
+        if len(np.unique(rdm_idx)) >= k_rdm \
+           and len(np.unique(pattern_idx)) >= 3 * k_pattern:
+            for i_rep in range(n_cv):
+                evals, cv_nc = _internal_cv(
+                    models, sample,
+                    pattern_descriptor, rdm_descriptor, pattern_idx,
+                    k_pattern, k_rdm,
+                    method, fitter)
+                noise_ceil[:, i_sample, i_rep] = cv_nc
+                evaluations[i_sample, :, :, i_rep] = evals[0]
+        else:  # sample does not allow desired crossvalidation
+            evaluations[i_sample, :, :] = np.nan
+            noise_ceil[:, i_sample] = np.nan
+    if boot_type == 'both':
+        cv_method = 'bootstrap_crossval'
+        dof = min(data.n_rdm, data.n_cond) - 1
+    elif boot_type == 'pattern':
+        cv_method = 'bootstrap_crossval_pattern'
+        dof = data.n_cond - 1
+    elif boot_type == 'rdm':
+        cv_method = 'bootstrap_crossval_rdm'
+        dof = data.n_rdm - 1
+    eval_ok = ~np.isnan(evaluations[:, 0, 0, 0])
+    if use_correction and n_cv > 1:
+        # we essentially project from the two points for 1 repetition and
+        # for n_cv repetitions to infinitely many cv repetitions
+        evals_mean = np.mean(np.mean(evaluations[eval_ok], -1), -1)
+        evals_1 = np.mean(evaluations[eval_ok], -2)
+        noise_ceil_mean = np.mean(noise_ceil[:, eval_ok], -1)
+        noise_ceil_1 = noise_ceil[:, eval_ok]
+        var_mean = np.cov(
+            np.concatenate([evals_mean.T, noise_ceil_mean]))
+        var_1 = []
+        for i in range(n_cv):
+            var_1.append(np.cov(np.concatenate([
+                evals_1[:, :, i].T, noise_ceil_1[:, :, i]])))
+        var_1 = np.mean(np.array(var_1), axis=0)
+        # this is the main formula for the correction:
+        variances = (n_cv * var_mean - var_1) / (n_cv - 1)
+    else:
+        if use_correction:
+            raise Warning('correction requested, but only one cv run'
+                          + ' per sample requested. This is invalid!'
+                          + ' We do not use the correction for now.')
+        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -1), -1)
+        noise_ceil_nonan = np.mean(noise_ceil[:, eval_ok], -1)
+        variances = np.cov(np.concatenate([evals_nonan.T, noise_ceil_nonan]))
+    result = Result(models, evaluations, method=method,
+                    cv_method=cv_method, noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=data.n_rdm,
+                    n_pattern=data.n_cond)
+    return result
+
+
+def eval_dual_bootstrap_random(
+        models, data, method='cosine', fitter=None,
+        n_pattern=None, n_rdm=None, N=1000, n_cv=2,
+        pattern_descriptor='index', rdm_descriptor='index',
+        boot_type='both', use_correction=True):
+    """evaluates a set of models by a evaluating a few random crossvalidation
+    folds per bootstrap.
+
+    If a k is set to 1 no crossvalidation is performed over the
+    corresponding dimension.
+
+    As especially crossvalidation over patterns/conditions creates
+    variance in the cv result for a single variance the default setting
+    of n_cv=1 inflates the estimated variance. Setting this value
+    higher will decrease this effect at the cost of more computation time.
+
+    by default ks are set by rsatoolbox.util.inference_util.default_k_pattern
+    and rsatoolbox.util.inference_util.default_k_rdm based on the number of
+    rdms and patterns provided. the ks are then in the range 2-5.
+
+    Args:
+        models(rsatoolbox.model.Model): models to be evaluated
+        data(rsatoolbox.rdm.RDMs): RDM data to use
+        method(string): comparison method to use
+        fitter(function): fitting method for models
+        k_pattern(int): #folds over patterns
+        k_rdm(int): #folds over rdms
+        N(int): number of bootstrap samples (default: 1000)
+        n_cv(int) : number of crossvalidation runs per sample (default: 1)
+        pattern_descriptor(string): descriptor to group patterns
+        rdm_descriptor(string): descriptor to group rdms
+        boot_type(String): which dimension to bootstrap over (default: 'both')
+            alternatives: 'rdm', 'pattern'
+        use_correction(bool): switch for the correction for the
+            variance caused by crossvalidation (default: True)
+
+    Returns:
+        numpy.ndarray: matrix of evaluations (N x k)
+
+    """
+    if n_pattern is None:
+        n_pattern_all = len(np.unique(data.pattern_descriptors[
+            pattern_descriptor]))
+        k_pattern = default_k_pattern((1 - 1 / np.exp(1)) * n_pattern_all)
+        n_pattern = int(np.floor(n_pattern_all / k_pattern))
+    if n_rdm is None:
+        n_rdm_all = len(np.unique(data.rdm_descriptors[
+            rdm_descriptor]))
+        k_rdm = default_k_rdm((1 - 1 / np.exp(1)) * n_rdm_all)
+        n_rdm = int(np.floor(n_rdm_all / k_rdm))
+    if isinstance(models, Model):
+        models = [models]
+    evaluations = np.zeros((N, len(models), n_cv))
+    noise_ceil = np.zeros((2, N, n_cv))
+    for i_sample in tqdm.trange(N):
+        if boot_type == 'both':
+            sample, rdm_idx, pattern_idx = bootstrap_sample(
+                data,
+                rdm_descriptor=rdm_descriptor,
+                pattern_descriptor=pattern_descriptor)
+        elif boot_type == 'pattern':
+            sample, pattern_idx = bootstrap_sample_pattern(
+                data,
+                pattern_descriptor=pattern_descriptor)
+            rdm_idx = np.unique(data.rdm_descriptors[rdm_descriptor])
+        elif boot_type == 'rdm':
+            sample, rdm_idx = bootstrap_sample_rdm(
+                data,
+                rdm_descriptor=rdm_descriptor)
+            pattern_idx = np.unique(
+                data.pattern_descriptors[pattern_descriptor])
+        else:
+            raise ValueError('boot_type not understood')
+        if len(np.unique(rdm_idx)) > n_rdm \
+           and len(np.unique(pattern_idx)) >= 3 + n_pattern:
+            train_set, test_set, ceil_set = sets_random(
+                sample,
+                pattern_descriptor=pattern_descriptor,
+                rdm_descriptor=rdm_descriptor,
+                n_pattern=n_pattern, n_rdm=n_rdm, n_cv=n_cv)
+            if n_rdm > 0 or n_pattern > 0:
+                nc = cv_noise_ceiling(
+                    sample, ceil_set, test_set,
+                    method=method,
+                    pattern_descriptor=pattern_descriptor)
+            else:
+                nc = boot_noise_ceiling(
+                    sample,
+                    method=method,
+                    rdm_descriptor=rdm_descriptor)
+            noise_ceil[:, i_sample] = nc
+            for test_s in test_set:
+                test_s[1] = _concat_sampling(pattern_idx, test_s[1])
+            for train_s in train_set:
+                train_s[1] = _concat_sampling(pattern_idx, train_s[1])
+            cv_result = crossval(
+                models, sample,
+                train_set, test_set,
+                method=method, fitter=fitter,
+                pattern_descriptor=pattern_descriptor,
+                calc_noise_ceil=False)
+            evaluations[i_sample, :, :] = cv_result.evaluations[0]
+        else:  # sample does not allow desired crossvalidation
+            evaluations[i_sample, :, :] = np.nan
+            noise_ceil[:, i_sample] = np.nan
+    if boot_type == 'both':
+        cv_method = 'bootstrap_crossval'
+        dof = min(data.n_rdm, data.n_cond) - 1
+    elif boot_type == 'pattern':
+        cv_method = 'bootstrap_crossval_pattern'
+        dof = data.n_cond - 1
+    elif boot_type == 'rdm':
+        cv_method = 'bootstrap_crossval_rdm'
+        dof = data.n_rdm - 1
+    eval_ok = ~np.isnan(evaluations[:, 0, 0])
+    if use_correction and n_cv > 1:
+        # we essentially project from the two points for 1 repetition and
+        # for n_cv repetitions to infinitely many cv repetitions
+        evals_mean = np.mean(evaluations[eval_ok], -1)
+        evals_1 = evaluations[eval_ok]
+        noise_ceil_mean = np.mean(noise_ceil[:, eval_ok], -1)
+        noise_ceil_1 = noise_ceil[:, eval_ok]
+        var_mean = np.cov(
+            np.concatenate([evals_mean.T, noise_ceil_mean]))
+        var_1 = []
+        for i in range(n_cv):
+            var_1.append(np.cov(np.concatenate([
+                evals_1[:, :, i].T, noise_ceil_1[:, :, i]])))
+        var_1 = np.mean(np.array(var_1), axis=0)
+        # this is the main formula for the correction:
+        variances = (n_cv * var_mean - var_1) / (n_cv - 1)
+    else:
+        if use_correction:
+            raise Warning('correction requested, but only one cv run'
+                          + ' per sample requested. This is invalid!'
+                          + ' We do not use the correction for now.')
+        evals_nonan = np.mean(np.mean(evaluations[eval_ok], -1), -1)
+        noise_ceil_nonan = np.mean(noise_ceil[:, eval_ok], -1)
+        variances = np.cov(np.concatenate([evals_nonan.T, noise_ceil_nonan]))
+    result = Result(models, evaluations, method=method,
+                    cv_method=cv_method, noise_ceiling=noise_ceil,
+                    variances=variances, dof=dof, n_rdm=data.n_rdm,
+                    n_pattern=data.n_cond)
+    return result
+
+
+def _concat_sampling(sample1, sample2):
+    """ computes an index vector for the sequential sampling with sample1
+    and sample2
+    """
+    sample_out = [[i_samp1 for i_samp1 in sample1 if i_samp1 == i_samp2]
+                  for i_samp2 in sample2]
+    return sum(sample_out, [])
+
+
+def _internal_cv(models, sample,
+                 pattern_descriptor, rdm_descriptor, pattern_idx,
+                 k_pattern, k_rdm,
+                 method, fitter):
+    """ runs a crossvalidation for use in bootstrap"""
+    train_set, test_set, ceil_set = sets_k_fold(
+        sample,
+        pattern_descriptor=pattern_descriptor,
+        rdm_descriptor=rdm_descriptor,
+        k_pattern=k_pattern, k_rdm=k_rdm, random=True)
+    if k_rdm > 1 or k_pattern > 1:
+        nc = cv_noise_ceiling(
+            sample, ceil_set, test_set,
+            method=method,
+            pattern_descriptor=pattern_descriptor)
+    else:
+        nc = boot_noise_ceiling(
+            sample,
+            method=method,
+            rdm_descriptor=rdm_descriptor)
+    for test_s in test_set:
+        test_s[1] = _concat_sampling(pattern_idx, test_s[1])
+    for train_s in train_set:
+        train_s[1] = _concat_sampling(pattern_idx, train_s[1])
+    cv_result = crossval(
+        models, sample,
+        train_set, test_set,
+        method=method, fitter=fitter,
+        pattern_descriptor=pattern_descriptor,
+        calc_noise_ceil=False)
+    return cv_result.evaluations, nc
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/noise_ceiling.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/noise_ceiling.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,78 +1,78 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-calculation of noise ceilings
-"""
-
-import numpy as np
-from rsatoolbox.util.inference_util import pool_rdm
-from rsatoolbox.rdm import compare
-from .crossvalsets import sets_leave_one_out_rdm
-
-
-def cv_noise_ceiling(rdms, ceil_set, test_set, method='cosine',
-                     pattern_descriptor='index'):
-    """ calculates the noise ceiling for crossvalidation.
-    The upper bound is calculated by pooling all rdms for the appropriate
-    patterns in the testsets.
-    the lower bound is calculated by using only the appropriate rdms
-    from ceil_set for training.
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): complete data
-        ceil_set(list): a list of the training RDMs with 2-tuple entries:
-            (RDMs, pattern_idx)
-        test_set(list): a list of the test RDMs with 2-tuple entries:
-            (RDMs, pattern_idx)
-        method(string): comparison method to use
-        pattern_descriptor(string): descriptor to group patterns
-
-    Returns:
-        list: lower nc-bound, upper nc-bound
-
-    """
-    assert len(ceil_set) == len(test_set), \
-        'train_set and test_set must have the same length'
-    noise_min = []
-    noise_max = []
-    for i in range(len(ceil_set)):
-        train = ceil_set[i]
-        test = test_set[i]
-        pred_train = pool_rdm(train[0], method=method)
-        pred_train = pred_train.subsample_pattern(by=pattern_descriptor,
-                                                  value=test[1])
-        pred_test = pool_rdm(rdms, method=method)
-        pred_test = pred_test.subsample_pattern(by=pattern_descriptor,
-                                                value=test[1])
-        noise_min.append(np.mean(compare(pred_train, test[0], method)))
-        noise_max.append(np.mean(compare(pred_test, test[0], method)))
-    noise_min = np.mean(np.array(noise_min))
-    noise_max = np.mean(np.array(noise_max))
-    return noise_min, noise_max
-
-
-def boot_noise_ceiling(rdms, method='cosine', rdm_descriptor='index'):
-    """ calculates a noise ceiling by leave one out & full set
-
-    Args:
-        rdms(rsatoolbox.rdm.RDMs): data to calculate noise ceiling
-        method(string): comparison method to use
-        rdm_descriptor(string): descriptor to group rdms
-
-    Returns:
-        list: [lower nc-bound, upper nc-bound]
-
-    """
-    _, test_set, ceil_set = sets_leave_one_out_rdm(rdms, rdm_descriptor)
-    pred_test = pool_rdm(rdms, method=method)
-    noise_min = []
-    noise_max = []
-    for i in range(len(ceil_set)):
-        train = ceil_set[i]
-        test = test_set[i]
-        pred_train = pool_rdm(train[0], method=method)
-        noise_min.append(np.mean(compare(pred_train, test[0], method)))
-        noise_max.append(np.mean(compare(pred_test, test[0], method)))
-    noise_min = np.mean(np.array(noise_min))
-    noise_max = np.mean(np.array(noise_max))
-    return noise_min, noise_max
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+calculation of noise ceilings
+"""
+
+import numpy as np
+from rsatoolbox.util.inference_util import pool_rdm
+from rsatoolbox.rdm import compare
+from .crossvalsets import sets_leave_one_out_rdm
+
+
+def cv_noise_ceiling(rdms, ceil_set, test_set, method='cosine',
+                     pattern_descriptor='index'):
+    """ calculates the noise ceiling for crossvalidation.
+    The upper bound is calculated by pooling all rdms for the appropriate
+    patterns in the testsets.
+    the lower bound is calculated by using only the appropriate rdms
+    from ceil_set for training.
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): complete data
+        ceil_set(list): a list of the training RDMs with 2-tuple entries:
+            (RDMs, pattern_idx)
+        test_set(list): a list of the test RDMs with 2-tuple entries:
+            (RDMs, pattern_idx)
+        method(string): comparison method to use
+        pattern_descriptor(string): descriptor to group patterns
+
+    Returns:
+        list: lower nc-bound, upper nc-bound
+
+    """
+    assert len(ceil_set) == len(test_set), \
+        'train_set and test_set must have the same length'
+    noise_min = []
+    noise_max = []
+    for i in range(len(ceil_set)):
+        train = ceil_set[i]
+        test = test_set[i]
+        pred_train = pool_rdm(train[0], method=method)
+        pred_train = pred_train.subsample_pattern(by=pattern_descriptor,
+                                                  value=test[1])
+        pred_test = pool_rdm(rdms, method=method)
+        pred_test = pred_test.subsample_pattern(by=pattern_descriptor,
+                                                value=test[1])
+        noise_min.append(np.mean(compare(pred_train, test[0], method)))
+        noise_max.append(np.mean(compare(pred_test, test[0], method)))
+    noise_min = np.mean(np.array(noise_min))
+    noise_max = np.mean(np.array(noise_max))
+    return noise_min, noise_max
+
+
+def boot_noise_ceiling(rdms, method='cosine', rdm_descriptor='index'):
+    """ calculates a noise ceiling by leave one out & full set
+
+    Args:
+        rdms(rsatoolbox.rdm.RDMs): data to calculate noise ceiling
+        method(string): comparison method to use
+        rdm_descriptor(string): descriptor to group rdms
+
+    Returns:
+        list: [lower nc-bound, upper nc-bound]
+
+    """
+    _, test_set, ceil_set = sets_leave_one_out_rdm(rdms, rdm_descriptor)
+    pred_test = pool_rdm(rdms, method=method)
+    noise_min = []
+    noise_max = []
+    for i in range(len(ceil_set)):
+        train = ceil_set[i]
+        test = test_set[i]
+        pred_train = pool_rdm(train[0], method=method)
+        noise_min.append(np.mean(compare(pred_train, test[0], method)))
+        noise_max.append(np.mean(compare(pred_test, test[0], method)))
+    noise_min = np.mean(np.array(noise_min))
+    noise_max = np.mean(np.array(noise_max))
+    return noise_min, noise_max
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/inference/result.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/inference/result.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,331 +1,333 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Result object definition
-"""
-
-import numpy as np
-import scipy.stats
-import rsatoolbox.model
-from rsatoolbox.io.hdf5 import read_dict_hdf5, write_dict_hdf5
-from rsatoolbox.io.pkl import read_dict_pkl, write_dict_pkl
-from rsatoolbox.util.file_io import remove_file
-from rsatoolbox.util.inference_util import extract_variances
-from rsatoolbox.util.inference_util import all_tests, pair_tests, nc_tests, zero_tests
-
-
-class Result:
-    """ Result class storing results for a set of models with the models,
-    the results matrix and the noise ceiling
-
-    Args:
-        models(list of rsatoolbox.model.Model):
-            the evaluated models
-        evaluations(numpy.ndarray):
-            evaluations of the models over bootstrap/crossvalidation
-            format: bootstrap_samples x models x crossval & others
-            such that np.mean(evaluations[i,j]) is a valid evaluation for the
-            jth model on the ith bootstrap-sample
-        method(String):
-            the evaluation method
-        cv_method(String):
-            crossvalidation specification
-        noise_ceiling(numpy.ndarray):
-            noise ceiling such that np.mean(noise_ceiling[0]) is the lower
-            bound and np.mean(noise_ceiling[1]) is the higher one.
-
-    Attributes:
-        as inputs
-
-    """
-
-    def __init__(self, models, evaluations, method, cv_method, noise_ceiling,
-                 variances=None, dof=1, fitter=None):
-        if isinstance(models, rsatoolbox.model.Model):
-            models = [models]
-        assert len(models) == evaluations.shape[1], 'evaluations shape does' \
-            + 'not match number of models'
-        self.models = models
-        self.n_model = len(models)
-        self.evaluations = np.array(evaluations)
-        self.method = method
-        self.cv_method = cv_method
-        self.noise_ceiling = np.array(noise_ceiling)
-        self.variances = variances
-        self.dof = dof
-        self.fitter = fitter
-        self.n_bootstraps = evaluations.shape[0]
-        if variances is not None:
-            # if the variances only refer to the models this should have the
-            # same number of entries as the models list.
-            if variances.ndim == 0:
-                nc_included = False
-            else:
-                nc_included = variances.shape[-1] != len(models)
-            self.model_var, self.diff_var, self.noise_ceil_var = \
-                extract_variances(variances, nc_included)
-        else:
-            self.model_var = None
-            self.diff_var = None
-            self.noise_ceil_var = None
-
-    def __repr__(self):
-        """ defines string which is printed for the object
-        """
-        return (f'rsatoolbox.inference.Result\n'
-                f'containing evaluations for {self.n_model} models\n'
-                f'evaluated using {self.cv_method} of {self.method}'
-                )
-
-    def __str__(self):
-        """ defines the output of print
-        """
-        return self.summary()
-
-    def summary(self, test_type='t-test'):
-        """
-        Human readable summary of the results
-
-        Args:
-            test_type(String):
-                What kind of tests to run.
-                See rsatoolbox.util.inference_util.all_tests for options
-        """
-        summary = f'Results for running {self.cv_method} evaluation for {self.method} '
-        summary += f'on {self.n_model} models:\n\n'
-        name_length = max([max(len(m.name) for m in self.models) + 1, 6])
-        means = self.get_means()
-        sems = self.get_sem()
-        if means is None:
-            means = np.nan * np.ones(self.n_model)
-        if sems is None:
-            sems = np.nan * np.ones(self.n_model)
-        try:
-            p_zero = self.test_zero(test_type=test_type)
-            p_noise = self.test_noise(test_type=test_type)
-        except ValueError:
-            p_zero = np.nan * np.ones(self.n_model)
-            p_noise = np.nan * np.ones(self.n_model)
-        # header of the results table
-        summary += 'Model' + (' ' * (name_length - 5))
-        summary += '|   Eval \u00B1 SEM   |'
-        summary += ' p (against 0) |'
-        summary += ' p (against NC) |\n'
-        summary += '-' * (name_length + 51)
-        summary += '\n'
-        for i, m in enumerate(self.models):
-            summary += m.name + (' ' * (name_length - len(m.name)))
-            summary += f'| {means[i]: 5.3f} \u00B1 {sems[i]:4.3f} |'
-            if p_zero[i] < 0.001:
-                summary += '      < 0.001  |'
-            else:
-                summary += f'{p_zero[i]:>13.3f}  |'
-            if p_noise[i] < 0.001:
-                summary += '       < 0.001  |'
-            else:
-                summary += f'{p_noise[i]:>14.3f}  |'
-            summary += '\n'
-        summary += '\n'
-        if self.cv_method == 'crossvalidation':
-            summary += 'No p-values available as crossvalidation provides no variance estimate'
-        elif test_type == 't-test':
-            summary += 'p-values are based on uncorrected t-tests'
-        elif test_type == 'bootstrap':
-            summary += 'p-values are based on percentiles of the bootstrap samples'
-        elif test_type == 'ranksum':
-            summary += 'p-values are based on ranksum tests'
-        return summary
-
-    def save(self, filename, file_type='hdf5', overwrite=False):
-        """ saves the results into a file.
-
-        Args:
-            filename(String): path to the file
-                [or opened file]
-            file_type(String): Type of file to create:
-                hdf5: hdf5 file
-                pkl: pickle file
-            overwrite(Boolean): overwrites file if it already exists
-
-        """
-        result_dict = self.to_dict()
-        if overwrite:
-            remove_file(filename)
-        if file_type == 'hdf5':
-            write_dict_hdf5(filename, result_dict)
-        elif file_type == 'pkl':
-            write_dict_pkl(filename, result_dict)
-
-    def to_dict(self):
-        """ Converts the RDMs object into a dict, which can be used for saving
-
-        Returns:
-            results_dict(dict): A dictionary with all the information needed
-                to regenerate the object
-
-        """
-        result_dict = {}
-        result_dict['evaluations'] = self.evaluations
-        result_dict['dof'] = self.dof
-        result_dict['variances'] = self.variances
-        result_dict['noise_ceiling'] = self.noise_ceiling
-        result_dict['method'] = self.method
-        result_dict['cv_method'] = self.cv_method
-        result_dict['models'] = {}
-        for i_model in range(len(self.models)):
-            key = 'model_%d' % i_model
-            result_dict['models'][key] = self.models[i_model].to_dict()
-        return result_dict
-
-    def test_all(self, test_type='t-test'):
-        """ returns all p-values: p_pairwise, p_zero & p_noise
-
-        Args:
-            test_type(String):
-                What kind of tests to run.
-                See rsatoolbox.util.inference_util.all_tests for options
-        """
-        p_pairwise, p_zero, p_noise = all_tests(
-            self.evaluations, self.noise_ceiling, test_type,
-            model_var=self.model_var, diff_var=self.diff_var,
-            noise_ceil_var=self.noise_ceil_var, dof=self.dof)
-        return p_pairwise, p_zero, p_noise
-
-    def test_pairwise(self, test_type='t-test'):
-        """returns the pairwise test p-values """
-        return pair_tests(self.evaluations, test_type, self.diff_var, self.dof)
-
-    def test_zero(self, test_type='t-test'):
-        """returns the p-values for the tests against 0 """
-        return zero_tests(self.evaluations, test_type, self.model_var, self.dof)
-
-    def test_noise(self, test_type='t-test'):
-        """returns the p-values for the tests against the noise ceiling"""
-        return nc_tests(self.evaluations, self.noise_ceiling,
-                        test_type, self.noise_ceil_var, self.dof)
-
-    def get_means(self):
-        """ returns the mean evaluations per model """
-        if self.cv_method == 'fixed':
-            perf = np.mean(self.evaluations, axis=0)
-            perf = np.nanmean(perf, axis=-1)
-        elif self.cv_method == 'crossvalidation':
-            perf = np.mean(self.evaluations, axis=0)
-            perf = np.nanmean(perf, axis=-1)
-        else:
-            perf = self.evaluations
-            while len(perf.shape) > 2:
-                perf = np.nanmean(perf, axis=-1)
-            perf = perf[~np.isnan(perf[:, 0])]
-            perf = np.mean(perf, axis=0)
-        return perf
-
-    def get_sem(self):
-        """ returns the SEM of the evaluation per model """
-        if self.model_var is None:
-            return None
-        return np.sqrt(np.maximum(self.model_var, 0))
-
-    def get_ci(self, ci_percent, test_type='t-test'):
-        """ returns confidence intervals for the evaluations"""
-        prop_cut = (1 - ci_percent) / 2
-        if test_type == 'bootstrap':
-            perf = self.evaluations
-            while len(perf.shape) > 2:
-                perf = np.nanmean(perf, axis=-1)
-            framed_evals = np.concatenate(
-                (np.tile(np.array(([-np.inf], [np.inf])),
-                         (1, self.n_model)),
-                 perf),
-                axis=0)
-            ci = [np.quantile(framed_evals, prop_cut, axis=0),
-                  np.quantile(framed_evals, 1 - prop_cut, axis=0)]
-        else:
-            tdist = scipy.stats.t
-            std_eval = self.get_sem()
-            means = self.get_means()
-            ci = [means + std_eval * tdist.ppf(prop_cut, self.dof),
-                  means - std_eval * tdist.ppf(prop_cut, self.dof)]
-        return ci
-
-    def get_errorbars(self, eb_type='sem', test_type='t-test'):
-        """ returns errorbars for the model evaluations"""
-        if eb_type.lower() == 'sem':
-            errorbar_low = self.get_sem()
-            errorbar_high = errorbar_low
-        elif eb_type[0:2].lower() == 'ci':
-            if len(eb_type) == 2:
-                ci_percent = 0.95
-            else:
-                ci_percent = float(eb_type[2:]) / 100
-            ci = self.get_ci(ci_percent, test_type)
-            means = self.get_means()
-            errorbar_low = means - ci[0]
-            errorbar_high = ci[1] - means
-            limits = np.concatenate((errorbar_low, errorbar_high))
-            if np.isnan(limits).any() or (abs(limits) == np.inf).any():
-                raise ValueError(
-                    'plot_model_comparison: Too few bootstrap samples for ' +
-                    'the requested confidence interval: ' + eb_type + '.')
-        return (errorbar_low, errorbar_high)
-
-    def get_model_var(self):
-        """ returns the variance of the evaluation per model """
-        return self.model_var
-
-    def get_noise_ceil(self):
-        """ returns the noise ceiling for the model evaluations """
-        return self.noise_ceiling
-
-
-def load_results(filename, file_type=None):
-    """ loads a Result object from disc
-
-    Args:
-        filename(String): path to the filelocation
-
-    """
-    if file_type is None:
-        if isinstance(filename, str):
-            if filename[-4:] == '.pkl':
-                file_type = 'pkl'
-            elif filename[-3:] == '.h5' or filename[-4:] == 'hdf5':
-                file_type = 'hdf5'
-    if file_type == 'hdf5':
-        data_dict = read_dict_hdf5(filename)
-    elif file_type == 'pkl':
-        data_dict = read_dict_pkl(filename)
-    else:
-        raise ValueError('filetype not understood')
-    return result_from_dict(data_dict)
-
-
-def result_from_dict(result_dict):
-    """ recreate Results object from dictionary
-
-    Args:
-        result_dict(dict): dictionary to regenerate
-
-    Returns:
-        result(Result): the recreated object
-
-    """
-    if 'variances' in result_dict.keys():
-        variances = result_dict['variances']
-    else:
-        variances = None
-    if 'dof' in result_dict.keys():
-        dof = result_dict['dof']
-    else:
-        dof = None
-    evaluations = result_dict['evaluations']
-    method = result_dict['method']
-    cv_method = result_dict['cv_method']
-    noise_ceiling = result_dict['noise_ceiling']
-    models = [None] * len(result_dict['models'])
-    for i_model in range(len(result_dict['models'])):
-        key = 'model_%d' % i_model
-        models[i_model] = rsatoolbox.model.model_from_dict(
-            result_dict['models'][key])
-    return Result(models, evaluations, method, cv_method, noise_ceiling,
-                  variances=variances, dof=dof)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Result object definition
+"""
+
+import numpy as np
+import scipy.stats
+import rsatoolbox.model
+from rsatoolbox.io.hdf5 import read_dict_hdf5, write_dict_hdf5
+from rsatoolbox.io.pkl import read_dict_pkl, write_dict_pkl
+from rsatoolbox.util.file_io import remove_file
+from rsatoolbox.util.inference_util import extract_variances
+from rsatoolbox.util.inference_util import all_tests, pair_tests, nc_tests, zero_tests
+
+
+class Result:
+    """ Result class storing results for a set of models with the models,
+    the results matrix and the noise ceiling
+
+    Args:
+        models(list of rsatoolbox.model.Model):
+            the evaluated models
+        evaluations(numpy.ndarray):
+            evaluations of the models over bootstrap/crossvalidation
+            format: bootstrap_samples x models x crossval & others
+            such that np.mean(evaluations[i,j]) is a valid evaluation for the
+            jth model on the ith bootstrap-sample
+        method(String):
+            the evaluation method
+        cv_method(String):
+            crossvalidation specification
+        noise_ceiling(numpy.ndarray):
+            noise ceiling such that np.mean(noise_ceiling[0]) is the lower
+            bound and np.mean(noise_ceiling[1]) is the higher one.
+
+    Attributes:
+        as inputs
+
+    """
+
+    def __init__(self, models, evaluations, method, cv_method, noise_ceiling,
+                 variances=None, dof=1, fitter=None, n_rdm=None, n_pattern=None):
+        if isinstance(models, rsatoolbox.model.Model):
+            models = [models]
+        assert len(models) == evaluations.shape[1], 'evaluations shape does' \
+            + 'not match number of models'
+        self.models = models
+        self.n_model = len(models)
+        self.evaluations = np.array(evaluations)
+        self.method = method
+        self.cv_method = cv_method
+        self.noise_ceiling = np.array(noise_ceiling)
+        self.variances = variances
+        self.dof = dof
+        self.fitter = fitter
+        self.n_bootstraps = evaluations.shape[0]
+        self.n_rdm = n_rdm
+        self.n_pattern = n_pattern
+        if variances is not None:
+            # if the variances only refer to the models this should have the
+            # same number of entries as the models list.
+            if variances.ndim == 0:
+                nc_included = False
+            else:
+                nc_included = variances.shape[-1] != len(models)
+            self.model_var, self.diff_var, self.noise_ceil_var = \
+                extract_variances(variances, nc_included, n_rdm, n_pattern)
+        else:
+            self.model_var = None
+            self.diff_var = None
+            self.noise_ceil_var = None
+
+    def __repr__(self):
+        """ defines string which is printed for the object
+        """
+        return (f'rsatoolbox.inference.Result\n'
+                f'containing evaluations for {self.n_model} models\n'
+                f'evaluated using {self.cv_method} of {self.method}'
+                )
+
+    def __str__(self):
+        """ defines the output of print
+        """
+        return self.summary()
+
+    def summary(self, test_type='t-test'):
+        """
+        Human readable summary of the results
+
+        Args:
+            test_type(String):
+                What kind of tests to run.
+                See rsatoolbox.util.inference_util.all_tests for options
+        """
+        summary = f'Results for running {self.cv_method} evaluation for {self.method} '
+        summary += f'on {self.n_model} models:\n\n'
+        name_length = max([max(len(m.name) for m in self.models) + 1, 6])
+        means = self.get_means()
+        sems = self.get_sem()
+        if means is None:
+            means = np.nan * np.ones(self.n_model)
+        if sems is None:
+            sems = np.nan * np.ones(self.n_model)
+        try:
+            p_zero = self.test_zero(test_type=test_type)
+            p_noise = self.test_noise(test_type=test_type)
+        except ValueError:
+            p_zero = np.nan * np.ones(self.n_model)
+            p_noise = np.nan * np.ones(self.n_model)
+        # header of the results table
+        summary += 'Model' + (' ' * (name_length - 5))
+        summary += '|   Eval \u00B1 SEM   |'
+        summary += ' p (against 0) |'
+        summary += ' p (against NC) |\n'
+        summary += '-' * (name_length + 51)
+        summary += '\n'
+        for i, m in enumerate(self.models):
+            summary += m.name + (' ' * (name_length - len(m.name)))
+            summary += f'| {means[i]: 5.3f} \u00B1 {sems[i]:4.3f} |'
+            if p_zero[i] < 0.001:
+                summary += '      < 0.001  |'
+            else:
+                summary += f'{p_zero[i]:>13.3f}  |'
+            if p_noise[i] < 0.001:
+                summary += '       < 0.001  |'
+            else:
+                summary += f'{p_noise[i]:>14.3f}  |'
+            summary += '\n'
+        summary += '\n'
+        if self.cv_method == 'crossvalidation':
+            summary += 'No p-values available as crossvalidation provides no variance estimate'
+        elif test_type == 't-test':
+            summary += 'p-values are based on uncorrected t-tests'
+        elif test_type == 'bootstrap':
+            summary += 'p-values are based on percentiles of the bootstrap samples'
+        elif test_type == 'ranksum':
+            summary += 'p-values are based on ranksum tests'
+        return summary
+
+    def save(self, filename, file_type='hdf5', overwrite=False):
+        """ saves the results into a file.
+
+        Args:
+            filename(String): path to the file
+                [or opened file]
+            file_type(String): Type of file to create:
+                hdf5: hdf5 file
+                pkl: pickle file
+            overwrite(Boolean): overwrites file if it already exists
+
+        """
+        result_dict = self.to_dict()
+        if overwrite:
+            remove_file(filename)
+        if file_type == 'hdf5':
+            write_dict_hdf5(filename, result_dict)
+        elif file_type == 'pkl':
+            write_dict_pkl(filename, result_dict)
+
+    def to_dict(self):
+        """ Converts the RDMs object into a dict, which can be used for saving
+
+        Returns:
+            results_dict(dict): A dictionary with all the information needed
+                to regenerate the object
+
+        """
+        result_dict = {}
+        result_dict['evaluations'] = self.evaluations
+        result_dict['dof'] = self.dof
+        result_dict['variances'] = self.variances
+        result_dict['noise_ceiling'] = self.noise_ceiling
+        result_dict['method'] = self.method
+        result_dict['cv_method'] = self.cv_method
+        result_dict['models'] = {}
+        for i_model in range(len(self.models)):
+            key = 'model_%d' % i_model
+            result_dict['models'][key] = self.models[i_model].to_dict()
+        return result_dict
+
+    def test_all(self, test_type='t-test'):
+        """ returns all p-values: p_pairwise, p_zero & p_noise
+
+        Args:
+            test_type(String):
+                What kind of tests to run.
+                See rsatoolbox.util.inference_util.all_tests for options
+        """
+        p_pairwise, p_zero, p_noise = all_tests(
+            self.evaluations, self.noise_ceiling, test_type,
+            model_var=self.model_var, diff_var=self.diff_var,
+            noise_ceil_var=self.noise_ceil_var, dof=self.dof)
+        return p_pairwise, p_zero, p_noise
+
+    def test_pairwise(self, test_type='t-test'):
+        """returns the pairwise test p-values """
+        return pair_tests(self.evaluations, test_type, self.diff_var, self.dof)
+
+    def test_zero(self, test_type='t-test'):
+        """returns the p-values for the tests against 0 """
+        return zero_tests(self.evaluations, test_type, self.model_var, self.dof)
+
+    def test_noise(self, test_type='t-test'):
+        """returns the p-values for the tests against the noise ceiling"""
+        return nc_tests(self.evaluations, self.noise_ceiling,
+                        test_type, self.noise_ceil_var, self.dof)
+
+    def get_means(self):
+        """ returns the mean evaluations per model """
+        if self.cv_method == 'fixed':
+            perf = np.mean(self.evaluations, axis=0)
+            perf = np.nanmean(perf, axis=-1)
+        elif self.cv_method == 'crossvalidation':
+            perf = np.mean(self.evaluations, axis=0)
+            perf = np.nanmean(perf, axis=-1)
+        else:
+            perf = self.evaluations
+            while len(perf.shape) > 2:
+                perf = np.nanmean(perf, axis=-1)
+            perf = perf[~np.isnan(perf[:, 0])]
+            perf = np.mean(perf, axis=0)
+        return perf
+
+    def get_sem(self):
+        """ returns the SEM of the evaluation per model """
+        if self.model_var is None:
+            return None
+        return np.sqrt(np.maximum(self.model_var, 0))
+
+    def get_ci(self, ci_percent, test_type='t-test'):
+        """ returns confidence intervals for the evaluations"""
+        prop_cut = (1 - ci_percent) / 2
+        if test_type == 'bootstrap':
+            perf = self.evaluations
+            while len(perf.shape) > 2:
+                perf = np.nanmean(perf, axis=-1)
+            framed_evals = np.concatenate(
+                (np.tile(np.array(([-np.inf], [np.inf])),
+                         (1, self.n_model)),
+                 perf),
+                axis=0)
+            ci = [np.quantile(framed_evals, prop_cut, axis=0),
+                  np.quantile(framed_evals, 1 - prop_cut, axis=0)]
+        else:
+            tdist = scipy.stats.t
+            std_eval = self.get_sem()
+            means = self.get_means()
+            ci = [means + std_eval * tdist.ppf(prop_cut, self.dof),
+                  means - std_eval * tdist.ppf(prop_cut, self.dof)]
+        return ci
+
+    def get_errorbars(self, eb_type='sem', test_type='t-test'):
+        """ returns errorbars for the model evaluations"""
+        if eb_type.lower() == 'sem':
+            errorbar_low = self.get_sem()
+            errorbar_high = errorbar_low
+        elif eb_type[0:2].lower() == 'ci':
+            if len(eb_type) == 2:
+                ci_percent = 0.95
+            else:
+                ci_percent = float(eb_type[2:]) / 100
+            ci = self.get_ci(ci_percent, test_type)
+            means = self.get_means()
+            errorbar_low = means - ci[0]
+            errorbar_high = ci[1] - means
+            limits = np.concatenate((errorbar_low, errorbar_high))
+            if np.isnan(limits).any() or (abs(limits) == np.inf).any():
+                raise ValueError(
+                    'plot_model_comparison: Too few bootstrap samples for ' +
+                    'the requested confidence interval: ' + eb_type + '.')
+        return (errorbar_low, errorbar_high)
+
+    def get_model_var(self):
+        """ returns the variance of the evaluation per model """
+        return self.model_var
+
+    def get_noise_ceil(self):
+        """ returns the noise ceiling for the model evaluations """
+        return self.noise_ceiling
+
+
+def load_results(filename, file_type=None):
+    """ loads a Result object from disc
+
+    Args:
+        filename(String): path to the filelocation
+
+    """
+    if file_type is None:
+        if isinstance(filename, str):
+            if filename[-4:] == '.pkl':
+                file_type = 'pkl'
+            elif filename[-3:] == '.h5' or filename[-4:] == 'hdf5':
+                file_type = 'hdf5'
+    if file_type == 'hdf5':
+        data_dict = read_dict_hdf5(filename)
+    elif file_type == 'pkl':
+        data_dict = read_dict_pkl(filename)
+    else:
+        raise ValueError('filetype not understood')
+    return result_from_dict(data_dict)
+
+
+def result_from_dict(result_dict):
+    """ recreate Results object from dictionary
+
+    Args:
+        result_dict(dict): dictionary to regenerate
+
+    Returns:
+        result(Result): the recreated object
+
+    """
+    if 'variances' in result_dict.keys():
+        variances = result_dict['variances']
+    else:
+        variances = None
+    if 'dof' in result_dict.keys():
+        dof = result_dict['dof']
+    else:
+        dof = None
+    evaluations = result_dict['evaluations']
+    method = result_dict['method']
+    cv_method = result_dict['cv_method']
+    noise_ceiling = result_dict['noise_ceiling']
+    models = [None] * len(result_dict['models'])
+    for i_model in range(len(result_dict['models'])):
+        key = 'model_%d' % i_model
+        models[i_model] = rsatoolbox.model.model_from_dict(
+            result_dict['models'][key])
+    return Result(models, evaluations, method, cv_method, noise_ceiling,
+                  variances=variances, dof=dof)
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/hdf5.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/hdf5.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,118 +1,118 @@
-"""
-saving to and reading from HDF5 files
-"""
-from __future__ import annotations
-from typing import Union, Dict, List, IO
-import os
-from collections.abc import Iterable
-try:  # drop:py37 (backport)
-    from importlib.metadata import version
-except ModuleNotFoundError:
-    from importlib_metadata import version
-from h5py import File, Group, Empty
-import numpy as np
-
-
-def write_dict_hdf5(fhandle: Union[str, IO], dictionary: Dict) -> None:
-    """ writes a nested dictionary containing strings & arrays as data into
-    a hdf5 file
-
-    Args:
-        file: a filename or opened writable file
-        dictionary(dict): the dict to be saved
-
-    """
-    if isinstance(fhandle, str):
-        if os.path.exists(fhandle):
-            raise ValueError('File already exists!')
-    file = File(fhandle, 'a')
-    file.attrs['rsatoolbox_version'] = version('rsatoolbox')
-    _write_to_group(file, dictionary)
-
-
-def _write_to_group(group: Group, dictionary: Dict) -> None:
-    """ writes a dictionary to a hdf5 group, which can recurse"""
-    for key in dictionary.keys():
-        value = dictionary[key]
-        if isinstance(value, str):
-            # needs another conversion to string to catch weird subtypes
-            # like numpy.str_
-            group.attrs[key] = str(value)
-        elif isinstance(value, np.ndarray):
-            if str(value.dtype)[:2] == '<U':
-                group[key] = value.astype('S')
-            else:
-                group[key] = value
-        elif isinstance(value, list):
-            _write_list(group, key, value)
-        elif isinstance(value, dict):
-            subgroup = group.create_group(key)
-            _write_to_group(subgroup, value)
-        elif value is None:
-            group[key] = Empty("f")
-        elif isinstance(value, Iterable):
-            if isinstance(value[0], str):
-                group.attrs[key] = value
-        else:
-            group[key] = value
-
-
-def _write_list(group: Group, key: str, value: List) -> None:
-    """
-    writes a list to a hdf5 file. First tries conversion to np.array.
-    If this fails the list is converted to a dict with integer keys.
-
-    Parameters
-    ----------
-    group : hdf5 group
-        where to write.
-    key :  hdf5 key
-    value : list
-        list to be written
-    """
-    try:
-        value = np.array(value)
-        if str(value.dtype)[:2] == '<U':
-            group[key] = value.astype('S')
-        else:
-            group[key] = value
-    except TypeError:
-        l_group = group.create_group(key)
-        for i, v in enumerate(value):
-            l_group[str(i)] = v
-
-
-def read_dict_hdf5(fhandle: Union[str, IO]) -> Dict:
-    """ writes a nested dictionary containing strings & arrays as data into
-    a hdf5 file
-
-    Args:
-        file: a filename or opened readable file
-
-    Returns:
-        dictionary(dict): the loaded dict
-
-    """
-    file = File(fhandle, 'r')
-    return _read_group(file)
-
-
-def _read_group(group: Group) -> Dict:
-    """ reads a group from a hdf5 file into a dict, which allows recursion"""
-    dictionary = {}
-    for key in group.keys():
-        sub_val = group[key]
-        if isinstance(sub_val, Group):
-            dictionary[key] = _read_group(sub_val)
-        elif sub_val.shape is None:
-            dictionary[key] = None
-        else:
-            dictionary[key] = np.array(sub_val)
-            if dictionary[key].dtype.type is np.string_:
-                dictionary[key] = np.array(sub_val).astype('unicode')
-            # if (len(dictionary[key].shape) == 1
-            #     and dictionary[key].shape[0] == 1):
-            #     dictionary[key] = dictionary[key][0]
-    for key in group.attrs.keys():
-        dictionary[key] = group.attrs[key]
-    return dictionary
+"""
+saving to and reading from HDF5 files
+"""
+from __future__ import annotations
+from typing import Union, Dict, List, IO
+import os
+from collections.abc import Iterable
+try:  # drop:py37 (backport)
+    from importlib.metadata import version
+except ModuleNotFoundError:
+    from importlib_metadata import version
+from h5py import File, Group, Empty
+import numpy as np
+
+
+def write_dict_hdf5(fhandle: Union[str, IO], dictionary: Dict) -> None:
+    """ writes a nested dictionary containing strings & arrays as data into
+    a hdf5 file
+
+    Args:
+        file: a filename or opened writable file
+        dictionary(dict): the dict to be saved
+
+    """
+    if isinstance(fhandle, str):
+        if os.path.exists(fhandle):
+            raise ValueError('File already exists!')
+    file = File(fhandle, 'a')
+    file.attrs['rsatoolbox_version'] = version('rsatoolbox')
+    _write_to_group(file, dictionary)
+
+
+def _write_to_group(group: Group, dictionary: Dict) -> None:
+    """ writes a dictionary to a hdf5 group, which can recurse"""
+    for key in dictionary.keys():
+        value = dictionary[key]
+        if isinstance(value, str):
+            # needs another conversion to string to catch weird subtypes
+            # like numpy.str_
+            group.attrs[key] = str(value)
+        elif isinstance(value, np.ndarray):
+            if str(value.dtype)[:2] == '<U':
+                group[key] = value.astype('S')
+            else:
+                group[key] = value
+        elif isinstance(value, list):
+            _write_list(group, key, value)
+        elif isinstance(value, dict):
+            subgroup = group.create_group(key)
+            _write_to_group(subgroup, value)
+        elif value is None:
+            group[key] = Empty("f")
+        elif isinstance(value, Iterable):
+            if isinstance(value[0], str):
+                group.attrs[key] = value
+        else:
+            group[key] = value
+
+
+def _write_list(group: Group, key: str, value: List) -> None:
+    """
+    writes a list to a hdf5 file. First tries conversion to np.array.
+    If this fails the list is converted to a dict with integer keys.
+
+    Parameters
+    ----------
+    group : hdf5 group
+        where to write.
+    key :  hdf5 key
+    value : list
+        list to be written
+    """
+    try:
+        value = np.array(value)
+        if str(value.dtype)[:2] == '<U':
+            group[key] = value.astype('S')
+        else:
+            group[key] = value
+    except TypeError:
+        l_group = group.create_group(key)
+        for i, v in enumerate(value):
+            l_group[str(i)] = v
+
+
+def read_dict_hdf5(fhandle: Union[str, IO]) -> Dict:
+    """ writes a nested dictionary containing strings & arrays as data into
+    a hdf5 file
+
+    Args:
+        file: a filename or opened readable file
+
+    Returns:
+        dictionary(dict): the loaded dict
+
+    """
+    file = File(fhandle, 'r')
+    return _read_group(file)
+
+
+def _read_group(group: Group) -> Dict:
+    """ reads a group from a hdf5 file into a dict, which allows recursion"""
+    dictionary = {}
+    for key in group.keys():
+        sub_val = group[key]
+        if isinstance(sub_val, Group):
+            dictionary[key] = _read_group(sub_val)
+        elif sub_val.shape is None:
+            dictionary[key] = None
+        else:
+            dictionary[key] = np.array(sub_val)
+            if dictionary[key].dtype.type is np.string_:
+                dictionary[key] = np.array(sub_val).astype('unicode')
+            # if (len(dictionary[key].shape) == 1
+            #     and dictionary[key].shape[0] == 1):
+            #     dictionary[key] = dictionary[key][0]
+    for key in group.attrs.keys():
+        dictionary[key] = group.attrs[key]
+    return dictionary
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/io/pkl.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/io/pkl.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,43 +1,43 @@
-"""
-saving to and reading from pickle files
-"""
-from __future__ import annotations
-from typing import Union, Dict, IO
-try:  # drop:py37 (backport)
-    from importlib.metadata import version
-except ModuleNotFoundError:
-    from importlib_metadata import version
-import pickle
-
-
-def write_dict_pkl(fhandle: Union[str, IO], dictionary: Dict) -> None:
-    """ writes a nested dictionary containing strings & arrays as data into
-    a pickle file
-
-    Args:
-        file: a filename or opened writable file
-        dictionary(dict): the dict to be saved
-
-    """
-    if isinstance(fhandle, str):
-        fhandle = open(fhandle, 'wb')
-    dictionary['rsatoolbox_version'] = version('rsatoolbox')
-    pickle.dump(dictionary, fhandle, protocol=-1)
-
-
-def read_dict_pkl(fhandle: Union[str, IO]) -> Dict:
-    """ writes a nested dictionary containing strings & arrays as data into
-    a pickle file
-
-    Args:
-        file: a filename or opened readable file
-
-    Returns:
-        dictionary(dict): the loaded dict
-
-
-    """
-    if isinstance(fhandle, str):
-        fhandle = open(fhandle, 'rb')
-    data = pickle.load(fhandle)
-    return data
+"""
+saving to and reading from pickle files
+"""
+from __future__ import annotations
+from typing import Union, Dict, IO
+try:  # drop:py37 (backport)
+    from importlib.metadata import version
+except ModuleNotFoundError:
+    from importlib_metadata import version
+import pickle
+
+
+def write_dict_pkl(fhandle: Union[str, IO], dictionary: Dict) -> None:
+    """ writes a nested dictionary containing strings & arrays as data into
+    a pickle file
+
+    Args:
+        file: a filename or opened writable file
+        dictionary(dict): the dict to be saved
+
+    """
+    if isinstance(fhandle, str):
+        fhandle = open(fhandle, 'wb')
+    dictionary['rsatoolbox_version'] = version('rsatoolbox')
+    pickle.dump(dictionary, fhandle, protocol=-1)
+
+
+def read_dict_pkl(fhandle: Union[str, IO]) -> Dict:
+    """ writes a nested dictionary containing strings & arrays as data into
+    a pickle file
+
+    Args:
+        file: a filename or opened readable file
+
+    Returns:
+        dictionary(dict): the loaded dict
+
+
+    """
+    if isinstance(fhandle, str):
+        fhandle = open(fhandle, 'rb')
+    data = pickle.load(fhandle)
+    return data
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/model/fitter.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/model/fitter.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,481 +1,481 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Parameter fitting methods for models
-"""
-
-import numpy as np
-import scipy.optimize as opt
-import scipy.sparse
-from rsatoolbox.rdm import compare
-from rsatoolbox.util.matrix import get_v
-from rsatoolbox.util.pooling import pool_rdm
-from rsatoolbox.util.rdm_utils import _parse_nan_vectors
-
-
-class Fitter:
-    """Object to specify a fitting function and parameters
-
-    Effectively this gives the user a convenient way to specify fitting
-    functions with different settings than the defaults.
-
-    Create this object with the fitting function to use and additional
-    keyword arguments for settings you wish to change. The resulting
-    object then behaves as the fitting function itself with the
-    keyword arguments set to the different values provided at object
-    creation.
-
-    Example:
-        generate Fitter-object for ridge regression:
-
-        ::
-
-            fit = pyrsa.model.Fitter(pyrsa.model.fit_regress, ridge_weight=1)
-
-        the resulting object 'fit' now does the same as
-        pyrsa.model.fit_regress when run with the additional argument
-        ``ridge_weight=1``, i.e. the following two lines now yield equal results:
-
-        ::
-
-            fit(model, data)
-            pyrsa.model.fit_regress(model, data, ridge_weight=1)
-
-    For a general introduction to flexible models see demo_flex.
-    """
-
-    def __init__(self, fit_fun, **kwargs):
-        self.fit_fun = fit_fun
-        self.kwargs = kwargs
-
-    def __call__(self, model, data, *args, **more_args):
-        return self.fit_fun(model, data, *args, **more_args, **self.kwargs)
-
-
-def fit_mock(model, data, method='cosine', pattern_idx=None,
-             pattern_descriptor=None, sigma_k=None):
-    """ formally acceptable fitting method which always returns a vector of
-    zeros
-
-    Args:
-        model(rsatoolbox.model.Model): model to be fit
-        data(rsatoolbox.rdm.RDMs): Data to fit to
-        method(String): Evaluation method
-        pattern_idx(numpy.ndarray): Which patterns are sampled
-        pattern_descriptor(String): Which descriptor is used
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        theta(numpy.ndarray): parameter vector
-
-    """
-    return np.zeros(model.n_param)
-
-
-def fit_select(model, data, method='cosine', pattern_idx=None,
-               pattern_descriptor=None, sigma_k=None):
-    """ fits selection models by evaluating each rdm and selcting the one
-    with best performance. Works only for ModelSelect
-
-    Args:
-        model(rsatoolbox.model.Model): model to be fit
-        data(rsatoolbox.rdm.RDMs): Data to fit to
-        method(String): Evaluation method
-        pattern_idx(numpy.ndarray): Which patterns are sampled
-        pattern_descriptor(String): Which descriptor is used
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        theta(int): parameter vector
-
-    """
-    evaluations = np.zeros(model.n_rdm)
-    for i_rdm in range(model.n_rdm):
-        pred = model.predict_rdm(i_rdm)
-        if not (pattern_idx is None or pattern_descriptor is None):
-            pred = pred.subsample_pattern(pattern_descriptor, pattern_idx)
-        evaluations[i_rdm] = np.mean(
-            compare(pred, data, method=method, sigma_k=sigma_k))
-    theta = np.argmax(evaluations)
-    return theta
-
-
-def fit_optimize(model, data, method='cosine', pattern_idx=None,
-                 pattern_descriptor=None, sigma_k=None, ridge_weight=0):
-    """
-    fitting theta using optimization
-    currently allowed for ModelWeighted only
-
-    Args:
-        model(Model): the model to be fit
-        data(rsatoolbox.rdm.RDMs): data to be fit
-        method(String, optional): evaluation metric The default is 'cosine'.
-        pattern_idx(numpy.ndarray, optional)
-            sampled patterns The default is None.
-        pattern_descriptor (String, optional)
-            descriptor used for fitting. The default is None.
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        numpy.ndarray: theta, parameter vector for the model
-
-    """
-    def _loss_opt(theta):
-        return _loss(theta, model, data, method=method,
-                     pattern_idx=pattern_idx,
-                     pattern_descriptor=pattern_descriptor,
-                     sigma_k=sigma_k, ridge_weight=ridge_weight)
-    thetas = []
-    losses = []
-    for _ in range(2 * model.n_param):
-        theta0 = np.random.rand(model.n_param)
-        theta = opt.minimize(
-            _loss_opt,
-            theta0,
-            method='BFGS',
-            tol=0.000001
-        )
-        thetas.append(theta.x)
-        losses.append(theta.fun)
-    id = np.argmin(losses)
-    theta = thetas[id]
-    norm = np.sum(theta ** 2)
-    if norm == 0:
-        return theta.flatten()
-    return theta.flatten() / np.sqrt(norm)
-
-
-def fit_optimize_positive(
-        model, data, method='cosine', pattern_idx=None,
-        pattern_descriptor=None, sigma_k=None, ridge_weight=0):
-    """
-    fitting theta using optimization enforcing positive weights
-    currently allowed for ModelWeighted only
-
-    Args:
-        model(Model): the model to be fit
-        data(pyrsa.rdm.RDMs): data to be fit
-        method(String, optional): evaluation metric The default is 'cosine'.
-        pattern_idx(numpy.ndarray, optional)
-            sampled patterns The default is None.
-        pattern_descriptor (String, optional)
-            descriptor used for fitting. The default is None.
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        numpy.ndarray: theta, parameter vector for the model
-
-    """
-    def _loss_opt(theta):
-        return _loss(theta ** 2, model, data, method=method,
-                     pattern_idx=pattern_idx,
-                     pattern_descriptor=pattern_descriptor,
-                     sigma_k=sigma_k, ridge_weight=ridge_weight)
-    theta0 = np.zeros(model.n_param)
-    thetas = [theta0]
-    losses = [_loss_opt(theta0)]
-    theta0 = np.random.rand(model.n_param)
-    theta = opt.minimize(
-        fun=_loss_opt,
-        x0=theta0,
-        method='BFGS',
-        tol=0.000001
-    )
-    thetas.append(theta.x)
-    losses.append(theta.fun)
-    for i in range(model.n_param):
-        theta0 = np.ones(model.n_param) * 0.001
-        theta0[i] = 1
-        theta = opt.minimize(
-            fun=_loss_opt,
-            x0=theta0,
-            method='BFGS',
-            tol=0.000001
-        )
-        thetas.append(theta.x)
-        losses.append(theta.fun)
-    id = np.argmin(losses)
-    theta = thetas[id] ** 2
-    norm = np.sum(theta ** 2)
-    if norm == 0:
-        return theta.flatten()
-    return theta.flatten() / np.sqrt(norm)
-
-
-def fit_interpolate(model, data, method='cosine', pattern_idx=None,
-                    pattern_descriptor=None, sigma_k=None):
-    """
-    fitting theta using bisection optimization
-    allowed for ModelInterpolate only
-
-    Args:
-        model(Model): the model to be fit
-        data(rsatoolbox.rdm.RDMs): data to be fit
-        method(String, optional): evaluation metric The default is 'cosine'.
-        pattern_idx(numpy.ndarray, optional)
-            sampled patterns The default is None.
-        pattern_descriptor (String, optional)
-            descriptor used for fitting. The default is None.
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        numpy.ndarray: theta, parameter vector for the model
-
-    """
-    results = []
-    for i_pair in range(model.n_rdm - 1):
-        def loss_opt(w):
-            theta = np.zeros(model.n_param)
-            theta[i_pair] = w
-            theta[i_pair + 1] = 1 - w
-            return _loss(theta, model, data, method=method,
-                         pattern_idx=pattern_idx,
-                         pattern_descriptor=pattern_descriptor,
-                         sigma_k=sigma_k)
-        results.append(
-            opt.minimize_scalar(loss_opt, np.array([.5]),
-                                method='bounded', bounds=(0, 1)))
-    losses = [r.fun for r in results]
-    i_pair = np.argmin(losses)
-    result = results[i_pair]
-    theta = np.zeros(model.n_rdm)
-    theta[i_pair] = result.x
-    theta[i_pair + 1] = 1 - result.x
-    return theta
-
-
-def fit_regress(model, data, method='cosine', pattern_idx=None,
-                pattern_descriptor=None, ridge_weight=0, sigma_k=None):
-    """
-    fitting theta using linear algebra solutions to the OLS problem
-    allowed for ModelWeighted only
-    This method first normalizes the data and model RDMs appropriately
-    for the measure to be optimized. For 'cosine' similarity this is a
-    normalization of the data-RDMs to vector length 1. For correlation
-    the mean is removed from both model and data rdms additionally.
-    Then the parameters are estimated using ordinary least squares.
-
-    Args:
-        model(Model): the model to be fit
-        data(pyrsa.rdm.RDMs): data to be fit
-        method(String, optional): evaluation metric The default is 'cosine'.
-        pattern_idx(numpy.ndarray, optional)
-            sampled patterns The default is None.
-        pattern_descriptor (String, optional)
-            descriptor used for fitting. The default is None.
-        ridge_weight (float, default=0)
-            weight for the ridge-regularisation of the regression
-            weight is in comparison to the final regression problem on
-            the appropriately normalized regressors
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        numpy.ndarray: theta, parameter vector for the model
-
-    """
-    if not (pattern_idx is None or pattern_descriptor is None):
-        pred = model.rdm_obj.subsample_pattern(pattern_descriptor, pattern_idx)
-    else:
-        pred = model.rdm_obj
-    vectors = pred.get_vectors()
-    data_mean = pool_rdm(data, method=method)
-    y = data_mean.get_vectors()
-    vectors, y, nan_idx = _parse_nan_vectors(vectors, y)
-    # Normalizations
-    if method == 'cosine':
-        v = None
-    elif method == 'corr':
-        vectors = vectors - np.mean(vectors, 1, keepdims=True)
-        v = None
-    elif method == 'cosine_cov':
-        v = get_v(pred.n_cond, sigma_k)
-        v = v[nan_idx[0]][:, nan_idx[0]]
-    elif method == 'corr_cov':
-        vectors = vectors - np.mean(vectors, 1, keepdims=True)
-        y = y - np.mean(y)
-        v = get_v(pred.n_cond, sigma_k)
-        v = v[nan_idx[0]][:, nan_idx[0]]
-    else:
-        raise ValueError('method argument invalid')
-    if v is None:
-        X = vectors @ vectors.T + ridge_weight * np.eye(vectors.shape[0])
-        y = vectors @ y.T
-    else:
-        v_inv_x = np.array([scipy.sparse.linalg.cg(v, vectors[i],
-                                                   atol=10 ** -9)[0]
-                            for i in range(vectors.shape[0])])
-        y = v_inv_x @ y.T
-        X = vectors @ v_inv_x.T + ridge_weight * np.eye(vectors.shape[0])
-    theta = np.linalg.solve(X, y)
-    norm = np.sum(theta ** 2)
-    if norm == 0:
-        return theta.flatten()
-    return theta.flatten() / np.sqrt(np.sum(theta ** 2))
-
-
-def fit_regress_nn(model, data, method='cosine', pattern_idx=None,
-                   pattern_descriptor=None, ridge_weight=0, sigma_k=None):
-    """
-    fitting theta using linear algebra solutions to the OLS problem
-    allowed for ModelWeighted only
-    This method first normalizes the data and model RDMs appropriately
-    for the measure to be optimized. For 'cosine' similarity this is a
-    normalization of the data-RDMs to vector length 1. For correlation
-    the mean is removed from both model and data rdms additionally.
-    Then the parameters are estimated using ordinary least squares.
-
-    Args:
-        model(Model): the model to be fit
-        data(pyrsa.rdm.RDMs): data to be fit
-        method(String, optional): evaluation metric The default is 'cosine'.
-        pattern_idx(numpy.ndarray, optional)
-            sampled patterns The default is None.
-        pattern_descriptor (String, optional)
-            descriptor used for fitting. The default is None.
-        ridge_weight (float, default=0)
-            weight for the ridge-regularisation of the regression
-            weight is in comparison to the final regression problem on
-            the appropriately normalized regressors
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-
-    Returns:
-        numpy.ndarray: theta, parameter vector for the model
-
-    """
-    if not (pattern_idx is None or pattern_descriptor is None):
-        pred = model.rdm_obj.subsample_pattern(pattern_descriptor, pattern_idx)
-    else:
-        pred = model.rdm_obj
-    vectors = pred.get_vectors()
-    data_mean = pool_rdm(data, method=method)
-    y = data_mean.get_vectors()
-    vectors, y, non_nan_mask = _parse_nan_vectors(vectors, y)
-    # Normalizations
-    if method == 'cosine':
-        v = None
-    elif method == 'corr':
-        vectors = vectors - np.mean(vectors, 1, keepdims=True)
-        v = None
-    elif method == 'cosine_cov':
-        v = get_v(pred.n_cond, sigma_k)
-        v = v[non_nan_mask[0]][:, non_nan_mask[0]]
-    elif method == 'corr_cov':
-        vectors = vectors - np.mean(vectors, 1, keepdims=True)
-        y = y - np.mean(y)
-        v = get_v(pred.n_cond, sigma_k)
-        v = v[non_nan_mask[0]][:, non_nan_mask[0]]
-    else:
-        raise ValueError('method argument invalid')
-    theta, _ = _nn_least_squares(vectors.T, y[0], ridge_weight=ridge_weight, V=v)
-    norm = np.sum(theta ** 2)
-    if norm == 0:
-        return theta.flatten()
-    return theta.flatten() / np.sqrt(np.sum(theta ** 2))
-
-
-def _loss(theta, model, data, method='cosine', sigma_k=None,
-          pattern_descriptor=None, pattern_idx=None,
-          ridge_weight=0):
-    """Method for calculating a loss for a model and parameter combination
-
-    Args:
-        theta(numpy.ndarray): evaluated parameter value
-        model(Model): the model to be fit
-        data(rsatoolbox.rdm.RDMs): data to be fit
-        method(String, optional): evaluation metric The default is 'cosine'.
-        pattern_idx(numpy.ndarray, optional)
-            sampled patterns The default is None.
-        pattern_descriptor (String, optional)
-            descriptor used for fitting. The default is None.
-        sigma_k(matrix): pattern-covariance matrix
-            used only for whitened distances (ending in _cov)
-            to compute the covariance matrix for rdms
-        ridge_weight(float): weight for a ridge regularisation
-
-    Returns:
-
-        numpy.ndarray: loss
-
-    """
-    pred = model.predict_rdm(theta)
-    if not (pattern_idx is None or pattern_descriptor is None):
-        pred = pred.subsample_pattern(pattern_descriptor, pattern_idx)
-    return -np.mean(compare(pred, data, method=method, sigma_k=sigma_k)) \
-        + np.sum(theta * theta) * ridge_weight
-
-
-def _nn_least_squares(A, y, ridge_weight=0, V=None):
-    """ non-negative least squares
-    essentially scipy.optimize.nnls extended to accept a ridge_regression
-    regularisation and/or a covariance matrix V.
-
-    The algorithm is discribed in detail here:
-    Bro, R., & Jong, S. D. (1997). A fast non-negativity-constrained
-    least squares algorithm. Journal of Chemometrics, 11, 9.
-
-
-    This is an active set algorithm which is somewhat optimized by
-    precomputing A^T V^-1 A and A^T V y such that during the optimization
-    only matricies of rank r need to be inverted.
-
-    This is tested against the scipy solution for ridge_weight=0 and V=None.
-    For other V the validation comes from fitting the same models using
-    general optimization.
-    """
-    assert A.shape[0] == y.shape[0]
-    assert y.ndim == 1
-    x = np.zeros(A.shape[1])
-    p = np.zeros(A.shape[1], bool)
-    if V is None:
-        w = A.T @ y
-        ATA = A.T @ A + ridge_weight * np.eye(A.shape[1])
-    else:
-        V_A = np.array([scipy.sparse.linalg.cg(V, A[:, i],
-                                               atol=10 ** -9)[0]
-                        for i in range(A.shape[1])])
-        y_V_A = V_A @ y
-        w = y_V_A
-        ATA = A.T @ V_A.T + ridge_weight * np.eye(A.shape[1])
-    while np.max(w) > 100 * np.finfo(float).eps:
-        p[np.argmax(w)] = True
-        if V is None:
-            s_p = np.linalg.solve(ATA[p][:, p], A[:, p].T @ y)
-        else:
-            s_p = np.linalg.solve(ATA[p][:, p], y_V_A[p])
-        while np.any(s_p < 0):
-            alphas = x[p] / (x[p] - s_p)
-            alphas[s_p > 0] = 1
-            i_alpha = np.argmin(alphas)
-            alpha = alphas[i_alpha]
-            x[p] = x[p] + alpha * (s_p - x[p])
-            i_alpha = np.where(p)[0][i_alpha]
-            x[i_alpha] = 0
-            p[i_alpha] = False
-            if V is None:
-                s_p = np.linalg.solve(ATA[p][:, p], A[:, p].T @ y)
-            else:
-                s_p = np.linalg.solve(ATA[p][:, p], y_V_A[p])
-        x[p] = s_p
-        if V is None:
-            w = A.T @ y - ATA @ x
-        else:
-            w = y_V_A - ATA @ x
-    if V is None:
-        loss = np.sum((y - A @ x) ** 2)
-    else:
-        loss = (y - A @ x).T @ V @ (y - A @ x)
-    return x, loss
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Parameter fitting methods for models
+"""
+
+import numpy as np
+import scipy.optimize as opt
+import scipy.sparse
+from rsatoolbox.rdm import compare
+from rsatoolbox.util.matrix import get_v
+from rsatoolbox.util.pooling import pool_rdm
+from rsatoolbox.util.rdm_utils import _parse_nan_vectors
+
+
+class Fitter:
+    """Object to specify a fitting function and parameters
+
+    Effectively this gives the user a convenient way to specify fitting
+    functions with different settings than the defaults.
+
+    Create this object with the fitting function to use and additional
+    keyword arguments for settings you wish to change. The resulting
+    object then behaves as the fitting function itself with the
+    keyword arguments set to the different values provided at object
+    creation.
+
+    Example:
+        generate Fitter-object for ridge regression:
+
+        ::
+
+            fit = pyrsa.model.Fitter(pyrsa.model.fit_regress, ridge_weight=1)
+
+        the resulting object 'fit' now does the same as
+        pyrsa.model.fit_regress when run with the additional argument
+        ``ridge_weight=1``, i.e. the following two lines now yield equal results:
+
+        ::
+
+            fit(model, data)
+            pyrsa.model.fit_regress(model, data, ridge_weight=1)
+
+    For a general introduction to flexible models see demo_flex.
+    """
+
+    def __init__(self, fit_fun, **kwargs):
+        self.fit_fun = fit_fun
+        self.kwargs = kwargs
+
+    def __call__(self, model, data, *args, **more_args):
+        return self.fit_fun(model, data, *args, **more_args, **self.kwargs)
+
+
+def fit_mock(model, data, method='cosine', pattern_idx=None,
+             pattern_descriptor=None, sigma_k=None):
+    """ formally acceptable fitting method which always returns a vector of
+    zeros
+
+    Args:
+        model(rsatoolbox.model.Model): model to be fit
+        data(rsatoolbox.rdm.RDMs): Data to fit to
+        method(String): Evaluation method
+        pattern_idx(numpy.ndarray): Which patterns are sampled
+        pattern_descriptor(String): Which descriptor is used
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        theta(numpy.ndarray): parameter vector
+
+    """
+    return np.zeros(model.n_param)
+
+
+def fit_select(model, data, method='cosine', pattern_idx=None,
+               pattern_descriptor=None, sigma_k=None):
+    """ fits selection models by evaluating each rdm and selcting the one
+    with best performance. Works only for ModelSelect
+
+    Args:
+        model(rsatoolbox.model.Model): model to be fit
+        data(rsatoolbox.rdm.RDMs): Data to fit to
+        method(String): Evaluation method
+        pattern_idx(numpy.ndarray): Which patterns are sampled
+        pattern_descriptor(String): Which descriptor is used
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        theta(int): parameter vector
+
+    """
+    evaluations = np.zeros(model.n_rdm)
+    for i_rdm in range(model.n_rdm):
+        pred = model.predict_rdm(i_rdm)
+        if not (pattern_idx is None or pattern_descriptor is None):
+            pred = pred.subsample_pattern(pattern_descriptor, pattern_idx)
+        evaluations[i_rdm] = np.mean(
+            compare(pred, data, method=method, sigma_k=sigma_k))
+    theta = np.argmax(evaluations)
+    return theta
+
+
+def fit_optimize(model, data, method='cosine', pattern_idx=None,
+                 pattern_descriptor=None, sigma_k=None, ridge_weight=0):
+    """
+    fitting theta using optimization
+    currently allowed for ModelWeighted only
+
+    Args:
+        model(Model): the model to be fit
+        data(rsatoolbox.rdm.RDMs): data to be fit
+        method(String, optional): evaluation metric The default is 'cosine'.
+        pattern_idx(numpy.ndarray, optional)
+            sampled patterns The default is None.
+        pattern_descriptor (String, optional)
+            descriptor used for fitting. The default is None.
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        numpy.ndarray: theta, parameter vector for the model
+
+    """
+    def _loss_opt(theta):
+        return _loss(theta, model, data, method=method,
+                     pattern_idx=pattern_idx,
+                     pattern_descriptor=pattern_descriptor,
+                     sigma_k=sigma_k, ridge_weight=ridge_weight)
+    thetas = []
+    losses = []
+    for _ in range(2 * model.n_param):
+        theta0 = np.random.rand(model.n_param)
+        theta = opt.minimize(
+            _loss_opt,
+            theta0,
+            method='BFGS',
+            tol=0.000001
+        )
+        thetas.append(theta.x)
+        losses.append(theta.fun)
+    id = np.argmin(losses)
+    theta = thetas[id]
+    norm = np.sum(theta ** 2)
+    if norm == 0:
+        return theta.flatten()
+    return theta.flatten() / np.sqrt(norm)
+
+
+def fit_optimize_positive(
+        model, data, method='cosine', pattern_idx=None,
+        pattern_descriptor=None, sigma_k=None, ridge_weight=0):
+    """
+    fitting theta using optimization enforcing positive weights
+    currently allowed for ModelWeighted only
+
+    Args:
+        model(Model): the model to be fit
+        data(pyrsa.rdm.RDMs): data to be fit
+        method(String, optional): evaluation metric The default is 'cosine'.
+        pattern_idx(numpy.ndarray, optional)
+            sampled patterns The default is None.
+        pattern_descriptor (String, optional)
+            descriptor used for fitting. The default is None.
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        numpy.ndarray: theta, parameter vector for the model
+
+    """
+    def _loss_opt(theta):
+        return _loss(theta ** 2, model, data, method=method,
+                     pattern_idx=pattern_idx,
+                     pattern_descriptor=pattern_descriptor,
+                     sigma_k=sigma_k, ridge_weight=ridge_weight)
+    theta0 = np.zeros(model.n_param)
+    thetas = [theta0]
+    losses = [_loss_opt(theta0)]
+    theta0 = np.random.rand(model.n_param)
+    theta = opt.minimize(
+        fun=_loss_opt,
+        x0=theta0,
+        method='BFGS',
+        tol=0.000001
+    )
+    thetas.append(theta.x)
+    losses.append(theta.fun)
+    for i in range(model.n_param):
+        theta0 = np.ones(model.n_param) * 0.001
+        theta0[i] = 1
+        theta = opt.minimize(
+            fun=_loss_opt,
+            x0=theta0,
+            method='BFGS',
+            tol=0.000001
+        )
+        thetas.append(theta.x)
+        losses.append(theta.fun)
+    id = np.argmin(losses)
+    theta = thetas[id] ** 2
+    norm = np.sum(theta ** 2)
+    if norm == 0:
+        return theta.flatten()
+    return theta.flatten() / np.sqrt(norm)
+
+
+def fit_interpolate(model, data, method='cosine', pattern_idx=None,
+                    pattern_descriptor=None, sigma_k=None):
+    """
+    fitting theta using bisection optimization
+    allowed for ModelInterpolate only
+
+    Args:
+        model(Model): the model to be fit
+        data(rsatoolbox.rdm.RDMs): data to be fit
+        method(String, optional): evaluation metric The default is 'cosine'.
+        pattern_idx(numpy.ndarray, optional)
+            sampled patterns The default is None.
+        pattern_descriptor (String, optional)
+            descriptor used for fitting. The default is None.
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        numpy.ndarray: theta, parameter vector for the model
+
+    """
+    results = []
+    for i_pair in range(model.n_rdm - 1):
+        def loss_opt(w):
+            theta = np.zeros(model.n_param)
+            theta[i_pair] = w
+            theta[i_pair + 1] = 1 - w
+            return _loss(theta, model, data, method=method,
+                         pattern_idx=pattern_idx,
+                         pattern_descriptor=pattern_descriptor,
+                         sigma_k=sigma_k)
+        results.append(
+            opt.minimize_scalar(loss_opt, np.array([.5]),
+                                method='bounded', bounds=(0, 1)))
+    losses = [r.fun for r in results]
+    i_pair = np.argmin(losses)
+    result = results[i_pair]
+    theta = np.zeros(model.n_rdm)
+    theta[i_pair] = result.x
+    theta[i_pair + 1] = 1 - result.x
+    return theta
+
+
+def fit_regress(model, data, method='cosine', pattern_idx=None,
+                pattern_descriptor=None, ridge_weight=0, sigma_k=None):
+    """
+    fitting theta using linear algebra solutions to the OLS problem
+    allowed for ModelWeighted only
+    This method first normalizes the data and model RDMs appropriately
+    for the measure to be optimized. For 'cosine' similarity this is a
+    normalization of the data-RDMs to vector length 1. For correlation
+    the mean is removed from both model and data rdms additionally.
+    Then the parameters are estimated using ordinary least squares.
+
+    Args:
+        model(Model): the model to be fit
+        data(pyrsa.rdm.RDMs): data to be fit
+        method(String, optional): evaluation metric The default is 'cosine'.
+        pattern_idx(numpy.ndarray, optional)
+            sampled patterns The default is None.
+        pattern_descriptor (String, optional)
+            descriptor used for fitting. The default is None.
+        ridge_weight (float, default=0)
+            weight for the ridge-regularisation of the regression
+            weight is in comparison to the final regression problem on
+            the appropriately normalized regressors
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        numpy.ndarray: theta, parameter vector for the model
+
+    """
+    if not (pattern_idx is None or pattern_descriptor is None):
+        pred = model.rdm_obj.subsample_pattern(pattern_descriptor, pattern_idx)
+    else:
+        pred = model.rdm_obj
+    vectors = pred.get_vectors()
+    data_mean = pool_rdm(data, method=method)
+    y = data_mean.get_vectors()
+    vectors, y, nan_idx = _parse_nan_vectors(vectors, y)
+    # Normalizations
+    if method == 'cosine':
+        v = None
+    elif method == 'corr':
+        vectors = vectors - np.mean(vectors, 1, keepdims=True)
+        v = None
+    elif method == 'cosine_cov':
+        v = get_v(pred.n_cond, sigma_k)
+        v = v[nan_idx[0]][:, nan_idx[0]]
+    elif method == 'corr_cov':
+        vectors = vectors - np.mean(vectors, 1, keepdims=True)
+        y = y - np.mean(y)
+        v = get_v(pred.n_cond, sigma_k)
+        v = v[nan_idx[0]][:, nan_idx[0]]
+    else:
+        raise ValueError('method argument invalid')
+    if v is None:
+        X = vectors @ vectors.T + ridge_weight * np.eye(vectors.shape[0])
+        y = vectors @ y.T
+    else:
+        v_inv_x = np.array([scipy.sparse.linalg.cg(v, vectors[i],
+                                                   atol=10 ** -9)[0]
+                            for i in range(vectors.shape[0])])
+        y = v_inv_x @ y.T
+        X = vectors @ v_inv_x.T + ridge_weight * np.eye(vectors.shape[0])
+    theta = np.linalg.solve(X, y)
+    norm = np.sum(theta ** 2)
+    if norm == 0:
+        return theta.flatten()
+    return theta.flatten() / np.sqrt(np.sum(theta ** 2))
+
+
+def fit_regress_nn(model, data, method='cosine', pattern_idx=None,
+                   pattern_descriptor=None, ridge_weight=0, sigma_k=None):
+    """
+    fitting theta using linear algebra solutions to the OLS problem
+    allowed for ModelWeighted only
+    This method first normalizes the data and model RDMs appropriately
+    for the measure to be optimized. For 'cosine' similarity this is a
+    normalization of the data-RDMs to vector length 1. For correlation
+    the mean is removed from both model and data rdms additionally.
+    Then the parameters are estimated using ordinary least squares.
+
+    Args:
+        model(Model): the model to be fit
+        data(pyrsa.rdm.RDMs): data to be fit
+        method(String, optional): evaluation metric The default is 'cosine'.
+        pattern_idx(numpy.ndarray, optional)
+            sampled patterns The default is None.
+        pattern_descriptor (String, optional)
+            descriptor used for fitting. The default is None.
+        ridge_weight (float, default=0)
+            weight for the ridge-regularisation of the regression
+            weight is in comparison to the final regression problem on
+            the appropriately normalized regressors
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+
+    Returns:
+        numpy.ndarray: theta, parameter vector for the model
+
+    """
+    if not (pattern_idx is None or pattern_descriptor is None):
+        pred = model.rdm_obj.subsample_pattern(pattern_descriptor, pattern_idx)
+    else:
+        pred = model.rdm_obj
+    vectors = pred.get_vectors()
+    data_mean = pool_rdm(data, method=method)
+    y = data_mean.get_vectors()
+    vectors, y, non_nan_mask = _parse_nan_vectors(vectors, y)
+    # Normalizations
+    if method == 'cosine':
+        v = None
+    elif method == 'corr':
+        vectors = vectors - np.mean(vectors, 1, keepdims=True)
+        v = None
+    elif method == 'cosine_cov':
+        v = get_v(pred.n_cond, sigma_k)
+        v = v[non_nan_mask[0]][:, non_nan_mask[0]]
+    elif method == 'corr_cov':
+        vectors = vectors - np.mean(vectors, 1, keepdims=True)
+        y = y - np.mean(y)
+        v = get_v(pred.n_cond, sigma_k)
+        v = v[non_nan_mask[0]][:, non_nan_mask[0]]
+    else:
+        raise ValueError('method argument invalid')
+    theta, _ = _nn_least_squares(vectors.T, y[0], ridge_weight=ridge_weight, V=v)
+    norm = np.sum(theta ** 2)
+    if norm == 0:
+        return theta.flatten()
+    return theta.flatten() / np.sqrt(np.sum(theta ** 2))
+
+
+def _loss(theta, model, data, method='cosine', sigma_k=None,
+          pattern_descriptor=None, pattern_idx=None,
+          ridge_weight=0):
+    """Method for calculating a loss for a model and parameter combination
+
+    Args:
+        theta(numpy.ndarray): evaluated parameter value
+        model(Model): the model to be fit
+        data(rsatoolbox.rdm.RDMs): data to be fit
+        method(String, optional): evaluation metric The default is 'cosine'.
+        pattern_idx(numpy.ndarray, optional)
+            sampled patterns The default is None.
+        pattern_descriptor (String, optional)
+            descriptor used for fitting. The default is None.
+        sigma_k(matrix): pattern-covariance matrix
+            used only for whitened distances (ending in _cov)
+            to compute the covariance matrix for rdms
+        ridge_weight(float): weight for a ridge regularisation
+
+    Returns:
+
+        numpy.ndarray: loss
+
+    """
+    pred = model.predict_rdm(theta)
+    if not (pattern_idx is None or pattern_descriptor is None):
+        pred = pred.subsample_pattern(pattern_descriptor, pattern_idx)
+    return -np.mean(compare(pred, data, method=method, sigma_k=sigma_k)) \
+        + np.sum(theta * theta) * ridge_weight
+
+
+def _nn_least_squares(A, y, ridge_weight=0, V=None):
+    """ non-negative least squares
+    essentially scipy.optimize.nnls extended to accept a ridge_regression
+    regularisation and/or a covariance matrix V.
+
+    The algorithm is discribed in detail here:
+    Bro, R., & Jong, S. D. (1997). A fast non-negativity-constrained
+    least squares algorithm. Journal of Chemometrics, 11, 9.
+
+
+    This is an active set algorithm which is somewhat optimized by
+    precomputing A^T V^-1 A and A^T V y such that during the optimization
+    only matricies of rank r need to be inverted.
+
+    This is tested against the scipy solution for ridge_weight=0 and V=None.
+    For other V the validation comes from fitting the same models using
+    general optimization.
+    """
+    assert A.shape[0] == y.shape[0]
+    assert y.ndim == 1
+    x = np.zeros(A.shape[1])
+    p = np.zeros(A.shape[1], bool)
+    if V is None:
+        w = A.T @ y
+        ATA = A.T @ A + ridge_weight * np.eye(A.shape[1])
+    else:
+        V_A = np.array([scipy.sparse.linalg.cg(V, A[:, i],
+                                               atol=10 ** -9)[0]
+                        for i in range(A.shape[1])])
+        y_V_A = V_A @ y
+        w = y_V_A
+        ATA = A.T @ V_A.T + ridge_weight * np.eye(A.shape[1])
+    while np.max(w) > 100 * np.finfo(float).eps:
+        p[np.argmax(w)] = True
+        if V is None:
+            s_p = np.linalg.solve(ATA[p][:, p], A[:, p].T @ y)
+        else:
+            s_p = np.linalg.solve(ATA[p][:, p], y_V_A[p])
+        while np.any(s_p < 0):
+            alphas = x[p] / (x[p] - s_p)
+            alphas[s_p > 0] = 1
+            i_alpha = np.argmin(alphas)
+            alpha = alphas[i_alpha]
+            x[p] = x[p] + alpha * (s_p - x[p])
+            i_alpha = np.where(p)[0][i_alpha]
+            x[i_alpha] = 0
+            p[i_alpha] = False
+            if V is None:
+                s_p = np.linalg.solve(ATA[p][:, p], A[:, p].T @ y)
+            else:
+                s_p = np.linalg.solve(ATA[p][:, p], y_V_A[p])
+        x[p] = s_p
+        if V is None:
+            w = A.T @ y - ATA @ x
+        else:
+            w = y_V_A - ATA @ x
+    if V is None:
+        loss = np.sum((y - A @ x) ** 2)
+    else:
+        loss = (y - A @ x).T @ V @ (y - A @ x)
+    return x, loss
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/model/model.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/model/model.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,367 +1,367 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Definition of RSA Model class and subclasses
-"""
-
-import numpy as np
-from rsatoolbox.rdm import RDMs
-from rsatoolbox.rdm import rdms_from_dict
-from rsatoolbox.util.rdm_utils import batch_to_vectors
-from .fitter import fit_mock, fit_optimize, fit_select, fit_interpolate
-
-
-class Model:
-    """
-    Abstract model class.
-    Defines members that every class needs to have, but does not implement any
-    interesting behavior. Inherit from this class to define specific model
-    types
-    """
-
-    def __init__(self, name):
-        self.name = name
-        self.n_param = 0
-        self.default_fitter = fit_mock
-        self.rdm_obj = None
-
-    def predict(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            numpy.ndarray: rdm vector
-        """
-        raise NotImplementedError(
-            "Predict function not implemented in used model class!")
-
-    def predict_rdm(self, theta=None):
-        """ Returns the predicted rdm as an object
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            numpy.ndarray: rdm object
-        """
-        raise NotImplementedError(
-            "Predict rdm function not implemented in used model class!")
-
-    def fit(self, data, method='cosine', pattern_idx=None,
-            pattern_descriptor=None, sigma_k=None):
-        """ fit the model to a RDM object data
-
-        Args:
-            data(RDM object): the RDMs to be fit with the model
-            method(String): how to measure rdm_similarity
-            patterrn_idx: which patterns to use
-            pattern_descriptor: which part of the dict to use to interpret
-                pattern_idx
-
-        Returns:
-            theta(numpy.ndarray): parameter vector (one dimensional)
-        """
-        return self.default_fitter(self, data, method=method,
-                                   pattern_idx=pattern_idx,
-                                   pattern_descriptor=pattern_descriptor,
-                                   sigma_k=sigma_k)
-
-    def to_dict(self):
-        """ Converts the model into a dictionary, which can be used for saving
-
-        Returns:
-            model_dict(dict): A dictionary containting all data needed to
-                recreate the object
-
-        """
-        model_dict = {}
-        if self.rdm_obj:
-            model_dict['rdm'] = self.rdm_obj.to_dict()
-        else:
-            model_dict['rdm'] = None
-        model_dict['name'] = self.name
-        model_dict['type'] = type(self).__name__
-        return model_dict
-
-
-class ModelFixed(Model):
-    def __init__(self, name, rdm):
-        """
-        Fixed model
-        This is a parameter-free model that simply predicts a fixed RDM
-        It takes rdm object, a vector or a matrix as input to define the RDM
-
-        Args:
-            Name(String): Model name
-            rdm(rsatoolbox.rdm.RDMs): rdms in one object
-        """
-        Model.__init__(self, name)
-        if isinstance(rdm, RDMs):
-            self.rdm_obj = rdm
-            self.rdm = np.mean(rdm.get_vectors(), axis=0)
-            self.n_cond = rdm.n_cond
-        elif rdm.ndim == 1:  # User passed a vector
-            self.rdm_obj = RDMs(np.array([rdm]))
-            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.size)) / 2
-            if self.n_cond % 1 != 0:
-                raise NameError(
-                    "RDM vector needs to have size of ncond*(ncond-1)/2")
-            self.rdm = rdm
-        else:  # User passed a matrix
-            self.rdm_obj = RDMs(np.array([rdm]))
-            self.rdm, _, self.n_cond = batch_to_vectors(np.array([rdm]))
-            self.rdm = self.rdm[0]
-        self.n_param = 0
-        self.default_fitter = fit_mock
-        self.rdm_obj.pattern_descriptors['index'] = np.arange(self.n_cond)
-
-    def predict(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        For the fixed model there are no parameters. theta is ignored.
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rdm vector
-
-        """
-        return self.rdm
-
-    def predict_rdm(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        For the fixed model there are no parameters.
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rsatoolbox.rdm.RDMs: rdm object
-
-        """
-        return self.rdm_obj
-
-
-class ModelSelect(Model):
-    """
-    Selection model
-    This model has a set of RDMs and selects one of them as its prediction.
-    theta should here be an integer index
-    """
-
-    # Model Constructor
-    def __init__(self, name, rdm):
-        Model.__init__(self, name)
-        if isinstance(rdm, RDMs):
-            self.rdm_obj = rdm
-            self.rdm = rdm.get_vectors()
-            self.n_cond = rdm.n_cond
-        elif rdm.ndim == 2:  # User supplied vectors
-            self.rdm_obj = RDMs(rdm)
-            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.shape[1])) / 2
-            if self.n_cond % 1 != 0:
-                raise NameError(
-                    "RDM vector needs to have size of ncond*(ncond-1)/2")
-            self.rdm = rdm
-        else:  # User passed matrixes
-            self.rdm_obj = RDMs(rdm)
-            self.rdm, _, self.n_cond = batch_to_vectors(rdm)
-        self.n_param = 1
-        self.n_rdm = self.rdm_obj.n_rdm
-        self.default_fitter = fit_select
-
-    def predict(self, theta=0):
-        """ Returns the predicted rdm vector
-
-        For the fixed model there are no parameters. theta is ignored.
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rdm vector
-
-        """
-        return self.rdm[theta]
-
-    def predict_rdm(self, theta=0):
-        """ Returns the predicted rdm vector
-
-        For the fixed model there are no parameters.
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rsatoolbox.rdm.RDMs: rdm object
-
-        """
-        return self.rdm_obj[theta]
-
-
-class ModelWeighted(Model):
-    """
-    weighted Model
-    models the RDM as a weighted sum of a set of RDMs
-    """
-
-    # Model Constructor
-    def __init__(self, name, rdm):
-        Model.__init__(self, name)
-        if isinstance(rdm, RDMs):
-            self.rdm_obj = rdm
-            self.rdm = rdm.get_vectors()
-            self.n_cond = rdm.n_cond
-        elif rdm.ndim == 2:  # User supplied vectors
-            self.rdm_obj = RDMs(rdm)
-            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.shape[1])) / 2
-            if self.n_cond % 1 != 0:
-                raise NameError(
-                    "RDM vector needs to have size of ncond*(ncond-1)/2")
-            self.rdm = rdm
-        else:  # User passed matrixes
-            self.rdm_obj = RDMs(rdm)
-            self.rdm, _, self.n_cond = batch_to_vectors(rdm)
-        self.n_param = self.rdm_obj.n_rdm
-        self.n_rdm = self.rdm_obj.n_rdm
-        self.default_fitter = fit_optimize
-
-    def predict(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        theta are the weights for the different rdms
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rdm vector
-
-        """
-        if theta is None:
-            theta = np.ones(self.n_rdm)
-        theta = np.array(theta)
-        return np.matmul(self.rdm.T, theta.reshape(-1))
-
-    def predict_rdm(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        For the fixed model there are no parameters.
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rsatoolbox.rdm.RDMs: rdm object
-
-        """
-        if theta is None:
-            theta = np.ones(self.n_rdm)
-        theta = np.array(theta)
-        dissimilarities = np.matmul(self.rdm.T, theta.reshape(-1))
-        rdms = RDMs(
-            dissimilarities.reshape(1, -1),
-            dissimilarity_measure=self.rdm_obj.dissimilarity_measure,
-            descriptors=self.rdm_obj.descriptors,
-            pattern_descriptors=self.rdm_obj.pattern_descriptors)
-        return rdms
-
-
-class ModelInterpolate(Model):
-    """
-    inpterpolation Model
-    models the RDM as an interpolation between 2 neigboring rdms
-    """
-
-    # Model Constructor
-    def __init__(self, name, rdm):
-        Model.__init__(self, name)
-        if isinstance(rdm, RDMs):
-            self.rdm_obj = rdm
-            self.rdm = rdm.get_vectors()
-            self.n_cond = rdm.n_cond
-        elif rdm.ndim == 2:  # User supplied vectors
-            self.rdm_obj = RDMs(rdm)
-            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.shape[1])) / 2
-            if self.n_cond % 1 != 0:
-                raise NameError(
-                    "RDM vector needs to have size of ncond*(ncond-1)/2")
-            self.rdm = rdm
-        else:  # User passed matrixes
-            self.rdm_obj = RDMs(rdm)
-            self.rdm, _, self.n_cond = batch_to_vectors(rdm)
-        self.n_param = self.rdm_obj.n_rdm
-        self.n_rdm = self.rdm_obj.n_rdm
-        self.default_fitter = fit_interpolate
-
-    def predict(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        theta are the weights for the different rdms
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rdm vector
-
-        """
-        if theta is None:
-            theta = np.zeros(self.n_rdm)
-            theta[0] = 0.5
-            theta[1] = 0.5
-        theta = np.array(theta)
-        return np.matmul(self.rdm.T, theta.reshape(-1))
-
-    def predict_rdm(self, theta=None):
-        """ Returns the predicted rdm vector
-
-        For the fixed model there are no parameters.
-
-        Args:
-            theta(numpy.ndarray): the model parameter vector (one dimensional)
-
-        Returns:
-            rsatoolbox.rdm.RDMs: rdm object
-
-        """
-        if theta is None:
-            theta = np.ones(self.n_rdm)
-        theta = np.maximum(theta, 0)
-        theta = np.array(theta)
-        dissimilarities = np.matmul(self.rdm.T, theta.reshape(-1))
-        rdms = RDMs(
-            dissimilarities.reshape(1, -1),
-            dissimilarity_measure=self.rdm_obj.dissimilarity_measure,
-            descriptors=self.rdm_obj.descriptors,
-            pattern_descriptors=self.rdm_obj.pattern_descriptors)
-        return rdms
-
-
-def model_from_dict(model_dict):
-    """ recreates a model object from a dictionary
-
-    Args:
-        model_dict(dict): The dictionary to be turned into a model
-
-    Returns
-        model(Model): The recreated model
-
-    """
-    if model_dict['rdm']:
-        rdm_obj = rdms_from_dict(model_dict['rdm'])
-    if model_dict['type'] == 'Model':
-        model = Model(model_dict['name'])
-    elif model_dict['type'] == 'ModelFixed':
-        model = ModelFixed(model_dict['name'], rdm_obj)
-    elif model_dict['type'] == 'ModelSelect':
-        model = ModelSelect(model_dict['name'], rdm_obj)
-    elif model_dict['type'] == 'ModelWeighted':
-        model = ModelWeighted(model_dict['name'], rdm_obj)
-    elif model_dict['type'] == 'ModelInterpolate':
-        model = ModelInterpolate(model_dict['name'], rdm_obj)
-    return model
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Definition of RSA Model class and subclasses
+"""
+
+import numpy as np
+from rsatoolbox.rdm import RDMs
+from rsatoolbox.rdm import rdms_from_dict
+from rsatoolbox.util.rdm_utils import batch_to_vectors
+from .fitter import fit_mock, fit_optimize, fit_select, fit_interpolate
+
+
+class Model:
+    """
+    Abstract model class.
+    Defines members that every class needs to have, but does not implement any
+    interesting behavior. Inherit from this class to define specific model
+    types
+    """
+
+    def __init__(self, name):
+        self.name = name
+        self.n_param = 0
+        self.default_fitter = fit_mock
+        self.rdm_obj = None
+
+    def predict(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            numpy.ndarray: rdm vector
+        """
+        raise NotImplementedError(
+            "Predict function not implemented in used model class!")
+
+    def predict_rdm(self, theta=None):
+        """ Returns the predicted rdm as an object
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            numpy.ndarray: rdm object
+        """
+        raise NotImplementedError(
+            "Predict rdm function not implemented in used model class!")
+
+    def fit(self, data, method='cosine', pattern_idx=None,
+            pattern_descriptor=None, sigma_k=None):
+        """ fit the model to a RDM object data
+
+        Args:
+            data(RDM object): the RDMs to be fit with the model
+            method(String): how to measure rdm_similarity
+            patterrn_idx: which patterns to use
+            pattern_descriptor: which part of the dict to use to interpret
+                pattern_idx
+
+        Returns:
+            theta(numpy.ndarray): parameter vector (one dimensional)
+        """
+        return self.default_fitter(self, data, method=method,
+                                   pattern_idx=pattern_idx,
+                                   pattern_descriptor=pattern_descriptor,
+                                   sigma_k=sigma_k)
+
+    def to_dict(self):
+        """ Converts the model into a dictionary, which can be used for saving
+
+        Returns:
+            model_dict(dict): A dictionary containting all data needed to
+                recreate the object
+
+        """
+        model_dict = {}
+        if self.rdm_obj:
+            model_dict['rdm'] = self.rdm_obj.to_dict()
+        else:
+            model_dict['rdm'] = None
+        model_dict['name'] = self.name
+        model_dict['type'] = type(self).__name__
+        return model_dict
+
+
+class ModelFixed(Model):
+    def __init__(self, name, rdm):
+        """
+        Fixed model
+        This is a parameter-free model that simply predicts a fixed RDM
+        It takes rdm object, a vector or a matrix as input to define the RDM
+
+        Args:
+            Name(String): Model name
+            rdm(rsatoolbox.rdm.RDMs): rdms in one object
+        """
+        Model.__init__(self, name)
+        if isinstance(rdm, RDMs):
+            self.rdm_obj = rdm
+            self.rdm = np.mean(rdm.get_vectors(), axis=0)
+            self.n_cond = rdm.n_cond
+        elif rdm.ndim == 1:  # User passed a vector
+            self.rdm_obj = RDMs(np.array([rdm]))
+            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.size)) / 2
+            if self.n_cond % 1 != 0:
+                raise NameError(
+                    "RDM vector needs to have size of ncond*(ncond-1)/2")
+            self.rdm = rdm
+        else:  # User passed a matrix
+            self.rdm_obj = RDMs(np.array([rdm]))
+            self.rdm, _, self.n_cond = batch_to_vectors(np.array([rdm]))
+            self.rdm = self.rdm[0]
+        self.n_param = 0
+        self.default_fitter = fit_mock
+        self.rdm_obj.pattern_descriptors['index'] = np.arange(self.n_cond)
+
+    def predict(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        For the fixed model there are no parameters. theta is ignored.
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rdm vector
+
+        """
+        return self.rdm
+
+    def predict_rdm(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        For the fixed model there are no parameters.
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rsatoolbox.rdm.RDMs: rdm object
+
+        """
+        return self.rdm_obj
+
+
+class ModelSelect(Model):
+    """
+    Selection model
+    This model has a set of RDMs and selects one of them as its prediction.
+    theta should here be an integer index
+    """
+
+    # Model Constructor
+    def __init__(self, name, rdm):
+        Model.__init__(self, name)
+        if isinstance(rdm, RDMs):
+            self.rdm_obj = rdm
+            self.rdm = rdm.get_vectors()
+            self.n_cond = rdm.n_cond
+        elif rdm.ndim == 2:  # User supplied vectors
+            self.rdm_obj = RDMs(rdm)
+            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.shape[1])) / 2
+            if self.n_cond % 1 != 0:
+                raise NameError(
+                    "RDM vector needs to have size of ncond*(ncond-1)/2")
+            self.rdm = rdm
+        else:  # User passed matrixes
+            self.rdm_obj = RDMs(rdm)
+            self.rdm, _, self.n_cond = batch_to_vectors(rdm)
+        self.n_param = 1
+        self.n_rdm = self.rdm_obj.n_rdm
+        self.default_fitter = fit_select
+
+    def predict(self, theta=0):
+        """ Returns the predicted rdm vector
+
+        For the fixed model there are no parameters. theta is ignored.
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rdm vector
+
+        """
+        return self.rdm[theta]
+
+    def predict_rdm(self, theta=0):
+        """ Returns the predicted rdm vector
+
+        For the fixed model there are no parameters.
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rsatoolbox.rdm.RDMs: rdm object
+
+        """
+        return self.rdm_obj[theta]
+
+
+class ModelWeighted(Model):
+    """
+    weighted Model
+    models the RDM as a weighted sum of a set of RDMs
+    """
+
+    # Model Constructor
+    def __init__(self, name, rdm):
+        Model.__init__(self, name)
+        if isinstance(rdm, RDMs):
+            self.rdm_obj = rdm
+            self.rdm = rdm.get_vectors()
+            self.n_cond = rdm.n_cond
+        elif rdm.ndim == 2:  # User supplied vectors
+            self.rdm_obj = RDMs(rdm)
+            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.shape[1])) / 2
+            if self.n_cond % 1 != 0:
+                raise NameError(
+                    "RDM vector needs to have size of ncond*(ncond-1)/2")
+            self.rdm = rdm
+        else:  # User passed matrixes
+            self.rdm_obj = RDMs(rdm)
+            self.rdm, _, self.n_cond = batch_to_vectors(rdm)
+        self.n_param = self.rdm_obj.n_rdm
+        self.n_rdm = self.rdm_obj.n_rdm
+        self.default_fitter = fit_optimize
+
+    def predict(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        theta are the weights for the different rdms
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rdm vector
+
+        """
+        if theta is None:
+            theta = np.ones(self.n_rdm)
+        theta = np.array(theta)
+        return np.matmul(self.rdm.T, theta.reshape(-1))
+
+    def predict_rdm(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        For the fixed model there are no parameters.
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rsatoolbox.rdm.RDMs: rdm object
+
+        """
+        if theta is None:
+            theta = np.ones(self.n_rdm)
+        theta = np.array(theta)
+        dissimilarities = np.matmul(self.rdm.T, theta.reshape(-1))
+        rdms = RDMs(
+            dissimilarities.reshape(1, -1),
+            dissimilarity_measure=self.rdm_obj.dissimilarity_measure,
+            descriptors=self.rdm_obj.descriptors,
+            pattern_descriptors=self.rdm_obj.pattern_descriptors)
+        return rdms
+
+
+class ModelInterpolate(Model):
+    """
+    inpterpolation Model
+    models the RDM as an interpolation between 2 neigboring rdms
+    """
+
+    # Model Constructor
+    def __init__(self, name, rdm):
+        Model.__init__(self, name)
+        if isinstance(rdm, RDMs):
+            self.rdm_obj = rdm
+            self.rdm = rdm.get_vectors()
+            self.n_cond = rdm.n_cond
+        elif rdm.ndim == 2:  # User supplied vectors
+            self.rdm_obj = RDMs(rdm)
+            self.n_cond = (1 + np.sqrt(1 + 8 * rdm.shape[1])) / 2
+            if self.n_cond % 1 != 0:
+                raise NameError(
+                    "RDM vector needs to have size of ncond*(ncond-1)/2")
+            self.rdm = rdm
+        else:  # User passed matrixes
+            self.rdm_obj = RDMs(rdm)
+            self.rdm, _, self.n_cond = batch_to_vectors(rdm)
+        self.n_param = self.rdm_obj.n_rdm
+        self.n_rdm = self.rdm_obj.n_rdm
+        self.default_fitter = fit_interpolate
+
+    def predict(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        theta are the weights for the different rdms
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rdm vector
+
+        """
+        if theta is None:
+            theta = np.zeros(self.n_rdm)
+            theta[0] = 0.5
+            theta[1] = 0.5
+        theta = np.array(theta)
+        return np.matmul(self.rdm.T, theta.reshape(-1))
+
+    def predict_rdm(self, theta=None):
+        """ Returns the predicted rdm vector
+
+        For the fixed model there are no parameters.
+
+        Args:
+            theta(numpy.ndarray): the model parameter vector (one dimensional)
+
+        Returns:
+            rsatoolbox.rdm.RDMs: rdm object
+
+        """
+        if theta is None:
+            theta = np.ones(self.n_rdm)
+        theta = np.maximum(theta, 0)
+        theta = np.array(theta)
+        dissimilarities = np.matmul(self.rdm.T, theta.reshape(-1))
+        rdms = RDMs(
+            dissimilarities.reshape(1, -1),
+            dissimilarity_measure=self.rdm_obj.dissimilarity_measure,
+            descriptors=self.rdm_obj.descriptors,
+            pattern_descriptors=self.rdm_obj.pattern_descriptors)
+        return rdms
+
+
+def model_from_dict(model_dict):
+    """ recreates a model object from a dictionary
+
+    Args:
+        model_dict(dict): The dictionary to be turned into a model
+
+    Returns
+        model(Model): The recreated model
+
+    """
+    if model_dict['rdm']:
+        rdm_obj = rdms_from_dict(model_dict['rdm'])
+    if model_dict['type'] == 'Model':
+        model = Model(model_dict['name'])
+    elif model_dict['type'] == 'ModelFixed':
+        model = ModelFixed(model_dict['name'], rdm_obj)
+    elif model_dict['type'] == 'ModelSelect':
+        model = ModelSelect(model_dict['name'], rdm_obj)
+    elif model_dict['type'] == 'ModelWeighted':
+        model = ModelWeighted(model_dict['name'], rdm_obj)
+    elif model_dict['type'] == 'ModelInterpolate':
+        model = ModelInterpolate(model_dict['name'], rdm_obj)
+    return model
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/calc.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/calc.py`

 * *Ordering differences only*

 * *Files 17% similar despite different names*

```diff
@@ -1,540 +1,540 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Calculation of RDMs from datasets
-@author: heiko, benjamin
-"""
-from __future__ import annotations
-from collections.abc import Iterable
-from copy import deepcopy
-from typing import TYPE_CHECKING, Optional, Tuple
-import numpy as np
-from rsatoolbox.rdm.rdms import RDMs
-from rsatoolbox.rdm.rdms import concat
-from rsatoolbox.rdm.combine import from_partials
-from rsatoolbox.data import average_dataset_by
-from rsatoolbox.util.rdm_utils import _extract_triu_
-if TYPE_CHECKING:
-    from rsatoolbox.data.base import DatasetBase
-    from numpy.typing import NDArray
-
-
-def calc_rdm(dataset, method='euclidean', descriptor=None, noise=None,
-             cv_descriptor=None, prior_lambda=1, prior_weight=0.1):
-    """
-    calculates an RDM from an input dataset
-
-    This should usually be called with the method and the descriptor argument
-    to specify the dissimilarity measure and which observations in the dataset
-    belong to which condition.
-
-    Args:
-        dataset (rsatoolbox.data.dataset.DatasetBase):
-            The dataset the RDM is computed from
-        method (String):
-            a description of the dissimilarity measure (e.g. 'Euclidean')
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-        noise (numpy.ndarray):
-            dataset.n_channel x dataset.n_channel
-            precision matrix used to calculate the RDM
-            used only for Mahalanobis and Crossnobis estimators
-            defaults to an identity matrix, i.e. euclidean distance
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    if isinstance(dataset, Iterable):
-        rdms = []
-        for i_dat, ds_i in enumerate(dataset):
-            if noise is None:
-                rdms.append(calc_rdm(
-                    ds_i, method=method,
-                    descriptor=descriptor,
-                    cv_descriptor=cv_descriptor,
-                    prior_lambda=prior_lambda, prior_weight=prior_weight))
-            elif isinstance(noise, np.ndarray) and noise.ndim == 2:
-                rdms.append(calc_rdm(
-                    ds_i, method=method,
-                    descriptor=descriptor,
-                    noise=noise,
-                    cv_descriptor=cv_descriptor,
-                    prior_lambda=prior_lambda, prior_weight=prior_weight))
-            elif isinstance(noise, Iterable):
-                rdms.append(calc_rdm(
-                    ds_i, method=method,
-                    descriptor=descriptor,
-                    noise=noise[i_dat],
-                    cv_descriptor=cv_descriptor,
-                    prior_lambda=prior_lambda, prior_weight=prior_weight))
-        if descriptor is None:
-            rdm = concat(rdms)
-        else:
-            rdm = from_partials(rdms, descriptor=descriptor)
-    else:
-        if method == 'euclidean':
-            rdm = calc_rdm_euclidean(dataset, descriptor)
-        elif method == 'correlation':
-            rdm = calc_rdm_correlation(dataset, descriptor)
-        elif method == 'mahalanobis':
-            rdm = calc_rdm_mahalanobis(dataset, descriptor, noise)
-        elif method == 'crossnobis':
-            rdm = calc_rdm_crossnobis(dataset, descriptor, noise,
-                                      cv_descriptor)
-        elif method == 'poisson':
-            rdm = calc_rdm_poisson(dataset, descriptor,
-                                   prior_lambda=prior_lambda,
-                                   prior_weight=prior_weight)
-        elif method == 'poisson_cv':
-            rdm = calc_rdm_poisson_cv(dataset, descriptor,
-                                      cv_descriptor=cv_descriptor,
-                                      prior_lambda=prior_lambda,
-                                      prior_weight=prior_weight)
-        else:
-            raise NotImplementedError
-        if descriptor is not None:
-            rdm.sort_by(**{descriptor: 'alpha'})
-    return rdm
-
-
-def calc_rdm_movie(
-        dataset, method='euclidean', descriptor=None, noise=None,
-        cv_descriptor=None, prior_lambda=1, prior_weight=0.1,
-        time_descriptor='time', bins=None):
-    """
-    calculates an RDM movie from an input TemporalDataset
-
-    Args:
-        dataset (rsatoolbox.data.dataset.TemporalDataset):
-            The dataset the RDM is computed from
-        method (String):
-            a description of the dissimilarity measure (e.g. 'Euclidean')
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-        noise (numpy.ndarray):
-            dataset.n_channel x dataset.n_channel
-            precision matrix used to calculate the RDM
-            used only for Mahalanobis and Crossnobis estimators
-            defaults to an identity matrix, i.e. euclidean distance
-        time_descriptor (String): descriptor key that points to the time
-            dimension in dataset.time_descriptors. Defaults to 'time'.
-        bins (array-like): list of bins, with bins[i] containing the vector
-            of time-points for the i-th bin. Defaults to no binning.
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with RDM movie
-    """
-
-    if isinstance(dataset, Iterable):
-        rdms = []
-        for i_dat, ds_i in enumerate(dataset):
-            if noise is None:
-                rdms.append(calc_rdm_movie(
-                    ds_i, method=method,
-                    descriptor=descriptor))
-            elif isinstance(noise, np.ndarray) and noise.ndim == 2:
-                rdms.append(calc_rdm_movie(
-                    ds_i, method=method,
-                    descriptor=descriptor,
-                    noise=noise))
-            elif isinstance(noise, Iterable):
-                rdms.append(calc_rdm_movie(
-                    ds_i, method=method,
-                    descriptor=descriptor,
-                    noise=noise[i_dat]))
-        rdm = concat(rdms)
-    else:
-        if bins is not None:
-            binned_data = dataset.bin_time(time_descriptor, bins)
-            splited_data = binned_data.split_time(time_descriptor)
-            time = binned_data.time_descriptors[time_descriptor]
-        else:
-            splited_data = dataset.split_time(time_descriptor)
-            time = dataset.time_descriptors[time_descriptor]
-
-        rdms = []
-        for dat in splited_data:
-            dat_single = dat.convert_to_dataset(time_descriptor)
-            rdms.append(calc_rdm(dat_single, method=method,
-                                 descriptor=descriptor, noise=noise,
-                                 cv_descriptor=cv_descriptor,
-                                 prior_lambda=prior_lambda,
-                                 prior_weight=prior_weight))
-
-        rdm = concat(rdms)
-        rdm.rdm_descriptors[time_descriptor] = time
-    return rdm
-
-
-def calc_rdm_euclidean(dataset, descriptor=None):
-    """
-    Args:
-        dataset (rsatoolbox.data.DatasetBase):
-            The dataset the RDM is computed from
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-            defaults to one row/column per row in the dataset
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-    """
-
-    measurements, desc = _parse_input(dataset, descriptor)
-    sum_sq_measurements = np.sum(measurements**2, axis=1, keepdims=True)
-    rdm = sum_sq_measurements + sum_sq_measurements.T \
-        - 2 * np.dot(measurements, measurements.T)
-    rdm = _extract_triu_(rdm) / measurements.shape[1]
-    return _build_rdms(rdm, dataset, 'squared euclidean', descriptor, desc)
-
-
-def calc_rdm_correlation(dataset, descriptor=None):
-    """
-    calculates an RDM from an input dataset using correlation distance
-    If multiple instances of the same condition are found in the dataset
-    they are averaged.
-
-    Args:
-        dataset (rsatoolbox.data.DatasetBase):
-            The dataset the RDM is computed from
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-            defaults to one row/column per row in the dataset
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    ma, desc = _parse_input(dataset, descriptor)
-    ma = ma - ma.mean(axis=1, keepdims=True)
-    ma /= np.sqrt(np.einsum('ij,ij->i', ma, ma))[:, None]
-    rdm = 1 - np.einsum('ik,jk', ma, ma)
-    return _build_rdms(rdm, dataset, 'correlation', descriptor, desc)
-
-
-def calc_rdm_mahalanobis(dataset, descriptor=None, noise=None):
-    """
-    calculates an RDM from an input dataset using mahalanobis distance
-    If multiple instances of the same condition are found in the dataset
-    they are averaged.
-
-    Args:
-        dataset (rsatoolbox.data.dataset.DatasetBase):
-            The dataset the RDM is computed from
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-            defaults to one row/column per row in the dataset
-        noise (numpy.ndarray):
-            dataset.n_channel x dataset.n_channel
-            precision matrix used to calculate the RDM
-            default: identity matrix, i.e. euclidean distance
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    if noise is None:
-        return calc_rdm_euclidean(dataset, descriptor)
-    measurements, desc = _parse_input(dataset, descriptor)
-    noise = _check_noise(noise, dataset.n_channel)
-    kernel = measurements @ noise @ measurements.T
-    rdm = np.expand_dims(np.diag(kernel), 0) + \
-        np.expand_dims(np.diag(kernel), 1) - 2 * kernel
-    rdm = _extract_triu_(rdm) / measurements.shape[1]
-    return _build_rdms(
-        rdm,
-        dataset,
-        'squared mahalanobis',
-        descriptor,
-        desc,
-        noise=noise
-    )
-
-
-def calc_rdm_crossnobis(dataset, descriptor, noise=None,
-                        cv_descriptor=None):
-    """
-    calculates an RDM from an input dataset using Cross-nobis distance
-    This performs leave one out crossvalidation over the cv_descriptor.
-
-    As the minimum input provide a dataset and a descriptor-name to
-    define the rows & columns of the RDM.
-    You may pass a noise precision. If you don't an identity is assumed.
-    Also a cv_descriptor can be passed to define the crossvalidation folds.
-    It is recommended to do this, to assure correct calculations. If you do
-    not, this function infers a split in order of the dataset, which is
-    guaranteed to fail if there are any unbalances.
-
-    This function also accepts a list of noise precision matricies.
-    It is then assumed that this is the precision of the mean from
-    the corresponding crossvalidation fold, i.e. if multiple measurements
-    enter a fold, please compute the resulting noise precision in advance!
-
-    To assert equal ordering in the folds the dataset is initially sorted
-    according to the descriptor used to define the patterns.
-
-    Args:
-        dataset (rsatoolbox.data.dataset.DatasetBase):
-            The dataset the RDM is computed from
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-            defaults to one row/column per row in the dataset
-        noise (numpy.ndarray):
-            dataset.n_channel x dataset.n_channel
-            precision matrix used to calculate the RDM
-            default: identity matrix, i.e. euclidean distance
-        cv_descriptor (String):
-            obs_descriptor which determines the cross-validation folds
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    noise = _check_noise(noise, dataset.n_channel)
-    if noise is None:
-        noise = np.eye(dataset.n_channel)
-    if descriptor is None:
-        raise ValueError('descriptor must be a string! Crossvalidation' +
-                         'requires multiple measurements to be grouped')
-    datasetCopy = deepcopy(dataset)
-    if cv_descriptor is None:
-        cv_desc = _gen_default_cv_descriptor(datasetCopy, descriptor)
-        datasetCopy.obs_descriptors['cv_desc'] = cv_desc
-        cv_descriptor = 'cv_desc'
-    datasetCopy.sort_by(descriptor)
-    cv_folds = np.unique(np.array(datasetCopy.obs_descriptors[cv_descriptor]))
-    rdms = []
-    if (noise is None) or (isinstance(noise, np.ndarray) and noise.ndim == 2):
-        for i_fold, fold in enumerate(cv_folds):
-            data_test = datasetCopy.subset_obs(cv_descriptor, fold)
-            data_train = datasetCopy.subset_obs(
-                cv_descriptor,
-                np.setdiff1d(cv_folds, fold)
-            )
-            measurements_train, _, _ = \
-                average_dataset_by(data_train, descriptor)
-            measurements_test, _, _ = \
-                average_dataset_by(data_test, descriptor)
-            rdm = _calc_rdm_crossnobis_single(
-                measurements_train, measurements_test, noise)
-            rdms.append(rdm)
-    else:  # a list of noises was provided
-        measurements = []
-        variances = []
-        for i, i_fold in enumerate(cv_folds):
-            data = datasetCopy.subset_obs(cv_descriptor, i_fold)
-            measurements.append(average_dataset_by(data, descriptor)[0])
-            variances.append(np.linalg.inv(noise[i]))
-        for i_fold in range(len(cv_folds)):
-            for j_fold in range(i_fold + 1, len(cv_folds)):
-                if i_fold != j_fold:
-                    rdm = _calc_rdm_crossnobis_single(
-                        measurements[i_fold], measurements[j_fold],
-                        np.linalg.inv(
-                            (variances[i_fold] + variances[j_fold]) / 2)
-                        )
-                    rdms.append(rdm)
-    rdms = np.array(rdms)
-    rdm = np.einsum('ij->j', rdms) / rdms.shape[0]
-    return _build_rdms(
-        rdm,
-        datasetCopy,
-        'crossnobis',
-        descriptor,
-        noise=noise,
-        cv=cv_descriptor
-    )
-
-
-def calc_rdm_poisson(dataset, descriptor=None, prior_lambda=1,
-                     prior_weight=0.1):
-    """
-    calculates an RDM from an input dataset using the symmetrized
-    KL-divergence assuming a poisson distribution.
-    If multiple instances of the same condition are found in the dataset
-    they are averaged.
-
-    Args:
-        dataset (rsatoolbox.data.DatasetBase):
-            The dataset the RDM is computed from
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-            defaults to one row/column per row in the dataset
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    measurements, desc = _parse_input(dataset, descriptor)
-    measurements = (measurements + prior_lambda * prior_weight) \
-        / (1 + prior_weight)
-    kernel = measurements @ np.log(measurements).T
-    rdm = np.expand_dims(np.diag(kernel), 0) + \
-        np.expand_dims(np.diag(kernel), 1) - kernel - kernel.T
-    rdm = _extract_triu_(rdm) / measurements.shape[1]
-    return _build_rdms(rdm, dataset, 'poisson', descriptor, desc)
-
-
-def calc_rdm_poisson_cv(dataset, descriptor=None, prior_lambda=1,
-                        prior_weight=0.1, cv_descriptor=None):
-    """
-    calculates an RDM from an input dataset using the crossvalidated
-    symmetrized KL-divergence assuming a poisson distribution
-
-    To assert equal ordering in the folds the dataset is initially sorted
-    according to the descriptor used to define the patterns.
-
-    Args:
-        dataset (rsatoolbox.data.DatasetBase):
-            The dataset the RDM is computed from
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-            defaults to one row/column per row in the dataset
-        cv_descriptor (str): The descriptor that indicates the folds
-            to use for crossvalidation
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    if descriptor is None:
-        raise ValueError('descriptor must be a string! Crossvalidation' +
-                         'requires multiple measurements to be grouped')
-    dataset = deepcopy(dataset)
-    if cv_descriptor is None:
-        cv_desc = _gen_default_cv_descriptor(dataset, descriptor)
-        dataset.obs_descriptors['cv_desc'] = cv_desc
-        cv_descriptor = 'cv_desc'
-    dataset.sort_by(descriptor)
-    cv_folds = np.unique(np.array(dataset.obs_descriptors[cv_descriptor]))
-    for i_fold in range(len(cv_folds)):
-        fold = cv_folds[i_fold]
-        data_test = dataset.subset_obs(cv_descriptor, fold)
-        data_train = dataset.subset_obs(cv_descriptor,
-                                        np.setdiff1d(cv_folds, fold))
-        measurements_train, _, _ = average_dataset_by(data_train, descriptor)
-        measurements_test, _, _ = average_dataset_by(data_test, descriptor)
-        measurements_train = (measurements_train
-                              + prior_lambda * prior_weight) \
-            / (1 + prior_weight)
-        measurements_test = (measurements_test
-                             + prior_lambda * prior_weight) \
-            / (1 + prior_weight)
-        kernel = measurements_train @ np.log(measurements_test).T
-        rdm = np.expand_dims(np.diag(kernel), 0) + \
-            np.expand_dims(np.diag(kernel), 1) - kernel - kernel.T
-        rdm = _extract_triu_(rdm) / measurements_train.shape[1]
-    return _build_rdms(rdm, dataset, 'poisson_cv', descriptor)
-
-
-def _calc_rdm_crossnobis_single(meas1, meas2, noise) -> NDArray:
-    kernel = meas1 @ noise @ meas2.T
-    rdm = np.expand_dims(np.diag(kernel), 0) + \
-        np.expand_dims(np.diag(kernel), 1) - kernel - kernel.T
-    return _extract_triu_(rdm) / meas1.shape[1]
-
-
-def _gen_default_cv_descriptor(dataset, descriptor) -> np.ndarray:
-    """ generates a default cv_descriptor for crossnobis
-    This assumes that the first occurence each descriptor value forms the
-    first group, the second occurence forms the second group, etc.
-    """
-    desc = dataset.obs_descriptors[descriptor]
-    values, counts = np.unique(desc, return_counts=True)
-    assert np.all(counts == counts[0]), (
-        'cv_descriptor generation failed:\n'
-        + 'different number of observations per pattern')
-    n_repeats = counts[0]
-    cv_descriptor = np.zeros_like(desc)
-    for i_val in values:
-        cv_descriptor[desc == i_val] = np.arange(n_repeats)
-    return cv_descriptor
-
-
-def _parse_input(
-            dataset: DatasetBase,
-            descriptor: Optional[str]
-        ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
-    if descriptor is None:
-        measurements = dataset.measurements
-        desc = None
-    else:
-        measurements, desc, _ = average_dataset_by(dataset, descriptor)
-    return measurements, desc
-
-
-def _check_noise(noise, n_channel):
-    """
-    checks that a noise pattern is a matrix with correct dimension
-    n_channel x n_channel
-
-    Args:
-        noise: noise input to be checked
-
-    Returns:
-        noise(np.ndarray): n_channel x n_channel noise precision matrix
-
-    """
-    if noise is None:
-        pass
-    elif isinstance(noise, np.ndarray) and noise.ndim == 2:
-        assert np.all(noise.shape == (n_channel, n_channel))
-    elif isinstance(noise, Iterable):
-        for idx, noise_i in enumerate(noise):
-            noise[idx] = _check_noise(noise_i, n_channel)
-    elif isinstance(noise, dict):
-        for key in noise.keys():
-            noise[key] = _check_noise(noise[key], n_channel)
-    else:
-        raise ValueError('noise(s) must have shape n_channel x n_channel')
-    return noise
-
-
-def _build_rdms(
-            utv: NDArray,
-            ds: DatasetBase,
-            method: str,
-            obs_desc_name: str | None,
-            obs_desc_vals: Optional[NDArray] = None,
-            cv: Optional[NDArray] = None,
-            noise: Optional[NDArray] = None
-        ) -> RDMs:
-    rdms = RDMs(
-        dissimilarities=np.array([utv]),
-        dissimilarity_measure=method,
-        rdm_descriptors=deepcopy(ds.descriptors)
-    )
-    if (obs_desc_vals is None) and (obs_desc_name is not None):
-        # obtain the unique values in the target obs descriptor
-        _, obs_desc_vals, _ = average_dataset_by(ds, obs_desc_name)
-
-    if _averaging_occurred(ds, obs_desc_name, obs_desc_vals):
-        orig_obs_desc_vals = np.asarray(ds.obs_descriptors[obs_desc_name])
-        for dname, dvals in ds.obs_descriptors.items():
-            dvals = np.asarray(dvals)
-            avg_dvals = np.full_like(obs_desc_vals, np.nan, dtype=dvals.dtype)
-            for i, v in enumerate(obs_desc_vals):
-                subset = dvals[orig_obs_desc_vals == v]
-                if len(set(subset)) > 1:
-                    break
-                avg_dvals[i] = subset[0]
-            else:
-                rdms.pattern_descriptors[dname] = avg_dvals
-    else:
-        rdms.pattern_descriptors = deepcopy(ds.obs_descriptors)
-    # Additional rdm_descriptors
-    if noise is not None:
-        rdms.descriptors['noise'] = noise
-    if cv is not None:
-        rdms.descriptors['cv_descriptor'] = cv
-    return rdms
-
-
-def _averaging_occurred(
-            ds: DatasetBase,
-            obs_desc_name: str | None,
-            obs_desc_vals: NDArray | None
-        ) -> bool:
-    if obs_desc_name is None:
-        return False
-    orig_obs_desc_vals = ds.obs_descriptors[obs_desc_name]
-    return len(obs_desc_vals) != len(orig_obs_desc_vals)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Calculation of RDMs from datasets
+@author: heiko, benjamin
+"""
+from __future__ import annotations
+from collections.abc import Iterable
+from copy import deepcopy
+from typing import TYPE_CHECKING, Optional, Tuple
+import numpy as np
+from rsatoolbox.rdm.rdms import RDMs
+from rsatoolbox.rdm.rdms import concat
+from rsatoolbox.rdm.combine import from_partials
+from rsatoolbox.data import average_dataset_by
+from rsatoolbox.util.rdm_utils import _extract_triu_
+if TYPE_CHECKING:
+    from rsatoolbox.data.base import DatasetBase
+    from numpy.typing import NDArray
+
+
+def calc_rdm(dataset, method='euclidean', descriptor=None, noise=None,
+             cv_descriptor=None, prior_lambda=1, prior_weight=0.1):
+    """
+    calculates an RDM from an input dataset
+
+    This should usually be called with the method and the descriptor argument
+    to specify the dissimilarity measure and which observations in the dataset
+    belong to which condition.
+
+    Args:
+        dataset (rsatoolbox.data.dataset.DatasetBase):
+            The dataset the RDM is computed from
+        method (String):
+            a description of the dissimilarity measure (e.g. 'Euclidean')
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+        noise (numpy.ndarray):
+            dataset.n_channel x dataset.n_channel
+            precision matrix used to calculate the RDM
+            used only for Mahalanobis and Crossnobis estimators
+            defaults to an identity matrix, i.e. euclidean distance
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    if isinstance(dataset, Iterable):
+        rdms = []
+        for i_dat, ds_i in enumerate(dataset):
+            if noise is None:
+                rdms.append(calc_rdm(
+                    ds_i, method=method,
+                    descriptor=descriptor,
+                    cv_descriptor=cv_descriptor,
+                    prior_lambda=prior_lambda, prior_weight=prior_weight))
+            elif isinstance(noise, np.ndarray) and noise.ndim == 2:
+                rdms.append(calc_rdm(
+                    ds_i, method=method,
+                    descriptor=descriptor,
+                    noise=noise,
+                    cv_descriptor=cv_descriptor,
+                    prior_lambda=prior_lambda, prior_weight=prior_weight))
+            elif isinstance(noise, Iterable):
+                rdms.append(calc_rdm(
+                    ds_i, method=method,
+                    descriptor=descriptor,
+                    noise=noise[i_dat],
+                    cv_descriptor=cv_descriptor,
+                    prior_lambda=prior_lambda, prior_weight=prior_weight))
+        if descriptor is None:
+            rdm = concat(rdms)
+        else:
+            rdm = from_partials(rdms, descriptor=descriptor)
+    else:
+        if method == 'euclidean':
+            rdm = calc_rdm_euclidean(dataset, descriptor)
+        elif method == 'correlation':
+            rdm = calc_rdm_correlation(dataset, descriptor)
+        elif method == 'mahalanobis':
+            rdm = calc_rdm_mahalanobis(dataset, descriptor, noise)
+        elif method == 'crossnobis':
+            rdm = calc_rdm_crossnobis(dataset, descriptor, noise,
+                                      cv_descriptor)
+        elif method == 'poisson':
+            rdm = calc_rdm_poisson(dataset, descriptor,
+                                   prior_lambda=prior_lambda,
+                                   prior_weight=prior_weight)
+        elif method == 'poisson_cv':
+            rdm = calc_rdm_poisson_cv(dataset, descriptor,
+                                      cv_descriptor=cv_descriptor,
+                                      prior_lambda=prior_lambda,
+                                      prior_weight=prior_weight)
+        else:
+            raise NotImplementedError
+        if descriptor is not None:
+            rdm.sort_by(**{descriptor: 'alpha'})
+    return rdm
+
+
+def calc_rdm_movie(
+        dataset, method='euclidean', descriptor=None, noise=None,
+        cv_descriptor=None, prior_lambda=1, prior_weight=0.1,
+        time_descriptor='time', bins=None):
+    """
+    calculates an RDM movie from an input TemporalDataset
+
+    Args:
+        dataset (rsatoolbox.data.dataset.TemporalDataset):
+            The dataset the RDM is computed from
+        method (String):
+            a description of the dissimilarity measure (e.g. 'Euclidean')
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+        noise (numpy.ndarray):
+            dataset.n_channel x dataset.n_channel
+            precision matrix used to calculate the RDM
+            used only for Mahalanobis and Crossnobis estimators
+            defaults to an identity matrix, i.e. euclidean distance
+        time_descriptor (String): descriptor key that points to the time
+            dimension in dataset.time_descriptors. Defaults to 'time'.
+        bins (array-like): list of bins, with bins[i] containing the vector
+            of time-points for the i-th bin. Defaults to no binning.
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with RDM movie
+    """
+
+    if isinstance(dataset, Iterable):
+        rdms = []
+        for i_dat, ds_i in enumerate(dataset):
+            if noise is None:
+                rdms.append(calc_rdm_movie(
+                    ds_i, method=method,
+                    descriptor=descriptor))
+            elif isinstance(noise, np.ndarray) and noise.ndim == 2:
+                rdms.append(calc_rdm_movie(
+                    ds_i, method=method,
+                    descriptor=descriptor,
+                    noise=noise))
+            elif isinstance(noise, Iterable):
+                rdms.append(calc_rdm_movie(
+                    ds_i, method=method,
+                    descriptor=descriptor,
+                    noise=noise[i_dat]))
+        rdm = concat(rdms)
+    else:
+        if bins is not None:
+            binned_data = dataset.bin_time(time_descriptor, bins)
+            splited_data = binned_data.split_time(time_descriptor)
+            time = binned_data.time_descriptors[time_descriptor]
+        else:
+            splited_data = dataset.split_time(time_descriptor)
+            time = dataset.time_descriptors[time_descriptor]
+
+        rdms = []
+        for dat in splited_data:
+            dat_single = dat.convert_to_dataset(time_descriptor)
+            rdms.append(calc_rdm(dat_single, method=method,
+                                 descriptor=descriptor, noise=noise,
+                                 cv_descriptor=cv_descriptor,
+                                 prior_lambda=prior_lambda,
+                                 prior_weight=prior_weight))
+
+        rdm = concat(rdms)
+        rdm.rdm_descriptors[time_descriptor] = time
+    return rdm
+
+
+def calc_rdm_euclidean(dataset, descriptor=None):
+    """
+    Args:
+        dataset (rsatoolbox.data.DatasetBase):
+            The dataset the RDM is computed from
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+            defaults to one row/column per row in the dataset
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+    """
+
+    measurements, desc = _parse_input(dataset, descriptor)
+    sum_sq_measurements = np.sum(measurements**2, axis=1, keepdims=True)
+    rdm = sum_sq_measurements + sum_sq_measurements.T \
+        - 2 * np.dot(measurements, measurements.T)
+    rdm = _extract_triu_(rdm) / measurements.shape[1]
+    return _build_rdms(rdm, dataset, 'squared euclidean', descriptor, desc)
+
+
+def calc_rdm_correlation(dataset, descriptor=None):
+    """
+    calculates an RDM from an input dataset using correlation distance
+    If multiple instances of the same condition are found in the dataset
+    they are averaged.
+
+    Args:
+        dataset (rsatoolbox.data.DatasetBase):
+            The dataset the RDM is computed from
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+            defaults to one row/column per row in the dataset
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    ma, desc = _parse_input(dataset, descriptor)
+    ma = ma - ma.mean(axis=1, keepdims=True)
+    ma /= np.sqrt(np.einsum('ij,ij->i', ma, ma))[:, None]
+    rdm = 1 - np.einsum('ik,jk', ma, ma)
+    return _build_rdms(rdm, dataset, 'correlation', descriptor, desc)
+
+
+def calc_rdm_mahalanobis(dataset, descriptor=None, noise=None):
+    """
+    calculates an RDM from an input dataset using mahalanobis distance
+    If multiple instances of the same condition are found in the dataset
+    they are averaged.
+
+    Args:
+        dataset (rsatoolbox.data.dataset.DatasetBase):
+            The dataset the RDM is computed from
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+            defaults to one row/column per row in the dataset
+        noise (numpy.ndarray):
+            dataset.n_channel x dataset.n_channel
+            precision matrix used to calculate the RDM
+            default: identity matrix, i.e. euclidean distance
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    if noise is None:
+        return calc_rdm_euclidean(dataset, descriptor)
+    measurements, desc = _parse_input(dataset, descriptor)
+    noise = _check_noise(noise, dataset.n_channel)
+    kernel = measurements @ noise @ measurements.T
+    rdm = np.expand_dims(np.diag(kernel), 0) + \
+        np.expand_dims(np.diag(kernel), 1) - 2 * kernel
+    rdm = _extract_triu_(rdm) / measurements.shape[1]
+    return _build_rdms(
+        rdm,
+        dataset,
+        'squared mahalanobis',
+        descriptor,
+        desc,
+        noise=noise
+    )
+
+
+def calc_rdm_crossnobis(dataset, descriptor, noise=None,
+                        cv_descriptor=None):
+    """
+    calculates an RDM from an input dataset using Cross-nobis distance
+    This performs leave one out crossvalidation over the cv_descriptor.
+
+    As the minimum input provide a dataset and a descriptor-name to
+    define the rows & columns of the RDM.
+    You may pass a noise precision. If you don't an identity is assumed.
+    Also a cv_descriptor can be passed to define the crossvalidation folds.
+    It is recommended to do this, to assure correct calculations. If you do
+    not, this function infers a split in order of the dataset, which is
+    guaranteed to fail if there are any unbalances.
+
+    This function also accepts a list of noise precision matricies.
+    It is then assumed that this is the precision of the mean from
+    the corresponding crossvalidation fold, i.e. if multiple measurements
+    enter a fold, please compute the resulting noise precision in advance!
+
+    To assert equal ordering in the folds the dataset is initially sorted
+    according to the descriptor used to define the patterns.
+
+    Args:
+        dataset (rsatoolbox.data.dataset.DatasetBase):
+            The dataset the RDM is computed from
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+            defaults to one row/column per row in the dataset
+        noise (numpy.ndarray):
+            dataset.n_channel x dataset.n_channel
+            precision matrix used to calculate the RDM
+            default: identity matrix, i.e. euclidean distance
+        cv_descriptor (String):
+            obs_descriptor which determines the cross-validation folds
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    noise = _check_noise(noise, dataset.n_channel)
+    if noise is None:
+        noise = np.eye(dataset.n_channel)
+    if descriptor is None:
+        raise ValueError('descriptor must be a string! Crossvalidation' +
+                         'requires multiple measurements to be grouped')
+    datasetCopy = deepcopy(dataset)
+    if cv_descriptor is None:
+        cv_desc = _gen_default_cv_descriptor(datasetCopy, descriptor)
+        datasetCopy.obs_descriptors['cv_desc'] = cv_desc
+        cv_descriptor = 'cv_desc'
+    datasetCopy.sort_by(descriptor)
+    cv_folds = np.unique(np.array(datasetCopy.obs_descriptors[cv_descriptor]))
+    rdms = []
+    if (noise is None) or (isinstance(noise, np.ndarray) and noise.ndim == 2):
+        for i_fold, fold in enumerate(cv_folds):
+            data_test = datasetCopy.subset_obs(cv_descriptor, fold)
+            data_train = datasetCopy.subset_obs(
+                cv_descriptor,
+                np.setdiff1d(cv_folds, fold)
+            )
+            measurements_train, _, _ = \
+                average_dataset_by(data_train, descriptor)
+            measurements_test, _, _ = \
+                average_dataset_by(data_test, descriptor)
+            rdm = _calc_rdm_crossnobis_single(
+                measurements_train, measurements_test, noise)
+            rdms.append(rdm)
+    else:  # a list of noises was provided
+        measurements = []
+        variances = []
+        for i, i_fold in enumerate(cv_folds):
+            data = datasetCopy.subset_obs(cv_descriptor, i_fold)
+            measurements.append(average_dataset_by(data, descriptor)[0])
+            variances.append(np.linalg.inv(noise[i]))
+        for i_fold in range(len(cv_folds)):
+            for j_fold in range(i_fold + 1, len(cv_folds)):
+                if i_fold != j_fold:
+                    rdm = _calc_rdm_crossnobis_single(
+                        measurements[i_fold], measurements[j_fold],
+                        np.linalg.inv(
+                            (variances[i_fold] + variances[j_fold]) / 2)
+                        )
+                    rdms.append(rdm)
+    rdms = np.array(rdms)
+    rdm = np.einsum('ij->j', rdms) / rdms.shape[0]
+    return _build_rdms(
+        rdm,
+        datasetCopy,
+        'crossnobis',
+        descriptor,
+        noise=noise,
+        cv=cv_descriptor
+    )
+
+
+def calc_rdm_poisson(dataset, descriptor=None, prior_lambda=1,
+                     prior_weight=0.1):
+    """
+    calculates an RDM from an input dataset using the symmetrized
+    KL-divergence assuming a poisson distribution.
+    If multiple instances of the same condition are found in the dataset
+    they are averaged.
+
+    Args:
+        dataset (rsatoolbox.data.DatasetBase):
+            The dataset the RDM is computed from
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+            defaults to one row/column per row in the dataset
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    measurements, desc = _parse_input(dataset, descriptor)
+    measurements = (measurements + prior_lambda * prior_weight) \
+        / (1 + prior_weight)
+    kernel = measurements @ np.log(measurements).T
+    rdm = np.expand_dims(np.diag(kernel), 0) + \
+        np.expand_dims(np.diag(kernel), 1) - kernel - kernel.T
+    rdm = _extract_triu_(rdm) / measurements.shape[1]
+    return _build_rdms(rdm, dataset, 'poisson', descriptor, desc)
+
+
+def calc_rdm_poisson_cv(dataset, descriptor=None, prior_lambda=1,
+                        prior_weight=0.1, cv_descriptor=None):
+    """
+    calculates an RDM from an input dataset using the crossvalidated
+    symmetrized KL-divergence assuming a poisson distribution
+
+    To assert equal ordering in the folds the dataset is initially sorted
+    according to the descriptor used to define the patterns.
+
+    Args:
+        dataset (rsatoolbox.data.DatasetBase):
+            The dataset the RDM is computed from
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+            defaults to one row/column per row in the dataset
+        cv_descriptor (str): The descriptor that indicates the folds
+            to use for crossvalidation
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    if descriptor is None:
+        raise ValueError('descriptor must be a string! Crossvalidation' +
+                         'requires multiple measurements to be grouped')
+    dataset = deepcopy(dataset)
+    if cv_descriptor is None:
+        cv_desc = _gen_default_cv_descriptor(dataset, descriptor)
+        dataset.obs_descriptors['cv_desc'] = cv_desc
+        cv_descriptor = 'cv_desc'
+    dataset.sort_by(descriptor)
+    cv_folds = np.unique(np.array(dataset.obs_descriptors[cv_descriptor]))
+    for i_fold in range(len(cv_folds)):
+        fold = cv_folds[i_fold]
+        data_test = dataset.subset_obs(cv_descriptor, fold)
+        data_train = dataset.subset_obs(cv_descriptor,
+                                        np.setdiff1d(cv_folds, fold))
+        measurements_train, _, _ = average_dataset_by(data_train, descriptor)
+        measurements_test, _, _ = average_dataset_by(data_test, descriptor)
+        measurements_train = (measurements_train
+                              + prior_lambda * prior_weight) \
+            / (1 + prior_weight)
+        measurements_test = (measurements_test
+                             + prior_lambda * prior_weight) \
+            / (1 + prior_weight)
+        kernel = measurements_train @ np.log(measurements_test).T
+        rdm = np.expand_dims(np.diag(kernel), 0) + \
+            np.expand_dims(np.diag(kernel), 1) - kernel - kernel.T
+        rdm = _extract_triu_(rdm) / measurements_train.shape[1]
+    return _build_rdms(rdm, dataset, 'poisson_cv', descriptor)
+
+
+def _calc_rdm_crossnobis_single(meas1, meas2, noise) -> NDArray:
+    kernel = meas1 @ noise @ meas2.T
+    rdm = np.expand_dims(np.diag(kernel), 0) + \
+        np.expand_dims(np.diag(kernel), 1) - kernel - kernel.T
+    return _extract_triu_(rdm) / meas1.shape[1]
+
+
+def _gen_default_cv_descriptor(dataset, descriptor) -> np.ndarray:
+    """ generates a default cv_descriptor for crossnobis
+    This assumes that the first occurence each descriptor value forms the
+    first group, the second occurence forms the second group, etc.
+    """
+    desc = dataset.obs_descriptors[descriptor]
+    values, counts = np.unique(desc, return_counts=True)
+    assert np.all(counts == counts[0]), (
+        'cv_descriptor generation failed:\n'
+        + 'different number of observations per pattern')
+    n_repeats = counts[0]
+    cv_descriptor = np.zeros_like(desc)
+    for i_val in values:
+        cv_descriptor[desc == i_val] = np.arange(n_repeats)
+    return cv_descriptor
+
+
+def _parse_input(
+            dataset: DatasetBase,
+            descriptor: Optional[str]
+        ) -> Tuple[np.ndarray, Optional[np.ndarray]]:
+    if descriptor is None:
+        measurements = dataset.measurements
+        desc = None
+    else:
+        measurements, desc, _ = average_dataset_by(dataset, descriptor)
+    return measurements, desc
+
+
+def _check_noise(noise, n_channel):
+    """
+    checks that a noise pattern is a matrix with correct dimension
+    n_channel x n_channel
+
+    Args:
+        noise: noise input to be checked
+
+    Returns:
+        noise(np.ndarray): n_channel x n_channel noise precision matrix
+
+    """
+    if noise is None:
+        pass
+    elif isinstance(noise, np.ndarray) and noise.ndim == 2:
+        assert np.all(noise.shape == (n_channel, n_channel))
+    elif isinstance(noise, Iterable):
+        for idx, noise_i in enumerate(noise):
+            noise[idx] = _check_noise(noise_i, n_channel)
+    elif isinstance(noise, dict):
+        for key in noise.keys():
+            noise[key] = _check_noise(noise[key], n_channel)
+    else:
+        raise ValueError('noise(s) must have shape n_channel x n_channel')
+    return noise
+
+
+def _build_rdms(
+            utv: NDArray,
+            ds: DatasetBase,
+            method: str,
+            obs_desc_name: str | None,
+            obs_desc_vals: Optional[NDArray] = None,
+            cv: Optional[NDArray] = None,
+            noise: Optional[NDArray] = None
+        ) -> RDMs:
+    rdms = RDMs(
+        dissimilarities=np.array([utv]),
+        dissimilarity_measure=method,
+        rdm_descriptors=deepcopy(ds.descriptors)
+    )
+    if (obs_desc_vals is None) and (obs_desc_name is not None):
+        # obtain the unique values in the target obs descriptor
+        _, obs_desc_vals, _ = average_dataset_by(ds, obs_desc_name)
+
+    if _averaging_occurred(ds, obs_desc_name, obs_desc_vals):
+        orig_obs_desc_vals = np.asarray(ds.obs_descriptors[obs_desc_name])
+        for dname, dvals in ds.obs_descriptors.items():
+            dvals = np.asarray(dvals)
+            avg_dvals = np.full_like(obs_desc_vals, np.nan, dtype=dvals.dtype)
+            for i, v in enumerate(obs_desc_vals):
+                subset = dvals[orig_obs_desc_vals == v]
+                if len(set(subset)) > 1:
+                    break
+                avg_dvals[i] = subset[0]
+            else:
+                rdms.pattern_descriptors[dname] = avg_dvals
+    else:
+        rdms.pattern_descriptors = deepcopy(ds.obs_descriptors)
+    # Additional rdm_descriptors
+    if noise is not None:
+        rdms.descriptors['noise'] = noise
+    if cv is not None:
+        rdms.descriptors['cv_descriptor'] = cv
+    return rdms
+
+
+def _averaging_occurred(
+            ds: DatasetBase,
+            obs_desc_name: str | None,
+            obs_desc_vals: NDArray | None
+        ) -> bool:
+    if obs_desc_name is None:
+        return False
+    orig_obs_desc_vals = ds.obs_descriptors[obs_desc_name]
+    return len(obs_desc_vals) != len(orig_obs_desc_vals)
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/calc_unbalanced.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/calc_unbalanced.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,203 +1,203 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Calculation of RDMs from unbalanced datasets, i.e. datasets with different
-channels or numbers of measurements per dissimilarity
-
-@author: heiko
-"""
-from __future__ import annotations
-from typing import TYPE_CHECKING, Tuple, Union, List
-from collections.abc import Iterable
-from copy import deepcopy
-import warnings
-import numpy as np
-from rsatoolbox.rdm.rdms import RDMs
-from rsatoolbox.rdm.rdms import concat
-from rsatoolbox.rdm.calc import _build_rdms
-from rsatoolbox.util.data_utils import get_unique_inverse
-from rsatoolbox.util.matrix import row_col_indicator_rdm
-from rsatoolbox.cengine.similarity import calc_one, calc
-if TYPE_CHECKING:
-    from rsatoolbox.data.base import DatasetBase
-    from numpy.typing import NDArray
-    SingleOrMultiDataset = Union[DatasetBase, List[DatasetBase]]
-
-
-def calc_rdm_unbalanced(dataset: SingleOrMultiDataset, method='euclidean',
-                        descriptor=None, noise=None, cv_descriptor=None,
-                        prior_lambda=1, prior_weight=0.1,
-                        weighting='number', enforce_same=False) -> RDMs:
-    """
-    calculate a RDM from an input dataset for unbalanced datasets.
-
-    Args:
-        dataset (rsatoolbox.data.dataset.DatasetBase):
-            The dataset the RDM is computed from
-        method (String):
-            a description of the dissimilarity measure (e.g. 'Euclidean')
-        descriptor (String):
-            obs_descriptor used to define the rows/columns of the RDM
-        noise (numpy.ndarray):
-            dataset.n_channel x dataset.n_channel
-            precision matrix used to calculate the RDM
-            used only for Mahalanobis and Crossnobis estimators
-            defaults to an identity matrix, i.e. euclidean distance
-
-    Returns:
-        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
-
-    """
-    if isinstance(dataset, Iterable):
-        rdms = []
-        for i_dat, dat in enumerate(dataset):
-            if noise is None:
-                rdms.append(calc_rdm_unbalanced(
-                    dat, method=method, descriptor=descriptor,
-                    cv_descriptor=cv_descriptor,
-                    prior_lambda=prior_lambda, prior_weight=prior_weight,
-                    weighting=weighting, enforce_same=enforce_same))
-            elif isinstance(noise, np.ndarray) and noise.ndim == 2:
-                rdms.append(calc_rdm_unbalanced(
-                    dat, method=method,
-                    descriptor=descriptor,
-                    noise=noise,
-                    cv_descriptor=cv_descriptor,
-                    prior_lambda=prior_lambda, prior_weight=prior_weight,
-                    weighting=weighting, enforce_same=enforce_same))
-            elif isinstance(noise, Iterable):
-                rdms.append(calc_rdm_unbalanced(
-                    dat, method=method,
-                    descriptor=descriptor,
-                    noise=noise[i_dat],
-                    cv_descriptor=cv_descriptor,
-                    prior_lambda=prior_lambda, prior_weight=prior_weight,
-                    weighting=weighting, enforce_same=enforce_same))
-        rdm = concat(rdms)
-    else:
-        if descriptor is None:
-            dataset = deepcopy(dataset)
-            dataset.obs_descriptors['index'] = np.arange(dataset.n_obs)
-            descriptor = 'index'
-        if method == 'crossnobis' or method == 'poisson_cv':
-            if cv_descriptor is None:
-                if 'index' not in dataset.obs_descriptors.keys():
-                    dataset.obs_descriptors['index'] = np.arange(dataset.n_obs)
-                cv_descriptor = 'index'
-                warnings.warn('cv_descriptor not set, using index for now.'
-                              + 'This will only remove self-similarities.'
-                              + 'Effectively this assumes independent trials')
-        unique_cond, cond_indices = get_unique_inverse(
-            dataset.obs_descriptors[descriptor])
-        # unique_cond = set(dataset.obs_descriptors[descriptor])
-        if cv_descriptor is None:
-            cv_desc_int = np.arange(dataset.n_obs, dtype=int)
-            crossval = 0
-        else:
-            _, indices = np.unique(
-                dataset.obs_descriptors[cv_descriptor],
-                return_inverse=True
-            )
-            cv_desc_int = indices.astype(int)
-            crossval = 1
-        if method == 'euclidean':
-            method_idx = 1
-        elif method == 'correlation':
-            method_idx = 2
-        elif method in ['mahalanobis', 'crossnobis']:
-            method_idx = 3
-        elif method in ['poisson', 'poisson_cv']:
-            method_idx = 4
-        else:
-            raise ValueError(f'Unknown method: {method}')
-        if weighting == 'equal':
-            weight_idx = 0
-        else:
-            weight_idx = 1
-        cond_indices_int = cond_indices.astype(int)
-        rdm = calc(
-            ensure_double(dataset.measurements),
-            cond_indices_int,
-            cv_desc_int, len(unique_cond),
-            method_idx, noise,
-            prior_lambda, prior_weight,
-            weight_idx, crossval)
-        self_sim = rdm[:len(unique_cond)]
-        rdm = rdm[len(unique_cond):]
-        row_idx, col_idx = row_col_indicator_rdm(len(unique_cond))
-        rdm = np.array(rdm)
-        self_sim = np.array(self_sim)
-        rdm = row_idx @ self_sim + col_idx @ self_sim - 2 * rdm
-        rdm = _build_rdms(rdm, dataset, method, descriptor, unique_cond,
-                          cv_desc_int, noise)
-    return rdm
-
-
-def calc_one_similarity(data_i: DatasetBase, data_j: DatasetBase,
-                        cv_desc_i: NDArray, cv_desc_j: NDArray,
-                        method='euclidean',
-                        noise=None, weighting='number',
-                        prior_lambda=1, prior_weight=0.1
-                        ) -> Tuple[NDArray, NDArray]:
-    """
-    finds all pairs of vectors to be compared and calculates one distance
-
-    Args:
-        data_i (rsatoolbox.data.DatasetBase):
-            dataset for condition i
-        data_j (rsatoolbox.data.DatasetBase):
-            dataset for condition j
-        cv_desc_i(numpy.ndarray):
-            crossvalidation descriptor for condition i
-        cv_desc_j(numpy.ndarray):
-            crossvalidation descriptor for condition j
-        method (string):
-            which dissimilarity to compute
-        noise : numpy.ndarray (n_channels x n_channels), optional
-            the covariance or precision matrix over channels
-            necessary for calculation of mahalanobis distances
-
-    Returns:
-        (np.ndarray, np.ndarray) : (value, weight)
-            value is the dissimilarity
-            weight is the weight of the samples
-
-    """
-    if method == 'euclidean':
-        method_idx = 1
-    elif method == 'correlation':
-        method_idx = 2
-    elif method in ['mahalanobis', 'crossnobis']:
-        method_idx = 3
-    elif method in ['poisson', 'poisson_cv']:
-        method_idx = 4
-    else:
-        raise ValueError(f'Unknown method: {method}')
-    if weighting == 'equal':
-        weight_idx = 0
-    else:
-        weight_idx = 1
-    return calc_one(
-        ensure_double(data_i.measurements),
-        ensure_double(data_j.measurements),
-        cv_desc_i, cv_desc_j,
-        data_i.n_obs, data_j.n_obs,
-        method_idx, noise=noise,
-        prior_lambda=prior_lambda, prior_weight=prior_weight,
-        weighting=weight_idx)
-
-
-def ensure_double(a: NDArray) -> NDArray[np.float64]:
-    """If required, will convert the array datatype to Float64
-
-    This ensures compatibility with the underlying c type "double".
-    If the array is already compatible, it will pass through.
-    If it is an integer, a converted copy will be made.
-
-    Args:
-        a (NDArray): Numeric numpy array
-
-    Returns:
-        NDArray[np.float64]: The float64 version of the array
-    """
-    return a.astype(np.float64)
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Calculation of RDMs from unbalanced datasets, i.e. datasets with different
+channels or numbers of measurements per dissimilarity
+
+@author: heiko
+"""
+from __future__ import annotations
+from typing import TYPE_CHECKING, Tuple, Union, List
+from collections.abc import Iterable
+from copy import deepcopy
+import warnings
+import numpy as np
+from rsatoolbox.rdm.rdms import RDMs
+from rsatoolbox.rdm.rdms import concat
+from rsatoolbox.rdm.calc import _build_rdms
+from rsatoolbox.util.data_utils import get_unique_inverse
+from rsatoolbox.util.matrix import row_col_indicator_rdm
+from rsatoolbox.cengine.similarity import calc_one, calc
+if TYPE_CHECKING:
+    from rsatoolbox.data.base import DatasetBase
+    from numpy.typing import NDArray
+    SingleOrMultiDataset = Union[DatasetBase, List[DatasetBase]]
+
+
+def calc_rdm_unbalanced(dataset: SingleOrMultiDataset, method='euclidean',
+                        descriptor=None, noise=None, cv_descriptor=None,
+                        prior_lambda=1, prior_weight=0.1,
+                        weighting='number', enforce_same=False) -> RDMs:
+    """
+    calculate a RDM from an input dataset for unbalanced datasets.
+
+    Args:
+        dataset (rsatoolbox.data.dataset.DatasetBase):
+            The dataset the RDM is computed from
+        method (String):
+            a description of the dissimilarity measure (e.g. 'Euclidean')
+        descriptor (String):
+            obs_descriptor used to define the rows/columns of the RDM
+        noise (numpy.ndarray):
+            dataset.n_channel x dataset.n_channel
+            precision matrix used to calculate the RDM
+            used only for Mahalanobis and Crossnobis estimators
+            defaults to an identity matrix, i.e. euclidean distance
+
+    Returns:
+        rsatoolbox.rdm.rdms.RDMs: RDMs object with the one RDM
+
+    """
+    if isinstance(dataset, Iterable):
+        rdms = []
+        for i_dat, dat in enumerate(dataset):
+            if noise is None:
+                rdms.append(calc_rdm_unbalanced(
+                    dat, method=method, descriptor=descriptor,
+                    cv_descriptor=cv_descriptor,
+                    prior_lambda=prior_lambda, prior_weight=prior_weight,
+                    weighting=weighting, enforce_same=enforce_same))
+            elif isinstance(noise, np.ndarray) and noise.ndim == 2:
+                rdms.append(calc_rdm_unbalanced(
+                    dat, method=method,
+                    descriptor=descriptor,
+                    noise=noise,
+                    cv_descriptor=cv_descriptor,
+                    prior_lambda=prior_lambda, prior_weight=prior_weight,
+                    weighting=weighting, enforce_same=enforce_same))
+            elif isinstance(noise, Iterable):
+                rdms.append(calc_rdm_unbalanced(
+                    dat, method=method,
+                    descriptor=descriptor,
+                    noise=noise[i_dat],
+                    cv_descriptor=cv_descriptor,
+                    prior_lambda=prior_lambda, prior_weight=prior_weight,
+                    weighting=weighting, enforce_same=enforce_same))
+        rdm = concat(rdms)
+    else:
+        if descriptor is None:
+            dataset = deepcopy(dataset)
+            dataset.obs_descriptors['index'] = np.arange(dataset.n_obs)
+            descriptor = 'index'
+        if method == 'crossnobis' or method == 'poisson_cv':
+            if cv_descriptor is None:
+                if 'index' not in dataset.obs_descriptors.keys():
+                    dataset.obs_descriptors['index'] = np.arange(dataset.n_obs)
+                cv_descriptor = 'index'
+                warnings.warn('cv_descriptor not set, using index for now.'
+                              + 'This will only remove self-similarities.'
+                              + 'Effectively this assumes independent trials')
+        unique_cond, cond_indices = get_unique_inverse(
+            dataset.obs_descriptors[descriptor])
+        # unique_cond = set(dataset.obs_descriptors[descriptor])
+        if cv_descriptor is None:
+            cv_desc_int = np.arange(dataset.n_obs, dtype=int)
+            crossval = 0
+        else:
+            _, indices = np.unique(
+                dataset.obs_descriptors[cv_descriptor],
+                return_inverse=True
+            )
+            cv_desc_int = indices.astype(int)
+            crossval = 1
+        if method == 'euclidean':
+            method_idx = 1
+        elif method == 'correlation':
+            method_idx = 2
+        elif method in ['mahalanobis', 'crossnobis']:
+            method_idx = 3
+        elif method in ['poisson', 'poisson_cv']:
+            method_idx = 4
+        else:
+            raise ValueError(f'Unknown method: {method}')
+        if weighting == 'equal':
+            weight_idx = 0
+        else:
+            weight_idx = 1
+        cond_indices_int = cond_indices.astype(int)
+        rdm = calc(
+            ensure_double(dataset.measurements),
+            cond_indices_int,
+            cv_desc_int, len(unique_cond),
+            method_idx, noise,
+            prior_lambda, prior_weight,
+            weight_idx, crossval)
+        self_sim = rdm[:len(unique_cond)]
+        rdm = rdm[len(unique_cond):]
+        row_idx, col_idx = row_col_indicator_rdm(len(unique_cond))
+        rdm = np.array(rdm)
+        self_sim = np.array(self_sim)
+        rdm = row_idx @ self_sim + col_idx @ self_sim - 2 * rdm
+        rdm = _build_rdms(rdm, dataset, method, descriptor, unique_cond,
+                          cv_desc_int, noise)
+    return rdm
+
+
+def calc_one_similarity(data_i: DatasetBase, data_j: DatasetBase,
+                        cv_desc_i: NDArray, cv_desc_j: NDArray,
+                        method='euclidean',
+                        noise=None, weighting='number',
+                        prior_lambda=1, prior_weight=0.1
+                        ) -> Tuple[NDArray, NDArray]:
+    """
+    finds all pairs of vectors to be compared and calculates one distance
+
+    Args:
+        data_i (rsatoolbox.data.DatasetBase):
+            dataset for condition i
+        data_j (rsatoolbox.data.DatasetBase):
+            dataset for condition j
+        cv_desc_i(numpy.ndarray):
+            crossvalidation descriptor for condition i
+        cv_desc_j(numpy.ndarray):
+            crossvalidation descriptor for condition j
+        method (string):
+            which dissimilarity to compute
+        noise : numpy.ndarray (n_channels x n_channels), optional
+            the covariance or precision matrix over channels
+            necessary for calculation of mahalanobis distances
+
+    Returns:
+        (np.ndarray, np.ndarray) : (value, weight)
+            value is the dissimilarity
+            weight is the weight of the samples
+
+    """
+    if method == 'euclidean':
+        method_idx = 1
+    elif method == 'correlation':
+        method_idx = 2
+    elif method in ['mahalanobis', 'crossnobis']:
+        method_idx = 3
+    elif method in ['poisson', 'poisson_cv']:
+        method_idx = 4
+    else:
+        raise ValueError(f'Unknown method: {method}')
+    if weighting == 'equal':
+        weight_idx = 0
+    else:
+        weight_idx = 1
+    return calc_one(
+        ensure_double(data_i.measurements),
+        ensure_double(data_j.measurements),
+        cv_desc_i, cv_desc_j,
+        data_i.n_obs, data_j.n_obs,
+        method_idx, noise=noise,
+        prior_lambda=prior_lambda, prior_weight=prior_weight,
+        weighting=weight_idx)
+
+
+def ensure_double(a: NDArray) -> NDArray[np.float64]:
+    """If required, will convert the array datatype to Float64
+
+    This ensures compatibility with the underlying c type "double".
+    If the array is already compatible, it will pass through.
+    If it is an integer, a converted copy will be made.
+
+    Args:
+        a (NDArray): Numeric numpy array
+
+    Returns:
+        NDArray[np.float64]: The float64 version of the array
+    """
+    return a.astype(np.float64)
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/combine.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/combine.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,205 +1,205 @@
-"""Functions operating on a set of related RDMs objects
-"""
-from __future__ import annotations
-from copy import deepcopy
-from typing import TYPE_CHECKING, List, Optional, Tuple
-import numpy as np
-from numpy import sqrt, nan, inf, ndarray
-from scipy.spatial.distance import squareform
-import rsatoolbox.rdm.rdms
-if TYPE_CHECKING:
-    from rsatoolbox.rdm.rdms import RDMs
-
-
-def from_partials(
-        list_of_rdms: List[RDMs],
-        all_patterns: Optional[List[str]] = None,
-        descriptor: str = 'conds') -> RDMs:
-    """Make larger RDMs with missing values where needed
-
-    Any object-level descriptors will be turned into rdm_descriptors
-    if they do not match across objects.
-
-    Args:
-        list_of_rdms (list): List of RDMs objects
-        all_patterns (list, optional): The full list of pattern
-            descriptors. Defaults to None, in which case the full
-            list is the union of all input rdms' values for the
-            pattern descriptor chosen.
-        descriptor (str, optional): The pattern descriptor on the basis
-            of which to expand. Defaults to 'conds'.
-
-    Returns:
-        RDMs: Object containing all input rdms on the larger scale,
-            with missing values where required
-    """
-
-    def pdescs(rdms, descriptor):
-        return list(rdms.pattern_descriptors.get(descriptor, []))
-    if all_patterns is None:
-        all_patterns = []
-        for rdms in list_of_rdms:
-            all_patterns += pdescs(rdms, descriptor)
-        all_patterns = list(dict.fromkeys(all_patterns).keys())
-
-    n_rdms = sum([rdms.n_rdm for rdms in list_of_rdms])
-    n_patterns = len(all_patterns)
-    rdm_desc_names = []
-    descriptors = deepcopy(list_of_rdms[0].descriptors)
-    desc_diff_names = []
-    for rdms in list_of_rdms[1:]:
-        rdm_desc_names += list(rdms.rdm_descriptors.keys())
-        delete = []
-        for k, v in descriptors.items():
-            if k not in rdms.descriptors.keys():
-                desc_diff_names.append(k)
-                delete.append(k)
-            elif not np.all(rdms.descriptors[k] == v):
-                desc_diff_names.append(k)
-                delete.append(k)
-        for k in delete:
-            descriptors.pop(k)
-        for k, v in rdms.descriptors.items():
-            if k not in descriptors.keys() and k not in desc_diff_names:
-                desc_diff_names.append(k)
-
-    rdm_desc_names = set(rdm_desc_names + list(desc_diff_names))
-    rdm_descriptors = dict([(n, [None]*n_rdms) for n in rdm_desc_names])
-    measure = None
-    vector_len = int(n_patterns * (n_patterns-1) / 2)
-    vectors = np.full((n_rdms, vector_len), np.nan)
-    rdm_id = 0
-    for rdms in list_of_rdms:
-        measure = rdms.dissimilarity_measure
-        pidx = [all_patterns.index(i) for i in pdescs(rdms, descriptor)]
-        for rdm_local_id, utv in enumerate(rdms.dissimilarities):
-            rdm = np.full((len(all_patterns), len(all_patterns)), np.nan)
-            rdm[np.ix_(pidx, pidx)] = squareform(utv, checks=False)
-            vectors[rdm_id, :] = squareform(rdm, checks=False)
-            for name in rdm_descriptors.keys():
-                if name == 'index':
-                    rdm_descriptors['index'][rdm_id] = rdm_id
-                elif name in rdms.rdm_descriptors:
-                    val = rdms.rdm_descriptors[name][rdm_local_id]
-                    rdm_descriptors[name][rdm_id] = val
-                elif name in rdms.descriptors:
-                    rdm_descriptors[name][rdm_id] = rdms.descriptors[name]
-                else:
-                    rdm_descriptors[name] = None
-            rdm_id += 1
-    return rsatoolbox.rdm.RDMs(
-        dissimilarities=vectors,
-        dissimilarity_measure=measure,
-        descriptors=descriptors,
-        rdm_descriptors=rdm_descriptors,
-        pattern_descriptors=dict([(descriptor, all_patterns)])
-    )
-
-
-def rescale(rdms, method: str = 'evidence', threshold=1e-8):
-    """Bring RDMs closer together
-
-    Iteratively scales RDMs based on pairs in-common.
-    Also adds an RDM descriptor with the weights used.
-
-    Args:
-        method (str, optional): One of 'evidence', 'setsize' or
-            'simple'. Defaults to 'evidence'.
-        threshold (float): Stop iterating when the sum of squares 
-            difference between iterations is smaller than this value.
-            A smaller value means more iterations, but the algorithm
-            may not always converge.
-
-    Returns:
-        RDMs: RDMs object with the aligned RDMs
-    """
-    aligned, weights = _rescale(rdms.dissimilarities, method, threshold)
-    rdm_descriptors = deepcopy(rdms.rdm_descriptors)
-    if weights is not None:
-        rdm_descriptors['rescalingWeights'] = weights
-    return rsatoolbox.rdm.rdms.RDMs(
-        dissimilarities=aligned,
-        dissimilarity_measure=rdms.dissimilarity_measure,
-        descriptors=deepcopy(rdms.descriptors),
-        rdm_descriptors=rdm_descriptors,
-        pattern_descriptors=deepcopy(rdms.pattern_descriptors)
-    )
-
-
-def _mean(vectors: ndarray, weights: ndarray = None) -> ndarray:
-    """Weighted mean of RDM vectors, ignores nans
-
-    See :meth:`rsatoolbox.rdm.rdms.RDMs.mean`
-
-    Args:
-        vectors (ndarray): dissimilarity vectors of shape (nrdms, nconds)
-        weights (ndarray, optional): Same shape as vectors.
-
-    Returns:
-        ndarray: Average vector of shape (nconds,)
-    """
-    if weights is None:
-        weights = np.ones(vectors.shape)
-        weights[np.isnan(vectors)] = np.nan
-    weighted_sum = np.nansum(vectors * weights, axis=0)
-    return weighted_sum / np.nansum(weights, axis=0)
-
-
-def _ss(vectors: ndarray) -> ndarray:
-    """Sum of squares on the last dimension
-
-    Args:
-        vectors (ndarray): 1- or 2-dimensional data
-
-    Returns:
-        ndarray: the sum of squares, with an extra empty dimension
-    """
-    summed_squares = np.nansum(vectors ** 2, axis=vectors.ndim-1)
-    return np.expand_dims(summed_squares, axis=vectors.ndim-1)
-
-
-def _scale(vectors: ndarray) -> ndarray:
-    """Divide by the root sum of squares
-
-    Args:
-        vectors (ndarray): 1- or 2-dimensional data
-
-    Returns:
-        ndarray: input scaled
-    """
-    return vectors / sqrt(_ss(vectors))
-
-
-def _rescale(dissim: ndarray, method: str, threshold=1e-8) -> Tuple[ndarray, ndarray]:
-    """Rescale RDM vectors
-
-    See :meth:`rsatoolbox.rdm.combine.rescale`
-
-    Args:
-        dissim (ndarray): dissimilarity vectors, shape = (rdms, conds)
-        method (str): one of 'evidence', 'setsize' or 'simple'.
-
-    Returns:
-        (ndarray, ndarray): Tuple of the aligned dissimilarity vectors
-            and the weights used
-    """
-    n_rdms, n_conds = dissim.shape
-    if method == 'evidence':
-        weights = (dissim ** 2).clip(0.2 ** 2)
-    elif method == 'setsize':
-        setsize = np.isfinite(dissim).sum(axis=1)
-        weights = np.tile(1 / setsize, [n_conds, 1]).T
-    else:
-        weights = np.ones(dissim.shape)
-    weights[np.isnan(dissim)] = np.nan
-
-    current_estimate = _scale(_mean(dissim))
-    prev_estimate = np.full([n_conds, ], -inf)
-    while _ss(current_estimate - prev_estimate) > threshold:
-        prev_estimate = current_estimate.copy()
-        tiled_estimate = np.tile(current_estimate, [n_rdms, 1])
-        tiled_estimate[np.isnan(dissim)] = nan
-        aligned = _scale(dissim) * sqrt(_ss(tiled_estimate))
-        current_estimate = _scale(_mean(aligned, weights))
-
-    return aligned, weights
+"""Functions operating on a set of related RDMs objects
+"""
+from __future__ import annotations
+from copy import deepcopy
+from typing import TYPE_CHECKING, List, Optional, Tuple
+import numpy as np
+from numpy import sqrt, nan, inf, ndarray
+from scipy.spatial.distance import squareform
+import rsatoolbox.rdm.rdms
+if TYPE_CHECKING:
+    from rsatoolbox.rdm.rdms import RDMs
+
+
+def from_partials(
+        list_of_rdms: List[RDMs],
+        all_patterns: Optional[List[str]] = None,
+        descriptor: str = 'conds') -> RDMs:
+    """Make larger RDMs with missing values where needed
+
+    Any object-level descriptors will be turned into rdm_descriptors
+    if they do not match across objects.
+
+    Args:
+        list_of_rdms (list): List of RDMs objects
+        all_patterns (list, optional): The full list of pattern
+            descriptors. Defaults to None, in which case the full
+            list is the union of all input rdms' values for the
+            pattern descriptor chosen.
+        descriptor (str, optional): The pattern descriptor on the basis
+            of which to expand. Defaults to 'conds'.
+
+    Returns:
+        RDMs: Object containing all input rdms on the larger scale,
+            with missing values where required
+    """
+
+    def pdescs(rdms, descriptor):
+        return list(rdms.pattern_descriptors.get(descriptor, []))
+    if all_patterns is None:
+        all_patterns = []
+        for rdms in list_of_rdms:
+            all_patterns += pdescs(rdms, descriptor)
+        all_patterns = list(dict.fromkeys(all_patterns).keys())
+
+    n_rdms = sum([rdms.n_rdm for rdms in list_of_rdms])
+    n_patterns = len(all_patterns)
+    rdm_desc_names = []
+    descriptors = deepcopy(list_of_rdms[0].descriptors)
+    desc_diff_names = []
+    for rdms in list_of_rdms[1:]:
+        rdm_desc_names += list(rdms.rdm_descriptors.keys())
+        delete = []
+        for k, v in descriptors.items():
+            if k not in rdms.descriptors.keys():
+                desc_diff_names.append(k)
+                delete.append(k)
+            elif not np.all(rdms.descriptors[k] == v):
+                desc_diff_names.append(k)
+                delete.append(k)
+        for k in delete:
+            descriptors.pop(k)
+        for k, v in rdms.descriptors.items():
+            if k not in descriptors.keys() and k not in desc_diff_names:
+                desc_diff_names.append(k)
+
+    rdm_desc_names = set(rdm_desc_names + list(desc_diff_names))
+    rdm_descriptors = dict([(n, [None]*n_rdms) for n in rdm_desc_names])
+    measure = None
+    vector_len = int(n_patterns * (n_patterns-1) / 2)
+    vectors = np.full((n_rdms, vector_len), np.nan)
+    rdm_id = 0
+    for rdms in list_of_rdms:
+        measure = rdms.dissimilarity_measure
+        pidx = [all_patterns.index(i) for i in pdescs(rdms, descriptor)]
+        for rdm_local_id, utv in enumerate(rdms.dissimilarities):
+            rdm = np.full((len(all_patterns), len(all_patterns)), np.nan)
+            rdm[np.ix_(pidx, pidx)] = squareform(utv, checks=False)
+            vectors[rdm_id, :] = squareform(rdm, checks=False)
+            for name in rdm_descriptors.keys():
+                if name == 'index':
+                    rdm_descriptors['index'][rdm_id] = rdm_id
+                elif name in rdms.rdm_descriptors:
+                    val = rdms.rdm_descriptors[name][rdm_local_id]
+                    rdm_descriptors[name][rdm_id] = val
+                elif name in rdms.descriptors:
+                    rdm_descriptors[name][rdm_id] = rdms.descriptors[name]
+                else:
+                    rdm_descriptors[name] = None
+            rdm_id += 1
+    return rsatoolbox.rdm.RDMs(
+        dissimilarities=vectors,
+        dissimilarity_measure=measure,
+        descriptors=descriptors,
+        rdm_descriptors=rdm_descriptors,
+        pattern_descriptors=dict([(descriptor, all_patterns)])
+    )
+
+
+def rescale(rdms, method: str = 'evidence', threshold=1e-8):
+    """Bring RDMs closer together
+
+    Iteratively scales RDMs based on pairs in-common.
+    Also adds an RDM descriptor with the weights used.
+
+    Args:
+        method (str, optional): One of 'evidence', 'setsize' or
+            'simple'. Defaults to 'evidence'.
+        threshold (float): Stop iterating when the sum of squares 
+            difference between iterations is smaller than this value.
+            A smaller value means more iterations, but the algorithm
+            may not always converge.
+
+    Returns:
+        RDMs: RDMs object with the aligned RDMs
+    """
+    aligned, weights = _rescale(rdms.dissimilarities, method, threshold)
+    rdm_descriptors = deepcopy(rdms.rdm_descriptors)
+    if weights is not None:
+        rdm_descriptors['rescalingWeights'] = weights
+    return rsatoolbox.rdm.rdms.RDMs(
+        dissimilarities=aligned,
+        dissimilarity_measure=rdms.dissimilarity_measure,
+        descriptors=deepcopy(rdms.descriptors),
+        rdm_descriptors=rdm_descriptors,
+        pattern_descriptors=deepcopy(rdms.pattern_descriptors)
+    )
+
+
+def _mean(vectors: ndarray, weights: ndarray = None) -> ndarray:
+    """Weighted mean of RDM vectors, ignores nans
+
+    See :meth:`rsatoolbox.rdm.rdms.RDMs.mean`
+
+    Args:
+        vectors (ndarray): dissimilarity vectors of shape (nrdms, nconds)
+        weights (ndarray, optional): Same shape as vectors.
+
+    Returns:
+        ndarray: Average vector of shape (nconds,)
+    """
+    if weights is None:
+        weights = np.ones(vectors.shape)
+        weights[np.isnan(vectors)] = np.nan
+    weighted_sum = np.nansum(vectors * weights, axis=0)
+    return weighted_sum / np.nansum(weights, axis=0)
+
+
+def _ss(vectors: ndarray) -> ndarray:
+    """Sum of squares on the last dimension
+
+    Args:
+        vectors (ndarray): 1- or 2-dimensional data
+
+    Returns:
+        ndarray: the sum of squares, with an extra empty dimension
+    """
+    summed_squares = np.nansum(vectors ** 2, axis=vectors.ndim-1)
+    return np.expand_dims(summed_squares, axis=vectors.ndim-1)
+
+
+def _scale(vectors: ndarray) -> ndarray:
+    """Divide by the root sum of squares
+
+    Args:
+        vectors (ndarray): 1- or 2-dimensional data
+
+    Returns:
+        ndarray: input scaled
+    """
+    return vectors / sqrt(_ss(vectors))
+
+
+def _rescale(dissim: ndarray, method: str, threshold=1e-8) -> Tuple[ndarray, ndarray]:
+    """Rescale RDM vectors
+
+    See :meth:`rsatoolbox.rdm.combine.rescale`
+
+    Args:
+        dissim (ndarray): dissimilarity vectors, shape = (rdms, conds)
+        method (str): one of 'evidence', 'setsize' or 'simple'.
+
+    Returns:
+        (ndarray, ndarray): Tuple of the aligned dissimilarity vectors
+            and the weights used
+    """
+    n_rdms, n_conds = dissim.shape
+    if method == 'evidence':
+        weights = (dissim ** 2).clip(0.2 ** 2)
+    elif method == 'setsize':
+        setsize = np.isfinite(dissim).sum(axis=1)
+        weights = np.tile(1 / setsize, [n_conds, 1]).T
+    else:
+        weights = np.ones(dissim.shape)
+    weights[np.isnan(dissim)] = np.nan
+
+    current_estimate = _scale(_mean(dissim))
+    prev_estimate = np.full([n_conds, ], -inf)
+    while _ss(current_estimate - prev_estimate) > threshold:
+        prev_estimate = current_estimate.copy()
+        tiled_estimate = np.tile(current_estimate, [n_rdms, 1])
+        tiled_estimate[np.isnan(dissim)] = nan
+        aligned = _scale(dissim) * sqrt(_ss(tiled_estimate))
+        current_estimate = _scale(_mean(aligned, weights))
+
+    return aligned, weights
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/pairs.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/pairs.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,40 +1,40 @@
-"""Functions to select pairs
-"""
-# pylint: disable=redefined-builtin
-from __future__ import annotations
-from typing import TYPE_CHECKING
-from pandas import DataFrame
-import numpy
-from scipy.stats import rankdata
-if TYPE_CHECKING:
-    from rsatoolbox.rdm.rdms import RDMs
-
-
-def pairs_by_percentile(rdms: RDMs, min: float = 0, max: float = 100,
-        **kwargs) -> DataFrame:
-    """Select pairs within a percentile range.
-
-    Filter pairs first by providing the `with_pattern` argument.
-
-    Args:
-        rdms (RDMs): RDMs object
-        min (float, optional): Lower percentile bound. Defaults to 0.
-        max (float, optional): Upper percentile bound. Defaults to 100.
-        kwargs: Pattern Descriptor value to match.
-
-    Returns:
-        DataFrame: Wide form DataFrame where each row represents a pair.
-    """
-    (desc, val) = list(kwargs.items())[0]
-    row_mask = rdms.pattern_descriptors[desc] == val
-    mats = rdms.get_matrices()
-    row = mats[0, row_mask, :].squeeze()
-    pair_dissims = row[~row_mask]
-    percs = rankdata(pair_dissims, 'average') / pair_dissims.size * 100
-    matches = numpy.logical_and(percs >= min, percs <= max)
-    matches_mask = numpy.full_like(row, False, dtype=bool)
-    matches_mask[~row_mask] = matches
-    columns = dict()
-    columns[desc] = rdms.pattern_descriptors[desc][matches_mask]
-    columns['dissim'] = row[matches_mask]
-    return DataFrame(columns)
+"""Functions to select pairs
+"""
+# pylint: disable=redefined-builtin
+from __future__ import annotations
+from typing import TYPE_CHECKING
+from pandas import DataFrame
+import numpy
+from scipy.stats import rankdata
+if TYPE_CHECKING:
+    from rsatoolbox.rdm.rdms import RDMs
+
+
+def pairs_by_percentile(rdms: RDMs, min: float = 0, max: float = 100,
+        **kwargs) -> DataFrame:
+    """Select pairs within a percentile range.
+
+    Filter pairs first by providing the `with_pattern` argument.
+
+    Args:
+        rdms (RDMs): RDMs object
+        min (float, optional): Lower percentile bound. Defaults to 0.
+        max (float, optional): Upper percentile bound. Defaults to 100.
+        kwargs: Pattern Descriptor value to match.
+
+    Returns:
+        DataFrame: Wide form DataFrame where each row represents a pair.
+    """
+    (desc, val) = list(kwargs.items())[0]
+    row_mask = rdms.pattern_descriptors[desc] == val
+    mats = rdms.get_matrices()
+    row = mats[0, row_mask, :].squeeze()
+    pair_dissims = row[~row_mask]
+    percs = rankdata(pair_dissims, 'average') / pair_dissims.size * 100
+    matches = numpy.logical_and(percs >= min, percs <= max)
+    matches_mask = numpy.full_like(row, False, dtype=bool)
+    matches_mask[~row_mask] = matches
+    columns = dict()
+    columns[desc] = rdms.pattern_descriptors[desc][matches_mask]
+    columns['dissim'] = row[matches_mask]
+    return DataFrame(columns)
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/rdms.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/rdms.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,663 +1,663 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Definition of RSA RDMs class and subclasses
-
-@author: baihan
-"""
-from __future__ import annotations
-from typing import Dict, Optional
-from copy import deepcopy
-from collections.abc import Iterable
-import numpy as np
-from rsatoolbox.io.pandas import rdms_to_df
-from rsatoolbox.rdm.combine import _mean
-from rsatoolbox.util.rdm_utils import batch_to_vectors
-from rsatoolbox.util.rdm_utils import batch_to_matrices
-from rsatoolbox.util.descriptor_utils import format_descriptor
-from rsatoolbox.util.descriptor_utils import num_index
-from rsatoolbox.util.descriptor_utils import subset_descriptor
-from rsatoolbox.util.descriptor_utils import check_descriptor_length_error
-from rsatoolbox.util.descriptor_utils import append_descriptor
-from rsatoolbox.util.descriptor_utils import dict_to_list
-from rsatoolbox.util.descriptor_utils import desc_eq
-from rsatoolbox.util.data_utils import extract_dict
-from rsatoolbox.io.hdf5 import read_dict_hdf5, write_dict_hdf5
-from rsatoolbox.io.pkl import read_dict_pkl, write_dict_pkl
-from rsatoolbox.util.file_io import remove_file
-
-
-class RDMs:
-    """ RDMs class
-
-    Args:
-        dissimilarities (numpy.ndarray):
-            either a 2d np-array (n_rdm x vectorform of dissimilarities)
-            or a 3d np-array (n_rdm x n_cond x n_cond)
-        dissimilarity_measure (String):
-            a description of the dissimilarity measure (e.g. 'Euclidean')
-        descriptors (dict):
-            descriptors with 1 value per RDMs object
-        rdm_descriptors (dict):
-            descriptors with 1 value per RDM
-        pattern_descriptors (dict):
-            descriptors with 1 value per RDM column
-
-    Attributes:
-        n_rdm(int): number of rdms
-        n_cond(int): number of patterns
-
-    """
-    dissimilarities: np.ndarray
-    dissimilarity_measure: Optional[str]
-    descriptors: Dict
-    rdm_descriptors: Dict
-    pattern_descriptors: Dict
-
-    def __init__(self, dissimilarities,
-                 dissimilarity_measure=None,
-                 descriptors=None,
-                 rdm_descriptors=None,
-                 pattern_descriptors=None):
-        self.dissimilarities, self.n_rdm, self.n_cond = \
-            batch_to_vectors(dissimilarities)
-        if descriptors is None:
-            self.descriptors = {}
-        else:
-            self.descriptors = descriptors
-        if rdm_descriptors is None:
-            self.rdm_descriptors = {}
-        else:
-            for k, v in rdm_descriptors.items():
-                if not isinstance(v, Iterable) or isinstance(v, str):
-                    rdm_descriptors[k] = [v]
-            check_descriptor_length_error(rdm_descriptors,
-                                          'rdm_descriptors',
-                                          self.n_rdm)
-            self.rdm_descriptors = rdm_descriptors
-        if pattern_descriptors is None:
-            self.pattern_descriptors = {}
-        else:
-            for k, v in pattern_descriptors.items():
-                if not isinstance(v, Iterable) or isinstance(v, str):
-                    pattern_descriptors[k] = [v]
-            check_descriptor_length_error(pattern_descriptors,
-                                          'pattern_descriptors',
-                                          self.n_cond)
-            self.pattern_descriptors = pattern_descriptors
-        if 'index' not in self.pattern_descriptors.keys():
-            self.pattern_descriptors['index'] = list(range(self.n_cond))
-        if 'index' not in self.rdm_descriptors.keys():
-            self.rdm_descriptors['index'] = list(range(self.n_rdm))
-        self.dissimilarity_measure = dissimilarity_measure
-
-    def __repr__(self):
-        """
-        defines string which is printed for the object
-        """
-        return (f'rsatoolbox.rdm.{self.__class__.__name__}(\n'
-                f'dissimilarity_measure = \n{self.dissimilarity_measure}\n'
-                f'dissimilarities = \n{self.dissimilarities}\n'
-                f'descriptors = \n{self.descriptors}\n'
-                f'rdm_descriptors = \n{self.rdm_descriptors}\n'
-                f'pattern_descriptors = \n{self.pattern_descriptors}\n'
-                )
-
-    def __eq__(self, other: RDMs) -> bool:
-        """Test for equality
-        This magic method gets called when you compare two
-        RDMs objects: `rdms1 == rdms2`.
-        True if the objects are of the same type, and
-        dissimilarities and descriptors are equal.
-
-        Args:
-            other (RDMs): The second RDMs object to
-                compare this one with
-
-        Returns:
-            bool: True if equal
-        """
-        return all([
-            isinstance(other, RDMs),
-            np.all(self.dissimilarities == other.dissimilarities),
-            self.descriptors == other.descriptors,
-            desc_eq(self.rdm_descriptors, other.rdm_descriptors),
-            desc_eq(self.pattern_descriptors, other.pattern_descriptors),
-        ])
-
-    def __str__(self):
-        """
-        defines the output of print
-        """
-        string_desc = format_descriptor(self.descriptors)
-        rdm_desc = format_descriptor(self.rdm_descriptors)
-        pattern_desc = format_descriptor(self.pattern_descriptors)
-        diss = self.get_matrices()[0]
-        return (f'rsatoolbox.rdm.{self.__class__.__name__}\n'
-                f'{self.n_rdm} RDM(s) over {self.n_cond} conditions\n\n'
-                f'dissimilarity_measure = \n{self.dissimilarity_measure}\n\n'
-                f'dissimilarities[0] = \n{diss}\n\n'
-                f'descriptors: \n{string_desc}\n'
-                f'rdm_descriptors: \n{rdm_desc}\n'
-                f'pattern_descriptors: \n{pattern_desc}\n'
-                )
-
-    def __getitem__(self, idx):
-        """
-        allows indexing with []
-        and iterating over RDMs with `for rdm in rdms:`
-        """
-        dissimilarities = self.dissimilarities[np.array(idx)].reshape(
-            -1, self.dissimilarities.shape[1])
-        rdm_descriptors = subset_descriptor(self.rdm_descriptors, idx)
-        rdms = RDMs(dissimilarities,
-                    dissimilarity_measure=self.dissimilarity_measure,
-                    descriptors=self.descriptors,
-                    rdm_descriptors=rdm_descriptors,
-                    pattern_descriptors=self.pattern_descriptors)
-        return rdms
-
-    def __len__(self) -> int:
-        """
-        The number of RDMs in this stack.
-        Together with __getitem__, allows `reversed(rdms)`.
-        """
-        return self.n_rdm
-
-    def get_vectors(self):
-        """ Returns RDMs as np.ndarray with each RDM as a vector
-
-        Returns:
-            numpy.ndarray: RDMs as a matrix with one row per RDM
-
-        """
-        return self.dissimilarities
-
-    def get_matrices(self):
-        """ Returns RDMs as np.ndarray with each RDM as a matrix
-
-        Returns:
-            numpy.ndarray: RDMs as a 3-Tensor with one matrix per RDM
-
-        """
-        matrices, _, _ = batch_to_matrices(self.dissimilarities)
-        return matrices
-
-    def copy(self) -> RDMs:
-        """Return a copy of this object, with all properties
-        equal to the original's
-
-        Returns:
-            RDMs: Value copy
-        """
-        return RDMs(
-            dissimilarities=self.dissimilarities.copy(),
-            dissimilarity_measure=self.dissimilarity_measure,
-            descriptors=deepcopy(self.descriptors),
-            rdm_descriptors=deepcopy(self.rdm_descriptors),
-            pattern_descriptors=deepcopy(self.pattern_descriptors)
-        )
-
-    def subset_pattern(self, by, value):
-        """ Returns a smaller RDMs with patterns with certain descriptor values
-
-        Args:
-            by(String): the descriptor by which the subset selection
-                        is made from pattern_descriptors
-            value:      the value by which the subset selection is made
-                        from pattern_descriptors
-
-        Returns:
-            RDMs object, with fewer patterns
-
-        """
-        if by is None:
-            by = 'index'
-        if not isinstance(value, Iterable):
-            value = [value]
-        selection = num_index(self.pattern_descriptors[by], value)
-        ix, iy = np.triu_indices(self.n_cond, 1)
-        pattern_in_value = np.array(
-            [p in value for p in self.pattern_descriptors[by]])
-        selection_xy = pattern_in_value[ix] & pattern_in_value[iy]
-        dissimilarities = self.dissimilarities[:, selection_xy]
-        descriptors = self.descriptors
-        pattern_descriptors = extract_dict(
-            self.pattern_descriptors, selection)
-        rdm_descriptors = self.rdm_descriptors
-        dissimilarity_measure = self.dissimilarity_measure
-        rdms = RDMs(dissimilarities=dissimilarities,
-                    descriptors=descriptors,
-                    rdm_descriptors=rdm_descriptors,
-                    pattern_descriptors=pattern_descriptors,
-                    dissimilarity_measure=dissimilarity_measure)
-        return rdms
-
-    def subsample_pattern(self, by, value):
-        """ Returns a subsampled RDMs with repetitions if values are repeated
-
-        This function now generates Nans where the off-diagonal 0s would
-        appear. These values are trivial to predict for models and thus
-        need to be marked and excluded from the evaluation.
-
-        Args:
-            by(String): the descriptor by which the subset selection
-                        is made from descriptors
-            value:      the value(s) by which the subset selection is made
-                        from descriptors
-
-        Returns:
-            RDMs object, with subsampled patterns
-
-        """
-        if by is None:
-            by = 'index'
-        desc = np.array(self.pattern_descriptors[by])  # desc is list-like
-        if isinstance(value, (list, tuple, np.ndarray)):
-            selection = [np.asarray(desc == i).nonzero()[0]
-                         for i in value]
-            selection = np.concatenate(selection)
-        else:
-            selection = np.where(desc == value)[0]
-        selection = np.sort(selection)
-        dissimilarities = self.get_matrices()
-        for i_rdm in range(self.n_rdm):
-            np.fill_diagonal(dissimilarities[i_rdm], np.nan)
-        selection = np.sort(selection)
-        dissimilarities = dissimilarities[:, selection][:, :, selection]
-        descriptors = self.descriptors
-        pattern_descriptors = extract_dict(
-            self.pattern_descriptors, selection)
-        rdm_descriptors = self.rdm_descriptors
-        dissimilarity_measure = self.dissimilarity_measure
-        rdms = RDMs(dissimilarities=dissimilarities,
-                    descriptors=descriptors,
-                    rdm_descriptors=rdm_descriptors,
-                    pattern_descriptors=pattern_descriptors,
-                    dissimilarity_measure=dissimilarity_measure)
-        return rdms
-
-    def subset(self, by, value):
-        """ Returns a set of fewer RDMs matching descriptor values
-
-        Args:
-            by(String): the descriptor by which the subset selection
-                        is made from descriptors
-            value:      the value by which the subset selection is made
-                        from descriptors
-
-        Returns:
-            RDMs object, with fewer RDMs
-
-        """
-        if by is None:
-            by = 'index'
-        selection = num_index(self.rdm_descriptors[by], value)
-        dissimilarities = self.dissimilarities[selection, :]
-        descriptors = self.descriptors
-        pattern_descriptors = self.pattern_descriptors
-        rdm_descriptors = extract_dict(self.rdm_descriptors, selection)
-        dissimilarity_measure = self.dissimilarity_measure
-        rdms = RDMs(dissimilarities=dissimilarities,
-                    descriptors=descriptors,
-                    rdm_descriptors=rdm_descriptors,
-                    pattern_descriptors=pattern_descriptors,
-                    dissimilarity_measure=dissimilarity_measure)
-        return rdms
-
-    def subsample(self, by, value):
-        """ Returns a subsampled RDMs with repetitions if values are repeated
-
-        Args:
-            by(String): the descriptor by which the subset selection
-                        is made from descriptors
-            value:      the value by which the subset selection is made
-                        from descriptors
-
-        Returns:
-            RDMs object, with subsampled RDMs
-
-        """
-        if by is None:
-            by = 'index'
-        desc = self.rdm_descriptors[by]
-        selection = []
-        if isinstance(value, (list, tuple, np.ndarray)):
-            for i in value:
-                for j, d in enumerate(desc):
-                    if d == i:
-                        selection.append(j)
-        else:
-            for j, d in enumerate(desc):
-                if d == value:
-                    selection.append(j)
-        dissimilarities = self.dissimilarities[selection, :]
-        descriptors = self.descriptors
-        pattern_descriptors = self.pattern_descriptors
-        rdm_descriptors = extract_dict(self.rdm_descriptors, selection)
-        dissimilarity_measure = self.dissimilarity_measure
-        rdms = RDMs(dissimilarities=dissimilarities,
-                    descriptors=descriptors,
-                    rdm_descriptors=rdm_descriptors,
-                    pattern_descriptors=pattern_descriptors,
-                    dissimilarity_measure=dissimilarity_measure)
-        return rdms
-
-    def append(self, rdm):
-        """ appends an rdm to the object
-        The rdm should have the same shape and type as this object.
-        Its pattern_descriptor and descriptor are ignored
-
-        Args:
-            rdm(rsatoolbox.rdm.RDMs): the rdm to append
-
-        Returns:
-
-        """
-        assert isinstance(rdm, RDMs), 'appended rdm should be an RDMs'
-        assert rdm.n_cond == self.n_cond, 'appended rdm had wrong shape'
-        assert rdm.dissimilarity_measure == self.dissimilarity_measure, \
-            'appended rdm had wrong dissimilarity measure'
-        self.dissimilarities = np.concatenate((
-            self.dissimilarities, rdm.dissimilarities), axis=0)
-        self.rdm_descriptors = append_descriptor(self.rdm_descriptors,
-                                                 rdm.rdm_descriptors)
-        self.n_rdm = self.n_rdm + rdm.n_rdm
-
-    def save(self, filename, file_type='hdf5', overwrite=False):
-        """ saves the RDMs object into a file
-
-        Args:
-            filename(String): path to file to save to
-                [or opened file]
-            file_type(String): Type of file to create:
-                hdf5: hdf5 file
-                pkl: pickle file
-            overwrite(Boolean): overwrites file if it already exists
-
-        """
-        rdm_dict = self.to_dict()
-        if overwrite:
-            remove_file(filename)
-        if file_type == 'hdf5':
-            write_dict_hdf5(filename, rdm_dict)
-        elif file_type == 'pkl':
-            write_dict_pkl(filename, rdm_dict)
-
-    def to_dict(self):
-        """ converts the object into a dictionary, which can be saved to disk
-
-        Returns:
-            rdm_dict(dict): dictionary containing all information required to
-                recreate the RDMs object
-        """
-        rdm_dict = {}
-        rdm_dict['dissimilarities'] = self.dissimilarities
-        rdm_dict['descriptors'] = self.descriptors
-        rdm_dict['rdm_descriptors'] = self.rdm_descriptors
-        rdm_dict['pattern_descriptors'] = self.pattern_descriptors
-        rdm_dict['dissimilarity_measure'] = self.dissimilarity_measure
-        return rdm_dict
-
-    def to_df(self):
-        """Return a new long-form pandas DataFrame representing this RDM
-
-        See `rsatoolbox.io.pandas.rdms_to_df` for details
-
-        Returns:
-            pandas.DataFrame: The DataFrame for this RDMs object
-        """
-        return rdms_to_df(self)
-
-    def reorder(self, new_order):
-        """Reorder the patterns according to the index in new_order
-
-        Args:
-            new_order (numpy.ndarray): new order of patterns,
-                vector of length equal to the number of patterns
-        """
-        matrices = self.get_matrices()
-        matrices = matrices[(slice(None),) + np.ix_(new_order, new_order)]
-        self.dissimilarities = batch_to_vectors(matrices)[0]
-        for dname, descriptors in self.pattern_descriptors.items():
-            self.pattern_descriptors[dname] = [descriptors[idx] for idx in new_order]
-
-    def sort_by(self, **kwargs):
-        """Reorder the patterns by sorting a descriptor
-
-        Pass keyword arguments that correspond to descriptors,
-        with value indicating the sort type. Supported methods:
-
-            'alpha': sort alphabetically (using np.sort)
-
-            list/np.array: specify the new order explicitly. Values should
-                correspond to the descriptor values
-
-        Examples:
-
-            The following code sorts the 'condition' descriptor alphabetically:
-
-            ::
-
-                rdms.sort_by(condition='alpha')
-
-            The following code sort the 'condition' descriptor in the order
-            1, 3, 2, 4, 5:
-
-            ::
-
-                rdms.sort_by(condition=[1, 3, 2, 4, 5])
-
-        Raises:
-            ValueError: Raised if the method chosen is not implemented
-        """
-        for dname, method in kwargs.items():
-            if method == 'alpha':
-                descriptor = self.pattern_descriptors[dname]
-                self.reorder(np.argsort(descriptor))
-            elif isinstance(method, (list, np.ndarray)):
-                # in this case, `method` is the desired descriptor order
-                new_order = method
-                descriptor = self.pattern_descriptors[dname]
-                if not set(descriptor).issubset(new_order):
-                    raise ValueError(f'Expected {method} to be a permutation \
-                            or subset of {descriptor}')
-                # convert to indices to use `reorder` method
-                self.reorder([list(descriptor).index(x) for x in new_order])
-            else:
-                raise ValueError(f'Unknown sorting method: {method}')
-
-    def mean(self, weights=None):
-        """Average rdm of all rdms contained
-
-        Args:
-            weights (str or ndarray, optional): One of:
-                None: No weighting applied
-                str: Use the weights contained in the `rdm_descriptor` with this name
-                ndarray: Weights array of the shape of RDMs.dissimilarities
-
-        Returns:
-            `rsatoolbox.rdm.rdms.RDMs`: New RDMs object with one vector
-        """
-        if str(weights) in self.rdm_descriptors:
-            new_descriptors = {
-                (k, v) for (k, v) in self.descriptors.items() if k != weights
-            }
-            weights = self.rdm_descriptors[weights]
-        else:
-            new_descriptors = deepcopy(self.descriptors)
-        return RDMs(
-            dissimilarities=np.array([_mean(self.dissimilarities, weights)]),
-            dissimilarity_measure=self.dissimilarity_measure,
-            descriptors=new_descriptors,
-            pattern_descriptors=deepcopy(self.pattern_descriptors)
-        )
-
-
-def rdms_from_dict(rdm_dict):
-    """ creates a RDMs object from a dictionary
-
-    Args:
-        rdm_dict(dict): dictionary with information
-
-    Returns:
-        rdms(RDMs): the regenerated RDMs object
-
-    """
-    rdms = RDMs(dissimilarities=rdm_dict['dissimilarities'],
-                descriptors=rdm_dict['descriptors'],
-                rdm_descriptors=dict_to_list(rdm_dict['rdm_descriptors']),
-                pattern_descriptors=dict_to_list(rdm_dict['pattern_descriptors']),
-                dissimilarity_measure=rdm_dict['dissimilarity_measure'])
-    return rdms
-
-
-def load_rdm(filename, file_type=None):
-    """ loads a RDMs object from disk
-
-    Args:
-        filename(String): path to file to load
-
-    """
-    if file_type is None:
-        if isinstance(filename, str):
-            if filename[-4:] == '.pkl':
-                file_type = 'pkl'
-            elif filename[-3:] == '.h5' or filename[-4:] == 'hdf5':
-                file_type = 'hdf5'
-    if file_type == 'hdf5':
-        rdm_dict = read_dict_hdf5(filename)
-    elif file_type == 'pkl':
-        rdm_dict = read_dict_pkl(filename)
-    else:
-        raise ValueError('filetype not understood')
-    return rdms_from_dict(rdm_dict)
-
-
-def concat(*rdms):
-    """ concatenates rdm objects
-    requires that the rdms have the same shape
-    descriptor and pattern descriptors are taken from the first rdms object
-    for rdm_descriptors concatenation is tried
-    the rdm index is reinitialized
-
-    Args:
-        rdms(iterable of pyrsa.rdm.RDMs): RDMs objects to be concatenated
-        or multiple RDMs as separate arguments
-
-    Returns:
-        rsatoolbox.rdm.RDMs: concatenated rdms object
-
-    """
-    if len(rdms) == 1:
-        if isinstance(rdms[0], RDMs):
-            rdms_list = [rdms[0]]
-        else:
-            rdms_list = list(rdms[0])
-    else:
-        rdms_list = list(rdms)
-    assert isinstance(rdms_list[0], RDMs), \
-        'Supply list of RDMs objects, or RDMs objects as separate arguments'
-    rdm_descriptors = deepcopy(rdms_list[0].rdm_descriptors)
-    for rdm_new in rdms_list[1:]:
-        assert isinstance(rdm_new, RDMs), 'rdm for concat should be an RDMs'
-        assert rdm_new.n_cond == rdms_list[0].n_cond, 'rdm for concat had wrong shape'
-        assert rdm_new.dissimilarity_measure == rdms_list[0].dissimilarity_measure, \
-            'appended rdm had wrong dissimilarity measure'
-        rdm_descriptors = append_descriptor(rdm_descriptors, rdm_new.rdm_descriptors)
-    dissimilarities = np.concatenate([
-        rdm.dissimilarities
-        for rdm in rdms_list
-        ], axis=0)
-    rdm = RDMs(
-        dissimilarities=dissimilarities,
-        rdm_descriptors=rdm_descriptors,
-        descriptors=rdms_list[0].descriptors,
-        pattern_descriptors=rdms_list[0].pattern_descriptors
-    )
-    return rdm
-
-
-def permute_rdms(rdms, p=None):
-    """ Permute rows, columns and corresponding pattern descriptors
-    of RDM matrices according to a permutation vector
-
-    Args:
-        p (numpy.ndarray):
-           permutation vector (values must be unique integers
-           from 0 to n_cond of RDM matrix).
-           If p = None, a random permutation vector is created.
-
-    Returns:
-        rdm_p(rsatoolbox.rdm.RDMs): the rdm object with a permuted matrix
-            and pattern descriptors
-
-    """
-    if p is None:
-        p = np.random.permutation(rdms.n_cond)
-        print('No permutation vector specified,'
-              + ' performing random permutation.')
-
-    assert p.dtype == 'int', "permutation vector must have integer entries."
-    assert min(p) == 0 and max(p) == rdms.n_cond-1, \
-        "permutation vector must have entries ranging from 0 to n_cond"
-    assert len(np.unique(p)) == rdms.n_cond, \
-        "permutation vector must only have unique integer entries"
-
-    rdm_mats = rdms.get_matrices()
-    descriptors = rdms.descriptors.copy()
-    rdm_descriptors = rdms.rdm_descriptors.copy()
-    pattern_descriptors = rdms.pattern_descriptors.copy()
-
-    # To easily reverse permutation later
-    p_inv = np.arange(len(p))[np.argsort(p)]
-    descriptors.update({'p_inv': p_inv})
-    rdm_mats = rdm_mats[:, p, :]
-    rdm_mats = rdm_mats[:, :, p]
-    stims = np.array(pattern_descriptors['index'])
-    pattern_descriptors.update({'index': list(stims[p].astype(np.str_))})
-
-    rdms_p = RDMs(
-        dissimilarities=rdm_mats,
-        descriptors=descriptors,
-        rdm_descriptors=rdm_descriptors,
-        pattern_descriptors=pattern_descriptors)
-    return rdms_p
-
-
-def inverse_permute_rdms(rdms):
-    """ Gimmick function to reverse the effect of permute_rdms() """
-
-    p_inv = rdms.descriptors['p_inv']
-    rdms_p = permute_rdms(rdms, p=p_inv)
-    return rdms_p
-
-
-def get_categorical_rdm(category_vector, category_name='category'):
-    """ generates an RDM object containing a categorical RDM, i.e. RDM = 0
-    if the category is the same and 1 if they are different
-
-    Args:
-        category_vector(iterable): a category index per condition
-        category_name(String): name for the descriptor in the object, defaults
-            to 'category'
-
-    Returns:
-        rsatoolbox.rdm.RDMs: constructed RDM
-
-    """
-    n = len(category_vector)
-    rdm_list = []
-    for i_cat in range(n):
-        for j_cat in range(i_cat + 1, n):
-            if isinstance(category_vector[i_cat], Iterable):
-                comparisons = [np.array(category_vector[i_cat][idx])
-                               != np.array(category_vector[j_cat][idx])
-                               for idx in range(len(category_vector[i_cat]))]
-                rdm_list.append(np.any(comparisons))
-            else:
-                rdm_list.append(
-                    category_vector[i_cat] != category_vector[j_cat])
-    rdm = RDMs(np.array(rdm_list, dtype=float),
-               pattern_descriptors={category_name: np.array(category_vector)})
-    return rdm
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Definition of RSA RDMs class and subclasses
+
+@author: baihan
+"""
+from __future__ import annotations
+from typing import Dict, Optional
+from copy import deepcopy
+from collections.abc import Iterable
+import numpy as np
+from rsatoolbox.io.pandas import rdms_to_df
+from rsatoolbox.rdm.combine import _mean
+from rsatoolbox.util.rdm_utils import batch_to_vectors
+from rsatoolbox.util.rdm_utils import batch_to_matrices
+from rsatoolbox.util.descriptor_utils import format_descriptor
+from rsatoolbox.util.descriptor_utils import num_index
+from rsatoolbox.util.descriptor_utils import subset_descriptor
+from rsatoolbox.util.descriptor_utils import check_descriptor_length_error
+from rsatoolbox.util.descriptor_utils import append_descriptor
+from rsatoolbox.util.descriptor_utils import dict_to_list
+from rsatoolbox.util.descriptor_utils import desc_eq
+from rsatoolbox.util.data_utils import extract_dict
+from rsatoolbox.io.hdf5 import read_dict_hdf5, write_dict_hdf5
+from rsatoolbox.io.pkl import read_dict_pkl, write_dict_pkl
+from rsatoolbox.util.file_io import remove_file
+
+
+class RDMs:
+    """ RDMs class
+
+    Args:
+        dissimilarities (numpy.ndarray):
+            either a 2d np-array (n_rdm x vectorform of dissimilarities)
+            or a 3d np-array (n_rdm x n_cond x n_cond)
+        dissimilarity_measure (String):
+            a description of the dissimilarity measure (e.g. 'Euclidean')
+        descriptors (dict):
+            descriptors with 1 value per RDMs object
+        rdm_descriptors (dict):
+            descriptors with 1 value per RDM
+        pattern_descriptors (dict):
+            descriptors with 1 value per RDM column
+
+    Attributes:
+        n_rdm(int): number of rdms
+        n_cond(int): number of patterns
+
+    """
+    dissimilarities: np.ndarray
+    dissimilarity_measure: Optional[str]
+    descriptors: Dict
+    rdm_descriptors: Dict
+    pattern_descriptors: Dict
+
+    def __init__(self, dissimilarities,
+                 dissimilarity_measure=None,
+                 descriptors=None,
+                 rdm_descriptors=None,
+                 pattern_descriptors=None):
+        self.dissimilarities, self.n_rdm, self.n_cond = \
+            batch_to_vectors(dissimilarities)
+        if descriptors is None:
+            self.descriptors = {}
+        else:
+            self.descriptors = descriptors
+        if rdm_descriptors is None:
+            self.rdm_descriptors = {}
+        else:
+            for k, v in rdm_descriptors.items():
+                if not isinstance(v, Iterable) or isinstance(v, str):
+                    rdm_descriptors[k] = [v]
+            check_descriptor_length_error(rdm_descriptors,
+                                          'rdm_descriptors',
+                                          self.n_rdm)
+            self.rdm_descriptors = rdm_descriptors
+        if pattern_descriptors is None:
+            self.pattern_descriptors = {}
+        else:
+            for k, v in pattern_descriptors.items():
+                if not isinstance(v, Iterable) or isinstance(v, str):
+                    pattern_descriptors[k] = [v]
+            check_descriptor_length_error(pattern_descriptors,
+                                          'pattern_descriptors',
+                                          self.n_cond)
+            self.pattern_descriptors = pattern_descriptors
+        if 'index' not in self.pattern_descriptors.keys():
+            self.pattern_descriptors['index'] = list(range(self.n_cond))
+        if 'index' not in self.rdm_descriptors.keys():
+            self.rdm_descriptors['index'] = list(range(self.n_rdm))
+        self.dissimilarity_measure = dissimilarity_measure
+
+    def __repr__(self):
+        """
+        defines string which is printed for the object
+        """
+        return (f'rsatoolbox.rdm.{self.__class__.__name__}(\n'
+                f'dissimilarity_measure = \n{self.dissimilarity_measure}\n'
+                f'dissimilarities = \n{self.dissimilarities}\n'
+                f'descriptors = \n{self.descriptors}\n'
+                f'rdm_descriptors = \n{self.rdm_descriptors}\n'
+                f'pattern_descriptors = \n{self.pattern_descriptors}\n'
+                )
+
+    def __eq__(self, other: RDMs) -> bool:
+        """Test for equality
+        This magic method gets called when you compare two
+        RDMs objects: `rdms1 == rdms2`.
+        True if the objects are of the same type, and
+        dissimilarities and descriptors are equal.
+
+        Args:
+            other (RDMs): The second RDMs object to
+                compare this one with
+
+        Returns:
+            bool: True if equal
+        """
+        return all([
+            isinstance(other, RDMs),
+            np.all(self.dissimilarities == other.dissimilarities),
+            self.descriptors == other.descriptors,
+            desc_eq(self.rdm_descriptors, other.rdm_descriptors),
+            desc_eq(self.pattern_descriptors, other.pattern_descriptors),
+        ])
+
+    def __str__(self):
+        """
+        defines the output of print
+        """
+        string_desc = format_descriptor(self.descriptors)
+        rdm_desc = format_descriptor(self.rdm_descriptors)
+        pattern_desc = format_descriptor(self.pattern_descriptors)
+        diss = self.get_matrices()[0]
+        return (f'rsatoolbox.rdm.{self.__class__.__name__}\n'
+                f'{self.n_rdm} RDM(s) over {self.n_cond} conditions\n\n'
+                f'dissimilarity_measure = \n{self.dissimilarity_measure}\n\n'
+                f'dissimilarities[0] = \n{diss}\n\n'
+                f'descriptors: \n{string_desc}\n'
+                f'rdm_descriptors: \n{rdm_desc}\n'
+                f'pattern_descriptors: \n{pattern_desc}\n'
+                )
+
+    def __getitem__(self, idx):
+        """
+        allows indexing with []
+        and iterating over RDMs with `for rdm in rdms:`
+        """
+        dissimilarities = self.dissimilarities[np.array(idx)].reshape(
+            -1, self.dissimilarities.shape[1])
+        rdm_descriptors = subset_descriptor(self.rdm_descriptors, idx)
+        rdms = RDMs(dissimilarities,
+                    dissimilarity_measure=self.dissimilarity_measure,
+                    descriptors=self.descriptors,
+                    rdm_descriptors=rdm_descriptors,
+                    pattern_descriptors=self.pattern_descriptors)
+        return rdms
+
+    def __len__(self) -> int:
+        """
+        The number of RDMs in this stack.
+        Together with __getitem__, allows `reversed(rdms)`.
+        """
+        return self.n_rdm
+
+    def get_vectors(self):
+        """ Returns RDMs as np.ndarray with each RDM as a vector
+
+        Returns:
+            numpy.ndarray: RDMs as a matrix with one row per RDM
+
+        """
+        return self.dissimilarities
+
+    def get_matrices(self):
+        """ Returns RDMs as np.ndarray with each RDM as a matrix
+
+        Returns:
+            numpy.ndarray: RDMs as a 3-Tensor with one matrix per RDM
+
+        """
+        matrices, _, _ = batch_to_matrices(self.dissimilarities)
+        return matrices
+
+    def copy(self) -> RDMs:
+        """Return a copy of this object, with all properties
+        equal to the original's
+
+        Returns:
+            RDMs: Value copy
+        """
+        return RDMs(
+            dissimilarities=self.dissimilarities.copy(),
+            dissimilarity_measure=self.dissimilarity_measure,
+            descriptors=deepcopy(self.descriptors),
+            rdm_descriptors=deepcopy(self.rdm_descriptors),
+            pattern_descriptors=deepcopy(self.pattern_descriptors)
+        )
+
+    def subset_pattern(self, by, value):
+        """ Returns a smaller RDMs with patterns with certain descriptor values
+
+        Args:
+            by(String): the descriptor by which the subset selection
+                        is made from pattern_descriptors
+            value:      the value by which the subset selection is made
+                        from pattern_descriptors
+
+        Returns:
+            RDMs object, with fewer patterns
+
+        """
+        if by is None:
+            by = 'index'
+        if not isinstance(value, Iterable):
+            value = [value]
+        selection = num_index(self.pattern_descriptors[by], value)
+        ix, iy = np.triu_indices(self.n_cond, 1)
+        pattern_in_value = np.array(
+            [p in value for p in self.pattern_descriptors[by]])
+        selection_xy = pattern_in_value[ix] & pattern_in_value[iy]
+        dissimilarities = self.dissimilarities[:, selection_xy]
+        descriptors = self.descriptors
+        pattern_descriptors = extract_dict(
+            self.pattern_descriptors, selection)
+        rdm_descriptors = self.rdm_descriptors
+        dissimilarity_measure = self.dissimilarity_measure
+        rdms = RDMs(dissimilarities=dissimilarities,
+                    descriptors=descriptors,
+                    rdm_descriptors=rdm_descriptors,
+                    pattern_descriptors=pattern_descriptors,
+                    dissimilarity_measure=dissimilarity_measure)
+        return rdms
+
+    def subsample_pattern(self, by, value):
+        """ Returns a subsampled RDMs with repetitions if values are repeated
+
+        This function now generates Nans where the off-diagonal 0s would
+        appear. These values are trivial to predict for models and thus
+        need to be marked and excluded from the evaluation.
+
+        Args:
+            by(String): the descriptor by which the subset selection
+                        is made from descriptors
+            value:      the value(s) by which the subset selection is made
+                        from descriptors
+
+        Returns:
+            RDMs object, with subsampled patterns
+
+        """
+        if by is None:
+            by = 'index'
+        desc = np.array(self.pattern_descriptors[by])  # desc is list-like
+        if isinstance(value, (list, tuple, np.ndarray)):
+            selection = [np.asarray(desc == i).nonzero()[0]
+                         for i in value]
+            selection = np.concatenate(selection)
+        else:
+            selection = np.where(desc == value)[0]
+        selection = np.sort(selection)
+        dissimilarities = self.get_matrices()
+        for i_rdm in range(self.n_rdm):
+            np.fill_diagonal(dissimilarities[i_rdm], np.nan)
+        selection = np.sort(selection)
+        dissimilarities = dissimilarities[:, selection][:, :, selection]
+        descriptors = self.descriptors
+        pattern_descriptors = extract_dict(
+            self.pattern_descriptors, selection)
+        rdm_descriptors = self.rdm_descriptors
+        dissimilarity_measure = self.dissimilarity_measure
+        rdms = RDMs(dissimilarities=dissimilarities,
+                    descriptors=descriptors,
+                    rdm_descriptors=rdm_descriptors,
+                    pattern_descriptors=pattern_descriptors,
+                    dissimilarity_measure=dissimilarity_measure)
+        return rdms
+
+    def subset(self, by, value):
+        """ Returns a set of fewer RDMs matching descriptor values
+
+        Args:
+            by(String): the descriptor by which the subset selection
+                        is made from descriptors
+            value:      the value by which the subset selection is made
+                        from descriptors
+
+        Returns:
+            RDMs object, with fewer RDMs
+
+        """
+        if by is None:
+            by = 'index'
+        selection = num_index(self.rdm_descriptors[by], value)
+        dissimilarities = self.dissimilarities[selection, :]
+        descriptors = self.descriptors
+        pattern_descriptors = self.pattern_descriptors
+        rdm_descriptors = extract_dict(self.rdm_descriptors, selection)
+        dissimilarity_measure = self.dissimilarity_measure
+        rdms = RDMs(dissimilarities=dissimilarities,
+                    descriptors=descriptors,
+                    rdm_descriptors=rdm_descriptors,
+                    pattern_descriptors=pattern_descriptors,
+                    dissimilarity_measure=dissimilarity_measure)
+        return rdms
+
+    def subsample(self, by, value):
+        """ Returns a subsampled RDMs with repetitions if values are repeated
+
+        Args:
+            by(String): the descriptor by which the subset selection
+                        is made from descriptors
+            value:      the value by which the subset selection is made
+                        from descriptors
+
+        Returns:
+            RDMs object, with subsampled RDMs
+
+        """
+        if by is None:
+            by = 'index'
+        desc = self.rdm_descriptors[by]
+        selection = []
+        if isinstance(value, (list, tuple, np.ndarray)):
+            for i in value:
+                for j, d in enumerate(desc):
+                    if d == i:
+                        selection.append(j)
+        else:
+            for j, d in enumerate(desc):
+                if d == value:
+                    selection.append(j)
+        dissimilarities = self.dissimilarities[selection, :]
+        descriptors = self.descriptors
+        pattern_descriptors = self.pattern_descriptors
+        rdm_descriptors = extract_dict(self.rdm_descriptors, selection)
+        dissimilarity_measure = self.dissimilarity_measure
+        rdms = RDMs(dissimilarities=dissimilarities,
+                    descriptors=descriptors,
+                    rdm_descriptors=rdm_descriptors,
+                    pattern_descriptors=pattern_descriptors,
+                    dissimilarity_measure=dissimilarity_measure)
+        return rdms
+
+    def append(self, rdm):
+        """ appends an rdm to the object
+        The rdm should have the same shape and type as this object.
+        Its pattern_descriptor and descriptor are ignored
+
+        Args:
+            rdm(rsatoolbox.rdm.RDMs): the rdm to append
+
+        Returns:
+
+        """
+        assert isinstance(rdm, RDMs), 'appended rdm should be an RDMs'
+        assert rdm.n_cond == self.n_cond, 'appended rdm had wrong shape'
+        assert rdm.dissimilarity_measure == self.dissimilarity_measure, \
+            'appended rdm had wrong dissimilarity measure'
+        self.dissimilarities = np.concatenate((
+            self.dissimilarities, rdm.dissimilarities), axis=0)
+        self.rdm_descriptors = append_descriptor(self.rdm_descriptors,
+                                                 rdm.rdm_descriptors)
+        self.n_rdm = self.n_rdm + rdm.n_rdm
+
+    def save(self, filename, file_type='hdf5', overwrite=False):
+        """ saves the RDMs object into a file
+
+        Args:
+            filename(String): path to file to save to
+                [or opened file]
+            file_type(String): Type of file to create:
+                hdf5: hdf5 file
+                pkl: pickle file
+            overwrite(Boolean): overwrites file if it already exists
+
+        """
+        rdm_dict = self.to_dict()
+        if overwrite:
+            remove_file(filename)
+        if file_type == 'hdf5':
+            write_dict_hdf5(filename, rdm_dict)
+        elif file_type == 'pkl':
+            write_dict_pkl(filename, rdm_dict)
+
+    def to_dict(self):
+        """ converts the object into a dictionary, which can be saved to disk
+
+        Returns:
+            rdm_dict(dict): dictionary containing all information required to
+                recreate the RDMs object
+        """
+        rdm_dict = {}
+        rdm_dict['dissimilarities'] = self.dissimilarities
+        rdm_dict['descriptors'] = self.descriptors
+        rdm_dict['rdm_descriptors'] = self.rdm_descriptors
+        rdm_dict['pattern_descriptors'] = self.pattern_descriptors
+        rdm_dict['dissimilarity_measure'] = self.dissimilarity_measure
+        return rdm_dict
+
+    def to_df(self):
+        """Return a new long-form pandas DataFrame representing this RDM
+
+        See `rsatoolbox.io.pandas.rdms_to_df` for details
+
+        Returns:
+            pandas.DataFrame: The DataFrame for this RDMs object
+        """
+        return rdms_to_df(self)
+
+    def reorder(self, new_order):
+        """Reorder the patterns according to the index in new_order
+
+        Args:
+            new_order (numpy.ndarray): new order of patterns,
+                vector of length equal to the number of patterns
+        """
+        matrices = self.get_matrices()
+        matrices = matrices[(slice(None),) + np.ix_(new_order, new_order)]
+        self.dissimilarities = batch_to_vectors(matrices)[0]
+        for dname, descriptors in self.pattern_descriptors.items():
+            self.pattern_descriptors[dname] = [descriptors[idx] for idx in new_order]
+
+    def sort_by(self, **kwargs):
+        """Reorder the patterns by sorting a descriptor
+
+        Pass keyword arguments that correspond to descriptors,
+        with value indicating the sort type. Supported methods:
+
+            'alpha': sort alphabetically (using np.sort)
+
+            list/np.array: specify the new order explicitly. Values should
+                correspond to the descriptor values
+
+        Examples:
+
+            The following code sorts the 'condition' descriptor alphabetically:
+
+            ::
+
+                rdms.sort_by(condition='alpha')
+
+            The following code sort the 'condition' descriptor in the order
+            1, 3, 2, 4, 5:
+
+            ::
+
+                rdms.sort_by(condition=[1, 3, 2, 4, 5])
+
+        Raises:
+            ValueError: Raised if the method chosen is not implemented
+        """
+        for dname, method in kwargs.items():
+            if method == 'alpha':
+                descriptor = self.pattern_descriptors[dname]
+                self.reorder(np.argsort(descriptor))
+            elif isinstance(method, (list, np.ndarray)):
+                # in this case, `method` is the desired descriptor order
+                new_order = method
+                descriptor = self.pattern_descriptors[dname]
+                if not set(descriptor).issubset(new_order):
+                    raise ValueError(f'Expected {method} to be a permutation \
+                            or subset of {descriptor}')
+                # convert to indices to use `reorder` method
+                self.reorder([list(descriptor).index(x) for x in new_order])
+            else:
+                raise ValueError(f'Unknown sorting method: {method}')
+
+    def mean(self, weights=None):
+        """Average rdm of all rdms contained
+
+        Args:
+            weights (str or ndarray, optional): One of:
+                None: No weighting applied
+                str: Use the weights contained in the `rdm_descriptor` with this name
+                ndarray: Weights array of the shape of RDMs.dissimilarities
+
+        Returns:
+            `rsatoolbox.rdm.rdms.RDMs`: New RDMs object with one vector
+        """
+        if str(weights) in self.rdm_descriptors:
+            new_descriptors = {
+                (k, v) for (k, v) in self.descriptors.items() if k != weights
+            }
+            weights = self.rdm_descriptors[weights]
+        else:
+            new_descriptors = deepcopy(self.descriptors)
+        return RDMs(
+            dissimilarities=np.array([_mean(self.dissimilarities, weights)]),
+            dissimilarity_measure=self.dissimilarity_measure,
+            descriptors=new_descriptors,
+            pattern_descriptors=deepcopy(self.pattern_descriptors)
+        )
+
+
+def rdms_from_dict(rdm_dict):
+    """ creates a RDMs object from a dictionary
+
+    Args:
+        rdm_dict(dict): dictionary with information
+
+    Returns:
+        rdms(RDMs): the regenerated RDMs object
+
+    """
+    rdms = RDMs(dissimilarities=rdm_dict['dissimilarities'],
+                descriptors=rdm_dict['descriptors'],
+                rdm_descriptors=dict_to_list(rdm_dict['rdm_descriptors']),
+                pattern_descriptors=dict_to_list(rdm_dict['pattern_descriptors']),
+                dissimilarity_measure=rdm_dict['dissimilarity_measure'])
+    return rdms
+
+
+def load_rdm(filename, file_type=None):
+    """ loads a RDMs object from disk
+
+    Args:
+        filename(String): path to file to load
+
+    """
+    if file_type is None:
+        if isinstance(filename, str):
+            if filename[-4:] == '.pkl':
+                file_type = 'pkl'
+            elif filename[-3:] == '.h5' or filename[-4:] == 'hdf5':
+                file_type = 'hdf5'
+    if file_type == 'hdf5':
+        rdm_dict = read_dict_hdf5(filename)
+    elif file_type == 'pkl':
+        rdm_dict = read_dict_pkl(filename)
+    else:
+        raise ValueError('filetype not understood')
+    return rdms_from_dict(rdm_dict)
+
+
+def concat(*rdms):
+    """ concatenates rdm objects
+    requires that the rdms have the same shape
+    descriptor and pattern descriptors are taken from the first rdms object
+    for rdm_descriptors concatenation is tried
+    the rdm index is reinitialized
+
+    Args:
+        rdms(iterable of pyrsa.rdm.RDMs): RDMs objects to be concatenated
+        or multiple RDMs as separate arguments
+
+    Returns:
+        rsatoolbox.rdm.RDMs: concatenated rdms object
+
+    """
+    if len(rdms) == 1:
+        if isinstance(rdms[0], RDMs):
+            rdms_list = [rdms[0]]
+        else:
+            rdms_list = list(rdms[0])
+    else:
+        rdms_list = list(rdms)
+    assert isinstance(rdms_list[0], RDMs), \
+        'Supply list of RDMs objects, or RDMs objects as separate arguments'
+    rdm_descriptors = deepcopy(rdms_list[0].rdm_descriptors)
+    for rdm_new in rdms_list[1:]:
+        assert isinstance(rdm_new, RDMs), 'rdm for concat should be an RDMs'
+        assert rdm_new.n_cond == rdms_list[0].n_cond, 'rdm for concat had wrong shape'
+        assert rdm_new.dissimilarity_measure == rdms_list[0].dissimilarity_measure, \
+            'appended rdm had wrong dissimilarity measure'
+        rdm_descriptors = append_descriptor(rdm_descriptors, rdm_new.rdm_descriptors)
+    dissimilarities = np.concatenate([
+        rdm.dissimilarities
+        for rdm in rdms_list
+        ], axis=0)
+    rdm = RDMs(
+        dissimilarities=dissimilarities,
+        rdm_descriptors=rdm_descriptors,
+        descriptors=rdms_list[0].descriptors,
+        pattern_descriptors=rdms_list[0].pattern_descriptors
+    )
+    return rdm
+
+
+def permute_rdms(rdms, p=None):
+    """ Permute rows, columns and corresponding pattern descriptors
+    of RDM matrices according to a permutation vector
+
+    Args:
+        p (numpy.ndarray):
+           permutation vector (values must be unique integers
+           from 0 to n_cond of RDM matrix).
+           If p = None, a random permutation vector is created.
+
+    Returns:
+        rdm_p(rsatoolbox.rdm.RDMs): the rdm object with a permuted matrix
+            and pattern descriptors
+
+    """
+    if p is None:
+        p = np.random.permutation(rdms.n_cond)
+        print('No permutation vector specified,'
+              + ' performing random permutation.')
+
+    assert p.dtype == 'int', "permutation vector must have integer entries."
+    assert min(p) == 0 and max(p) == rdms.n_cond-1, \
+        "permutation vector must have entries ranging from 0 to n_cond"
+    assert len(np.unique(p)) == rdms.n_cond, \
+        "permutation vector must only have unique integer entries"
+
+    rdm_mats = rdms.get_matrices()
+    descriptors = rdms.descriptors.copy()
+    rdm_descriptors = rdms.rdm_descriptors.copy()
+    pattern_descriptors = rdms.pattern_descriptors.copy()
+
+    # To easily reverse permutation later
+    p_inv = np.arange(len(p))[np.argsort(p)]
+    descriptors.update({'p_inv': p_inv})
+    rdm_mats = rdm_mats[:, p, :]
+    rdm_mats = rdm_mats[:, :, p]
+    stims = np.array(pattern_descriptors['index'])
+    pattern_descriptors.update({'index': list(stims[p].astype(np.str_))})
+
+    rdms_p = RDMs(
+        dissimilarities=rdm_mats,
+        descriptors=descriptors,
+        rdm_descriptors=rdm_descriptors,
+        pattern_descriptors=pattern_descriptors)
+    return rdms_p
+
+
+def inverse_permute_rdms(rdms):
+    """ Gimmick function to reverse the effect of permute_rdms() """
+
+    p_inv = rdms.descriptors['p_inv']
+    rdms_p = permute_rdms(rdms, p=p_inv)
+    return rdms_p
+
+
+def get_categorical_rdm(category_vector, category_name='category'):
+    """ generates an RDM object containing a categorical RDM, i.e. RDM = 0
+    if the category is the same and 1 if they are different
+
+    Args:
+        category_vector(iterable): a category index per condition
+        category_name(String): name for the descriptor in the object, defaults
+            to 'category'
+
+    Returns:
+        rsatoolbox.rdm.RDMs: constructed RDM
+
+    """
+    n = len(category_vector)
+    rdm_list = []
+    for i_cat in range(n):
+        for j_cat in range(i_cat + 1, n):
+            if isinstance(category_vector[i_cat], Iterable):
+                comparisons = [np.array(category_vector[i_cat][idx])
+                               != np.array(category_vector[j_cat][idx])
+                               for idx in range(len(category_vector[i_cat]))]
+                rdm_list.append(np.any(comparisons))
+            else:
+                rdm_list.append(
+                    category_vector[i_cat] != category_vector[j_cat])
+    rdm = RDMs(np.array(rdm_list, dtype=float),
+               pattern_descriptors={category_name: np.array(category_vector)})
+    return rdm
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/rdm/transform.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/rdm/transform.py`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,112 +1,112 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-""" transforms, which can be applied to RDMs
-"""
-from __future__ import annotations
-from copy import deepcopy
-import numpy as np
-from scipy.stats import rankdata
-from .rdms import RDMs
-
-
-def rank_transform(rdms: RDMs, method='average'):
-    """ applies a rank_transform and generates a new RDMs object
-    This assigns a rank to each dissimilarity estimate in the RDM,
-    deals with rank ties and saves ranks as new dissimilarity estimates.
-    As an effect, all non-diagonal entries of the RDM will
-    range from 1 to (n_dim-n_dim)/2, if the RDM has the dimensions
-    n_dim x n_dim.
-
-    Args:
-        rdms(RDMs): RDMs object
-        method(String):
-            controls how ranks are assigned to equal values
-            options are: average, min, max, dense, ordinal
-
-    Returns:
-        rdms_new(RDMs): RDMs object with rank transformed dissimilarities
-
-    """
-    dissimilarities = rdms.get_vectors()
-    dissimilarities = np.array([rankdata(dissimilarities[i], method=method)
-                                for i in range(rdms.n_rdm)])
-    measure = rdms.dissimilarity_measure or ''
-    if '(ranks)' not in measure:
-        measure = (measure + ' (ranks)').strip()
-    rdms_new = RDMs(dissimilarities,
-                    dissimilarity_measure=measure,
-                    descriptors=deepcopy(rdms.descriptors),
-                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
-                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
-    return rdms_new
-
-
-def sqrt_transform(rdms):
-    """ applies a square root transform and generates a new RDMs object
-    This sets values blow 0 to 0 and takes a square root of each entry.
-    It also adds a sqrt to the dissimilarity_measure entry.
-
-    Args:
-        rdms(RDMs): RDMs object
-
-    Returns:
-        rdms_new(RDMs): RDMs object with sqrt transformed dissimilarities
-
-    """
-    dissimilarities = rdms.get_vectors()
-    dissimilarities[dissimilarities < 0] = 0
-    dissimilarities = np.sqrt(dissimilarities)
-    if rdms.dissimilarity_measure == 'squared euclidean':
-        dissimilarity_measure = 'euclidean'
-    elif rdms.dissimilarity_measure == 'squared mahalanobis':
-        dissimilarity_measure = 'mahalanobis'
-    else:
-        dissimilarity_measure = 'sqrt of' + rdms.dissimilarity_measure
-    rdms_new = RDMs(dissimilarities,
-                    dissimilarity_measure=dissimilarity_measure,
-                    descriptors=deepcopy(rdms.descriptors),
-                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
-                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
-    return rdms_new
-
-
-def positive_transform(rdms):
-    """ sets all negative entries in an RDM to zero and returns a new RDMs
-
-    Args:
-        rdms(RDMs): RDMs object
-
-    Returns:
-        rdms_new(RDMs): RDMs object with sqrt transformed dissimilarities
-
-    """
-    dissimilarities = rdms.get_vectors()
-    dissimilarities[dissimilarities < 0] = 0
-    rdms_new = RDMs(dissimilarities,
-                    dissimilarity_measure=rdms.dissimilarity_measure,
-                    descriptors=deepcopy(rdms.descriptors),
-                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
-                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
-    return rdms_new
-
-
-def transform(rdms, fun):
-    """ applies an arbitray function ``fun`` to the dissimilarities and
-    returns a new RDMs object.
-
-    Args:
-        rdms(RDMs): RDMs object
-
-    Returns:
-        rdms_new(RDMs): RDMs object with sqrt transformed dissimilarities
-
-    """
-    dissimilarities = rdms.get_vectors()
-    dissimilarities = fun(dissimilarities)
-    meas = 'transformed ' + rdms.dissimilarity_measure
-    rdms_new = RDMs(dissimilarities,
-                    dissimilarity_measure=meas,
-                    descriptors=deepcopy(rdms.descriptors),
-                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
-                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
-    return rdms_new
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+""" transforms, which can be applied to RDMs
+"""
+from __future__ import annotations
+from copy import deepcopy
+import numpy as np
+from scipy.stats import rankdata
+from .rdms import RDMs
+
+
+def rank_transform(rdms: RDMs, method='average'):
+    """ applies a rank_transform and generates a new RDMs object
+    This assigns a rank to each dissimilarity estimate in the RDM,
+    deals with rank ties and saves ranks as new dissimilarity estimates.
+    As an effect, all non-diagonal entries of the RDM will
+    range from 1 to (n_dim-n_dim)/2, if the RDM has the dimensions
+    n_dim x n_dim.
+
+    Args:
+        rdms(RDMs): RDMs object
+        method(String):
+            controls how ranks are assigned to equal values
+            options are: average, min, max, dense, ordinal
+
+    Returns:
+        rdms_new(RDMs): RDMs object with rank transformed dissimilarities
+
+    """
+    dissimilarities = rdms.get_vectors()
+    dissimilarities = np.array([rankdata(dissimilarities[i], method=method)
+                                for i in range(rdms.n_rdm)])
+    measure = rdms.dissimilarity_measure or ''
+    if '(ranks)' not in measure:
+        measure = (measure + ' (ranks)').strip()
+    rdms_new = RDMs(dissimilarities,
+                    dissimilarity_measure=measure,
+                    descriptors=deepcopy(rdms.descriptors),
+                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
+                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
+    return rdms_new
+
+
+def sqrt_transform(rdms):
+    """ applies a square root transform and generates a new RDMs object
+    This sets values blow 0 to 0 and takes a square root of each entry.
+    It also adds a sqrt to the dissimilarity_measure entry.
+
+    Args:
+        rdms(RDMs): RDMs object
+
+    Returns:
+        rdms_new(RDMs): RDMs object with sqrt transformed dissimilarities
+
+    """
+    dissimilarities = rdms.get_vectors()
+    dissimilarities[dissimilarities < 0] = 0
+    dissimilarities = np.sqrt(dissimilarities)
+    if rdms.dissimilarity_measure == 'squared euclidean':
+        dissimilarity_measure = 'euclidean'
+    elif rdms.dissimilarity_measure == 'squared mahalanobis':
+        dissimilarity_measure = 'mahalanobis'
+    else:
+        dissimilarity_measure = 'sqrt of' + rdms.dissimilarity_measure
+    rdms_new = RDMs(dissimilarities,
+                    dissimilarity_measure=dissimilarity_measure,
+                    descriptors=deepcopy(rdms.descriptors),
+                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
+                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
+    return rdms_new
+
+
+def positive_transform(rdms):
+    """ sets all negative entries in an RDM to zero and returns a new RDMs
+
+    Args:
+        rdms(RDMs): RDMs object
+
+    Returns:
+        rdms_new(RDMs): RDMs object with sqrt transformed dissimilarities
+
+    """
+    dissimilarities = rdms.get_vectors()
+    dissimilarities[dissimilarities < 0] = 0
+    rdms_new = RDMs(dissimilarities,
+                    dissimilarity_measure=rdms.dissimilarity_measure,
+                    descriptors=deepcopy(rdms.descriptors),
+                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
+                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
+    return rdms_new
+
+
+def transform(rdms, fun):
+    """ applies an arbitray function ``fun`` to the dissimilarities and
+    returns a new RDMs object.
+
+    Args:
+        rdms(RDMs): RDMs object
+
+    Returns:
+        rdms_new(RDMs): RDMs object with sqrt transformed dissimilarities
+
+    """
+    dissimilarities = rdms.get_vectors()
+    dissimilarities = fun(dissimilarities)
+    meas = 'transformed ' + rdms.dissimilarity_measure
+    rdms_new = RDMs(dissimilarities,
+                    dissimilarity_measure=meas,
+                    descriptors=deepcopy(rdms.descriptors),
+                    rdm_descriptors=deepcopy(rdms.rdm_descriptors),
+                    pattern_descriptors=deepcopy(rdms.pattern_descriptors))
+    return rdms_new
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/test.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/test.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,102 +1,102 @@
-"""Skeleton test module
-
-The full collection of unit and acceptance tests for rsatoolbox is kept
-in a separate package that is not part of our distributables. It can be run
-by checking out the rsatoolbox git repository.
-
-The tests in this module are a limited number of basic so-called skeleton
-tests, which check that the library and all its dependencies are installed
-correctly. It is not exhaustive and assumes that unittests have passed
-for other most package formats.
-If rsatoolbox is installed, the tests can be run with:
-
-`python -m unittest rsatoolbox.test`
-
-These tests have to:
-
-- not have any dependencies outside of direct rsatoolbox runtime dependencies
-- be fast (a few seconds)
-- test interfaces that depend on external packages
-- test compiled code
-
-In other words they have to check that all the moving parts are there,
-without looking at very specific calculation outcomes.
-"""
-# pylint: disable=import-outside-toplevel
-from unittest import TestCase
-
-
-class SkeletonTests(TestCase):
-    """Toolbox skeleton tests to ensure correct packaging and installation
-    """
-
-    def setUp(self):
-        """Create basic RDMs and Dataset objects for all tests
-        """
-        from numpy import asarray, ones
-        from numpy.testing import assert_almost_equal
-        from rsatoolbox.data.dataset import Dataset
-        from rsatoolbox.rdm.rdms import RDMs
-        self.data = Dataset(asarray([[0, 0], [1, 1], [2.0, 2.0]]))
-        self.rdms = RDMs(asarray([[1.0, 2, 3], [3, 4, 5]]))
-        self.larger_rdms = RDMs(ones([3, 10]))
-        self.array = asarray
-        self.arrayAlmostEqual = assert_almost_equal
-
-    def test_calc_compiled(self):
-        """Covers similarity calculation with compiled code
-        """
-        from rsatoolbox.rdm.calc_unbalanced import calc_rdm_unbalanced
-        rdms = calc_rdm_unbalanced(self.data)
-        self.arrayAlmostEqual(rdms.dissimilarities, self.array([[1, 4, 1]]))
-
-    def test_model_fit(self):
-        """Covers model fitting with scipy
-        """
-        from rsatoolbox.model.model import ModelWeighted
-        from rsatoolbox.model.fitter import fit_optimize
-        theta = fit_optimize(ModelWeighted('F', self.rdms), self.rdms)
-        self.arrayAlmostEqual(theta, [0.88, 0.47], decimal=2)
-
-    def test_plotting_with_mpl(self):
-        """Covers Matplotlib usage
-        """
-        from rsatoolbox.vis.rdm_plot import show_rdm
-        show_rdm(self.rdms)
-
-    def test_mds(self):
-        """Covers sklearn and Matplotlib usage
-        """
-        from rsatoolbox.vis.scatter_plot import show_MDS
-        show_MDS(self.rdms)
-
-    def test_evaluate(self):
-        """Covers tqdm usage and evaluate functionality
-        """
-        from rsatoolbox.inference import eval_fixed
-        from rsatoolbox.model import ModelFixed
-        model = ModelFixed('G', self.array(list(range(10))))
-        result = eval_fixed(model, self.larger_rdms)
-        self.assertAlmostEqual(result.test_zero()[0], 0)
-
-    def test_pandas_io(self):
-        """Covers pandas usage
-        """
-        df = self.rdms.to_df()
-        self.arrayAlmostEqual(
-            self.array(df.loc[:, 'dissimilarity'].values),
-            self.rdms.dissimilarities.ravel()
-        )
-
-    def test_hdf_io(self):
-        """Covers h5py library use
-        """
-        from io import BytesIO
-        from rsatoolbox.rdm.rdms import load_rdm
-        fhandle = BytesIO()
-        self.rdms.save(fhandle, file_type='hdf5')
-        reconstituted_rdms = load_rdm(fhandle, file_type='hdf5')
-        self.arrayAlmostEqual(
-            self.rdms.dissimilarities,
-            reconstituted_rdms.dissimilarities
-        )
+"""Skeleton test module
+
+The full collection of unit and acceptance tests for rsatoolbox is kept
+in a separate package that is not part of our distributables. It can be run
+by checking out the rsatoolbox git repository.
+
+The tests in this module are a limited number of basic so-called skeleton
+tests, which check that the library and all its dependencies are installed
+correctly. It is not exhaustive and assumes that unittests have passed
+for other most package formats.
+If rsatoolbox is installed, the tests can be run with:
+
+`python -m unittest rsatoolbox.test`
+
+These tests have to:
+
+- not have any dependencies outside of direct rsatoolbox runtime dependencies
+- be fast (a few seconds)
+- test interfaces that depend on external packages
+- test compiled code
+
+In other words they have to check that all the moving parts are there,
+without looking at very specific calculation outcomes.
+"""
+# pylint: disable=import-outside-toplevel
+from unittest import TestCase
+
+
+class SkeletonTests(TestCase):
+    """Toolbox skeleton tests to ensure correct packaging and installation
+    """
+
+    def setUp(self):
+        """Create basic RDMs and Dataset objects for all tests
+        """
+        from numpy import asarray, ones
+        from numpy.testing import assert_almost_equal
+        from rsatoolbox.data.dataset import Dataset
+        from rsatoolbox.rdm.rdms import RDMs
+        self.data = Dataset(asarray([[0, 0], [1, 1], [2.0, 2.0]]))
+        self.rdms = RDMs(asarray([[1.0, 2, 3], [3, 4, 5]]))
+        self.larger_rdms = RDMs(ones([3, 10]))
+        self.array = asarray
+        self.arrayAlmostEqual = assert_almost_equal
+
+    def test_calc_compiled(self):
+        """Covers similarity calculation with compiled code
+        """
+        from rsatoolbox.rdm.calc_unbalanced import calc_rdm_unbalanced
+        rdms = calc_rdm_unbalanced(self.data)
+        self.arrayAlmostEqual(rdms.dissimilarities, self.array([[1, 4, 1]]))
+
+    def test_model_fit(self):
+        """Covers model fitting with scipy
+        """
+        from rsatoolbox.model.model import ModelWeighted
+        from rsatoolbox.model.fitter import fit_optimize
+        theta = fit_optimize(ModelWeighted('F', self.rdms), self.rdms)
+        self.arrayAlmostEqual(theta, [0.88, 0.47], decimal=2)
+
+    def test_plotting_with_mpl(self):
+        """Covers Matplotlib usage
+        """
+        from rsatoolbox.vis.rdm_plot import show_rdm
+        show_rdm(self.rdms)
+
+    def test_mds(self):
+        """Covers sklearn and Matplotlib usage
+        """
+        from rsatoolbox.vis.scatter_plot import show_MDS
+        show_MDS(self.rdms)
+
+    def test_evaluate(self):
+        """Covers tqdm usage and evaluate functionality
+        """
+        from rsatoolbox.inference import eval_fixed
+        from rsatoolbox.model import ModelFixed
+        model = ModelFixed('G', self.array(list(range(10))))
+        result = eval_fixed(model, self.larger_rdms)
+        self.assertAlmostEqual(result.test_zero()[0], 0)
+
+    def test_pandas_io(self):
+        """Covers pandas usage
+        """
+        df = self.rdms.to_df()
+        self.arrayAlmostEqual(
+            self.array(df.loc[:, 'dissimilarity'].values),
+            self.rdms.dissimilarities.ravel()
+        )
+
+    def test_hdf_io(self):
+        """Covers h5py library use
+        """
+        from io import BytesIO
+        from rsatoolbox.rdm.rdms import load_rdm
+        fhandle = BytesIO()
+        self.rdms.save(fhandle, file_type='hdf5')
+        reconstituted_rdms = load_rdm(fhandle, file_type='hdf5')
+        self.arrayAlmostEqual(
+            self.rdms.dissimilarities,
+            reconstituted_rdms.dissimilarities
+        )
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/descriptor_utils.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/descriptor_utils.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,218 +1,218 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Descriptor handling.
-Note: descriptor is assumed to be a list, to accommodate objects that don't fit well into strings,
-such as arrays of varying sizes.
-Some of these methods may convert numpy-array descriptors to list-types.
-
-@author: adkipnis
-"""
-from __future__ import annotations
-from typing import TYPE_CHECKING, Dict, List
-from collections.abc import Iterable
-import numpy as np
-if TYPE_CHECKING:
-    DescriptorDict = Dict[str, List | np.ndarray]
-
-
-def bool_index(descriptor, value):
-    """
-    creates a boolean index vector where a descriptor has a value
-
-    Args:
-        descriptor (list-like): descriptor vector
-        value:                  value or list of values to mark
-
-    Returns:
-        numpy.ndarray:
-            bool_index: boolean index vector where descriptor == value
-
-    """
-    descriptor = np.array(descriptor)
-    if isinstance(value, (list, tuple, np.ndarray)):
-        index = np.array([descriptor == v for v in value])
-        index = np.any(index, axis=0)
-    else:
-        index = np.array(descriptor == value)
-    return index
-
-
-def num_index(descriptor, value):
-    """
-    creates a boolean index vector where a descriptor has a value
-
-    Args:
-        descriptor (list-like): descriptor vector
-        value:                  value or list of values to mark
-
-    Returns:
-        numpy.ndarray:
-            bool_index: boolean index vector where descriptor == value
-
-    """
-    return np.where(bool_index(descriptor, value))[0]
-
-
-def format_descriptor(descriptors):
-    """ formats a descriptor dictionary
-
-    Args:
-        descriptors(dict): the descriptor dictionary
-
-    Returns:
-        String: formatted string to show dict
-
-    """
-    string_descriptors = ''
-    for entry in descriptors:
-        string_descriptors = (string_descriptors +
-                              f'{entry} = {descriptors[entry]}\n'
-                              )
-    return string_descriptors
-
-
-def parse_input_descriptor(descriptors):
-    """ parse input descriptor checks whether an input descriptors dictionary
-    is a dictionary. If it is None instead it is replaced by an empty dict.
-    Otherwise an error is raised.
-
-    Args:
-        descriptors(dict/None): the descriptor dictionary
-
-    Returns:
-        dict: descriptor dictionary
-
-    """
-    if descriptors is None:
-        descriptors = {}
-    elif not isinstance(descriptors, dict):
-        raise ValueError('Descriptors must be dictionaries!')
-    return descriptors
-
-
-def check_descriptor_length(descriptor, n_element):
-    """
-    Checks whether the entries of a descriptor dictionary have the right length.
-    Converts single-strings to a list of 1 element.
-
-    Args:
-        descriptor(dict): the descriptor dictionary
-        n_element: the correct length of the descriptors
-
-    Returns:
-        bool
-
-    """
-    for k, v in descriptor.items():
-        if isinstance(v, str):
-            v = [v]
-        if isinstance(v, Iterable) and len(v) != n_element:
-            return False
-    return True
-
-
-def subset_descriptor(descriptor, indices):
-    """
-    Retrieves a subset of a descriptor given by indices.
-
-    Args:
-        descriptor(dict): the descriptor dictionary
-        indices: the indices to be extracted
-
-    Returns:
-        extracted_descriptor(dict): the selected subset of the descriptor
-
-    """
-    extracted_descriptor = {}
-    if isinstance(indices, Iterable):
-        for k, v in descriptor.items():
-            extracted_descriptor[k] = [v[index] for index in indices]
-    else:
-        for k, v in descriptor.items():
-            extracted_descriptor[k] = [v[indices]]
-    return extracted_descriptor
-
-
-def append_descriptor(descriptor, desc_new):
-    """
-    appends a descriptor to an existing one
-
-    Args:
-        descriptor(dict): the descriptor dictionary, with list-like values
-        desc_new(dict): the descriptor dictionary to append
-
-    Returns:
-        descriptor(dict): the longer descriptor
-
-    """
-    for k, v in descriptor.items():
-        assert k in desc_new.keys(), f'appended descriptors misses key {k}'
-        descriptor[k] = list(v) + list(desc_new[k])
-    descriptor['index'] = list(range(len(descriptor['index'])))
-    return descriptor
-
-
-def check_descriptor_length_error(descriptor, name, n_element):
-    """
-    Raises an error if the given descriptor does not have the right length
-
-    Args:
-        descriptor(dict/None): the descriptor dictionary
-        name(String): Descriptor name used for error message
-        n_element: the desired descriptor length
-
-    Returns:
-        ---
-
-    """
-    if descriptor is not None:
-        if not check_descriptor_length(descriptor, n_element):
-            raise AttributeError(
-                name + " have mismatched dimension with measurements.")
-
-
-def append_obs_descriptors(dict_orig, dict_addit):
-    """
-    Merge two dictionaries of observation descriptors with matching keys and
-    numpy arrays as values.
-    """
-    assert list(dict_orig.keys()) == list(dict_addit.keys()), \
-        "Provided observation descriptors have different keys."
-    dict_merged = {}
-    keys = list(dict_orig.keys())
-    for k in keys:
-        values = list(np.append(dict_orig[k], dict_addit[k]))
-        dict_merged.update({k: values})
-    return dict_merged
-
-
-def dict_to_list(d_dict):
-    """
-    converts a dictionary from a hdf5 file to a list
-    """
-    for k, v in d_dict.items():
-        if isinstance(v, dict):
-            d_dict[k] = [
-                d_dict[k][str(i)]
-                for i in range(len(d_dict[k]))]
-        else:
-            d_dict[k] = list(d_dict[k])
-    return d_dict
-
-
-def desc_eq(a: DescriptorDict, b: DescriptorDict) -> bool:
-    """Whether the two descriptor-style dictionaries are equal
-
-    Args:
-        a (dict): Dictionary with list or array values
-        b (dict): Dictionary with list or array values
-
-    Returns:
-        bool: True if the two dictionaries have the same keys and values
-    """
-    if set(a.keys()) == set(b.keys()):
-        return all(
-            all(np.asarray(a[k]) == np.asarray(b[k])) for k in a.keys()
-        )
-    return False
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Descriptor handling.
+Note: descriptor is assumed to be a list, to accommodate objects that don't fit well into strings,
+such as arrays of varying sizes.
+Some of these methods may convert numpy-array descriptors to list-types.
+
+@author: adkipnis
+"""
+from __future__ import annotations
+from typing import TYPE_CHECKING, Dict, List
+from collections.abc import Iterable
+import numpy as np
+if TYPE_CHECKING:
+    DescriptorDict = Dict[str, List | np.ndarray]
+
+
+def bool_index(descriptor, value):
+    """
+    creates a boolean index vector where a descriptor has a value
+
+    Args:
+        descriptor (list-like): descriptor vector
+        value:                  value or list of values to mark
+
+    Returns:
+        numpy.ndarray:
+            bool_index: boolean index vector where descriptor == value
+
+    """
+    descriptor = np.array(descriptor)
+    if isinstance(value, (list, tuple, np.ndarray)):
+        index = np.array([descriptor == v for v in value])
+        index = np.any(index, axis=0)
+    else:
+        index = np.array(descriptor == value)
+    return index
+
+
+def num_index(descriptor, value):
+    """
+    creates a boolean index vector where a descriptor has a value
+
+    Args:
+        descriptor (list-like): descriptor vector
+        value:                  value or list of values to mark
+
+    Returns:
+        numpy.ndarray:
+            bool_index: boolean index vector where descriptor == value
+
+    """
+    return np.where(bool_index(descriptor, value))[0]
+
+
+def format_descriptor(descriptors):
+    """ formats a descriptor dictionary
+
+    Args:
+        descriptors(dict): the descriptor dictionary
+
+    Returns:
+        String: formatted string to show dict
+
+    """
+    string_descriptors = ''
+    for entry in descriptors:
+        string_descriptors = (string_descriptors +
+                              f'{entry} = {descriptors[entry]}\n'
+                              )
+    return string_descriptors
+
+
+def parse_input_descriptor(descriptors):
+    """ parse input descriptor checks whether an input descriptors dictionary
+    is a dictionary. If it is None instead it is replaced by an empty dict.
+    Otherwise an error is raised.
+
+    Args:
+        descriptors(dict/None): the descriptor dictionary
+
+    Returns:
+        dict: descriptor dictionary
+
+    """
+    if descriptors is None:
+        descriptors = {}
+    elif not isinstance(descriptors, dict):
+        raise ValueError('Descriptors must be dictionaries!')
+    return descriptors
+
+
+def check_descriptor_length(descriptor, n_element):
+    """
+    Checks whether the entries of a descriptor dictionary have the right length.
+    Converts single-strings to a list of 1 element.
+
+    Args:
+        descriptor(dict): the descriptor dictionary
+        n_element: the correct length of the descriptors
+
+    Returns:
+        bool
+
+    """
+    for k, v in descriptor.items():
+        if isinstance(v, str):
+            v = [v]
+        if isinstance(v, Iterable) and len(v) != n_element:
+            return False
+    return True
+
+
+def subset_descriptor(descriptor, indices):
+    """
+    Retrieves a subset of a descriptor given by indices.
+
+    Args:
+        descriptor(dict): the descriptor dictionary
+        indices: the indices to be extracted
+
+    Returns:
+        extracted_descriptor(dict): the selected subset of the descriptor
+
+    """
+    extracted_descriptor = {}
+    if isinstance(indices, Iterable):
+        for k, v in descriptor.items():
+            extracted_descriptor[k] = [v[index] for index in indices]
+    else:
+        for k, v in descriptor.items():
+            extracted_descriptor[k] = [v[indices]]
+    return extracted_descriptor
+
+
+def append_descriptor(descriptor, desc_new):
+    """
+    appends a descriptor to an existing one
+
+    Args:
+        descriptor(dict): the descriptor dictionary, with list-like values
+        desc_new(dict): the descriptor dictionary to append
+
+    Returns:
+        descriptor(dict): the longer descriptor
+
+    """
+    for k, v in descriptor.items():
+        assert k in desc_new.keys(), f'appended descriptors misses key {k}'
+        descriptor[k] = list(v) + list(desc_new[k])
+    descriptor['index'] = list(range(len(descriptor['index'])))
+    return descriptor
+
+
+def check_descriptor_length_error(descriptor, name, n_element):
+    """
+    Raises an error if the given descriptor does not have the right length
+
+    Args:
+        descriptor(dict/None): the descriptor dictionary
+        name(String): Descriptor name used for error message
+        n_element: the desired descriptor length
+
+    Returns:
+        ---
+
+    """
+    if descriptor is not None:
+        if not check_descriptor_length(descriptor, n_element):
+            raise AttributeError(
+                name + " have mismatched dimension with measurements.")
+
+
+def append_obs_descriptors(dict_orig, dict_addit):
+    """
+    Merge two dictionaries of observation descriptors with matching keys and
+    numpy arrays as values.
+    """
+    assert list(dict_orig.keys()) == list(dict_addit.keys()), \
+        "Provided observation descriptors have different keys."
+    dict_merged = {}
+    keys = list(dict_orig.keys())
+    for k in keys:
+        values = list(np.append(dict_orig[k], dict_addit[k]))
+        dict_merged.update({k: values})
+    return dict_merged
+
+
+def dict_to_list(d_dict):
+    """
+    converts a dictionary from a hdf5 file to a list
+    """
+    for k, v in d_dict.items():
+        if isinstance(v, dict):
+            d_dict[k] = [
+                d_dict[k][str(i)]
+                for i in range(len(d_dict[k]))]
+        else:
+            d_dict[k] = list(d_dict[k])
+    return d_dict
+
+
+def desc_eq(a: DescriptorDict, b: DescriptorDict) -> bool:
+    """Whether the two descriptor-style dictionaries are equal
+
+    Args:
+        a (dict): Dictionary with list or array values
+        b (dict): Dictionary with list or array values
+
+    Returns:
+        bool: True if the two dictionaries have the same keys and values
+    """
+    if set(a.keys()) == set(b.keys()):
+        return all(
+            all(np.asarray(a[k]) == np.asarray(b[k])) for k in a.keys()
+        )
+    return False
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/inference_util.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/inference_util.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,688 +1,751 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Inference module utilities
-"""
-
-from collections.abc import Iterable
-import numpy as np
-from scipy import stats
-from scipy.stats import rankdata, wilcoxon
-from scipy.stats import t as tdist
-from rsatoolbox.model import Model
-from rsatoolbox.rdm import RDMs
-from .matrix import pairwise_contrast
-from .rdm_utils import batch_to_matrices
-
-
-def input_check_model(models, theta=None, fitter=None, N=1):
-    """ Checks whether model related inputs to evaluations are valid and
-    generates an evaluation-matrix of fitting size.
-
-    Args:
-        model : [list of] rsatoolbox.rdm.RDMs
-            the models to be evaluated
-        theta : numpy.ndarray or list , optional
-            Parameter(s) for the model(s). The default is None.
-        fitter : [list of] function, optional
-            fitting function to overwrite the model default.
-            The default is None, i.e. keep default
-        N : int, optional
-            number of samples/rows in evaluations matrix. The default is 1.
-
-    Returns:
-        evaluations : numpy.ndarray
-            empty evaluations-matrix
-        theta : list
-            the processed and checked model parameters
-        fitter : [list of] functions
-            checked and processed fitter functions
-
-    """
-    if isinstance(models, Model):
-        models = [models]
-    elif not isinstance(models, Iterable):
-        raise ValueError('model should be an rsatoolbox.model.Model or a list of'
-                         + ' such objects')
-    if N > 1:
-        evaluations = np.zeros((N, len(models)))
-    else:
-        evaluations = np.zeros(len(models))
-    if theta is not None:
-        assert isinstance(theta, Iterable), 'If a list of models is' \
-            + ' passed theta must be a list of parameters'
-        assert len(models) == len(theta), 'there should equally many' \
-            + ' models as parameters'
-    else:
-        theta = [None] * len(models)
-    if fitter is None:
-        fitter = [None] * len(models)
-    elif isinstance(fitter, Iterable):
-        assert len(fitter) == len(models), 'if fitters are passed ' \
-            + 'there should be as many as models'
-    else:
-        fitter = [fitter] * len(models)
-    for k, model in enumerate(models):
-        if fitter[k] is None:
-            fitter[k] = model.default_fitter
-    return models, evaluations, theta, fitter
-
-
-def pool_rdm(rdms, method='cosine'):
-    """pools multiple RDMs into the one with maximal performance under a given
-    evaluation metric
-    rdm_descriptors of the generated rdms are empty
-
-    Args:
-        rdms (rsatoolbox.rdm.RDMs):
-            RDMs to be pooled
-        method : String, optional
-            Which comparison method to optimize for. The default is 'cosine'.
-
-    Returns:
-        rsatoolbox.rdm.RDMs: the pooled RDM, i.e. a RDM with maximal performance
-            under the chosen method
-
-    """
-    rdm_vec = rdms.get_vectors()
-    if method == 'euclid':
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'neg_riem_dist':
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'cosine':
-        rdm_vec = rdm_vec / np.sqrt(np.nanmean(rdm_vec ** 2, axis=1,
-                                               keepdims=True))
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'corr':
-        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
-        rdm_vec = rdm_vec / np.nanstd(rdm_vec, axis=1, keepdims=True)
-        rdm_vec = _nan_mean(rdm_vec)
-        rdm_vec = rdm_vec - np.nanmin(rdm_vec)
-    elif method == 'cosine_cov':
-        rdm_vec = rdm_vec / np.sqrt(np.nanmean(rdm_vec ** 2, axis=1,
-                                               keepdims=True))
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'corr_cov':
-        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
-        rdm_vec = rdm_vec / np.nanstd(rdm_vec, axis=1, keepdims=True)
-        rdm_vec = _nan_mean(rdm_vec)
-        rdm_vec = rdm_vec - np.nanmin(rdm_vec)
-    elif method in ('spearman', 'rho-a'):
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'rho-a':
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method in ('kendall', 'tau-b'):
-        Warning('Noise ceiling for tau based on averaged ranks!')
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'tau-a':
-        Warning('Noise ceiling for tau based on averaged ranks!')
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    else:
-        raise ValueError('Unknown RDM comparison method requested!')
-    return RDMs(rdm_vec,
-                dissimilarity_measure=rdms.dissimilarity_measure,
-                descriptors=rdms.descriptors,
-                rdm_descriptors=None,
-                pattern_descriptors=rdms.pattern_descriptors)
-
-
-def _nan_mean(rdm_vector):
-    """ takes the average over a rdm_vector with nans for masked entries
-    without a warning
-
-    Args:
-        rdm_vector(numpy.ndarray): set of rdm_vectors to be averaged
-
-    Returns:
-        rdm_mean(numpy.ndarray): the mean rdm
-
-    """
-    nan_idx = ~np.isnan(rdm_vector[0])
-    mean_values = np.mean(rdm_vector[:, nan_idx], axis=0)
-    rdm_mean = np.empty((1, rdm_vector.shape[1]))
-    rdm_mean.fill(np.nan)
-    rdm_mean[:, nan_idx] = mean_values
-    return rdm_mean
-
-
-def _nan_rank_data(rdm_vector):
-    """ rank_data for vectors with nan entries
-
-    Args:
-        rdm_vector(numpy.ndarray): the vector to be rank_transformed
-
-    Returns:
-        ranks(numpy.ndarray): the ranks with nans where the original vector
-            had nans
-
-    """
-    ranks_no_nan = rankdata(rdm_vector[~np.isnan(rdm_vector)])
-    ranks = np.ones_like(rdm_vector) * np.nan
-    ranks[~np.isnan(rdm_vector)] = ranks_no_nan
-    return ranks
-
-
-def all_tests(evaluations, noise_ceil, test_type='t-test',
-              model_var=None, diff_var=None, noise_ceil_var=None,
-              dof=1):
-    """wrapper running all tests necessary for the model plot
-    -> pairwise tests, tests against 0 and against noise ceiling
-
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-            (should be 3D: bootstrap x models x subjects or repeats)
-        noise_ceil (numpy.ndarray):
-            noise_ceiling estimate(s) to compare against
-        test_type(String):
-            't-test' : t-test bases tests using variances
-            'bootstrap' : Direct bootstrap sample based tests
-            'ranksum' : Wilcoxon signed rank-sum tests
-        model_var, diff_var, noise_ceil_var:
-            variance estimates from the results object.
-            These correspond to the variances of the individual model evals,
-            pairs of model evals and differences from the noise ceiling.
-
-    Returns:
-        numpy.ndarrays: p_pairwise, p_zero, p_noise
-
-    """
-    if test_type == 't-test':
-        p_pairwise = t_tests(evaluations, diff_var, dof=dof)
-        p_zero = t_test_0(evaluations, model_var, dof=dof)
-        p_noise = t_test_nc(evaluations, noise_ceil_var[:, 0],
-                            np.mean(noise_ceil[0]), dof)
-    elif test_type == 'bootstrap':
-        if len(noise_ceil.shape) > 1:
-            noise_lower_bs = noise_ceil[0]
-            noise_lower_bs.shape = (noise_ceil.shape[0], 1)
-        else:
-            noise_lower_bs = noise_ceil[0].reshape(1, 1)
-        p_pairwise = bootstrap_pair_tests(evaluations)
-        p_zero = ((evaluations <= 0).sum(axis=0) + 1) / evaluations.shape[0]
-        diffs = noise_lower_bs - evaluations
-        p_noise = ((diffs <= 0).sum(axis=0) + 1) / evaluations.shape[0]
-    elif test_type == 'ranksum':
-        noise_c = np.mean(noise_ceil[0])
-        p_pairwise = ranksum_pair_test(evaluations)
-        p_zero = ranksum_value_test(evaluations, 0)
-        p_noise = ranksum_value_test(evaluations, noise_c)
-    else:
-        raise ValueError('test_type not recognized.\n'
-                         + 'Options are: t-test, bootstrap, ranksum')
-    return p_pairwise, p_zero, p_noise
-
-
-def pair_tests(evaluations, test_type='t-test', diff_var=None, dof=1):
-    """wrapper running pair tests
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-            (should be 3D: bootstrap x models x subjects or repeats)
-        test_type(Strinng):
-            't-test' : t-test bases tests using variances
-            'bootstrap' : Direct bootstrap sample based tests
-            'ranksum' : Wilcoxon signed rank-sum tests
-
-    Returns:
-        numpy.ndarray: p_pairwise
-
-    """
-    if test_type == 't-test':
-        p_pairwise = t_tests(evaluations, diff_var, dof=dof)
-    elif test_type == 'bootstrap':
-        p_pairwise = bootstrap_pair_tests(evaluations)
-    elif test_type == 'ranksum':
-        p_pairwise = ranksum_pair_test(evaluations)
-    else:
-        raise ValueError('test_type not recognized.\n'
-                         + 'Options are: t-test, bootstrap, ranksum')
-    return p_pairwise
-
-
-def zero_tests(evaluations, test_type='t-test',
-               model_var=None, dof=1):
-    """wrapper running tests against 0
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-            (should be 3D: bootstrap x models x subjects or repeats)
-        test_type(Strinng):
-            't-test' : t-test bases tests using variances
-            'bootstrap' : Direct bootstrap sample based tests
-            'ranksum' : Wilcoxon signed rank-sum tests
-
-    Returns:
-        numpy.ndarray: p_zero
-
-    """
-    if test_type == 't-test':
-        p_zero = t_test_0(evaluations, model_var, dof=dof)
-    elif test_type == 'bootstrap':
-        p_zero = ((evaluations <= 0).sum(axis=0) + 1) / evaluations.shape[0]
-    elif test_type == 'ranksum':
-        p_zero = ranksum_value_test(evaluations, 0)
-    else:
-        raise ValueError('test_type not recognized.\n'
-                         + 'Options are: t-test, bootstrap, ranksum')
-    return p_zero
-
-
-def nc_tests(evaluations, noise_ceil, test_type='t-test',
-             noise_ceil_var=None, dof=1):
-    """wrapper running tests against noise ceiling
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-            (should be 3D: bootstrap x models x subjects or repeats)
-        noise_ceil (numpy.ndarray):
-            noise_ceiling estimate(s) to compare against
-        test_type(Strinng):
-            't-test' : t-test bases tests using variances
-            'bootstrap' : Direct bootstrap sample based tests
-            'ranksum' : Wilcoxon signed rank-sum tests
-        noise_ceil_var:
-            variance estimate from the results object.
-            These correspond to the variances of the
-            differences from the noise ceiling.
-
-    Returns:
-        numpy.ndarrays: p_noise
-
-    """
-    if test_type == 't-test':
-        p_noise = t_test_nc(evaluations, noise_ceil_var[:, 0],
-                            np.mean(noise_ceil[0]), dof)
-    elif test_type == 'bootstrap':
-        if len(noise_ceil.shape) > 1:
-            noise_lower_bs = noise_ceil[0]
-            noise_lower_bs.shape = (noise_ceil.shape[0], 1)
-        else:
-            noise_lower_bs = noise_ceil[0].reshape(1, 1)
-        diffs = noise_lower_bs - evaluations
-        p_noise = ((diffs <= 0).sum(axis=0) + 1) / evaluations.shape[0]
-    elif test_type == 'ranksum':
-        noise_c = np.mean(noise_ceil[0])
-        p_noise = ranksum_value_test(evaluations, noise_c)
-    else:
-        raise ValueError('test_type not recognized.\n'
-                         + 'Options are: t-test, bootstrap, ranksum')
-    return p_noise
-
-
-def ranksum_pair_test(evaluations):
-    """pairwise tests between models using the wilcoxon signed rank test
-
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-            (should be 3D: bootstrap x models x subjects or repeats)
-
-    Returns:
-        numpy.ndarray: matrix of proportions of opposit conclusions, i.e.
-            p-values for the test
-
-    """
-    # check that the dimensionality is correct
-    assert evaluations.ndim == 3, \
-        'provided evaluations array has wrong dimensionality'
-    n_model = evaluations.shape[1]
-    # ignore bootstraps
-    evaluations = np.nanmean(evaluations, 0)
-    pvalues = np.empty((n_model, n_model))
-    for i_model in range(n_model - 1):
-        for j_model in range(i_model + 1, n_model):
-            pvalues[i_model, j_model] = wilcoxon(
-                evaluations[i_model], evaluations[j_model]).pvalue
-            pvalues[j_model, i_model] = pvalues[i_model, j_model]
-    np.fill_diagonal(pvalues, 1)
-    return pvalues
-
-
-def ranksum_value_test(evaluations, comp_value=0):
-    """nonparametric wilcoxon signed rank test against a fixed value
-
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-            (should be 3D: bootstrap x models x subjects or repeats)
-        comp_value(float):
-            value to compare against
-
-    Returns:
-        float: p-value
-
-    """
-    # check that the dimensionality is correct
-    assert evaluations.ndim == 3, \
-        'provided evaluations array has wrong dimensionality'
-    n_model = evaluations.shape[1]
-    # ignore bootstraps
-    evaluations = np.nanmean(evaluations, 0)
-    pvalues = np.empty(n_model)
-    for i_model in range(n_model):
-        pvalues[i_model] = wilcoxon(
-            evaluations[i_model] - comp_value).pvalue
-    return pvalues
-
-
-def bootstrap_pair_tests(evaluations):
-    """pairwise bootstrapping significance tests for a difference in model
-    performance.
-    Tests add 1/len(evaluations) to each p-value and are computed as
-    two sided tests, i.e. as 2 * the smaller proportion
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be compared
-
-    Returns:
-        numpy.ndarray: matrix of proportions of opposit conclusions, i.e.
-            p-values for the bootstrap test
-    """
-    proportions = np.zeros((evaluations.shape[1], evaluations.shape[1]))
-    while len(evaluations.shape) > 2:
-        evaluations = np.mean(evaluations, axis=-1)
-    for i_model in range(evaluations.shape[1]-1):
-        for j_model in range(i_model + 1, evaluations.shape[1]):
-            proportions[i_model, j_model] = np.sum(
-                evaluations[:, i_model] < evaluations[:, j_model]) \
-                / (evaluations.shape[0] -
-                   np.sum(evaluations[:, i_model] == evaluations[:, j_model]))
-            proportions[j_model, i_model] = proportions[i_model, j_model]
-    proportions = np.minimum(proportions, 1 - proportions) * 2
-    proportions = (len(evaluations) - 1) / len(evaluations) * proportions \
-        + 1 / len(evaluations)
-    np.fill_diagonal(proportions, 1)
-    return proportions
-
-
-def t_tests(evaluations, variances, dof=1):
-    """pairwise t_test based significant tests for a difference in model
-    performance
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be tested, typically from a results object
-        variances (numpy.ndarray):
-            vector of the variances of model evaluation differences
-        dof (integer):
-            degrees of freedom used for the test (default=1)
-
-    Returns:
-        numpy.ndarray: matrix of p-values for the test
-
-    """
-    if variances is None:
-        raise ValueError('No variance estimates provided for t_test!')
-    n_model = evaluations.shape[1]
-    evaluations = np.mean(evaluations, 0)
-    while evaluations.ndim > 1:
-        evaluations = np.mean(evaluations, axis=-1)
-    C = pairwise_contrast(np.arange(n_model))
-    diffs = C @ evaluations
-    t = diffs / np.sqrt(np.maximum(variances, np.finfo(float).eps))
-    t = batch_to_matrices(np.array([t]))[0][0]
-    p = 2 * (1 - stats.t.cdf(np.abs(t), dof))
-    return p
-
-
-def t_test_0(evaluations, variances, dof=1):
-    """
-    t-tests against 0 performance.
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be tested, typically from a results object
-        variances (numpy.ndarray):
-            vector of model evaluation variances
-        dof (integer):
-            degrees of freedom used for the test (default=1)
-
-    Returns:
-        numpy.ndarray: p-values for the raw t-test of each model against 0.
-
-    """
-    if variances is None:
-        raise ValueError('No variance estimates provided for t_test!')
-    evaluations = np.mean(evaluations, 0)
-    while evaluations.ndim > 1:
-        evaluations = np.mean(evaluations, axis=-1)
-    t = evaluations / np.sqrt(np.maximum(variances, np.finfo(float).eps))
-    p = 1 - stats.t.cdf(t, dof)
-    return p
-
-
-def t_test_nc(evaluations, variances, noise_ceil, dof=1):
-    """
-    t-tests against noise_ceiling.
-    Technically this can be used to test evaluations against any fixed
-    number.
-
-    Args:
-        evaluations (numpy.ndarray):
-            model evaluations to be tested, typically from a results object
-        variances (numpy.ndarray):
-            variance estimates for the comparisons to the noise ceiling
-        noise_ceil (float):
-            the average noise ceiling to test against.
-        noise_ceil_var (numpy.ndarray):
-            variance or covariance of the noise ceiling
-        dof (integer):
-            degrees of freedom used for the test (default=1)
-
-    Returns:
-        numpy.ndarray: p-values for the raw t-test of each model against
-        the noise ceiling.
-
-    """
-    if variances is None:
-        raise ValueError('No variance estimates provided for t_test!')
-    evaluations = np.mean(evaluations, 0)
-    while evaluations.ndim > 1:
-        evaluations = np.mean(evaluations, axis=-1)
-    p = np.empty(len(evaluations))
-    for i, eval_i in enumerate(evaluations):
-        t = (eval_i - noise_ceil) / np.sqrt(
-            np.maximum(variances[i], np.finfo(float).eps))
-        p[i] = 2 * (1 - stats.t.cdf(np.abs(t), dof))
-    return p
-
-
-def extract_variances(variance, nc_included=True):
-    """ extracts the variances for the individual model evaluations,
-    differences between model evaluations and for the comparison to
-    the noise ceiling
-
-    for 1D arrays we assume a diagonal covariance is meant
-
-    for 2D arrays this is taken as the covariance of the model evals
-
-    for 3D arrays we assume this is the result of a dual bootstrap and
-    perform the correction. Then there should be three covariances given
-    from double, rdm & pattern bootstrap in that order.
-
-    nc_included=True jields the result if the last two columns correspond
-    to the noise ceiling results
-
-    nc_included=False assumes that the noise ceiling is fixed instead.
-    """
-    if variance.ndim == 0:
-        variance = np.array([variance])
-    if variance.ndim == 1:
-        # model evaluations assumed independent
-        if nc_included:
-            C = pairwise_contrast(np.arange(variance.shape[0] - 2))
-            model_variances = variance[:-2]
-            nc_variances = np.expand_dims(model_variances, -1) \
-                + np.expand_dims(variance[-2:], 0)
-            diff_variances = np.diag(C @ np.diag(variance[:-2]) @ C.T)
-        else:
-            C = pairwise_contrast(np.arange(variance.shape[0]))
-            model_variances = variance
-            nc_variances = np.array([variance, variance]).T
-            diff_variances = np.diag(C @ np.diag(variance) @ C.T)
-    elif variance.ndim == 2:
-        # a single covariance matrix
-        if nc_included:
-            C = pairwise_contrast(np.arange(variance.shape[0] - 2))
-            model_variances = np.diag(variance)[:-2]
-            nc_variances = np.expand_dims(model_variances, -1) \
-                - 2 * variance[:-2, -2:] \
-                + np.expand_dims(np.diag(variance[-2:, -2:]), 0)
-            diff_variances = np.diag(C @ variance[:-2, :-2] @ C.T)
-        else:
-            C = pairwise_contrast(np.arange(variance.shape[0]))
-            model_variances = np.diag(variance)
-            nc_variances = np.array([model_variances, model_variances]).T
-            diff_variances = np.diag(C @ variance @ C.T)
-    elif variance.ndim == 3:
-        # general transform for multiple covariance matrices
-        if nc_included:
-            C = pairwise_contrast(np.arange(variance.shape[1] - 2))
-            model_variances = np.einsum('ijj->ij', variance)[:, :-2]
-            nc_variances = np.expand_dims(model_variances, -1) \
-                - 2 * variance[:, :-2, -2:] \
-                + np.expand_dims(np.einsum('ijj->ij',
-                                           variance[:, -2:, -2:]), 1)
-            # np.diag(C@variances@C.T)
-            diff_variances = np.einsum(
-                'ij,kjl,il->ki', C, variance[:, :-2, :-2], C)
-        else:
-            C = pairwise_contrast(np.arange(variance.shape[1]))
-            model_variances = np.einsum('ijj->ij', variance)
-            nc_variances = np.array([model_variances, model_variances]
-                                    ).transpose(1, 2, 0)
-            diff_variances = np.einsum('ij,kjl,il->ki', C, variance, C)
-        # dual bootstrap variance estimate from 3 covariance matrices
-        model_variances = _dual_bootstrap(model_variances)
-        nc_variances = _dual_bootstrap(nc_variances)
-        diff_variances = _dual_bootstrap(diff_variances)
-    return model_variances, diff_variances, nc_variances
-
-
-def get_errorbars(model_var, evaluations, dof, error_bars='sem',
-                  test_type='t-test'):
-    """ computes errorbars for the model-evaluations from a results object
-
-    Args:
-        results : rsatoolbox.inference.Results
-            the results object
-        eb_type : str, optional
-            which errorbars to compute
-
-    Returns:
-        limits (np.array)
-    """
-    if model_var is None:
-        return np.full((2, evaluations.shape[1]), np.nan)
-    if error_bars.lower() == 'sem':
-        errorbar_low = np.sqrt(np.maximum(model_var, 0))
-        errorbar_high = np.sqrt(np.maximum(model_var, 0))
-    elif error_bars[0:2].lower() == 'ci':
-        if len(error_bars) == 2:
-            CI_percent = 95.0
-        else:
-            CI_percent = float(error_bars[2:])
-        prop_cut = (1 - CI_percent / 100) / 2
-        if test_type == 'bootstrap':
-            n_models = evaluations.shape[1]
-            framed_evals = np.concatenate(
-                (np.tile(np.array((-np.inf, np.inf)).reshape(2, 1),
-                         (1, n_models)),
-                 evaluations),
-                axis=0)
-            perf = np.mean(evaluations, 0)
-            while perf.ndim > 1:
-                perf = np.mean(perf, -1)
-            errorbar_low = -(np.quantile(framed_evals, prop_cut, axis=0)
-                             - perf)
-            errorbar_high = (np.quantile(framed_evals, 1 - prop_cut,
-                                         axis=0)
-                             - perf)
-        else:
-            std_eval = np.sqrt(np.maximum(model_var, 0))
-            errorbar_low = std_eval \
-                * tdist.ppf(prop_cut, dof)
-            errorbar_high = std_eval \
-                * tdist.ppf(prop_cut, dof)
-    else:
-        raise Exception('computing errorbars: Argument ' +
-                        'error_bars is incorrectly defined as '
-                        + str(error_bars) + '.')
-    limits = np.stack((errorbar_low, errorbar_high))
-    if np.isnan(limits).any() or (abs(limits) == np.inf).any():
-        raise Exception(
-            'computing errorbars: Too few bootstrap samples for the ' +
-            'requested confidence interval: ' + error_bars + '.')
-    return limits
-
-
-def _dual_bootstrap(variances):
-    """ helper function to perform the dual bootstrap
-
-    Takes a 3x... array of variances and computes the corrections assuming:
-    variances[0] are the variances in the double bootstrap
-    variances[1] are the variances in the rdm bootstrap
-    variances[2] are the variances in the pattern bootstrap
-    """
-    variance = 2 * (variances[1] + variances[2]) \
-        - variances[0]
-    variance = np.maximum(np.maximum(
-        variance, variances[1]), variances[2])
-    variance = np.minimum(
-        variance, variances[0])
-    return variance
-
-
-def default_k_pattern(n_pattern):
-    """ the default number of pattern divisions for crossvalidation
-    minimum number of patterns is 3*k_pattern. Thus for n_pattern <=9 this
-    returns 2. From there it grows gradually until 5 groups are made for 40
-    patterns. From this point onwards the number of groups is kept at 5.
-
-    bootstrapped crossvalidation also uses this function to set k, but scales
-    n_rdm to the expected proportion of samples retained when bootstrapping
-    (1-np.exp(-1))
-    """
-    if n_pattern < 12:
-        k_pattern = 2
-    elif n_pattern < 24:
-        k_pattern = 3
-    elif n_pattern < 40:
-        k_pattern = 4
-    else:
-        k_pattern = 5
-    return k_pattern
-
-
-def default_k_rdm(n_rdm):
-    """ the default number of rdm groupsfor crossvalidation
-    minimum number of subjects is k_rdm. We switch to more groups whenever
-    the groups all contain more rdms, e.g. we make 3 groups of 2 instead of
-    2 groups of 3. We follow this scheme until we reach 5 groups of 4.
-    From there on this function returns 5 groups forever.
-
-    bootstrapped crossvalidation also uses this function to set k, but scales
-    n_rdm to the expected proportion of samples retained when bootstrapping
-    (1-np.exp(-1))
-    """
-    if n_rdm < 6:
-        k_rdm = 2
-    elif n_rdm < 12:
-        k_rdm = 3
-    elif n_rdm < 20:
-        k_rdm = 4
-    else:
-        k_rdm = 5
-    return k_rdm
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Inference module utilities
+"""
+from __future__ import annotations
+from collections.abc import Iterable
+from typing import TYPE_CHECKING, Optional
+import warnings
+import numpy as np
+from scipy import stats
+from scipy.stats import rankdata, wilcoxon
+from scipy.stats import t as tdist
+from rsatoolbox.model import Model
+from rsatoolbox.rdm import RDMs
+from .matrix import pairwise_contrast
+from .rdm_utils import batch_to_matrices
+if TYPE_CHECKING:
+    from numpy.typing import NDArray
+
+
+def input_check_model(models, theta=None, fitter=None, N=1):
+    """ Checks whether model related inputs to evaluations are valid and
+    generates an evaluation-matrix of fitting size.
+
+    Args:
+        model : [list of] rsatoolbox.rdm.RDMs
+            the models to be evaluated
+        theta : numpy.ndarray or list , optional
+            Parameter(s) for the model(s). The default is None.
+        fitter : [list of] function, optional
+            fitting function to overwrite the model default.
+            The default is None, i.e. keep default
+        N : int, optional
+            number of samples/rows in evaluations matrix. The default is 1.
+
+    Returns:
+        evaluations : numpy.ndarray
+            empty evaluations-matrix
+        theta : list
+            the processed and checked model parameters
+        fitter : [list of] functions
+            checked and processed fitter functions
+
+    """
+    if isinstance(models, Model):
+        models = [models]
+    elif not isinstance(models, Iterable):
+        raise ValueError('model should be an rsatoolbox.model.Model or a list of'
+                         + ' such objects')
+    if N > 1:
+        evaluations = np.zeros((N, len(models)))
+    else:
+        evaluations = np.zeros(len(models))
+    if theta is not None:
+        assert isinstance(theta, Iterable), 'If a list of models is' \
+            + ' passed theta must be a list of parameters'
+        assert len(models) == len(theta), 'there should equally many' \
+            + ' models as parameters'
+    else:
+        theta = [None] * len(models)
+    if fitter is None:
+        fitter = [None] * len(models)
+    elif isinstance(fitter, Iterable):
+        assert len(fitter) == len(models), 'if fitters are passed ' \
+            + 'there should be as many as models'
+    else:
+        fitter = [fitter] * len(models)
+    for k, model in enumerate(models):
+        if fitter[k] is None:
+            fitter[k] = model.default_fitter
+    return models, evaluations, theta, fitter
+
+
+def pool_rdm(rdms, method: str = 'cosine'):
+    """pools multiple RDMs into the one with maximal performance under a given
+    evaluation metric
+    rdm_descriptors of the generated rdms are empty
+
+    Args:
+        rdms (rsatoolbox.rdm.RDMs):
+            RDMs to be pooled
+        method : String, optional
+            Which comparison method to optimize for. The default is 'cosine'.
+
+    Returns:
+        rsatoolbox.rdm.RDMs: the pooled RDM, i.e. a RDM with maximal performance
+            under the chosen method
+
+    """
+    rdm_vec = rdms.get_vectors()
+    if method == 'euclid':
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'neg_riem_dist':
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'cosine':
+        rdm_vec = rdm_vec / np.sqrt(np.nanmean(rdm_vec ** 2, axis=1,
+                                               keepdims=True))
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'corr':
+        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
+        rdm_vec = rdm_vec / np.nanstd(rdm_vec, axis=1, keepdims=True)
+        rdm_vec = _nan_mean(rdm_vec)
+        rdm_vec = rdm_vec - np.nanmin(rdm_vec)
+    elif method == 'cosine_cov':
+        rdm_vec = rdm_vec / np.sqrt(np.nanmean(rdm_vec ** 2, axis=1,
+                                               keepdims=True))
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'corr_cov':
+        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
+        rdm_vec = rdm_vec / np.nanstd(rdm_vec, axis=1, keepdims=True)
+        rdm_vec = _nan_mean(rdm_vec)
+        rdm_vec = rdm_vec - np.nanmin(rdm_vec)
+    elif method in ('spearman', 'rho-a'):
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'rho-a':
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method in ('kendall', 'tau-b'):
+        warnings.warn('Noise ceiling for tau based on averaged ranks!')
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'tau-a':
+        warnings.warn('Noise ceiling for tau based on averaged ranks!')
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    else:
+        raise ValueError('Unknown RDM comparison method requested!')
+    return RDMs(rdm_vec,
+                dissimilarity_measure=rdms.dissimilarity_measure,
+                descriptors=rdms.descriptors,
+                rdm_descriptors=None,
+                pattern_descriptors=rdms.pattern_descriptors)
+
+
+def _nan_mean(rdm_vector: NDArray) -> NDArray:
+    """ takes the average over a rdm_vector with nans for masked entries
+    without a warning
+
+    Args:
+        rdm_vector(numpy.ndarray): set of rdm_vectors to be averaged
+
+    Returns:
+        rdm_mean(numpy.ndarray): the mean rdm
+
+    """
+    nan_idx = ~np.isnan(rdm_vector[0])
+    mean_values = np.mean(rdm_vector[:, nan_idx], axis=0)
+    rdm_mean = np.empty((1, rdm_vector.shape[1]))
+    rdm_mean.fill(np.nan)
+    rdm_mean[:, nan_idx] = mean_values
+    return rdm_mean
+
+
+def _nan_rank_data(rdm_vector: NDArray) -> NDArray:
+    """ rank_data for vectors with nan entries
+
+    Args:
+        rdm_vector(numpy.ndarray): the vector to be rank_transformed
+
+    Returns:
+        ranks(numpy.ndarray): the ranks with nans where the original vector
+            had nans
+
+    """
+    ranks_no_nan = rankdata(rdm_vector[~np.isnan(rdm_vector)])
+    ranks = np.ones_like(rdm_vector) * np.nan
+    ranks[~np.isnan(rdm_vector)] = ranks_no_nan
+    return ranks
+
+
+def all_tests(
+        evaluations: NDArray,
+        noise_ceil: NDArray,
+        test_type: str = 't-test',
+        model_var: Optional[NDArray] = None,
+        diff_var: Optional[NDArray] = None,
+        noise_ceil_var: Optional[NDArray] = None,
+        dof: int = 1):
+    """wrapper running all tests necessary for the model plot
+    -> pairwise tests, tests against 0 and against noise ceiling
+
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+            (should be 3D: bootstrap x models x subjects or repeats)
+        noise_ceil (numpy.ndarray):
+            noise_ceiling estimate(s) to compare against
+        test_type(String):
+            't-test' : t-test bases tests using variances
+            'bootstrap' : Direct bootstrap sample based tests
+            'ranksum' : Wilcoxon signed rank-sum tests
+        model_var, diff_var, noise_ceil_var:
+            variance estimates from the results object.
+            These correspond to the variances of the individual model evals,
+            pairs of model evals and differences from the noise ceiling.
+
+    Returns:
+        numpy.ndarrays: p_pairwise, p_zero, p_noise
+
+    """
+    if test_type == 't-test':
+        p_pairwise = t_tests(evaluations, diff_var, dof=dof)
+        p_zero = t_test_0(evaluations, model_var, dof=dof)
+        p_noise = t_test_nc(evaluations, noise_ceil_var[:, 0],
+                            np.mean(noise_ceil[0]), dof)
+    elif test_type == 'bootstrap':
+        if len(noise_ceil.shape) > 1:
+            noise_lower_bs = noise_ceil[0]
+            noise_lower_bs.shape = (noise_ceil.shape[0], 1)
+        else:
+            noise_lower_bs = noise_ceil[0].reshape(1, 1)
+        p_pairwise = bootstrap_pair_tests(evaluations)
+        p_zero = ((evaluations <= 0).sum(axis=0) + 1) / evaluations.shape[0]
+        diffs = noise_lower_bs - evaluations
+        p_noise = ((diffs <= 0).sum(axis=0) + 1) / evaluations.shape[0]
+    elif test_type == 'ranksum':
+        noise_c = np.mean(noise_ceil[0])
+        p_pairwise = ranksum_pair_test(evaluations)
+        p_zero = ranksum_value_test(evaluations, 0)
+        p_noise = ranksum_value_test(evaluations, noise_c)
+    else:
+        raise ValueError('test_type not recognized.\n'
+                         + 'Options are: t-test, bootstrap, ranksum')
+    return p_pairwise, p_zero, p_noise
+
+
+def pair_tests(
+        evaluations: NDArray,
+        test_type: str = 't-test',
+        diff_var: Optional[NDArray] = None,
+        dof: int = 1):
+    """wrapper running pair tests
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+            (should be 3D: bootstrap x models x subjects or repeats)
+        test_type(Strinng):
+            't-test' : t-test bases tests using variances
+            'bootstrap' : Direct bootstrap sample based tests
+            'ranksum' : Wilcoxon signed rank-sum tests
+
+    Returns:
+        numpy.ndarray: p_pairwise
+
+    """
+    if test_type == 't-test':
+        p_pairwise = t_tests(evaluations, diff_var, dof=dof)
+    elif test_type == 'bootstrap':
+        p_pairwise = bootstrap_pair_tests(evaluations)
+    elif test_type == 'ranksum':
+        p_pairwise = ranksum_pair_test(evaluations)
+    else:
+        raise ValueError('test_type not recognized.\n'
+                         + 'Options are: t-test, bootstrap, ranksum')
+    return p_pairwise
+
+
+def zero_tests(evaluations, test_type='t-test',
+               model_var=None, dof=1):
+    """wrapper running tests against 0
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+            (should be 3D: bootstrap x models x subjects or repeats)
+        test_type(Strinng):
+            't-test' : t-test bases tests using variances
+            'bootstrap' : Direct bootstrap sample based tests
+            'ranksum' : Wilcoxon signed rank-sum tests
+
+    Returns:
+        numpy.ndarray: p_zero
+
+    """
+    if test_type == 't-test':
+        p_zero = t_test_0(evaluations, model_var, dof=dof)
+    elif test_type == 'bootstrap':
+        p_zero = ((evaluations <= 0).sum(axis=0) + 1) / evaluations.shape[0]
+    elif test_type == 'ranksum':
+        p_zero = ranksum_value_test(evaluations, 0)
+    else:
+        raise ValueError('test_type not recognized.\n'
+                         + 'Options are: t-test, bootstrap, ranksum')
+    return p_zero
+
+
+def nc_tests(evaluations, noise_ceil, test_type='t-test',
+             noise_ceil_var=None, dof=1):
+    """wrapper running tests against noise ceiling
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+            (should be 3D: bootstrap x models x subjects or repeats)
+        noise_ceil (numpy.ndarray):
+            noise_ceiling estimate(s) to compare against
+        test_type(Strinng):
+            't-test' : t-test bases tests using variances
+            'bootstrap' : Direct bootstrap sample based tests
+            'ranksum' : Wilcoxon signed rank-sum tests
+        noise_ceil_var:
+            variance estimate from the results object.
+            These correspond to the variances of the
+            differences from the noise ceiling.
+
+    Returns:
+        numpy.ndarrays: p_noise
+
+    """
+    if test_type == 't-test':
+        p_noise = t_test_nc(evaluations, noise_ceil_var[:, 0],
+                            np.mean(noise_ceil[0]), dof)
+    elif test_type == 'bootstrap':
+        if len(noise_ceil.shape) > 1:
+            noise_lower_bs = noise_ceil[0]
+            noise_lower_bs.shape = (noise_ceil.shape[0], 1)
+        else:
+            noise_lower_bs = noise_ceil[0].reshape(1, 1)
+        diffs = noise_lower_bs - evaluations
+        p_noise = ((diffs <= 0).sum(axis=0) + 1) / evaluations.shape[0]
+    elif test_type == 'ranksum':
+        noise_c = np.mean(noise_ceil[0])
+        p_noise = ranksum_value_test(evaluations, noise_c)
+    else:
+        raise ValueError('test_type not recognized.\n'
+                         + 'Options are: t-test, bootstrap, ranksum')
+    return p_noise
+
+
+def ranksum_pair_test(evaluations):
+    """pairwise tests between models using the wilcoxon signed rank test
+
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+            (should be 3D: bootstrap x models x subjects or repeats)
+
+    Returns:
+        numpy.ndarray: matrix of proportions of opposit conclusions, i.e.
+            p-values for the test
+
+    """
+    # check that the dimensionality is correct
+    assert evaluations.ndim == 3, \
+        'provided evaluations array has wrong dimensionality'
+    n_model = evaluations.shape[1]
+    # ignore bootstraps
+    evaluations = np.nanmean(evaluations, 0)
+    pvalues = np.empty((n_model, n_model))
+    for i_model in range(n_model - 1):
+        for j_model in range(i_model + 1, n_model):
+            pvalues[i_model, j_model] = wilcoxon(
+                evaluations[i_model], evaluations[j_model]).pvalue
+            pvalues[j_model, i_model] = pvalues[i_model, j_model]
+    np.fill_diagonal(pvalues, 1)
+    return pvalues
+
+
+def ranksum_value_test(evaluations, comp_value=0):
+    """nonparametric wilcoxon signed rank test against a fixed value
+
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+            (should be 3D: bootstrap x models x subjects or repeats)
+        comp_value(float):
+            value to compare against
+
+    Returns:
+        float: p-value
+
+    """
+    # check that the dimensionality is correct
+    assert evaluations.ndim == 3, \
+        'provided evaluations array has wrong dimensionality'
+    n_model = evaluations.shape[1]
+    # ignore bootstraps
+    evaluations = np.nanmean(evaluations, 0)
+    pvalues = np.empty(n_model)
+    for i_model in range(n_model):
+        pvalues[i_model] = wilcoxon(
+            evaluations[i_model] - comp_value).pvalue
+    return pvalues
+
+
+def bootstrap_pair_tests(evaluations):
+    """pairwise bootstrapping significance tests for a difference in model
+    performance.
+    Tests add 1/len(evaluations) to each p-value and are computed as
+    two sided tests, i.e. as 2 * the smaller proportion
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be compared
+
+    Returns:
+        numpy.ndarray: matrix of proportions of opposit conclusions, i.e.
+            p-values for the bootstrap test
+    """
+    proportions = np.zeros((evaluations.shape[1], evaluations.shape[1]))
+    while len(evaluations.shape) > 2:
+        evaluations = np.mean(evaluations, axis=-1)
+    for i_model in range(evaluations.shape[1] - 1):
+        for j_model in range(i_model + 1, evaluations.shape[1]):
+            proportions[i_model, j_model] = np.sum(
+                evaluations[:, i_model] < evaluations[:, j_model]) \
+                / (evaluations.shape[0] -
+                   np.sum(evaluations[:, i_model] == evaluations[:, j_model]))
+            proportions[j_model, i_model] = proportions[i_model, j_model]
+    proportions = np.minimum(proportions, 1 - proportions) * 2
+    proportions = (len(evaluations) - 1) / len(evaluations) * proportions \
+        + 1 / len(evaluations)
+    np.fill_diagonal(proportions, 1)
+    return proportions
+
+
+def t_tests(evaluations, variances, dof=1):
+    """pairwise t_test based significant tests for a difference in model
+    performance
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be tested, typically from a results object
+        variances (numpy.ndarray):
+            vector of the variances of model evaluation differences
+        dof (integer):
+            degrees of freedom used for the test (default=1)
+
+    Returns:
+        numpy.ndarray: matrix of p-values for the test
+
+    """
+    if variances is None:
+        raise ValueError('No variance estimates provided for t_test!')
+    n_model = evaluations.shape[1]
+    evaluations = np.mean(evaluations, 0)
+    while evaluations.ndim > 1:
+        evaluations = np.mean(evaluations, axis=-1)
+    C = pairwise_contrast(np.arange(n_model))
+    diffs = C @ evaluations
+    t = diffs / np.sqrt(np.maximum(variances, np.finfo(float).eps))
+    t = batch_to_matrices(np.array([t]))[0][0]
+    p = 2 * (1 - stats.t.cdf(np.abs(t), dof))
+    return p
+
+
+def t_test_0(evaluations, variances, dof=1):
+    """
+    t-tests against 0 performance.
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be tested, typically from a results object
+        variances (numpy.ndarray):
+            vector of model evaluation variances
+        dof (integer):
+            degrees of freedom used for the test (default=1)
+
+    Returns:
+        numpy.ndarray: p-values for the raw t-test of each model against 0.
+
+    """
+    if variances is None:
+        raise ValueError('No variance estimates provided for t_test!')
+    evaluations = np.mean(evaluations, 0)
+    while evaluations.ndim > 1:
+        evaluations = np.mean(evaluations, axis=-1)
+    t = evaluations / np.sqrt(np.maximum(variances, np.finfo(float).eps))
+    p = 1 - stats.t.cdf(t, dof)
+    return p
+
+
+def t_test_nc(evaluations, variances, noise_ceil, dof=1):
+    """
+    t-tests against noise_ceiling.
+    Technically this can be used to test evaluations against any fixed
+    number.
+
+    Args:
+        evaluations (numpy.ndarray):
+            model evaluations to be tested, typically from a results object
+        variances (numpy.ndarray):
+            variance estimates for the comparisons to the noise ceiling
+        noise_ceil (float):
+            the average noise ceiling to test against.
+        noise_ceil_var (numpy.ndarray):
+            variance or covariance of the noise ceiling
+        dof (integer):
+            degrees of freedom used for the test (default=1)
+
+    Returns:
+        numpy.ndarray: p-values for the raw t-test of each model against
+        the noise ceiling.
+
+    """
+    if variances is None:
+        raise ValueError('No variance estimates provided for t_test!')
+    evaluations = np.mean(evaluations, 0)
+    while evaluations.ndim > 1:
+        evaluations = np.mean(evaluations, axis=-1)
+    p = np.empty(len(evaluations))
+    for i, eval_i in enumerate(evaluations):
+        t = (eval_i - noise_ceil) / np.sqrt(
+            np.maximum(variances[i], np.finfo(float).eps))
+        p[i] = 2 * (1 - stats.t.cdf(np.abs(t), dof))
+    return p
+
+
+def extract_variances(
+        variance,
+        nc_included: bool = True,
+        n_rdm: Optional[int] = None,
+        n_pattern: Optional[int] = None):
+    """ extracts the variances for the individual model evaluations,
+    differences between model evaluations and for the comparison to
+    the noise ceiling
+
+    for 1D arrays we assume a diagonal covariance is meant
+
+    for 2D arrays this is taken as the covariance of the model evals
+
+    for 3D arrays we assume this is the result of a dual bootstrap and
+    perform the correction. Then there should be three covariances given
+    from double, rdm & pattern bootstrap in that order.
+
+    nc_included=True jields the result if the last two columns correspond
+    to the noise ceiling results
+
+    nc_included=False assumes that the noise ceiling is fixed instead.
+
+    To get the more accurate estimates that take into account
+    the number of subjects and/or the numbers of stimuli
+    can be passed as n_rdm and n_pattern respectively.
+    This function corrects for all ns that are passed. If you bootstrapped
+    only one factor only pass the N for that factor!
+    """
+    if variance.ndim == 0:
+        variance = np.array([variance])
+    if variance.ndim == 1:
+        # model evaluations assumed independent
+        if nc_included:
+            C = pairwise_contrast(np.arange(variance.shape[0] - 2))
+            model_variances = variance[:-2]
+            nc_variances = np.expand_dims(model_variances, -1) \
+                + np.expand_dims(variance[-2:], 0)
+            diff_variances = np.diag(C @ np.diag(variance[:-2]) @ C.T)
+        else:
+            C = pairwise_contrast(np.arange(variance.shape[0]))
+            model_variances = variance
+            nc_variances = np.array([variance, variance]).T
+            diff_variances = np.diag(C @ np.diag(variance) @ C.T)
+        model_variances = _correct_1d(model_variances, n_pattern, n_rdm)
+        nc_variances = _correct_1d(nc_variances, n_pattern, n_rdm)
+        diff_variances = _correct_1d(diff_variances, n_pattern, n_rdm)
+    elif variance.ndim == 2:
+        # a single covariance matrix
+        if nc_included:
+            C = pairwise_contrast(np.arange(variance.shape[0] - 2))
+            model_variances = np.diag(variance)[:-2]
+            nc_variances = np.expand_dims(model_variances, -1) \
+                - 2 * variance[:-2, -2:] \
+                + np.expand_dims(np.diag(variance[-2:, -2:]), 0)
+            diff_variances = np.diag(C @ variance[:-2, :-2] @ C.T)
+        else:
+            C = pairwise_contrast(np.arange(variance.shape[0]))
+            model_variances = np.diag(variance)
+            nc_variances = np.array([model_variances, model_variances]).T
+            diff_variances = np.diag(C @ variance @ C.T)
+        model_variances = _correct_1d(model_variances, n_pattern, n_rdm)
+        nc_variances = _correct_1d(nc_variances, n_pattern, n_rdm)
+        diff_variances = _correct_1d(diff_variances, n_pattern, n_rdm)
+    elif variance.ndim == 3:
+        # general transform for multiple covariance matrices
+        if nc_included:
+            C = pairwise_contrast(np.arange(variance.shape[1] - 2))
+            model_variances = np.einsum('ijj->ij', variance)[:, :-2]
+            nc_variances = np.expand_dims(model_variances, -1) \
+                - 2 * variance[:, :-2, -2:] \
+                + np.expand_dims(np.einsum('ijj->ij',
+                                           variance[:, -2:, -2:]), 1)
+            # np.diag(C@variances@C.T)
+            diff_variances = np.einsum(
+                'ij,kjl,il->ki', C, variance[:, :-2, :-2], C)
+        else:
+            C = pairwise_contrast(np.arange(variance.shape[1]))
+            model_variances = np.einsum('ijj->ij', variance)
+            nc_variances = np.array([model_variances, model_variances]
+                                    ).transpose(1, 2, 0)
+            diff_variances = np.einsum('ij,kjl,il->ki', C, variance, C)
+        # dual bootstrap variance estimate from 3 covariance matrices
+        model_variances = _dual_bootstrap(model_variances, n_rdm, n_pattern)
+        nc_variances = _dual_bootstrap(nc_variances, n_rdm, n_pattern)
+        diff_variances = _dual_bootstrap(diff_variances, n_rdm, n_pattern)
+    return model_variances, diff_variances, nc_variances
+
+
+def _correct_1d(
+        variance: NDArray,
+        n_pattern: Optional[int] = None,
+        n_rdm: Optional[int] = None):
+    if (n_pattern is not None) and (n_rdm is not None):
+        # uncorrected dual bootstrap?
+        n = min(n_rdm, n_pattern)
+    elif n_pattern is not None:
+        n = n_pattern
+    elif n_rdm is not None:
+        n = n_rdm
+    else:
+        n = None
+    if n is not None:
+        variance = (n / (n - 1)) * variance
+    return variance
+
+
+def get_errorbars(model_var, evaluations, dof, error_bars='sem',
+                  test_type='t-test'):
+    """ computes errorbars for the model-evaluations from a results object
+
+    Args:
+        results : rsatoolbox.inference.Results
+            the results object
+        eb_type : str, optional
+            which errorbars to compute
+
+    Returns:
+        limits (np.array)
+    """
+    if model_var is None:
+        return np.full((2, evaluations.shape[1]), np.nan)
+    if error_bars.lower() == 'sem':
+        errorbar_low = np.sqrt(np.maximum(model_var, 0))
+        errorbar_high = np.sqrt(np.maximum(model_var, 0))
+    elif error_bars[0:2].lower() == 'ci':
+        if len(error_bars) == 2:
+            CI_percent = 95.0
+        else:
+            CI_percent = float(error_bars[2:])
+        prop_cut = (1 - CI_percent / 100) / 2
+        if test_type == 'bootstrap':
+            n_models = evaluations.shape[1]
+            framed_evals = np.concatenate(
+                (np.tile(np.array((-np.inf, np.inf)).reshape(2, 1),
+                         (1, n_models)),
+                 evaluations),
+                axis=0)
+            perf = np.mean(evaluations, 0)
+            while perf.ndim > 1:
+                perf = np.mean(perf, -1)
+            errorbar_low = -(np.quantile(framed_evals, prop_cut, axis=0)
+                             - perf)
+            errorbar_high = (np.quantile(framed_evals, 1 - prop_cut,
+                                         axis=0)
+                             - perf)
+        else:
+            std_eval = np.sqrt(np.maximum(model_var, 0))
+            errorbar_low = std_eval \
+                * tdist.ppf(prop_cut, dof)
+            errorbar_high = std_eval \
+                * tdist.ppf(prop_cut, dof)
+    else:
+        raise ValueError('computing errorbars: Argument ' +
+                         'error_bars is incorrectly defined as '
+                         + str(error_bars) + '.')
+    limits = np.stack((errorbar_low, errorbar_high))
+    if np.isnan(limits).any() or (abs(limits) == np.inf).any():
+        raise ValueError(
+            'computing errorbars: Too few bootstrap samples for the ' +
+            'requested confidence interval: ' + error_bars + '.')
+    return limits
+
+
+def _dual_bootstrap(variances, n_rdm=None, n_pattern=None):
+    """ helper function to perform the dual bootstrap
+
+    Takes a 3x... array of variances and computes the corrections assuming:
+    variances[0] are the variances in the double bootstrap
+    variances[1] are the variances in the rdm bootstrap
+    variances[2] are the variances in the pattern bootstrap
+
+    If both n_rdm and n_pattern are given this uses
+    the more accurate small sample formula.
+    """
+    if n_rdm is None or n_pattern is None:
+        variance = 2 * (variances[1] + variances[2]) \
+            - variances[0]
+        variance = np.maximum(np.maximum(
+            variance, variances[1]), variances[2])
+        variance = np.minimum(
+            variance, variances[0])
+    else:
+        variance = (
+            (n_rdm / (n_rdm - 1)) * variances[1]
+            + (n_pattern / (n_pattern - 1)) * variances[2]
+            - ((n_pattern * n_rdm / (n_pattern - 1) / (n_rdm - 1))
+               * (variances[0] - variances[1] - variances[2])))
+        variance = np.maximum(np.maximum(
+            variance,
+            (n_rdm / (n_rdm - 1)) * variances[1]),
+            (n_pattern / (n_pattern - 1)) * variances[2])
+        variance = np.minimum(
+            variance, variances[0])
+    return variance
+
+
+def default_k_pattern(n_pattern):
+    """ the default number of pattern divisions for crossvalidation
+    minimum number of patterns is 3*k_pattern. Thus for n_pattern <=9 this
+    returns 2. From there it grows gradually until 5 groups are made for 40
+    patterns. From this point onwards the number of groups is kept at 5.
+
+    bootstrapped crossvalidation also uses this function to set k, but scales
+    n_rdm to the expected proportion of samples retained when bootstrapping
+    (1-np.exp(-1))
+    """
+    if n_pattern < 12:
+        k_pattern = 2
+    elif n_pattern < 24:
+        k_pattern = 3
+    elif n_pattern < 40:
+        k_pattern = 4
+    else:
+        k_pattern = 5
+    return k_pattern
+
+
+def default_k_rdm(n_rdm):
+    """ the default number of rdm groupsfor crossvalidation
+    minimum number of subjects is k_rdm. We switch to more groups whenever
+    the groups all contain more rdms, e.g. we make 3 groups of 2 instead of
+    2 groups of 3. We follow this scheme until we reach 5 groups of 4.
+    From there on this function returns 5 groups forever.
+
+    bootstrapped crossvalidation also uses this function to set k, but scales
+    n_rdm to the expected proportion of samples retained when bootstrapping
+    (1-np.exp(-1))
+    """
+    if n_rdm < 6:
+        k_rdm = 2
+    elif n_rdm < 12:
+        k_rdm = 3
+    elif n_rdm < 20:
+        k_rdm = 4
+    else:
+        k_rdm = 5
+    return k_rdm
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/pooling.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/pooling.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,121 +1,121 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on Tue Oct 20 23:58:28 2020
-
-@author: heiko
-"""
-
-import numpy as np
-import scipy.sparse
-from scipy.stats import rankdata
-from rsatoolbox.rdm import RDMs
-from rsatoolbox.util.matrix import get_v
-
-
-def pool_rdm(rdms, method='cosine', sigma_k=None):
-    """pools multiple RDMs into the one with maximal performance under a given
-    evaluation metric
-    rdm_descriptors of the generated rdms are empty
-
-    Args:
-        rdms (pyrsa.rdm.RDMs):
-            RDMs to be pooled
-        method : String, optional
-            Which comparison method to optimize for. The default is 'cosine'.
-
-    Returns:
-        pyrsa.rdm.RDMs: the pooled RDM, i.e. a RDM with maximal performance
-            under the chosen method
-
-    """
-    rdm_vec = rdms.get_vectors()
-    if method == 'euclid':
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'cosine':
-        rdm_vec = rdm_vec / np.sqrt(np.nanmean(rdm_vec ** 2, axis=1,
-                                               keepdims=True))
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'corr':
-        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
-        rdm_vec = rdm_vec / np.nanstd(rdm_vec, axis=1, keepdims=True)
-        rdm_vec = _nan_mean(rdm_vec)
-        rdm_vec = rdm_vec - np.nanmin(rdm_vec) + 0.01
-    elif method == 'cosine_cov':
-        v = get_v(rdms.n_cond, sigma_k=sigma_k)
-        ok_idx = np.all(np.isfinite(rdm_vec), axis=0)
-        v = v[ok_idx][:, ok_idx]
-        rdm_vec_nonan = rdm_vec[:, ok_idx]
-        v_inv_x = np.array([scipy.sparse.linalg.cg(v, rdm_vec_nonan[i],
-                                                   atol=10 ** -9)[0]
-                            for i in range(rdms.n_rdm)])
-        rdm_norms = np.einsum('ij, ij->i', rdm_vec_nonan, v_inv_x).reshape(
-            [rdms.n_rdm, 1])
-        rdm_vec = rdm_vec / np.sqrt(rdm_norms)
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'corr_cov':
-        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
-        v = get_v(rdms.n_cond, sigma_k=sigma_k)
-        ok_idx = np.all(np.isfinite(rdm_vec), axis=0)
-        v = v[ok_idx][:, ok_idx]
-        rdm_vec_nonan = rdm_vec[:, ok_idx]
-        v_inv_x = np.array([scipy.sparse.linalg.cg(v, rdm_vec_nonan[i],
-                                                   atol=10 ** -9)[0]
-                            for i in range(rdms.n_rdm)])
-        rdm_norms = np.einsum('ij, ij->i', rdm_vec_nonan, v_inv_x).reshape(
-            [rdms.n_rdm, 1])
-        rdm_vec = rdm_vec / np.sqrt(rdm_norms)
-        rdm_vec = _nan_mean(rdm_vec)
-        rdm_vec = rdm_vec - np.nanmin(rdm_vec) + 0.01
-    elif method in ('spearman', 'rho-a'):
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method == 'rho-a':
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    elif method in ('kendall', 'tau-b', 'tau-a'):
-        Warning('Noise ceiling for tau based on averaged ranks!')
-        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
-        rdm_vec = _nan_mean(rdm_vec)
-    else:
-        raise ValueError('Unknown RDM comparison method requested!')
-    return RDMs(rdm_vec,
-                dissimilarity_measure=rdms.dissimilarity_measure,
-                descriptors=rdms.descriptors,
-                rdm_descriptors=None,
-                pattern_descriptors=rdms.pattern_descriptors)
-
-
-def _nan_mean(rdm_vector):
-    """ takes the average over a rdm_vector with nans for masked entries
-    without a warning
-
-    Args:
-        rdm_vector(numpy.ndarray): set of rdm_vectors to be averaged
-
-    Returns:
-        rdm_mean(numpy.ndarray): the mean rdm
-
-    """
-    nan_idx = ~np.isnan(rdm_vector[0])
-    mean_values = np.mean(rdm_vector[:, nan_idx], axis=0)
-    rdm_mean = np.empty((1, rdm_vector.shape[1])) * np.nan
-    rdm_mean[:, nan_idx] = mean_values
-    return rdm_mean
-
-
-def _nan_rank_data(rdm_vector):
-    """ rank_data for vectors with nan entries
-
-    Args:
-        rdm_vector(numpy.ndarray): the vector to be rank_transformed
-
-    Returns:
-        ranks(numpy.ndarray): the ranks with nans where the original vector
-            had nans
-
-    """
-    ranks_no_nan = rankdata(rdm_vector[~np.isnan(rdm_vector)])
-    ranks = np.ones_like(rdm_vector) * np.nan
-    ranks[~np.isnan(rdm_vector)] = ranks_no_nan
-    return ranks
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on Tue Oct 20 23:58:28 2020
+
+@author: heiko
+"""
+
+import numpy as np
+import scipy.sparse
+from scipy.stats import rankdata
+from rsatoolbox.rdm import RDMs
+from rsatoolbox.util.matrix import get_v
+
+
+def pool_rdm(rdms, method='cosine', sigma_k=None):
+    """pools multiple RDMs into the one with maximal performance under a given
+    evaluation metric
+    rdm_descriptors of the generated rdms are empty
+
+    Args:
+        rdms (pyrsa.rdm.RDMs):
+            RDMs to be pooled
+        method : String, optional
+            Which comparison method to optimize for. The default is 'cosine'.
+
+    Returns:
+        pyrsa.rdm.RDMs: the pooled RDM, i.e. a RDM with maximal performance
+            under the chosen method
+
+    """
+    rdm_vec = rdms.get_vectors()
+    if method == 'euclid':
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'cosine':
+        rdm_vec = rdm_vec / np.sqrt(np.nanmean(rdm_vec ** 2, axis=1,
+                                               keepdims=True))
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'corr':
+        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
+        rdm_vec = rdm_vec / np.nanstd(rdm_vec, axis=1, keepdims=True)
+        rdm_vec = _nan_mean(rdm_vec)
+        rdm_vec = rdm_vec - np.nanmin(rdm_vec) + 0.01
+    elif method == 'cosine_cov':
+        v = get_v(rdms.n_cond, sigma_k=sigma_k)
+        ok_idx = np.all(np.isfinite(rdm_vec), axis=0)
+        v = v[ok_idx][:, ok_idx]
+        rdm_vec_nonan = rdm_vec[:, ok_idx]
+        v_inv_x = np.array([scipy.sparse.linalg.cg(v, rdm_vec_nonan[i],
+                                                   atol=10 ** -9)[0]
+                            for i in range(rdms.n_rdm)])
+        rdm_norms = np.einsum('ij, ij->i', rdm_vec_nonan, v_inv_x).reshape(
+            [rdms.n_rdm, 1])
+        rdm_vec = rdm_vec / np.sqrt(rdm_norms)
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'corr_cov':
+        rdm_vec = rdm_vec - np.nanmean(rdm_vec, axis=1, keepdims=True)
+        v = get_v(rdms.n_cond, sigma_k=sigma_k)
+        ok_idx = np.all(np.isfinite(rdm_vec), axis=0)
+        v = v[ok_idx][:, ok_idx]
+        rdm_vec_nonan = rdm_vec[:, ok_idx]
+        v_inv_x = np.array([scipy.sparse.linalg.cg(v, rdm_vec_nonan[i],
+                                                   atol=10 ** -9)[0]
+                            for i in range(rdms.n_rdm)])
+        rdm_norms = np.einsum('ij, ij->i', rdm_vec_nonan, v_inv_x).reshape(
+            [rdms.n_rdm, 1])
+        rdm_vec = rdm_vec / np.sqrt(rdm_norms)
+        rdm_vec = _nan_mean(rdm_vec)
+        rdm_vec = rdm_vec - np.nanmin(rdm_vec) + 0.01
+    elif method in ('spearman', 'rho-a'):
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method == 'rho-a':
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    elif method in ('kendall', 'tau-b', 'tau-a'):
+        Warning('Noise ceiling for tau based on averaged ranks!')
+        rdm_vec = np.array([_nan_rank_data(v) for v in rdm_vec])
+        rdm_vec = _nan_mean(rdm_vec)
+    else:
+        raise ValueError('Unknown RDM comparison method requested!')
+    return RDMs(rdm_vec,
+                dissimilarity_measure=rdms.dissimilarity_measure,
+                descriptors=rdms.descriptors,
+                rdm_descriptors=None,
+                pattern_descriptors=rdms.pattern_descriptors)
+
+
+def _nan_mean(rdm_vector):
+    """ takes the average over a rdm_vector with nans for masked entries
+    without a warning
+
+    Args:
+        rdm_vector(numpy.ndarray): set of rdm_vectors to be averaged
+
+    Returns:
+        rdm_mean(numpy.ndarray): the mean rdm
+
+    """
+    nan_idx = ~np.isnan(rdm_vector[0])
+    mean_values = np.mean(rdm_vector[:, nan_idx], axis=0)
+    rdm_mean = np.empty((1, rdm_vector.shape[1])) * np.nan
+    rdm_mean[:, nan_idx] = mean_values
+    return rdm_mean
+
+
+def _nan_rank_data(rdm_vector):
+    """ rank_data for vectors with nan entries
+
+    Args:
+        rdm_vector(numpy.ndarray): the vector to be rank_transformed
+
+    Returns:
+        ranks(numpy.ndarray): the ranks with nans where the original vector
+            had nans
+
+    """
+    ranks_no_nan = rankdata(rdm_vector[~np.isnan(rdm_vector)])
+    ranks = np.ones_like(rdm_vector) * np.nan
+    ranks[~np.isnan(rdm_vector)] = ranks_no_nan
+    return ranks
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/searchlight.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/searchlight.py`

 * *Ordering differences only*

 * *Files 21% similar despite different names*

```diff
@@ -1,209 +1,209 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-This code was initially inspired by the following :
-https://github.com/machow/pysearchlight
-
-@author: Daniel Lindh
-"""
-import numpy as np
-from scipy.spatial.distance import cdist
-from tqdm import tqdm
-from joblib import Parallel, delayed
-from rsatoolbox.data.dataset import Dataset
-from rsatoolbox.rdm.calc import calc_rdm
-from rsatoolbox.rdm import RDMs
-
-
-def _get_searchlight_neighbors(mask, center, radius=3):
-    """Return indices for searchlight where distance
-        between a voxel and their center < radius (in voxels)
-
-    Args:
-        center (index):  point around which to make searchlight sphere
-
-    Returns:
-        list: the list of volume indices that respect the
-                searchlight radius for the input center.
-    """
-    center = np.array(center)
-    mask_shape = mask.shape
-    cx, cy, cz = np.array(center)
-    x = np.arange(mask_shape[0])
-    y = np.arange(mask_shape[1])
-    z = np.arange(mask_shape[2])
-
-    # First mask the obvious points
-    # - may actually slow down your calculation depending.
-    x = x[abs(x - cx) < radius]
-    y = y[abs(y - cy) < radius]
-    z = z[abs(z - cz) < radius]
-
-    # Generate grid of points
-    X, Y, Z = np.meshgrid(x, y, z)
-    data = np.vstack((X.ravel(), Y.ravel(), Z.ravel())).T
-    distance = cdist(data, center.reshape(1, -1), 'euclidean').ravel()
-
-    return tuple(data[distance < radius].T.tolist())
-
-
-def get_volume_searchlight(mask, radius=2, threshold=1.0):
-    """
-    Searches through the non-zero voxels of the mask, selects centers where
-    proportion of sphere voxels >= self.threshold.
-
-    Args:
-
-        mask ([numpy array]): binary brain mask
-
-        radius (int, optional): the radius of each searchlight, defined in voxels.
-        Defaults to 2.
-
-        threshold (float, optional): Threshold of the proportion of voxels that need to
-        be inside the brain mask in order for it to be
-        considered a good searchlight center.
-        Values go between 0.0 - 1.0 where 1.0 means that
-        100% of the voxels need to be inside
-        the brain mask.
-        Defaults to 1.0.
-
-    Returns:
-        numpy array: array of centers of size n_centers x 3
-
-        list: list of lists with neighbors - the length of the list will correspond to:
-        n_centers x 3 x n_neighbors
-    """
-
-    mask = np.array(mask)
-    assert mask.ndim == 3, "Mask needs to be a 3-dimensional numpy array"
-
-    centers = list(zip(*np.nonzero(mask)))
-    good_centers = []
-    good_neighbors = []
-
-    for center in tqdm(centers, desc='Finding searchlights...'):
-        neighbors = _get_searchlight_neighbors(mask, center, radius)
-        if mask[neighbors].mean() >= threshold:
-            good_centers.append(center)
-            good_neighbors.append(neighbors)
-
-    good_centers = np.array(good_centers)
-    assert good_centers.shape[0] == len(good_neighbors),\
-        "number of centers and sets of neighbors do not match"
-    print(f'Found {len(good_neighbors)} searchlights')
-
-    # turn the 3-dim coordinates to array coordinates
-    centers = np.ravel_multi_index(good_centers.T, mask.shape)
-    neighbors = [np.ravel_multi_index(n, mask.shape) for n in good_neighbors]
-
-    return centers, neighbors
-
-
-def get_searchlight_RDMs(data_2d, centers, neighbors, events,
-                         method='correlation', verbose=True):
-    """Iterates over all the searchlight centers and calculates the RDM
-
-    Args:
-
-        data_2d (2D numpy array): brain data,
-        shape n_observations x n_channels (i.e. voxels/vertices)
-
-        centers (1D numpy array): center indices for all searchlights as provided
-        by rsatoolbox.util.searchlight.get_volume_searchlight
-
-        neighbors (list): list of lists with neighbor voxel indices for all searchlights
-        as provided by rsatoolbox.util.searchlight.get_volume_searchlight
-
-        events (1D numpy array): 1D array of length n_observations
-
-        method (str, optional): distance metric,
-        see rsatoolbox.rdm.calc for options. Defaults to 'correlation'.
-
-        verbose (bool, optional): Defaults to True.
-
-    Returns:
-        RDM [rsatoolbox.rdm.RDMs]: RDMs object with the RDM for each searchlight
-                              the RDM.rdm_descriptors['voxel_index']
-                              describes the center voxel index each RDM is associated with
-    """
-
-    data_2d, centers = np.array(data_2d), np.array(centers)
-    n_centers = centers.shape[0]
-
-    # For memory reasons, we chunk the data if we have more than 1000 RDMs
-    if n_centers > 1000:
-        # we can't run all centers at once, that will take too much memory
-        # so lets to some chunking
-        chunked_center = np.split(np.arange(n_centers),
-                                  np.linspace(0, n_centers,
-                                              101, dtype=int)[1:-1])
-
-        # loop over chunks
-        n_conds = len(np.unique(events))
-        RDM = np.zeros((n_centers, n_conds * (n_conds - 1) // 2))
-        for chunks in tqdm(chunked_center, desc='Calculating RDMs...'):
-            center_data = []
-            for c in chunks:
-                # grab this center and neighbors
-                center = centers[c]
-                center_neighbors = neighbors[c]
-                # create a database object with this data
-                ds = Dataset(data_2d[:, center_neighbors],
-                             descriptors={'center': center},
-                             obs_descriptors={'events': events},
-                             channel_descriptors={'voxels': center_neighbors})
-                center_data.append(ds)
-
-            RDM_corr = calc_rdm(center_data, method=method,
-                                descriptor='events')
-            RDM[chunks, :] = RDM_corr.dissimilarities
-    else:
-        center_data = []
-        for c in range(n_centers):
-            # grab this center and neighbors
-            center = centers[c]
-            nb = neighbors[c]
-            # create a database object with this data
-            ds = Dataset(data_2d[:, nb],
-                         descriptors={'center': c},
-                         obs_descriptors={'events': events},
-                         channel_descriptors={'voxels': nb})
-            center_data.append(ds)
-        # calculate RDMs for each database object
-        RDM = calc_rdm(center_data, method=method,
-                       descriptor='events').dissimilarities
-
-    SL_rdms = RDMs(RDM,
-                   rdm_descriptors={'voxel_index': centers},
-                   dissimilarity_measure=method)
-
-    return SL_rdms
-
-
-def evaluate_models_searchlight(sl_RDM, models, eval_function, method='corr', theta=None, n_jobs=1):
-    """evaluates each searchlighth with the given model/models
-
-    Args:
-
-        sl_RDM ([rsatoolbox.rdm.RDMs]): RDMs object
-        as computed by rsatoolbox.util.searchlight.get_searchlight_RDMs
-
-        models ([rsatoolbox.model]: models to evaluate - can also be list of models
-
-        eval_function (rsatoolbox.inference evaluation-function): [description]
-
-        method (str, optional): see rsatoolbox.rdm.compare for specifics. Defaults to 'corr'.
-
-        n_jobs (int, optional): how many jobs to run. Defaults to 1.
-
-    Returns:
-
-        list: list of with the model evaluation for each searchlight center
-    """
-
-    results = Parallel(n_jobs=n_jobs)(
-        delayed(eval_function)(
-            models, x, method=method, theta=theta) for x in tqdm(
-            sl_RDM, desc='Evaluating models for each searchlight'))
-
-    return results
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+This code was initially inspired by the following :
+https://github.com/machow/pysearchlight
+
+@author: Daniel Lindh
+"""
+import numpy as np
+from scipy.spatial.distance import cdist
+from tqdm import tqdm
+from joblib import Parallel, delayed
+from rsatoolbox.data.dataset import Dataset
+from rsatoolbox.rdm.calc import calc_rdm
+from rsatoolbox.rdm import RDMs
+
+
+def _get_searchlight_neighbors(mask, center, radius=3):
+    """Return indices for searchlight where distance
+        between a voxel and their center < radius (in voxels)
+
+    Args:
+        center (index):  point around which to make searchlight sphere
+
+    Returns:
+        list: the list of volume indices that respect the
+                searchlight radius for the input center.
+    """
+    center = np.array(center)
+    mask_shape = mask.shape
+    cx, cy, cz = np.array(center)
+    x = np.arange(mask_shape[0])
+    y = np.arange(mask_shape[1])
+    z = np.arange(mask_shape[2])
+
+    # First mask the obvious points
+    # - may actually slow down your calculation depending.
+    x = x[abs(x - cx) < radius]
+    y = y[abs(y - cy) < radius]
+    z = z[abs(z - cz) < radius]
+
+    # Generate grid of points
+    X, Y, Z = np.meshgrid(x, y, z)
+    data = np.vstack((X.ravel(), Y.ravel(), Z.ravel())).T
+    distance = cdist(data, center.reshape(1, -1), 'euclidean').ravel()
+
+    return tuple(data[distance < radius].T.tolist())
+
+
+def get_volume_searchlight(mask, radius=2, threshold=1.0):
+    """
+    Searches through the non-zero voxels of the mask, selects centers where
+    proportion of sphere voxels >= self.threshold.
+
+    Args:
+
+        mask ([numpy array]): binary brain mask
+
+        radius (int, optional): the radius of each searchlight, defined in voxels.
+        Defaults to 2.
+
+        threshold (float, optional): Threshold of the proportion of voxels that need to
+        be inside the brain mask in order for it to be
+        considered a good searchlight center.
+        Values go between 0.0 - 1.0 where 1.0 means that
+        100% of the voxels need to be inside
+        the brain mask.
+        Defaults to 1.0.
+
+    Returns:
+        numpy array: array of centers of size n_centers x 3
+
+        list: list of lists with neighbors - the length of the list will correspond to:
+        n_centers x 3 x n_neighbors
+    """
+
+    mask = np.array(mask)
+    assert mask.ndim == 3, "Mask needs to be a 3-dimensional numpy array"
+
+    centers = list(zip(*np.nonzero(mask)))
+    good_centers = []
+    good_neighbors = []
+
+    for center in tqdm(centers, desc='Finding searchlights...'):
+        neighbors = _get_searchlight_neighbors(mask, center, radius)
+        if mask[neighbors].mean() >= threshold:
+            good_centers.append(center)
+            good_neighbors.append(neighbors)
+
+    good_centers = np.array(good_centers)
+    assert good_centers.shape[0] == len(good_neighbors),\
+        "number of centers and sets of neighbors do not match"
+    print(f'Found {len(good_neighbors)} searchlights')
+
+    # turn the 3-dim coordinates to array coordinates
+    centers = np.ravel_multi_index(good_centers.T, mask.shape)
+    neighbors = [np.ravel_multi_index(n, mask.shape) for n in good_neighbors]
+
+    return centers, neighbors
+
+
+def get_searchlight_RDMs(data_2d, centers, neighbors, events,
+                         method='correlation', verbose=True):
+    """Iterates over all the searchlight centers and calculates the RDM
+
+    Args:
+
+        data_2d (2D numpy array): brain data,
+        shape n_observations x n_channels (i.e. voxels/vertices)
+
+        centers (1D numpy array): center indices for all searchlights as provided
+        by rsatoolbox.util.searchlight.get_volume_searchlight
+
+        neighbors (list): list of lists with neighbor voxel indices for all searchlights
+        as provided by rsatoolbox.util.searchlight.get_volume_searchlight
+
+        events (1D numpy array): 1D array of length n_observations
+
+        method (str, optional): distance metric,
+        see rsatoolbox.rdm.calc for options. Defaults to 'correlation'.
+
+        verbose (bool, optional): Defaults to True.
+
+    Returns:
+        RDM [rsatoolbox.rdm.RDMs]: RDMs object with the RDM for each searchlight
+                              the RDM.rdm_descriptors['voxel_index']
+                              describes the center voxel index each RDM is associated with
+    """
+
+    data_2d, centers = np.array(data_2d), np.array(centers)
+    n_centers = centers.shape[0]
+
+    # For memory reasons, we chunk the data if we have more than 1000 RDMs
+    if n_centers > 1000:
+        # we can't run all centers at once, that will take too much memory
+        # so lets to some chunking
+        chunked_center = np.split(np.arange(n_centers),
+                                  np.linspace(0, n_centers,
+                                              101, dtype=int)[1:-1])
+
+        # loop over chunks
+        n_conds = len(np.unique(events))
+        RDM = np.zeros((n_centers, n_conds * (n_conds - 1) // 2))
+        for chunks in tqdm(chunked_center, desc='Calculating RDMs...'):
+            center_data = []
+            for c in chunks:
+                # grab this center and neighbors
+                center = centers[c]
+                center_neighbors = neighbors[c]
+                # create a database object with this data
+                ds = Dataset(data_2d[:, center_neighbors],
+                             descriptors={'center': center},
+                             obs_descriptors={'events': events},
+                             channel_descriptors={'voxels': center_neighbors})
+                center_data.append(ds)
+
+            RDM_corr = calc_rdm(center_data, method=method,
+                                descriptor='events')
+            RDM[chunks, :] = RDM_corr.dissimilarities
+    else:
+        center_data = []
+        for c in range(n_centers):
+            # grab this center and neighbors
+            center = centers[c]
+            nb = neighbors[c]
+            # create a database object with this data
+            ds = Dataset(data_2d[:, nb],
+                         descriptors={'center': c},
+                         obs_descriptors={'events': events},
+                         channel_descriptors={'voxels': nb})
+            center_data.append(ds)
+        # calculate RDMs for each database object
+        RDM = calc_rdm(center_data, method=method,
+                       descriptor='events').dissimilarities
+
+    SL_rdms = RDMs(RDM,
+                   rdm_descriptors={'voxel_index': centers},
+                   dissimilarity_measure=method)
+
+    return SL_rdms
+
+
+def evaluate_models_searchlight(sl_RDM, models, eval_function, method='corr', theta=None, n_jobs=1):
+    """evaluates each searchlighth with the given model/models
+
+    Args:
+
+        sl_RDM ([rsatoolbox.rdm.RDMs]): RDMs object
+        as computed by rsatoolbox.util.searchlight.get_searchlight_RDMs
+
+        models ([rsatoolbox.model]: models to evaluate - can also be list of models
+
+        eval_function (rsatoolbox.inference evaluation-function): [description]
+
+        method (str, optional): see rsatoolbox.rdm.compare for specifics. Defaults to 'corr'.
+
+        n_jobs (int, optional): how many jobs to run. Defaults to 1.
+
+    Returns:
+
+        list: list of with the model evaluation for each searchlight center
+    """
+
+    results = Parallel(n_jobs=n_jobs)(
+        delayed(eval_function)(
+            models, x, method=method, theta=theta) for x in tqdm(
+            sl_RDM, desc='Evaluating models for each searchlight'))
+
+    return results
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/util/vis_utils.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/util/vis_utils.py`

 * *Ordering differences only*

 * *Files 14% similar despite different names*

```diff
@@ -1,515 +1,515 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""Collection of helper methods for vis module
-
-+ Weighted_MDS:       an MDS class that incorporates weighting
-+ weight_to_matrices: batch squareform() to weight matrices
-
-@author: baihan
-
-Notice:
-
-The functions of MDS in this module are modified from
-the Python package scikit-learn, originally written by
-Nelle Varoquaux <nelle.varoquaux@gmail.com> under BSD
-licence <https://en.wikipedia.org/wiki/BSD_licenses>.
-We modified the MDS function to include an additional
-functionality of having an important matrix as an input.
-"""
-
-import warnings
-import numpy as np
-from joblib import Parallel, delayed, effective_n_jobs
-from sklearn.base import BaseEstimator
-from sklearn.metrics import euclidean_distances
-from sklearn.utils import check_random_state, check_array, check_symmetric
-from sklearn.isotonic import IsotonicRegression
-from scipy.spatial.distance import squareform
-from rsatoolbox.util.rdm_utils import _get_n_from_reduced_vectors
-
-
-def weight_to_matrices(x):
-    """converts a *stack* of weights in vector or matrix form into matrix form
-
-    Args:
-        **x** (np.ndarray): stack of weight matrices or weight vectors
-
-    Returns:
-        tuple: **v** (np.ndarray): 3D, matrix form of the stack of weight matrices
-    """
-    if x.ndim == 2:
-        v = x
-        n_rdm = x.shape[0]
-        n_cond = _get_n_from_reduced_vectors(x)
-        m = np.ndarray((n_rdm, n_cond, n_cond))
-        for idx in np.arange(n_rdm):
-            m[idx, :, :] = squareform(v[idx, :])
-    elif x.ndim == 3:
-        m = x
-    return m
-
-
-def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
-                   max_iter=300, verbose=0, eps=1e-3, random_state=None,
-                   weight=None):
-    """Computes multidimensional scaling using SMACOF algorithm.
-
-    Parameters
-    ----------
-    dissimilarities : ndarray of shape (n_samples, n_samples)
-        Pairwise dissimilarities between the points. Must be symmetric.
-
-    metric : bool, default=True
-        Compute metric or nonmetric SMACOF algorithm.
-
-    n_components : int, default=2
-        Number of dimensions in which to immerse the dissimilarities. If an
-        ``init`` array is provided, this option is overridden and the shape of
-        ``init`` is used to determine the dimensionality of the embedding
-        space.
-
-    init : ndarray of shape (n_samples, n_components), default=None
-        Starting configuration of the embedding to initialize the algorithm. By
-        default, the algorithm is initialized with a randomly chosen array.
-
-    max_iter : int, default=300
-        Maximum number of iterations of the SMACOF algorithm for a single run.
-
-    verbose : int, default=0
-        Level of verbosity.
-
-    eps : float, default=1e-3
-        Relative tolerance with respect to stress at which to declare
-        convergence.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines the random number generator used to initialize the centers.
-        Pass an int for reproducible results across multiple function calls.
-        See :term: `Glossary <random_state>`.
-
-    weight : ndarray of shape (n_samples, n_samples), default=None
-        symmetric weighting matrix of similarities.
-        In default, all weights are 1.
-
-    Returns
-    -------
-    X : ndarray of shape (n_samples, n_components)
-        Coordinates of the points in a ``n_components``-space.
-
-    stress : float
-        The final value of the stress (sum of squared distance of the
-        disparities and the distances for all constrained points).
-
-    n_iter : int
-        The number of iterations corresponding to the best stress.
-    """
-    dissimilarities = check_symmetric(dissimilarities, raise_exception=True)
-
-    n_samples = dissimilarities.shape[0]
-    random_state = check_random_state(random_state)
-
-    sim_flat = ((1 - np.tri(n_samples)) * dissimilarities).ravel()
-    sim_flat_w = sim_flat[sim_flat != 0]
-    if init is None:
-        # Randomly choose initial configuration
-        X = random_state.rand(n_samples * n_components)
-        X = X.reshape((n_samples, n_components))
-    else:
-        # overrides the parameter p
-        n_components = init.shape[1]
-        if n_samples != init.shape[0]:
-            raise ValueError("init matrix should be of shape (%d, %d)" %
-                             (n_samples, n_components))
-        X = init
-
-    old_stress = None
-    ir = IsotonicRegression()
-    for it in range(max_iter):
-        # Compute distance and monotonic regression
-        dis = euclidean_distances(X)
-
-        if metric:
-            disparities = dissimilarities
-        else:
-            dis_flat = dis.ravel()
-            # dissimilarities with 0 are considered as missing values
-            dis_flat_w = dis_flat[sim_flat != 0]
-
-            # Compute the disparities using a monotonic regression
-            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
-            disparities = dis_flat.copy()
-            disparities[sim_flat != 0] = disparities_flat
-            disparities = disparities.reshape((n_samples, n_samples))
-            disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) /
-                                   (disparities ** 2).sum())
-
-        # Compute stress
-        stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2
-
-        # Update X using the Guttman transform
-        dis[dis == 0] = 1e-5
-        if weight is None:
-            ratio = disparities / dis
-            B = - ratio
-            B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
-            X = 1. / n_samples * np.dot(B, X)
-        else:
-            ratio = weight * disparities / dis
-            B = - ratio
-            B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
-            V = np.zeros((n_samples, n_samples))
-            for nn in range(n_samples):
-                for mm in range(nn, n_samples):
-                    v = np.zeros((n_samples, 1))
-                    v[nn], v[mm] = 1, -1
-                    V += weight[nn, mm] * np.dot(v, v.T)
-            X = np.dot(np.linalg.pinv(V), np.dot(B, X))
-
-        dis = np.sqrt((X ** 2).sum(axis=1)).sum()
-        if verbose >= 2:
-            print('it: %d, stress %s' % (it, stress))
-        if old_stress is not None:
-            if(old_stress - stress / dis) < eps:
-                if verbose:
-                    print('breaking at iteration %d with stress %s' % (it,
-                                                                       stress))
-                break
-        old_stress = stress / dis
-
-    return X, stress, it + 1
-
-
-def smacof(dissimilarities, *, metric=True, n_components=2, init=None,
-           n_init=8, n_jobs=None, max_iter=300, verbose=0, eps=1e-3,
-           random_state=None, return_n_iter=False, weight=None):
-    """Computes multidimensional scaling using the SMACOF algorithm.
-
-    The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a
-    multidimensional scaling algorithm which minimizes an objective function
-    (the *stress*) using a majorization technique. Stress majorization, also
-    known as the Guttman Transform, guarantees a monotone convergence of
-    stress, and is more powerful than traditional techniques such as gradient
-    descent.
-
-    The SMACOF algorithm for metric MDS can summarized by the following steps:
-
-    1. Set an initial start configuration, randomly or not.
-    2. Compute the stress
-    3. Compute the Guttman Transform
-    4. Iterate 2 and 3 until convergence.
-
-    The nonmetric algorithm adds a monotonic regression step before computing
-    the stress.
-
-    Parameters
-    ----------
-    dissimilarities : ndarray of shape (n_samples, n_samples)
-        Pairwise dissimilarities between the points. Must be symmetric.
-
-    metric : bool, default=True
-        Compute metric or nonmetric SMACOF algorithm.
-
-    n_components : int, default=2
-        Number of dimensions in which to immerse the dissimilarities. If an
-        ``init`` array is provided, this option is overridden and the shape of
-        ``init`` is used to determine the dimensionality of the embedding
-        space.
-
-    init : ndarray of shape (n_samples, n_components), default=None
-        Starting configuration of the embedding to initialize the algorithm. By
-        default, the algorithm is initialized with a randomly chosen array.
-
-    n_init : int, default=8
-        Number of times the SMACOF algorithm will be run with different
-        initializations. The final results will be the best output of the runs,
-        determined by the run with the smallest final stress. If ``init`` is
-        provided, this option is overridden and a single run is performed.
-
-    n_jobs : int, default=None
-        The number of jobs to use for the computation. If multiple
-        initializations are used (``n_init``), each run of the algorithm is
-        computed in parallel.
-
-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
-        ``-1`` means using all processors.
-
-    max_iter : int, default=300
-        Maximum number of iterations of the SMACOF algorithm for a single run.
-
-    verbose : int, default=0
-        Level of verbosity.
-
-    eps : float, default=1e-3
-        Relative tolerance with respect to stress at which to declare
-        convergence.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines the random number generator used to initialize the centers.
-        Pass an int for reproducible results across multiple function calls.
-        See :term: `Glossary <random_state>`.
-
-    return_n_iter : bool, default=False
-        Whether or not to return the number of iterations.
-
-    weight : ndarray of shape (n_samples, n_samples), default=None
-        symmetric weighting matrix of similarities.
-        In default, all weights are 1.
-
-    Returns
-    -------
-    X : ndarray of shape (n_samples, n_components)
-        Coordinates of the points in a ``n_components``-space.
-
-    stress : float
-        The final value of the stress (sum of squared distance of the
-        disparities and the distances for all constrained points).
-
-    n_iter : int
-        The number of iterations corresponding to the best stress. Returned
-        only if ``return_n_iter`` is set to ``True``.
-
-    Notes
-    -----
-    "Modern Multidimensional Scaling - Theory and Applications" Borg, I.;
-    Groenen P. Springer Series in Statistics (1997)
-
-    "Nonmetric multidimensional scaling: a numerical method" Kruskal, J.
-    Psychometrika, 29 (1964)
-
-    "Multidimensional scaling by optimizing goodness of fit to a nonmetric
-    hypothesis" Kruskal, J. Psychometrika, 29, (1964)
-    """
-
-    dissimilarities = check_array(dissimilarities)
-    random_state = check_random_state(random_state)
-
-    if hasattr(init, '__array__'):
-        init = np.asarray(init).copy()
-        if not n_init == 1:
-            warnings.warn(
-                'Explicit initial positions passed: '
-                'performing only one init of the MDS instead of %d'
-                % n_init)
-            n_init = 1
-
-    best_pos, best_stress = None, None
-
-    if effective_n_jobs(n_jobs) == 1:
-        for it in range(n_init):
-            pos, stress, n_iter_ = _smacof_single(
-                dissimilarities, metric=metric,
-                n_components=n_components, init=init,
-                max_iter=max_iter, verbose=verbose,
-                eps=eps, random_state=random_state,
-                weight=weight)
-            if best_stress is None or stress < best_stress:
-                best_stress = stress
-                best_pos = pos.copy()
-                best_iter = n_iter_
-    else:
-        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
-        results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(
-            delayed(_smacof_single)(
-                dissimilarities, metric=metric, n_components=n_components,
-                init=init, max_iter=max_iter, verbose=verbose, eps=eps,
-                random_state=seed, weight=weight)
-            for seed in seeds)
-        positions, stress, n_iters = zip(*results)
-        best = np.argmin(stress)
-        best_stress = stress[best]
-        best_pos = positions[best]
-        best_iter = n_iters[best]
-
-    if return_n_iter:
-        return best_pos, best_stress, best_iter
-    else:
-        return best_pos, best_stress
-
-
-class Weighted_MDS(BaseEstimator):
-    """Multidimensional scaling with weighting options.
-
-    Parameters
-    ----------
-    n_components : int, default=2
-        Number of dimensions in which to immerse the dissimilarities.
-
-    metric : bool, default=True
-        If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.
-
-    n_init : int, default=4
-        Number of times the SMACOF algorithm will be run with different
-        initializations. The final results will be the best output of the runs,
-        determined by the run with the smallest final stress.
-
-    max_iter : int, default=300
-        Maximum number of iterations of the SMACOF algorithm for a single run.
-
-    verbose : int, default=0
-        Level of verbosity.
-
-    eps : float, default=1e-3
-        Relative tolerance with respect to stress at which to declare
-        convergence.
-
-    n_jobs : int, default=None
-        The number of jobs to use for the computation. If multiple
-        initializations are used (``n_init``), each run of the algorithm is
-        computed in parallel.
-
-        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
-        ``-1`` means using all processors.
-
-    random_state : int, RandomState instance or None, default=None
-        Determines the random number generator used to initialize the centers.
-        Pass an int for reproducible results across multiple function calls.
-        See :term: `Glossary <random_state>`.
-
-    dissimilarity : {'euclidean', 'precomputed'}, default='euclidean'
-        Dissimilarity measure to use:
-
-        - 'euclidean':
-            Pairwise Euclidean distances between points in the dataset.
-
-        - 'precomputed':
-            Pre-computed dissimilarities are passed directly to ``fit`` and
-            ``fit_transform``.
-
-    Attributes
-    ----------
-    embedding_ : ndarray of shape (n_samples, n_components)
-        Stores the position of the dataset in the embedding space.
-
-    stress_ : float
-        The final value of the stress (sum of squared distance of the
-        disparities and the distances for all constrained points).
-
-    dissimilarity_matrix_ : ndarray of shape (n_samples, n_samples)
-        Pairwise dissimilarities between the points. Symmetric matrix that:
-
-        - either uses a custom dissimilarity matrix by setting `dissimilarity`
-          to 'precomputed';
-        - or constructs a dissimilarity matrix from data using
-          Euclidean distances.
-
-    n_iter_ : int
-        The number of iterations corresponding to the best stress.
-
-    Examples
-    --------
-    >>> from sklearn.datasets import load_digits
-    >>> from sklearn.manifold import MDS
-    >>> X, _ = load_digits(return_X_y=True)
-    >>> X.shape
-    (1797, 64)
-    >>> embedding = MDS(n_components=2)
-    >>> X_transformed = embedding.fit_transform(X[:100])
-    >>> X_transformed.shape
-    (100, 2)
-
-    References
-    ----------
-    "Modern Multidimensional Scaling - Theory and Applications" Borg, I.;
-    Groenen P. Springer Series in Statistics (1997)
-
-    "Nonmetric multidimensional scaling: a numerical method" Kruskal, J.
-    Psychometrika, 29 (1964)
-
-    "Multidimensional scaling by optimizing goodness of fit to a nonmetric
-    hypothesis" Kruskal, J. Psychometrika, 29, (1964)
-
-    """
-
-    def __init__(self, n_components=2, *, metric=True, n_init=4,
-                 max_iter=300, verbose=0, eps=1e-3, n_jobs=None,
-                 random_state=None, dissimilarity="euclidean",
-                 normalized_stress='auto'):
-        self.n_components = n_components
-        self.dissimilarity = dissimilarity
-        self.metric = metric
-        self.n_init = n_init
-        self.max_iter = max_iter
-        self.eps = eps
-        self.verbose = verbose
-        self.n_jobs = n_jobs
-        self.random_state = random_state
-        self.dissimilarity_matrix_ = None
-        self.embedding_ = None
-        self.stress_ = None
-        self.n_iter_ = None
-        # not in use, declared for consistency with sklearn:
-        self.normalized_stress = normalized_stress
-
-    @property
-    def _pairwise(self):
-        return self.dissimilarity == "precomputed"
-
-    def fit(self, X, y=None, init=None, weight=None):
-        """
-        Computes the position of the points in the embedding space.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features) or \
-                (n_samples, n_samples)
-            Input data. If ``dissimilarity=='precomputed'``, the input should
-            be the dissimilarity matrix.
-
-        y : Ignored
-
-        init : ndarray of shape (n_samples,), default=None
-            Starting configuration of the embedding to initialize the SMACOF
-            algorithm. By default, the algorithm is initialized with a randomly
-            chosen array.
-
-        weight : ndarray of shape (n_samples, n_samples), default=None
-            symmetric weighting matrix of similarities.
-            In default, all weights are 1.
-        """
-        self.fit_transform(X, init=init, weight=weight)
-        return self
-
-    def fit_transform(self, X, y=None, init=None, weight=None):
-        """
-        Fit the data from X, and returns the embedded coordinates.
-
-        Parameters
-        ----------
-        X : array-like of shape (n_samples, n_features) or \
-                (n_samples, n_samples)
-            Input data. If ``dissimilarity=='precomputed'``, the input should
-            be the dissimilarity matrix.
-
-        y : Ignored
-
-        init : ndarray of shape (n_samples,), default=None
-            Starting configuration of the embedding to initialize the SMACOF
-            algorithm. By default, the algorithm is initialized with a randomly
-            chosen array.
-        weight : ndarray of shape (n_samples, n_samples), default=None
-            symmetric weighting matrix of similarities.
-            In default, all weights are 1.
-        """
-        X = self._validate_data(X)
-        if X.shape[0] == X.shape[1] and self.dissimilarity != "precomputed":
-            warnings.warn("The MDS API has changed. ``fit`` now constructs an"
-                          " dissimilarity matrix from data. To use a custom "
-                          "dissimilarity matrix, set "
-                          "``dissimilarity='precomputed'``.")
-
-        if self.dissimilarity == "precomputed":
-            self.dissimilarity_matrix_ = X
-        elif self.dissimilarity == "euclidean":
-            self.dissimilarity_matrix_ = euclidean_distances(X)
-        else:
-            raise ValueError("Proximity must be 'precomputed' or 'euclidean'."
-                             " Got %s instead" % str(self.dissimilarity))
-
-        self.embedding_, self.stress_, self.n_iter_ = smacof(
-            self.dissimilarity_matrix_, metric=self.metric,
-            n_components=self.n_components, init=init, n_init=self.n_init,
-            n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose,
-            eps=self.eps, random_state=self.random_state,
-            return_n_iter=True, weight=weight)
-
-        return self.embedding_
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""Collection of helper methods for vis module
+
++ Weighted_MDS:       an MDS class that incorporates weighting
++ weight_to_matrices: batch squareform() to weight matrices
+
+@author: baihan
+
+Notice:
+
+The functions of MDS in this module are modified from
+the Python package scikit-learn, originally written by
+Nelle Varoquaux <nelle.varoquaux@gmail.com> under BSD
+licence <https://en.wikipedia.org/wiki/BSD_licenses>.
+We modified the MDS function to include an additional
+functionality of having an important matrix as an input.
+"""
+
+import warnings
+import numpy as np
+from joblib import Parallel, delayed, effective_n_jobs
+from sklearn.base import BaseEstimator
+from sklearn.metrics import euclidean_distances
+from sklearn.utils import check_random_state, check_array, check_symmetric
+from sklearn.isotonic import IsotonicRegression
+from scipy.spatial.distance import squareform
+from rsatoolbox.util.rdm_utils import _get_n_from_reduced_vectors
+
+
+def weight_to_matrices(x):
+    """converts a *stack* of weights in vector or matrix form into matrix form
+
+    Args:
+        **x** (np.ndarray): stack of weight matrices or weight vectors
+
+    Returns:
+        tuple: **v** (np.ndarray): 3D, matrix form of the stack of weight matrices
+    """
+    if x.ndim == 2:
+        v = x
+        n_rdm = x.shape[0]
+        n_cond = _get_n_from_reduced_vectors(x)
+        m = np.ndarray((n_rdm, n_cond, n_cond))
+        for idx in np.arange(n_rdm):
+            m[idx, :, :] = squareform(v[idx, :])
+    elif x.ndim == 3:
+        m = x
+    return m
+
+
+def _smacof_single(dissimilarities, metric=True, n_components=2, init=None,
+                   max_iter=300, verbose=0, eps=1e-3, random_state=None,
+                   weight=None):
+    """Computes multidimensional scaling using SMACOF algorithm.
+
+    Parameters
+    ----------
+    dissimilarities : ndarray of shape (n_samples, n_samples)
+        Pairwise dissimilarities between the points. Must be symmetric.
+
+    metric : bool, default=True
+        Compute metric or nonmetric SMACOF algorithm.
+
+    n_components : int, default=2
+        Number of dimensions in which to immerse the dissimilarities. If an
+        ``init`` array is provided, this option is overridden and the shape of
+        ``init`` is used to determine the dimensionality of the embedding
+        space.
+
+    init : ndarray of shape (n_samples, n_components), default=None
+        Starting configuration of the embedding to initialize the algorithm. By
+        default, the algorithm is initialized with a randomly chosen array.
+
+    max_iter : int, default=300
+        Maximum number of iterations of the SMACOF algorithm for a single run.
+
+    verbose : int, default=0
+        Level of verbosity.
+
+    eps : float, default=1e-3
+        Relative tolerance with respect to stress at which to declare
+        convergence.
+
+    random_state : int, RandomState instance or None, default=None
+        Determines the random number generator used to initialize the centers.
+        Pass an int for reproducible results across multiple function calls.
+        See :term: `Glossary <random_state>`.
+
+    weight : ndarray of shape (n_samples, n_samples), default=None
+        symmetric weighting matrix of similarities.
+        In default, all weights are 1.
+
+    Returns
+    -------
+    X : ndarray of shape (n_samples, n_components)
+        Coordinates of the points in a ``n_components``-space.
+
+    stress : float
+        The final value of the stress (sum of squared distance of the
+        disparities and the distances for all constrained points).
+
+    n_iter : int
+        The number of iterations corresponding to the best stress.
+    """
+    dissimilarities = check_symmetric(dissimilarities, raise_exception=True)
+
+    n_samples = dissimilarities.shape[0]
+    random_state = check_random_state(random_state)
+
+    sim_flat = ((1 - np.tri(n_samples)) * dissimilarities).ravel()
+    sim_flat_w = sim_flat[sim_flat != 0]
+    if init is None:
+        # Randomly choose initial configuration
+        X = random_state.rand(n_samples * n_components)
+        X = X.reshape((n_samples, n_components))
+    else:
+        # overrides the parameter p
+        n_components = init.shape[1]
+        if n_samples != init.shape[0]:
+            raise ValueError("init matrix should be of shape (%d, %d)" %
+                             (n_samples, n_components))
+        X = init
+
+    old_stress = None
+    ir = IsotonicRegression()
+    for it in range(max_iter):
+        # Compute distance and monotonic regression
+        dis = euclidean_distances(X)
+
+        if metric:
+            disparities = dissimilarities
+        else:
+            dis_flat = dis.ravel()
+            # dissimilarities with 0 are considered as missing values
+            dis_flat_w = dis_flat[sim_flat != 0]
+
+            # Compute the disparities using a monotonic regression
+            disparities_flat = ir.fit_transform(sim_flat_w, dis_flat_w)
+            disparities = dis_flat.copy()
+            disparities[sim_flat != 0] = disparities_flat
+            disparities = disparities.reshape((n_samples, n_samples))
+            disparities *= np.sqrt((n_samples * (n_samples - 1) / 2) /
+                                   (disparities ** 2).sum())
+
+        # Compute stress
+        stress = ((dis.ravel() - disparities.ravel()) ** 2).sum() / 2
+
+        # Update X using the Guttman transform
+        dis[dis == 0] = 1e-5
+        if weight is None:
+            ratio = disparities / dis
+            B = - ratio
+            B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
+            X = 1. / n_samples * np.dot(B, X)
+        else:
+            ratio = weight * disparities / dis
+            B = - ratio
+            B[np.arange(len(B)), np.arange(len(B))] += ratio.sum(axis=1)
+            V = np.zeros((n_samples, n_samples))
+            for nn in range(n_samples):
+                for mm in range(nn, n_samples):
+                    v = np.zeros((n_samples, 1))
+                    v[nn], v[mm] = 1, -1
+                    V += weight[nn, mm] * np.dot(v, v.T)
+            X = np.dot(np.linalg.pinv(V), np.dot(B, X))
+
+        dis = np.sqrt((X ** 2).sum(axis=1)).sum()
+        if verbose >= 2:
+            print('it: %d, stress %s' % (it, stress))
+        if old_stress is not None:
+            if(old_stress - stress / dis) < eps:
+                if verbose:
+                    print('breaking at iteration %d with stress %s' % (it,
+                                                                       stress))
+                break
+        old_stress = stress / dis
+
+    return X, stress, it + 1
+
+
+def smacof(dissimilarities, *, metric=True, n_components=2, init=None,
+           n_init=8, n_jobs=None, max_iter=300, verbose=0, eps=1e-3,
+           random_state=None, return_n_iter=False, weight=None):
+    """Computes multidimensional scaling using the SMACOF algorithm.
+
+    The SMACOF (Scaling by MAjorizing a COmplicated Function) algorithm is a
+    multidimensional scaling algorithm which minimizes an objective function
+    (the *stress*) using a majorization technique. Stress majorization, also
+    known as the Guttman Transform, guarantees a monotone convergence of
+    stress, and is more powerful than traditional techniques such as gradient
+    descent.
+
+    The SMACOF algorithm for metric MDS can summarized by the following steps:
+
+    1. Set an initial start configuration, randomly or not.
+    2. Compute the stress
+    3. Compute the Guttman Transform
+    4. Iterate 2 and 3 until convergence.
+
+    The nonmetric algorithm adds a monotonic regression step before computing
+    the stress.
+
+    Parameters
+    ----------
+    dissimilarities : ndarray of shape (n_samples, n_samples)
+        Pairwise dissimilarities between the points. Must be symmetric.
+
+    metric : bool, default=True
+        Compute metric or nonmetric SMACOF algorithm.
+
+    n_components : int, default=2
+        Number of dimensions in which to immerse the dissimilarities. If an
+        ``init`` array is provided, this option is overridden and the shape of
+        ``init`` is used to determine the dimensionality of the embedding
+        space.
+
+    init : ndarray of shape (n_samples, n_components), default=None
+        Starting configuration of the embedding to initialize the algorithm. By
+        default, the algorithm is initialized with a randomly chosen array.
+
+    n_init : int, default=8
+        Number of times the SMACOF algorithm will be run with different
+        initializations. The final results will be the best output of the runs,
+        determined by the run with the smallest final stress. If ``init`` is
+        provided, this option is overridden and a single run is performed.
+
+    n_jobs : int, default=None
+        The number of jobs to use for the computation. If multiple
+        initializations are used (``n_init``), each run of the algorithm is
+        computed in parallel.
+
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors.
+
+    max_iter : int, default=300
+        Maximum number of iterations of the SMACOF algorithm for a single run.
+
+    verbose : int, default=0
+        Level of verbosity.
+
+    eps : float, default=1e-3
+        Relative tolerance with respect to stress at which to declare
+        convergence.
+
+    random_state : int, RandomState instance or None, default=None
+        Determines the random number generator used to initialize the centers.
+        Pass an int for reproducible results across multiple function calls.
+        See :term: `Glossary <random_state>`.
+
+    return_n_iter : bool, default=False
+        Whether or not to return the number of iterations.
+
+    weight : ndarray of shape (n_samples, n_samples), default=None
+        symmetric weighting matrix of similarities.
+        In default, all weights are 1.
+
+    Returns
+    -------
+    X : ndarray of shape (n_samples, n_components)
+        Coordinates of the points in a ``n_components``-space.
+
+    stress : float
+        The final value of the stress (sum of squared distance of the
+        disparities and the distances for all constrained points).
+
+    n_iter : int
+        The number of iterations corresponding to the best stress. Returned
+        only if ``return_n_iter`` is set to ``True``.
+
+    Notes
+    -----
+    "Modern Multidimensional Scaling - Theory and Applications" Borg, I.;
+    Groenen P. Springer Series in Statistics (1997)
+
+    "Nonmetric multidimensional scaling: a numerical method" Kruskal, J.
+    Psychometrika, 29 (1964)
+
+    "Multidimensional scaling by optimizing goodness of fit to a nonmetric
+    hypothesis" Kruskal, J. Psychometrika, 29, (1964)
+    """
+
+    dissimilarities = check_array(dissimilarities)
+    random_state = check_random_state(random_state)
+
+    if hasattr(init, '__array__'):
+        init = np.asarray(init).copy()
+        if not n_init == 1:
+            warnings.warn(
+                'Explicit initial positions passed: '
+                'performing only one init of the MDS instead of %d'
+                % n_init)
+            n_init = 1
+
+    best_pos, best_stress = None, None
+
+    if effective_n_jobs(n_jobs) == 1:
+        for it in range(n_init):
+            pos, stress, n_iter_ = _smacof_single(
+                dissimilarities, metric=metric,
+                n_components=n_components, init=init,
+                max_iter=max_iter, verbose=verbose,
+                eps=eps, random_state=random_state,
+                weight=weight)
+            if best_stress is None or stress < best_stress:
+                best_stress = stress
+                best_pos = pos.copy()
+                best_iter = n_iter_
+    else:
+        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)
+        results = Parallel(n_jobs=n_jobs, verbose=max(verbose - 1, 0))(
+            delayed(_smacof_single)(
+                dissimilarities, metric=metric, n_components=n_components,
+                init=init, max_iter=max_iter, verbose=verbose, eps=eps,
+                random_state=seed, weight=weight)
+            for seed in seeds)
+        positions, stress, n_iters = zip(*results)
+        best = np.argmin(stress)
+        best_stress = stress[best]
+        best_pos = positions[best]
+        best_iter = n_iters[best]
+
+    if return_n_iter:
+        return best_pos, best_stress, best_iter
+    else:
+        return best_pos, best_stress
+
+
+class Weighted_MDS(BaseEstimator):
+    """Multidimensional scaling with weighting options.
+
+    Parameters
+    ----------
+    n_components : int, default=2
+        Number of dimensions in which to immerse the dissimilarities.
+
+    metric : bool, default=True
+        If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.
+
+    n_init : int, default=4
+        Number of times the SMACOF algorithm will be run with different
+        initializations. The final results will be the best output of the runs,
+        determined by the run with the smallest final stress.
+
+    max_iter : int, default=300
+        Maximum number of iterations of the SMACOF algorithm for a single run.
+
+    verbose : int, default=0
+        Level of verbosity.
+
+    eps : float, default=1e-3
+        Relative tolerance with respect to stress at which to declare
+        convergence.
+
+    n_jobs : int, default=None
+        The number of jobs to use for the computation. If multiple
+        initializations are used (``n_init``), each run of the algorithm is
+        computed in parallel.
+
+        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
+        ``-1`` means using all processors.
+
+    random_state : int, RandomState instance or None, default=None
+        Determines the random number generator used to initialize the centers.
+        Pass an int for reproducible results across multiple function calls.
+        See :term: `Glossary <random_state>`.
+
+    dissimilarity : {'euclidean', 'precomputed'}, default='euclidean'
+        Dissimilarity measure to use:
+
+        - 'euclidean':
+            Pairwise Euclidean distances between points in the dataset.
+
+        - 'precomputed':
+            Pre-computed dissimilarities are passed directly to ``fit`` and
+            ``fit_transform``.
+
+    Attributes
+    ----------
+    embedding_ : ndarray of shape (n_samples, n_components)
+        Stores the position of the dataset in the embedding space.
+
+    stress_ : float
+        The final value of the stress (sum of squared distance of the
+        disparities and the distances for all constrained points).
+
+    dissimilarity_matrix_ : ndarray of shape (n_samples, n_samples)
+        Pairwise dissimilarities between the points. Symmetric matrix that:
+
+        - either uses a custom dissimilarity matrix by setting `dissimilarity`
+          to 'precomputed';
+        - or constructs a dissimilarity matrix from data using
+          Euclidean distances.
+
+    n_iter_ : int
+        The number of iterations corresponding to the best stress.
+
+    Examples
+    --------
+    >>> from sklearn.datasets import load_digits
+    >>> from sklearn.manifold import MDS
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> X.shape
+    (1797, 64)
+    >>> embedding = MDS(n_components=2)
+    >>> X_transformed = embedding.fit_transform(X[:100])
+    >>> X_transformed.shape
+    (100, 2)
+
+    References
+    ----------
+    "Modern Multidimensional Scaling - Theory and Applications" Borg, I.;
+    Groenen P. Springer Series in Statistics (1997)
+
+    "Nonmetric multidimensional scaling: a numerical method" Kruskal, J.
+    Psychometrika, 29 (1964)
+
+    "Multidimensional scaling by optimizing goodness of fit to a nonmetric
+    hypothesis" Kruskal, J. Psychometrika, 29, (1964)
+
+    """
+
+    def __init__(self, n_components=2, *, metric=True, n_init=4,
+                 max_iter=300, verbose=0, eps=1e-3, n_jobs=None,
+                 random_state=None, dissimilarity="euclidean",
+                 normalized_stress='auto'):
+        self.n_components = n_components
+        self.dissimilarity = dissimilarity
+        self.metric = metric
+        self.n_init = n_init
+        self.max_iter = max_iter
+        self.eps = eps
+        self.verbose = verbose
+        self.n_jobs = n_jobs
+        self.random_state = random_state
+        self.dissimilarity_matrix_ = None
+        self.embedding_ = None
+        self.stress_ = None
+        self.n_iter_ = None
+        # not in use, declared for consistency with sklearn:
+        self.normalized_stress = normalized_stress
+
+    @property
+    def _pairwise(self):
+        return self.dissimilarity == "precomputed"
+
+    def fit(self, X, y=None, init=None, weight=None):
+        """
+        Computes the position of the points in the embedding space.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features) or \
+                (n_samples, n_samples)
+            Input data. If ``dissimilarity=='precomputed'``, the input should
+            be the dissimilarity matrix.
+
+        y : Ignored
+
+        init : ndarray of shape (n_samples,), default=None
+            Starting configuration of the embedding to initialize the SMACOF
+            algorithm. By default, the algorithm is initialized with a randomly
+            chosen array.
+
+        weight : ndarray of shape (n_samples, n_samples), default=None
+            symmetric weighting matrix of similarities.
+            In default, all weights are 1.
+        """
+        self.fit_transform(X, init=init, weight=weight)
+        return self
+
+    def fit_transform(self, X, y=None, init=None, weight=None):
+        """
+        Fit the data from X, and returns the embedded coordinates.
+
+        Parameters
+        ----------
+        X : array-like of shape (n_samples, n_features) or \
+                (n_samples, n_samples)
+            Input data. If ``dissimilarity=='precomputed'``, the input should
+            be the dissimilarity matrix.
+
+        y : Ignored
+
+        init : ndarray of shape (n_samples,), default=None
+            Starting configuration of the embedding to initialize the SMACOF
+            algorithm. By default, the algorithm is initialized with a randomly
+            chosen array.
+        weight : ndarray of shape (n_samples, n_samples), default=None
+            symmetric weighting matrix of similarities.
+            In default, all weights are 1.
+        """
+        X = self._validate_data(X)
+        if X.shape[0] == X.shape[1] and self.dissimilarity != "precomputed":
+            warnings.warn("The MDS API has changed. ``fit`` now constructs an"
+                          " dissimilarity matrix from data. To use a custom "
+                          "dissimilarity matrix, set "
+                          "``dissimilarity='precomputed'``.")
+
+        if self.dissimilarity == "precomputed":
+            self.dissimilarity_matrix_ = X
+        elif self.dissimilarity == "euclidean":
+            self.dissimilarity_matrix_ = euclidean_distances(X)
+        else:
+            raise ValueError("Proximity must be 'precomputed' or 'euclidean'."
+                             " Got %s instead" % str(self.dissimilarity))
+
+        self.embedding_, self.stress_, self.n_iter_ = smacof(
+            self.dissimilarity_matrix_, metric=self.metric,
+            n_components=self.n_components, init=init, n_init=self.n_init,
+            n_jobs=self.n_jobs, max_iter=self.max_iter, verbose=self.verbose,
+            eps=self.eps, random_state=self.random_state,
+            return_n_iter=True, weight=weight)
+
+        return self.embedding_
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/colors.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/colors.py`

 * *Ordering differences only*

 * *Files 26% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-"""
-Classic colormap ported from matlab rsatoolbox
-
-@author: iancharest
-"""
-from __future__ import annotations
-import numpy as np
-from skimage.color import rgb2hsv, hsv2rgb
-import matplotlib.pyplot as plt
-from matplotlib.colors import ListedColormap
-from scipy.interpolate import interp1d
-
-
-def color_scale(n_cols: int, anchor_cols=None, monitor=False):
-    """ linearly interpolates between a set of given
-    anchor colours to give n_cols and displays them
-    if monitor is set
-
-    Args:
-        n_cols (int): number of colors for the colormap
-        anchor_cols (numpy.ndarray, optional): what color space to
-            interpolate. Defaults to None.
-        monitor (boolean, optional): quick visualisation of the
-            resulting colormap. Defaults to False.
-
-    Returns:
-        numpy.ndarray: n_cols x 3 RGB array.
-
-    """
-
-    if anchor_cols is None:
-        # if no anchor_cols provided, use red to blue
-        anchor_cols = np.array([[1, 0, 0], [0, 0, 1]])
-
-    # define color scale
-    n_anchors = anchor_cols.shape[0]
-
-    # simple 1D interpolation
-    fn = interp1d(
-        range(n_anchors),
-        anchor_cols.T,
-    )
-    cols = fn(np.linspace(0, n_anchors - 1, n_cols)).T
-
-    # optional visuals
-    if monitor:
-        reshaped_cols = cols.reshape((n_cols, 1, 3))
-        width = int(n_cols / 2)
-        mapping = np.tile(reshaped_cols, (width, 1))
-        plt.imshow(mapping)
-        plt.show()
-
-    return cols
-
-
-def rdm_colormap_classic(n_cols: int = 256, monitor: bool = False):
-    """this function provides a convenient colormap for visualizing
-    dissimilarity matrices. it goes from blue to yellow and has grey for
-    intermediate values.
-
-    Args:
-        n_cols (int, optional): precision of the colormap.
-        Defaults to 256.
-
-    Returns:
-        [matplotlib ListedColormap]: this matplotlib color object can be
-        used as a cmap in any plot.
-
-    Example:
-        .. code-block:: python
-
-            import numpy as np
-            import matplotlib.pyplot as plt
-            from rsatoolbox.vis.colors import rdm_colormap_classic
-            plt.imshow(np.random.rand(10,10),cmap=rdm_colormap_classic())
-            plt.colorbar()
-            plt.show()
-
-    (ported from Niko Kriegeskorte's RDMcolormap.m)
-    """
-
-    # blue-cyan-gray-red-yellow with increasing V (BCGRYincV)
-    anchor_cols = np.array([
-        [0, 0, 1],
-        [0, 1, 1],
-        [.5, .5, .5],
-        [1, 0, 0],
-        [1, 1, 0],
-    ])
-
-    # skimage rgb2hsv is intended for 3d images (RGB)
-    # here we add a new axis to our 2d anchorCols to satisfy
-    # skimage, and then squeeze
-    anchor_cols_hsv = rgb2hsv(anchor_cols[np.newaxis, :]).squeeze()
-
-    inc_v_weight = 1
-    anchor_cols_hsv[:, 2] = (1 - inc_v_weight) * anchor_cols_hsv[:, 2] + \
-        inc_v_weight * np.linspace(0.5, 1, anchor_cols.shape[0]).T
-
-    # anchorCols = brightness(anchorCols)
-    anchor_cols = hsv2rgb(anchor_cols_hsv[np.newaxis, :]).squeeze()
-
-    cols = color_scale(n_cols, anchor_cols, monitor)
-
-    cmap = ListedColormap(cols)
-    cmap.set_bad('white')
-
-    return cmap
+"""
+Classic colormap ported from matlab rsatoolbox
+
+@author: iancharest
+"""
+from __future__ import annotations
+import numpy as np
+from skimage.color import rgb2hsv, hsv2rgb
+import matplotlib.pyplot as plt
+from matplotlib.colors import ListedColormap
+from scipy.interpolate import interp1d
+
+
+def color_scale(n_cols: int, anchor_cols=None, monitor=False):
+    """ linearly interpolates between a set of given
+    anchor colours to give n_cols and displays them
+    if monitor is set
+
+    Args:
+        n_cols (int): number of colors for the colormap
+        anchor_cols (numpy.ndarray, optional): what color space to
+            interpolate. Defaults to None.
+        monitor (boolean, optional): quick visualisation of the
+            resulting colormap. Defaults to False.
+
+    Returns:
+        numpy.ndarray: n_cols x 3 RGB array.
+
+    """
+
+    if anchor_cols is None:
+        # if no anchor_cols provided, use red to blue
+        anchor_cols = np.array([[1, 0, 0], [0, 0, 1]])
+
+    # define color scale
+    n_anchors = anchor_cols.shape[0]
+
+    # simple 1D interpolation
+    fn = interp1d(
+        range(n_anchors),
+        anchor_cols.T,
+    )
+    cols = fn(np.linspace(0, n_anchors - 1, n_cols)).T
+
+    # optional visuals
+    if monitor:
+        reshaped_cols = cols.reshape((n_cols, 1, 3))
+        width = int(n_cols / 2)
+        mapping = np.tile(reshaped_cols, (width, 1))
+        plt.imshow(mapping)
+        plt.show()
+
+    return cols
+
+
+def rdm_colormap_classic(n_cols: int = 256, monitor: bool = False):
+    """this function provides a convenient colormap for visualizing
+    dissimilarity matrices. it goes from blue to yellow and has grey for
+    intermediate values.
+
+    Args:
+        n_cols (int, optional): precision of the colormap.
+        Defaults to 256.
+
+    Returns:
+        [matplotlib ListedColormap]: this matplotlib color object can be
+        used as a cmap in any plot.
+
+    Example:
+        .. code-block:: python
+
+            import numpy as np
+            import matplotlib.pyplot as plt
+            from rsatoolbox.vis.colors import rdm_colormap_classic
+            plt.imshow(np.random.rand(10,10),cmap=rdm_colormap_classic())
+            plt.colorbar()
+            plt.show()
+
+    (ported from Niko Kriegeskorte's RDMcolormap.m)
+    """
+
+    # blue-cyan-gray-red-yellow with increasing V (BCGRYincV)
+    anchor_cols = np.array([
+        [0, 0, 1],
+        [0, 1, 1],
+        [.5, .5, .5],
+        [1, 0, 0],
+        [1, 1, 0],
+    ])
+
+    # skimage rgb2hsv is intended for 3d images (RGB)
+    # here we add a new axis to our 2d anchorCols to satisfy
+    # skimage, and then squeeze
+    anchor_cols_hsv = rgb2hsv(anchor_cols[np.newaxis, :]).squeeze()
+
+    inc_v_weight = 1
+    anchor_cols_hsv[:, 2] = (1 - inc_v_weight) * anchor_cols_hsv[:, 2] + \
+        inc_v_weight * np.linspace(0.5, 1, anchor_cols.shape[0]).T
+
+    # anchorCols = brightness(anchorCols)
+    anchor_cols = hsv2rgb(anchor_cols_hsv[np.newaxis, :]).squeeze()
+
+    cols = color_scale(n_cols, anchor_cols, monitor)
+
+    cmap = ListedColormap(cols)
+    cmap.set_bad('white')
+
+    return cmap
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/model_map.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/model_map.py`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,839 +1,839 @@
-# -*- coding: utf-8 -*-
-"""
-Created on Thu Sep 17 17:09:00 2020
-"""
-
-import sys
-import numpy as np
-import matplotlib.pyplot as plt
-from matplotlib import cm
-import scipy.spatial.distance as ssd
-import scipy.stats as sst
-from tqdm import trange
-
-from rsatoolbox.util.inference_util import get_errorbars, all_tests
-from rsatoolbox.util.rdm_utils import batch_to_vectors
-
-fs_small, fs, fs_large = 12, 18, 22
-fig_width, dpi = 10, 300
-
-# from matplotlib import rc
-# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
-# rc('font',**{'family':'serif','serif':['Times']})
-# rc('text', usetex=True)
-
-
-def map_model_comparison(result, rdms_data=None,
-                         colors=None, alpha=0.01, test_type='t-test',
-                         test_pair_comparisons=True, bias_correction=True,
-                         multiple_pair_testing='fdr', test_above_0=True,
-                         test_below_noise_ceil=True, error_bars='ci99',
-                         label_orientation='tangential', fliplr=False,
-                         verbose=0):
-    """ Maps the model RDMs around the data RDM to reveal the pairwise
-    similarities among the model-predicted RDMs along with each
-    model-predicted RDM's similarity to the data RDM. The model map
-    additionally contains inferential information, indicating significant
-    differences among models in terms of their ability to explain the data and
-    significant deviations of the models from the data (noise floor).
-
-    Args (All strings case insensitive):
-        result (rsatoolbox.inference.result.Result):
-            model evaluation result
-            result.models (list of rsatoolbox.model objects): the models in the
-                order in which they were passed to the inference function to
-                produce the result argument. These need to be fixed RDM models.
-                (Flexible models will be handled in the near future.)
-        rdms_data (rsatoolbox.rdm.rdms.RDMs): single-subject data RDMs
-        colors (list of lists, numpy array, matplotlib colormap):
-            None (default): default blue for all bars
-            single color: list or numpy array of 3 or 4 values (RGB, RGBA)
-                specifying the color for all bars
-            multiple colors: list of lists or numpy array (number of colors by
-                3 or 4 channels -- RGB, RGBA). If the number of colors matches
-                the number of models, each color is used for the bar
-                corresponding to one model (in the order of the models as
-                passed). If the number of colors does not match the number of
-                models, the list is linearly interpolated to assign a color to
-                each model (in the order of the models as passed). For example,
-                two colors will become a gradation, unless there are exactly
-                two model. Instead of a list of lists or numpy array, a
-                matplotlib colormap object may also be passed (e.g. colors =
-                cm.coolwarm).
-        alpha (float):
-            significance threshold (p threshold or FDR q threshold)
-        test_pair_comparisons (Boolean or string):
-            False or None: do not plot pairwise model comparison results
-            True (default): plot pairwise model comparison results using
-                default settings
-        multiple_pair_testing (Boolean or string):
-            False or 'none': do not adjust for multiple testing for the
-                pairwise model comparisons
-            'FDR' or 'fdr' (default): control the false-discorvery rate at
-                q = alpha
-            'FWER',' fwer', or 'Bonferroni': control the familywise error rate
-            using the Bonferroni method
-        test_above_0 (Boolean or string):
-            False or None: do not plot results of statistical comparison of
-                each model performance against 0
-            True (default): plot results of statistical comparison of each
-                model performance against 0 using default settings ('dewdrops')
-            Tests are one-sided, use the global alpha threshold and are
-            automatically Bonferroni-corrected for the number of models tested.
-        test_below_noise_ceil (Boolean or string):
-            False or None: do not plot results of statistical comparison of
-                each model performance against the lower-bound estimate of the
-                noise ceiling
-            True (default): plot results of statistical comparison of each
-                model performance against the lower-bound estimate of the noise
-                ceiling using default settings
-            Tests are one-sided, use the global alpha threshold and are
-            automatically Bonferroni-corrected for the number of models tested.
-        error_bars (Boolean or string):
-            False or None: do not plot error bars
-            True (default) or 'SEM': plot the standard error of the mean
-            'CI': plot 95%-confidence intervals (exluding 2.5% on each side)
-            'CI[x]': plot x%-confidence intervals (exluding 2.5% on each side)
-            Confidence intervals are based on the bootstrap procedure,
-            reflecting variability of the estimate across subjects and/or
-            experimental conditions.
-        label_orientation (string): 'tangential' (default) for tangential model
-            labels, 'radial' for radial model labels.
-        fliplr (Boolean): If this is False (default) the arrangement will map
-            the first model to the left of the second model (in reading
-            direction). If this argument is True, the map will left-right
-            flipped about the vertical axis.
-
-    Returns:
-        ---
-
-    """
-    if result.cv_method == 'crossvalidation':
-        print('Tests deactivated because crossvalidation alone gives no uncertainty estimate.\n')
-        test_pair_comparisons = False
-        test_above_0 = False
-        test_below_noise_ceil = False
-    # %% Prepare and sort data
-    evaluations = result.evaluations
-    models = result.models
-    noise_ceiling = result.noise_ceiling
-    n_models = result.n_model
-    names = [m.name for m in models]
-
-    # average the bootstrap evaluations
-    while len(evaluations.shape) > 2:
-        # average across trailing dimensions
-        evaluations = np.nanmean(evaluations, axis=-1)
-    evaluations = evaluations[~np.isnan(evaluations[:, 0])]
-    noise_ceiling = np.array(noise_ceiling)
-    perf = np.mean(evaluations, axis=0)  # average across bootstrap samples
-    n_bootstraps = evaluations.shape[0]
-
-    if multiple_pair_testing is None:
-        multiple_pair_testing = 'uncorrected'
-
-    # run tests
-    if any([test_pair_comparisons, test_above_0, test_below_noise_ceil]):
-        p_pairwise, p_zero, p_noise = all_tests(
-            evaluations, noise_ceiling, test_type,
-            model_var=result.model_var, diff_var=result.diff_var,
-            noise_ceil_var=result.noise_ceil_var, dof=result.dof)
-
-    # %% Test each model RDM for relatedness to and distinctness from the data RDM
-    # test if model RDMs are significantly related to data RDM
-    # (above 0)
-    if test_above_0:  # one-sided test
-        model_significant = p_zero < alpha / n_models  # Bonferroni-corrected
-    else:
-        model_significant = None
-
-    # test if model RDMs are significantly distinct from the data RDM
-    # (below the noise ceiling's lower bound)
-    if test_below_noise_ceil:  # one-sided test
-        model_below_lower_bound = p_noise < alpha / n_models  # Bonferroni-corrected
-    else:
-        model_below_lower_bound = None
-
-    # %% Perform pairwise model comparisons
-    n_tests = int((n_models**2 - n_models) / 2)
-    if test_pair_comparisons:
-        if multiple_pair_testing.lower() == 'bonferroni' or \
-           multiple_pair_testing.lower() == 'fwer':
-            significant = p_pairwise < (alpha / n_tests)
-        elif multiple_pair_testing.lower() == 'fdr':
-            ps = batch_to_vectors(np.array([p_pairwise]))[0][0]
-            ps = np.sort(ps)
-            criterion = alpha * (np.arange(ps.shape[0]) + 1) / ps.shape[0]
-            k_ok = ps < criterion
-            if np.any(k_ok):
-                k_max = np.where(k_ok)[0][-1]
-                crit = criterion[k_max]
-            else:
-                crit = 0
-            significant = p_pairwise < crit
-        else:
-            if 'uncorrected' not in multiple_pair_testing.lower():
-                raise Exception(
-                    'plot_model_comparison: Argument ' +
-                    'multiple_pair_testing is incorrectly defined as ' +
-                    multiple_pair_testing + '.')
-            significant = p_pairwise < alpha
-    else:
-        significant = None
-
-    # %% Compute the errorbars
-    limits = get_errorbars(result.model_var, evaluations, result.dof, error_bars)
-    inference_descr = _get_description(
-        test_pair_comparisons, multiple_pair_testing, error_bars,
-        test_above_0, test_below_noise_ceil,
-        result.cv_method, result.method,
-        alpha, n_tests, n_models, n_bootstraps)
-    print(inference_descr)
-
-    # %% Compute data-model distance estimates
-    # We assume performance estimates p are correlations or cosines.
-    # We convert to correlation distance or cosine distance: 1-p.
-    # Correlation distance and cosine distance are porportional to squared
-    # Euclidean distances after the respective approriate pattern
-    # normalizations.
-    # For each model, we estimate the data-model distance as
-    # d = sqrt( (1-p) - (1-u) ) = sqrt(u-p),
-    # where p is the performance and u the upper bound of the noise ceiling.
-
-    noise_lower = np.nanmean(noise_ceiling[0])
-    noise_upper = np.nanmean(noise_ceiling[1])
-
-    # use upper noise ceiling as reference
-    # (conservative: distances will still be positively biased)
-    # data_model_dists = np.sqrt( 2 * (noise_upper - perf) )
-    # noise_halo_rad = np.sqrt( 2 * (noise_upper - noise_lower) )
-
-    # estimate true model-data distances
-    # (assuming noise and intersubject variation displace RDMs orthogonally to
-    # the model-brain RDM-space axis)
-    if verbose > 0:
-        print('\nEstimating the bias due to noise of the inter-RDM distance estimates...')
-    if bias_correction is None or bias_correction == 'none':
-        correction = 1 / noise_upper
-    elif bias_correction == 'bootstrap' or bias_correction:
-        correction = _correct_model_dist(rdms_data, method=result.method)
-    # DEBUG
-    # bias_of_sq_data_model_dist = 0  # no correction: upper bound is at the center (0)
-    # bias_of_sq_data_model_dist = 2*(noise_upper - noise_lower)  # collapse
-    # the noise ceiling: lower bound is at the center (0)
-
-    data_model_dists = np.sqrt(2 * np.maximum(
-        1 - correction * perf, np.finfo(float).eps))
-    noise_halo_rad = np.sqrt(2 * np.maximum(
-        1 - correction * noise_lower, np.finfo(float).eps))
-    errbar_dist_low = np.sqrt(2 * np.maximum(
-        1 - correction * (perf - limits[0]), np.finfo(float).eps))
-    errbar_dist_high = np.sqrt(2 * np.maximum(
-        1 - correction * (perf + limits[1]), np.finfo(float).eps))
-
-    # data_model_dists = np.sqrt(2 * (noise_upper - perf)
-    #                    - bias_of_sq_data_model_dist)
-    # noise_halo_rad = np.sqrt(2 * (noise_upper - noise_lower) -
-    #                    - bias_of_sq_data_model_dist)
-    # errbar_dist_low = np.sqrt(2 * (noise_upper - (perf - limits[0]))
-    #                    - bias_of_sq_data_model_dist)
-    # errbar_dist_high = np.sqrt(2 * (noise_upper - (perf + limits[1]))
-    #                    - bias_of_sq_data_model_dist)
-
-    eb_low_high = np.array((errbar_dist_low, errbar_dist_high))
-
-    # %% Compute intermodel distances
-    n_dissim = int(models[0].n_cond * (models[0].n_cond - 1) / 2)
-    modelRDMs = np.empty((n_models, n_dissim))
-    for idx, model_i in enumerate(models):
-        if rdms_data is not None:
-            theta = model_i.fit(rdms_data)
-            modelRDMs[idx, :] = model_i.predict(theta)
-        else:
-            modelRDMs[idx, :] = model_i.predict()
-
-    if result.method == 'corr':
-        modelRDMs = modelRDMs - modelRDMs.mean(axis=1, keepdims=True)
-        modelRDMs /= np.sqrt(np.einsum('ij,ij->i', modelRDMs, modelRDMs))[:, None]
-    elif result.method == 'cosine':
-        modelRDMs /= np.sqrt(np.einsum('ij,ij->i', modelRDMs, modelRDMs))[:, None]
-    intermodelDists = ssd.squareform(
-        ssd.pdist(modelRDMs, metric='euclidean'))
-    # the below yield identical results...
-    # intermodelDists2 = np.sqrt(2*(1 - np.einsum('ik,jk', modelRDMs, modelRDMs)))
-    # intermodelDists3 = ssd.squareform(np.sqrt(2*ssd.pdist(modelRDMs, metric='correlation')))
-
-    # %% Assemble second-order distance matrix (distances among RDMs)
-    rdm_dists = np.zeros((n_models + 1, n_models + 1))
-    rdm_dists[1:, 0] = data_model_dists
-    rdm_dists[0, 1:] = data_model_dists
-    rdm_dists[1:, 1:] = intermodelDists
-    rdm_dists[np.eye(n_models + 1) == 1] = 0
-
-    # optionally use squared distances (e.g. correlation distances, rather than their square roots)
-    # rdm_dists = rdm_dists**2
-
-    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
-    plt.imshow(rdm_dists, cmap='Greys')
-    plt.title(
-        'Matrix of inter-RDM distances\n(0: data RDM, 1..n: model RDMs)', fontsize=fs_large)
-    plt.colorbar()
-    plt.xticks(fontsize=fs_small)
-    plt.yticks(fontsize=fs_small)
-    plt.show()
-
-    # %% Perform MDS to map model RDMs around the data RDM
-    if verbose > 0:
-        print('\nPerforming MDS to map model RDMs around the data RDM, using ' +
-              'custom MDS...', flush=True)
-        sys.stdout.flush()
-    locs2d = custom_MDS(rdm_dists, n_init=10, n_iter=500, verbose=verbose)
-
-    # ensure canonical reflection
-    if bool(locs2d[1, 0] < locs2d[2, 0]) == fliplr:
-        locs2d[:, 0] *= -1
-
-    # show Shepard plot and rubberband plot of mapping distortions
-    r, r_model_data, Spearman_r_model_data = show_Shepard_plot(
-        locs2d, rdm_dists, colors)
-    print('Pearson r(RDM dist, 2d-map dist): {:.4f}'.format(r))
-    print(
-        'Pearson r(RDM dist, 2d-map dist) for model-data dists: {:.4f}'.format(r_model_data))
-    print(
-        'Spearman r(RDM dist, 2d-map dist) for model-data dists: {:.4f}'.format(
-            Spearman_r_model_data))
-    # show Elastic plot
-    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
-    plot_model_map_elastic(locs2d, rdm_dists, names, colors)
-    plt.show()
-
-    # compute scale bar
-    approx_frac_of_max = 0.15
-    dists_2d_vec = ssd.pdist(locs2d)
-    rdm_dists_vec = ssd.squareform(rdm_dists)
-    distortion_facs = dists_2d_vec / rdm_dists_vec
-    percent_covered = 100
-    prop_cut = (1 - percent_covered / 100) / 2
-    qnts = np.quantile(distortion_facs, [prop_cut, 1 - prop_cut], axis=0)
-    np.max(distortion_facs)
-    dist_2d_max = np.max(dists_2d_vec)
-    scalebar_length = round(approx_frac_of_max * dist_2d_max * 10) / 10
-    scalebar_descr = '{:.1f} [a.u.]\n'.format(scalebar_length)
-    scalebar_descr += ('normalized-pattern Eucl. dist.')
-    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
-    plot_model_map(locs2d, significant, model_significant, model_below_lower_bound,
-                   eb_low_high, noise_halo_rad, scalebar_length, scalebar_descr, qnts,
-                   colors, label_orientation, names)
-    plt.show()
-
-
-def plot_model_map(locs2d, significant, model_significant, model_below_lower_bound,
-                   eb_low_high, noise_halo_rad, scalebar_length, scalebar_descr, qnts,
-                   colors=None, label_orientation='tangential', names=None):
-    """ Plots the model map result illustration """
-    n_models = locs2d.shape[0] - 1
-    # Define the model colors
-    colors = _parse_colors(colors, n_models)
-    # Draw the map with error bars
-    rdm_dot_size = 200  # size of dots representing RDMs [pt]
-    noise_halo_col = [0.7, 0.7, 0.7, 1]  # color of the noise halo
-    orbit_col = [0.85, 0.85, 0.85, 0.4]   # color of the orbits
-    ns_lw = 6  # line width for non-significance cords and rays
-    ns_col = [0, 0, 0, 0.15]  # color for non-significance cords and rays
-
-    # prepare axes for map
-    l, b, w, h = 0.15, 0.15, 0.85, 0.85
-    ax = plt.axes((l, b, w, h))
-    r_max = np.max(np.sqrt(np.sum(np.array(locs2d)**2, axis=1)))
-    clearance_fac = 1.08  # factor by which the maximum radius is multiplied
-    # to determine the inner edge of the model labels...
-    rng = r_max * clearance_fac  # ...and minimum extent shown in the axes.
-    # plt.axis([-rng, rng, -rng, rng])
-    # ensure axes shows the largest orbit completely
-    plt.scatter([-rng, 0, rng, 0], [0, rng, 0, -rng], c='none')
-
-    # plot non-significance arches
-    if significant is not None:
-        for i in range(n_models - 1):
-            for j in range(i + 1, n_models):
-                if not significant[i, j]:
-                    # draw non-significance arch
-                    xi, yi = np.array(locs2d[i + 1]).squeeze()
-                    xj, yj = np.array(locs2d[j + 1]).squeeze()
-                    rad_i, rad_j = np.sqrt(xi**2 + yi**2), np.sqrt(xj**2 + yj**2)
-                    angle_i, angle_j = np.arctan2(xi, yi), np.arctan2(xj, yj)
-                    angle_diff = angle_j - angle_i  # pos if angle_j is greater
-                    if (angle_diff > 0 and abs(angle_diff) <= np.pi) or \
-                       (angle_diff <= 0 and abs(angle_diff) > np.pi):
-                        # clockwise from i to j is shorter
-                        angles = np.linspace(
-                            angle_i,
-                            angle_i + min(
-                                abs(angle_diff),
-                                2 * np.pi - abs(angle_diff)),
-                            360)
-                        radii = np.linspace(rad_i, rad_j, 360)
-                    else:
-                        # clockwise from j to i is shorter
-                        angles = np.linspace(
-                            angle_j, angle_j + min(
-                                abs(angle_diff),
-                                2 * np.pi - abs(angle_diff)),
-                            360)
-                        radii = np.linspace(rad_j, rad_i, 360)
-                    xx, yy = np.sin(angles) * radii, np.cos(angles) * radii
-                    plt.plot(xx, yy, color=ns_col, linewidth=ns_lw)
-
-    # plot orbits, relatedness and distinctness tests, error bars, and model
-    # labels
-    for model_i in range(n_models):
-        # orbits
-        v = locs2d[model_i + 1]
-        rad = np.sqrt(v * v.T)
-        orbit = plt.Circle((0, 0), rad, color='none', ec=orbit_col, lw=1, zorder=12)
-        ax.add_artist(orbit)
-        # plt.plot(v[0, 0], v[0, 1], ms=200, color='r')
-
-        # indicate whether model RDM is significantly related to data RDM
-        if model_significant is not None:
-            if not model_significant[model_i]:
-                x0, y0 = locs2d[model_i + 1, 0], locs2d[model_i + 1, 1]
-                x1, y1 = np.array((x0, y0)) / np.sqrt(np.sum(
-                    x0**2 + y0**2)) * r_max * clearance_fac * 0.98
-                # plot outer non-significance ray
-                plt.plot([x0, x1], [y0, y1], color=ns_col, linewidth=ns_lw)
-
-        # indicate whether model RDM is significantly distinct from data RDM
-        if model_below_lower_bound is not None:
-            if not model_below_lower_bound[model_i]:
-                x, y = locs2d[model_i + 1, 0], locs2d[model_i + 1, 1]
-                # plot outer non-significance ray
-                plt.plot([0, x], [0, y], color=ns_col, linewidth=ns_lw)
-
-        # error bars
-        vn = v / rad
-        edlx = vn[0, 0] * eb_low_high[:, model_i]
-        edly = vn[0, 1] * eb_low_high[:, model_i]
-        plt.plot(edlx, edly, color=colors[model_i], zorder=15)
-
-        # model labels
-        if names is not None:
-            tx = vn[0, 0] * r_max * clearance_fac
-            ty = vn[0, 1] * r_max * clearance_fac
-            if label_orientation == 'tangential':
-                angle_deg = (np.arctan2(vn[0, 1], vn[0, 0]) /
-                             np.pi * 180) % 180 - 90  # tangential
-                horAlign = 'center'
-                verAlign = 'bottom' if ty > 0 else 'top'
-            elif label_orientation == 'radial':
-                angle_deg = (np.arctan2(vn[0, 1], vn[0, 0]) /
-                             np.pi * 180 - 90) % 180 - 90  # radial
-                horAlign = 'right' if tx <= 1e-5 else 'left'
-                verAlign = 'center'
-            plt.text(tx, ty, names[model_i], va=verAlign, ha=horAlign,
-                     rotation_mode='anchor', family='sans-serif', size=fs,
-                     rotation=angle_deg)
-
-    # plot the data RDM with its noise halo
-    noise_halo = plt.Circle((0, 0), noise_halo_rad,
-                            color=noise_halo_col, zorder=5)
-    ax.add_artist(noise_halo)
-    plt.scatter(0, 0, s=rdm_dot_size, c='k', zorder=20)
-
-    # plot the model RDMs
-    x, y = np.array(locs2d[1:, 0]).squeeze(), np.array(locs2d[1:, 1]).squeeze()
-    plt.scatter(x, y, s=rdm_dot_size, c=colors, zorder=15)
-
-    # add a scalebar
-    plt.plot([-rng, -rng + scalebar_length],
-             [-rng, -rng], color='k', linewidth=6)
-    plt.plot([-rng + scalebar_length * qnts[0], -rng + scalebar_length * qnts[1]], [-rng, -rng],
-             color=[0.5, 0.5, 0.5], linewidth=2)
-    plt.text(-rng + 0.5 * scalebar_length, -rng * 1.02, scalebar_descr, va='top', ha='center',
-             family='sans-serif', size=fs_small)
-    plt.axis('equal')
-    plt.axis('off')
-
-
-def _parse_colors(colors, n_models):
-    """ parses a color argument into an array of RGB values
-    """
-    if colors is None:  # no color passed...
-        colors = np.array([0, 0.4, 0.9, 1])[None, :]  # use default blue
-    elif isinstance(colors, cm.colors.LinearSegmentedColormap):
-        cmap = cm.get_cmap(colors)
-        colors = cmap(np.linspace(0, 1, 100))[np.newaxis, :, :3].squeeze()
-    colors = np.array([np.array(col) for col in colors])
-    if colors.ndim == 1:  # one color passed...
-        n_col, n_chan = 1, colors.shape[0]
-        colors = colors.reshape(n_col, n_chan)
-    elif colors.ndim == 2 and colors.shape[0] == 1:
-        n_col, n_chan = colors.shape
-    else:  # multiple colors passed...
-        n_col, n_chan = colors.shape
-        if n_col == n_models:  # one color passed for each model...
-            cols2 = colors
-        else:  # number of colors passed does not match number of models...
-            # interpolate colors to define a color for each model
-            cols2 = np.empty((n_models, n_chan))
-            for c in range(n_chan):
-                cols2[:, c] = np.interp(np.arange(n_models),
-                                        np.arange(n_col) /
-                                        (n_col - 1) * (n_models - 1),
-                                        colors[:, c])
-        colors = cols2
-    # if there is no alpha channel, make opaque
-    if colors.shape[1] == 3:
-        colors = np.concatenate((colors, np.ones((colors.shape[0], 1))),
-                                axis=1)
-    if colors.shape[0] == 1:
-        colors = np.tile(colors, (n_models, 1))
-    return colors
-
-
-def _get_description(test_pair_comparisons, multiple_pair_testing, error_bars,
-                     test_above_0, test_below_noise_ceil,
-                     cv_method, method,
-                     alpha, n_tests, n_models, n_bootstraps):
-    inference_descr = ''
-    if test_pair_comparisons:
-        inference_descr += 'Model comparisons: two-tailed, '
-    if multiple_pair_testing.lower() == 'bonferroni' or \
-       multiple_pair_testing.lower() == 'fwer':
-        inference_descr += ('p < {:<.5g}'.format(alpha) +
-                            ', Bonferroni-corrected for ' +
-                            str(n_tests) +
-                            ' model-pair comparisons')
-    elif multiple_pair_testing.lower() == 'fdr':
-        inference_descr += ('FDR q < {:<.5g}'.format(alpha) +
-                            ' (' + str(n_tests) +
-                            ' model-pair comparisons)')
-    else:
-        inference_descr = (inference_descr +
-                           'p < {:<.5g}'.format(alpha) +
-                           ', uncorrected (' + str(n_tests) +
-                           ' model-pair comparisons)')
-    if cv_method in ['bootstrap_rdm', 'bootstrap_pattern', 'bootstrap_crossval']:
-        inference_descr = inference_descr + \
-            '\nInference by bootstrap resampling ' + \
-            '({:<,.0f}'.format(n_bootstraps) + ' bootstrap samples) of '
-    if cv_method == 'bootstrap_rdm':
-        inference_descr = inference_descr + 'subjects. '
-    elif cv_method == 'bootstrap_pattern':
-        inference_descr = inference_descr + 'experimental conditions. '
-    elif cv_method in ['bootstrap', 'bootstrap_crossval']:
-        inference_descr = inference_descr + \
-            'subjects and experimental conditions. '
-
-    # %% Print description of inferential methods
-    inference_descr += '\nError bars indicate the'
-    if error_bars[0:2].lower() == 'ci':
-        if len(error_bars) == 2:
-            CI_percent = 95.0
-        else:
-            CI_percent = float(error_bars[2:])
-        inference_descr += ' ' + str(CI_percent) + '% confidence interval.'
-    elif error_bars.lower() == 'sem':
-        inference_descr += ' standard error of the mean.'
-    if test_above_0 or test_below_noise_ceil:
-        inference_descr += '\nOne-sided comparisons of each model performance '
-    if test_above_0:
-        inference_descr += 'against 0 '
-    if test_above_0 and test_below_noise_ceil:
-        inference_descr += 'and '
-    if test_below_noise_ceil:
-        inference_descr += 'against the lower-bound estimate of the noise ceiling '
-    if test_above_0 or test_below_noise_ceil:
-        inference_descr += ('are Bonferroni-corrected for ' +
-                            str(n_models) + ' models.\n')
-    inference_descr += 'Inter-RDM distances were measured by the '
-    if method == 'corr':
-        inference_descr += (
-            'Pearson correlation distance '
-            + '(proportional to squared Euclidean distance'
-            + ' after RDM centering and divisive normalization).')
-    elif method == 'cosine':
-        inference_descr += (
-            'cosine distance '
-            + '(proportional to squared Euclidean distance after RDM divisive normalizaton). ')
-    else:
-        raise Exception('rsatoolbox.vis.map_model_comparison: result.method ' +
-                        method + ' not yet handled.')
-    inference_descr += 'Inter-RDM distances are mapped as '
-    inference_descr += (
-        'Euclidean distance '
-        + '(proportional to the square root of correlation or cosine distance'
-        + ' if RDMs were appropriately normalized).')
-    return inference_descr
-
-
-def custom_MDS(rdm_dists, n_init=100, n_iter=500, verbose=0):
-    """ Custom multidimensional scaling
-
-    Performs multidimensional scaling (MDS) using the metric stress cost
-    function (sum of squared distance deviations) for the intermodel RDM
-    distances while exactly preserving the model-data RDM distances.
-    Assumes that the data RDM has index 0 in the passed vectorized matrix of
-    RDM distances (second-order distances). The data RDM is placed at the
-    origin. The best fitting model is placed straight above it at the exact
-    RDM distance. Since the remaining models must be placed at radii exactly
-    matching the model-data RDM distances, only the angles are free parameters.
-    The remaining model-RDMs are placed in random order. Each is placed
-    by line search to minimize the sum of squared deviations from the RDM-
-    distance to the already placed models. After this initial round of
-    placements, each model (except the best one, which remains in its initial
-    position) is re-placed in random order. Models are adjusted until
-    convergence in random order. The entire process, including the
-    initialization, is repeated n_init times. The best arrangement (the one
-    with minimum sum of squared deviations) is returned.
-
-    """
-    # Preparations
-    n_models = rdm_dists.shape[0] - 1
-    best_model_i = np.argmin(rdm_dists[0, 1:])
-    other_model_is = [i for i in range(0, n_models) if i != best_model_i]
-    print(rdm_dists.shape)
-    rdm_dists_vec = ssd.squareform(rdm_dists)
-    ssqd_min = np.inf
-    two = 2  # minimizes sum(abs(errors)**two)
-
-    # Run repeatedly with random initialization
-    if verbose > 0:
-        iterator = trange(n_init)
-    else:
-        iterator = range(n_init)
-    for _ in iterator:
-        locs2d = np.full((n_models + 1, 2), np.NaN)
-        # place data RDM at the origin and the best model straight above it
-        locs2d[0] = 0, 0
-        # ...and best model straight above it
-        locs2d[best_model_i + 1] = 0, rdm_dists[0, best_model_i + 1]
-        # Initialize by locally optimal placing of each of the remaining models
-        # in random order
-        rand_perm_other_model_is = np.random.permutation(other_model_is)
-        for model_i in rand_perm_other_model_is:
-            locs2d[model_i + 1,
-                   :] = place_model(model_i, locs2d, rdm_dists, 1, two)
-        # Initialize by placing the remaining models at random angles
-        # (This random-angle approach is less reliable than the random-order initialization above.)
-        # for model_i in other_model_is:
-        #     angle = np.random.rand() * 2 * np.pi
-        #     rad = rdm_dists[0, model_i+1]
-        #     locs2d[model_i+1, :] = np.sin(angle) * rad, np.cos(angle) * rad
-
-        # Adjust each model (except the best) in random order
-        n_scales, n_scales_max = 1, 2
-        for iter_i in range(n_iter):
-            locs2d_prev = locs2d.copy()
-            rand_perm_other_model_is = np.random.permutation(other_model_is)
-            for model_i in rand_perm_other_model_is:
-                locs2d[model_i + 1,
-                       :] = place_model(model_i, locs2d, rdm_dists, n_scales, two)
-            if np.max(abs(locs2d_prev - locs2d)) < 1e-8:
-                if verbose > 0:
-                    print('MDS converged after {:.0f} iterations using {:.0f} scales.'.format(
-                        iter_i, n_scales))
-                n_scales += 1
-                if n_scales > n_scales_max:
-                    break
-        dists_2d = ssd.pdist(locs2d, metric='euclidean')
-        ssqd = np.sum(abs(rdm_dists_vec - dists_2d)**two)
-        if ssqd < ssqd_min:
-            ssqd_min = ssqd
-            # r = np.corrcoef(rdm_dists_vec, dists_2d)[0, 1]
-            r_0fixed = np.sum(rdm_dists_vec * dists_2d) \
-                / (np.sum(rdm_dists_vec ** 2) + np.sum(dists_2d ** 2))
-            locs2d_best = locs2d.copy()
-            if verbose > 0:
-                print(' SSQD: {:.4f}, corr_0fixed(RDM dist, map dist): {:.4f}'.format(
-                    ssqd, r_0fixed))
-        if iter_i == n_iter - 1:
-            print(' MDS did not converge. Doubling number of iterations.')
-            n_iter *= 2
-    return np.matrix(locs2d_best)
-
-
-def place_model(model_i, locs2d, rdm_dists, n_scales=3, two=2):
-    """ place a model
-
-    Args:
-        model_i : int
-            which model to update.
-        locs2d : np.array
-            The current positions in 2D space.
-        rdm_dists : np.array
-            The desired distances between models.
-        n_scales : int, optional
-            How many scales to test at. The default is 3.
-        two : TYPE, optional
-            Exponent for the error weighting. The default is 2.
-
-    """
-    n_angles = 180
-    n_models = rdm_dists.shape[0] - 1
-    radius = rdm_dists[0, model_i + 1]
-    start, stop = 0, 2 * np.pi
-
-    for _ in range(n_scales):
-        angles = np.linspace(start, stop, n_angles)
-        cand_locs = np.concatenate([np.sin(angles)[:, None], np.cos(angles)[:, None]], axis=1) \
-            * radius
-        dists_2d = np.sqrt(np.sum(
-            (cand_locs.reshape(n_angles, 1, 2) -
-             locs2d.reshape(1, n_models + 1, 2)) ** 2,
-            axis=2))
-        ssqd = np.nansum(abs(
-            dists_2d - rdm_dists[model_i + 1, :].reshape(1, n_models + 1)) ** two, axis=1)
-        best_angle_i = np.argmin(ssqd)
-        start, stop = angles[(best_angle_i + np.array([-1, 1])) % n_angles]
-        n_angles = int(n_angles / 3)
-
-    return cand_locs[best_angle_i, :]
-
-
-def plot_model_map_elastic(locs2d, rdm_dists, names, colors=None):
-    """ Plots the models with an indiciation how strongly their distances were
-    distorted.
-    """
-    # parsing input
-    n_models = locs2d.shape[0] - 1
-    dists_2d_vec = ssd.pdist(locs2d, metric='euclidean')
-    dists_2d = ssd.squareform(dists_2d_vec)
-    rdm_dists_vec = ssd.squareform(rdm_dists)
-    # Define the model colors
-    colors = _parse_colors(colors, n_models)
-    # distance-distortion plot
-    r_max = np.max(np.sqrt(np.sum(np.array(locs2d)**2, axis=1)))
-    rng = r_max * 1.2  # ...and minimum extent shown in the axes.
-    # plt.axis([-rng, rng, -rng, rng])
-    # ensure axes shows the largest orbit completely
-    plt.scatter([-rng, 0, rng, 0], [0, rng, 0, -rng], c='none')
-
-    # model labels
-    clearance_fac = 1.05  # factor by which the maximum radius is multiplied
-    for model_i in range(n_models):
-        v = locs2d[model_i + 1]
-        rad = np.sqrt(v * v.T)
-        vn = v / rad
-        tx = vn[0, 0] * r_max * clearance_fac
-        ty = vn[0, 1] * r_max * clearance_fac
-        angle_deg = (np.arctan2(vn[0, 1], vn[0, 0]) /
-                     np.pi * 180) % 180 - 90  # tangential
-        horAlign = 'center'
-        verAlign = 'bottom' if ty > 0 else 'top'
-        plt.text(tx, ty, names[model_i], va=verAlign, ha=horAlign,
-                 rotation_mode='anchor', family='sans-serif',
-                 size=fs, rotation=angle_deg)
-
-    # plot the data RDM
-    rdm_dot_size = 50
-    plt.scatter(0, 0, s=rdm_dot_size, c='k', zorder=20)
-
-    # plot the distortion-indicating rubberbands
-    stretched_col = np.array([0, 0.5, 1])
-    squeezed_col = np.array([0.8, 0, 0])
-    correct_col = np.array([0.5, 0.5, 0.5])
-    lw_undistorted = 3
-
-    # slackstringplot(rdm_dists)
-    for i in range(n_models - 1):
-        for j in range(i + 1, n_models):
-            distortion = dists_2d[i + 1, j + 1] - rdm_dists[i + 1, j + 1]
-            w = abs(distortion) / np.max(abs(dists_2d_vec - rdm_dists_vec))
-            if distortion <= 0:
-                col = w * squeezed_col + (1 - w) * correct_col
-            else:
-                col = w * stretched_col + (1 - w) * correct_col
-            # if abs(distortion) < 0.1:
-            #     col = correct_col
-            # else:
-            #     col = stretched_col if distortion > 0 else squeezed_col
-            lw = lw_undistorted / dists_2d[i +
-                                           1, j + 1] * rdm_dists[i + 1, j + 1]
-            plt.plot(locs2d[[i + 1, j + 1], 0], locs2d[[i + 1, j + 1], 1],
-                     color=col, linewidth=lw, solid_capstyle='round')
-
-    # plot the model RDMs
-    x, y = np.array(locs2d[1:, 0]).squeeze(), np.array(locs2d[1:, 1]).squeeze()
-    plt.scatter(x, y, s=rdm_dot_size, c=colors, zorder=10)
-
-    plt.title('Rubberband plot of inter-RDM-distance distortions\n' +
-              '(thin & blue: stretched, thick & red: squeezed)', fontsize=fs_large)
-    plt.axis('equal')
-    plt.axis('off')
-
-
-def show_Shepard_plot(locs2d, rdm_dists, colors=None):
-    """ Show shepard plot """
-    rdm_dists_vec = ssd.squareform(rdm_dists)
-    n_models = locs2d.shape[0] - 1
-    dists_2d_vec = ssd.pdist(locs2d, metric='euclidean')
-    mx = max(dists_2d_vec)
-    # Define the model colors
-    colors = _parse_colors(colors, n_models)
-
-    r = np.corrcoef(rdm_dists_vec, dists_2d_vec)[0, 1]
-    r_model_data = np.corrcoef(
-        rdm_dists_vec[:n_models], dists_2d_vec[:n_models])[0, 1]
-    Spearman_r_model_data = sst.spearmanr(rdm_dists_vec[:n_models],
-                                          dists_2d_vec[:n_models]).correlation
-
-    # Shepard plot
-    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
-    plt.plot((0, mx), (0, mx), color=[0.5, 0.5, 0.5, 0.5])
-    plt.plot(rdm_dists_vec, dists_2d_vec, '.', color=[0, 0, 0, 0.8],
-             mec=[1, 1, 1], ms=20, label='model-model distances')
-    plt.plot(rdm_dists_vec[:n_models], dists_2d_vec[:n_models], '.',
-             color=[0.8, 0, 0], mec=[1, 1, 1, 1], ms=20, label='model-data distances')
-    plt.title('Shepard plot\n(Pearson r: {:.3f}, Pearson r for model-data dists: {:.1f})'.format(
-        r, r_model_data), fontsize=fs_large)
-    plt.xlabel('distance between RDMs', fontsize=fs)
-    plt.ylabel('2d distance', fontsize=fs)
-    plt.xticks(fontsize=fs_small)
-    plt.yticks(fontsize=fs_small)
-    plt.axis([0, mx, 0, mx])
-    plt.gca().set_aspect('equal', adjustable='box')
-    plt.legend(fontsize=fs)
-
-    # plot distortion factor range
-    approx_frac_of_max = 0.15
-    distortion_facs = dists_2d_vec / rdm_dists_vec
-    percent_covered = 100
-    prop_cut = (1 - percent_covered / 100) / 2
-    qnts = np.quantile(distortion_facs, [prop_cut, 1 - prop_cut], axis=0)
-    dist_2d_max = np.max(dists_2d_vec)
-    scalebar_length = round(approx_frac_of_max * dist_2d_max * 10) / 10
-    plt.plot([0, mx], [0, mx * qnts[0]],
-             color=[0.5, 0.5, 0.5, 0.2], linewidth=1)
-    plt.plot([0, mx], [0, mx * qnts[1]],
-             color=[0.5, 0.5, 0.5, 0.2], linewidth=1)
-    plt.plot([scalebar_length, scalebar_length], [0, scalebar_length],
-             color=[0, 0, 0, 0.7], linewidth=6)
-    plt.plot([scalebar_length, scalebar_length],
-             [scalebar_length * qnts[0], scalebar_length * qnts[1]],
-             color=[0.5, 0.5, 0.5], linewidth=2)
-    return r, r_model_data, Spearman_r_model_data
-
-
-def _correct_model_dist(rdms_data, method='corr'):
-    if rdms_data is None:
-        print('No data RDMs passed. Omitting noise correction.'
-              + ' Data-model RDM distances will be positively biased.')
-        return 1
-    N = rdms_data.n_rdm
-    if method in ['corr', 'cosine']:
-        rdms = rdms_data.dissimilarities
-        if method == 'corr':
-            rdms -= rdms.mean(axis=1, keepdims=True)
-        rdms /= np.sqrt(np.einsum('ij,ij->i', rdms, rdms))[:, None]
-        mean_rdm = rdms.mean(axis=0, keepdims=True)
-        mean_rdm /= np.sqrt(np.sum(mean_rdm**2))
-        dist = np.sum((rdms - mean_rdm)**2) / (N-1)
-        print(
-            '\nFormula-based average distance: {:.4f}'.format(dist))
-        correction = 2 / (2 - dist)
-    else:
-        raise Exception(
-            'rsatoolbox.vis.map_model_comparison:'
-            + ' RDM comparison method must be "corr" or "cosine" for current implementation.')
-    return correction
+# -*- coding: utf-8 -*-
+"""
+Created on Thu Sep 17 17:09:00 2020
+"""
+
+import sys
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib import cm
+import scipy.spatial.distance as ssd
+import scipy.stats as sst
+from tqdm import trange
+
+from rsatoolbox.util.inference_util import get_errorbars, all_tests
+from rsatoolbox.util.rdm_utils import batch_to_vectors
+
+fs_small, fs, fs_large = 12, 18, 22
+fig_width, dpi = 10, 300
+
+# from matplotlib import rc
+# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})
+# rc('font',**{'family':'serif','serif':['Times']})
+# rc('text', usetex=True)
+
+
+def map_model_comparison(result, rdms_data=None,
+                         colors=None, alpha=0.01, test_type='t-test',
+                         test_pair_comparisons=True, bias_correction=True,
+                         multiple_pair_testing='fdr', test_above_0=True,
+                         test_below_noise_ceil=True, error_bars='ci99',
+                         label_orientation='tangential', fliplr=False,
+                         verbose=0):
+    """ Maps the model RDMs around the data RDM to reveal the pairwise
+    similarities among the model-predicted RDMs along with each
+    model-predicted RDM's similarity to the data RDM. The model map
+    additionally contains inferential information, indicating significant
+    differences among models in terms of their ability to explain the data and
+    significant deviations of the models from the data (noise floor).
+
+    Args (All strings case insensitive):
+        result (rsatoolbox.inference.result.Result):
+            model evaluation result
+            result.models (list of rsatoolbox.model objects): the models in the
+                order in which they were passed to the inference function to
+                produce the result argument. These need to be fixed RDM models.
+                (Flexible models will be handled in the near future.)
+        rdms_data (rsatoolbox.rdm.rdms.RDMs): single-subject data RDMs
+        colors (list of lists, numpy array, matplotlib colormap):
+            None (default): default blue for all bars
+            single color: list or numpy array of 3 or 4 values (RGB, RGBA)
+                specifying the color for all bars
+            multiple colors: list of lists or numpy array (number of colors by
+                3 or 4 channels -- RGB, RGBA). If the number of colors matches
+                the number of models, each color is used for the bar
+                corresponding to one model (in the order of the models as
+                passed). If the number of colors does not match the number of
+                models, the list is linearly interpolated to assign a color to
+                each model (in the order of the models as passed). For example,
+                two colors will become a gradation, unless there are exactly
+                two model. Instead of a list of lists or numpy array, a
+                matplotlib colormap object may also be passed (e.g. colors =
+                cm.coolwarm).
+        alpha (float):
+            significance threshold (p threshold or FDR q threshold)
+        test_pair_comparisons (Boolean or string):
+            False or None: do not plot pairwise model comparison results
+            True (default): plot pairwise model comparison results using
+                default settings
+        multiple_pair_testing (Boolean or string):
+            False or 'none': do not adjust for multiple testing for the
+                pairwise model comparisons
+            'FDR' or 'fdr' (default): control the false-discorvery rate at
+                q = alpha
+            'FWER',' fwer', or 'Bonferroni': control the familywise error rate
+            using the Bonferroni method
+        test_above_0 (Boolean or string):
+            False or None: do not plot results of statistical comparison of
+                each model performance against 0
+            True (default): plot results of statistical comparison of each
+                model performance against 0 using default settings ('dewdrops')
+            Tests are one-sided, use the global alpha threshold and are
+            automatically Bonferroni-corrected for the number of models tested.
+        test_below_noise_ceil (Boolean or string):
+            False or None: do not plot results of statistical comparison of
+                each model performance against the lower-bound estimate of the
+                noise ceiling
+            True (default): plot results of statistical comparison of each
+                model performance against the lower-bound estimate of the noise
+                ceiling using default settings
+            Tests are one-sided, use the global alpha threshold and are
+            automatically Bonferroni-corrected for the number of models tested.
+        error_bars (Boolean or string):
+            False or None: do not plot error bars
+            True (default) or 'SEM': plot the standard error of the mean
+            'CI': plot 95%-confidence intervals (exluding 2.5% on each side)
+            'CI[x]': plot x%-confidence intervals (exluding 2.5% on each side)
+            Confidence intervals are based on the bootstrap procedure,
+            reflecting variability of the estimate across subjects and/or
+            experimental conditions.
+        label_orientation (string): 'tangential' (default) for tangential model
+            labels, 'radial' for radial model labels.
+        fliplr (Boolean): If this is False (default) the arrangement will map
+            the first model to the left of the second model (in reading
+            direction). If this argument is True, the map will left-right
+            flipped about the vertical axis.
+
+    Returns:
+        ---
+
+    """
+    if result.cv_method == 'crossvalidation':
+        print('Tests deactivated because crossvalidation alone gives no uncertainty estimate.\n')
+        test_pair_comparisons = False
+        test_above_0 = False
+        test_below_noise_ceil = False
+    # %% Prepare and sort data
+    evaluations = result.evaluations
+    models = result.models
+    noise_ceiling = result.noise_ceiling
+    n_models = result.n_model
+    names = [m.name for m in models]
+
+    # average the bootstrap evaluations
+    while len(evaluations.shape) > 2:
+        # average across trailing dimensions
+        evaluations = np.nanmean(evaluations, axis=-1)
+    evaluations = evaluations[~np.isnan(evaluations[:, 0])]
+    noise_ceiling = np.array(noise_ceiling)
+    perf = np.mean(evaluations, axis=0)  # average across bootstrap samples
+    n_bootstraps = evaluations.shape[0]
+
+    if multiple_pair_testing is None:
+        multiple_pair_testing = 'uncorrected'
+
+    # run tests
+    if any([test_pair_comparisons, test_above_0, test_below_noise_ceil]):
+        p_pairwise, p_zero, p_noise = all_tests(
+            evaluations, noise_ceiling, test_type,
+            model_var=result.model_var, diff_var=result.diff_var,
+            noise_ceil_var=result.noise_ceil_var, dof=result.dof)
+
+    # %% Test each model RDM for relatedness to and distinctness from the data RDM
+    # test if model RDMs are significantly related to data RDM
+    # (above 0)
+    if test_above_0:  # one-sided test
+        model_significant = p_zero < alpha / n_models  # Bonferroni-corrected
+    else:
+        model_significant = None
+
+    # test if model RDMs are significantly distinct from the data RDM
+    # (below the noise ceiling's lower bound)
+    if test_below_noise_ceil:  # one-sided test
+        model_below_lower_bound = p_noise < alpha / n_models  # Bonferroni-corrected
+    else:
+        model_below_lower_bound = None
+
+    # %% Perform pairwise model comparisons
+    n_tests = int((n_models**2 - n_models) / 2)
+    if test_pair_comparisons:
+        if multiple_pair_testing.lower() == 'bonferroni' or \
+           multiple_pair_testing.lower() == 'fwer':
+            significant = p_pairwise < (alpha / n_tests)
+        elif multiple_pair_testing.lower() == 'fdr':
+            ps = batch_to_vectors(np.array([p_pairwise]))[0][0]
+            ps = np.sort(ps)
+            criterion = alpha * (np.arange(ps.shape[0]) + 1) / ps.shape[0]
+            k_ok = ps < criterion
+            if np.any(k_ok):
+                k_max = np.where(k_ok)[0][-1]
+                crit = criterion[k_max]
+            else:
+                crit = 0
+            significant = p_pairwise < crit
+        else:
+            if 'uncorrected' not in multiple_pair_testing.lower():
+                raise Exception(
+                    'plot_model_comparison: Argument ' +
+                    'multiple_pair_testing is incorrectly defined as ' +
+                    multiple_pair_testing + '.')
+            significant = p_pairwise < alpha
+    else:
+        significant = None
+
+    # %% Compute the errorbars
+    limits = get_errorbars(result.model_var, evaluations, result.dof, error_bars)
+    inference_descr = _get_description(
+        test_pair_comparisons, multiple_pair_testing, error_bars,
+        test_above_0, test_below_noise_ceil,
+        result.cv_method, result.method,
+        alpha, n_tests, n_models, n_bootstraps)
+    print(inference_descr)
+
+    # %% Compute data-model distance estimates
+    # We assume performance estimates p are correlations or cosines.
+    # We convert to correlation distance or cosine distance: 1-p.
+    # Correlation distance and cosine distance are porportional to squared
+    # Euclidean distances after the respective approriate pattern
+    # normalizations.
+    # For each model, we estimate the data-model distance as
+    # d = sqrt( (1-p) - (1-u) ) = sqrt(u-p),
+    # where p is the performance and u the upper bound of the noise ceiling.
+
+    noise_lower = np.nanmean(noise_ceiling[0])
+    noise_upper = np.nanmean(noise_ceiling[1])
+
+    # use upper noise ceiling as reference
+    # (conservative: distances will still be positively biased)
+    # data_model_dists = np.sqrt( 2 * (noise_upper - perf) )
+    # noise_halo_rad = np.sqrt( 2 * (noise_upper - noise_lower) )
+
+    # estimate true model-data distances
+    # (assuming noise and intersubject variation displace RDMs orthogonally to
+    # the model-brain RDM-space axis)
+    if verbose > 0:
+        print('\nEstimating the bias due to noise of the inter-RDM distance estimates...')
+    if bias_correction is None or bias_correction == 'none':
+        correction = 1 / noise_upper
+    elif bias_correction == 'bootstrap' or bias_correction:
+        correction = _correct_model_dist(rdms_data, method=result.method)
+    # DEBUG
+    # bias_of_sq_data_model_dist = 0  # no correction: upper bound is at the center (0)
+    # bias_of_sq_data_model_dist = 2*(noise_upper - noise_lower)  # collapse
+    # the noise ceiling: lower bound is at the center (0)
+
+    data_model_dists = np.sqrt(2 * np.maximum(
+        1 - correction * perf, np.finfo(float).eps))
+    noise_halo_rad = np.sqrt(2 * np.maximum(
+        1 - correction * noise_lower, np.finfo(float).eps))
+    errbar_dist_low = np.sqrt(2 * np.maximum(
+        1 - correction * (perf - limits[0]), np.finfo(float).eps))
+    errbar_dist_high = np.sqrt(2 * np.maximum(
+        1 - correction * (perf + limits[1]), np.finfo(float).eps))
+
+    # data_model_dists = np.sqrt(2 * (noise_upper - perf)
+    #                    - bias_of_sq_data_model_dist)
+    # noise_halo_rad = np.sqrt(2 * (noise_upper - noise_lower) -
+    #                    - bias_of_sq_data_model_dist)
+    # errbar_dist_low = np.sqrt(2 * (noise_upper - (perf - limits[0]))
+    #                    - bias_of_sq_data_model_dist)
+    # errbar_dist_high = np.sqrt(2 * (noise_upper - (perf + limits[1]))
+    #                    - bias_of_sq_data_model_dist)
+
+    eb_low_high = np.array((errbar_dist_low, errbar_dist_high))
+
+    # %% Compute intermodel distances
+    n_dissim = int(models[0].n_cond * (models[0].n_cond - 1) / 2)
+    modelRDMs = np.empty((n_models, n_dissim))
+    for idx, model_i in enumerate(models):
+        if rdms_data is not None:
+            theta = model_i.fit(rdms_data)
+            modelRDMs[idx, :] = model_i.predict(theta)
+        else:
+            modelRDMs[idx, :] = model_i.predict()
+
+    if result.method == 'corr':
+        modelRDMs = modelRDMs - modelRDMs.mean(axis=1, keepdims=True)
+        modelRDMs /= np.sqrt(np.einsum('ij,ij->i', modelRDMs, modelRDMs))[:, None]
+    elif result.method == 'cosine':
+        modelRDMs /= np.sqrt(np.einsum('ij,ij->i', modelRDMs, modelRDMs))[:, None]
+    intermodelDists = ssd.squareform(
+        ssd.pdist(modelRDMs, metric='euclidean'))
+    # the below yield identical results...
+    # intermodelDists2 = np.sqrt(2*(1 - np.einsum('ik,jk', modelRDMs, modelRDMs)))
+    # intermodelDists3 = ssd.squareform(np.sqrt(2*ssd.pdist(modelRDMs, metric='correlation')))
+
+    # %% Assemble second-order distance matrix (distances among RDMs)
+    rdm_dists = np.zeros((n_models + 1, n_models + 1))
+    rdm_dists[1:, 0] = data_model_dists
+    rdm_dists[0, 1:] = data_model_dists
+    rdm_dists[1:, 1:] = intermodelDists
+    rdm_dists[np.eye(n_models + 1) == 1] = 0
+
+    # optionally use squared distances (e.g. correlation distances, rather than their square roots)
+    # rdm_dists = rdm_dists**2
+
+    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
+    plt.imshow(rdm_dists, cmap='Greys')
+    plt.title(
+        'Matrix of inter-RDM distances\n(0: data RDM, 1..n: model RDMs)', fontsize=fs_large)
+    plt.colorbar()
+    plt.xticks(fontsize=fs_small)
+    plt.yticks(fontsize=fs_small)
+    plt.show()
+
+    # %% Perform MDS to map model RDMs around the data RDM
+    if verbose > 0:
+        print('\nPerforming MDS to map model RDMs around the data RDM, using ' +
+              'custom MDS...', flush=True)
+        sys.stdout.flush()
+    locs2d = custom_MDS(rdm_dists, n_init=10, n_iter=500, verbose=verbose)
+
+    # ensure canonical reflection
+    if bool(locs2d[1, 0] < locs2d[2, 0]) == fliplr:
+        locs2d[:, 0] *= -1
+
+    # show Shepard plot and rubberband plot of mapping distortions
+    r, r_model_data, Spearman_r_model_data = show_Shepard_plot(
+        locs2d, rdm_dists, colors)
+    print('Pearson r(RDM dist, 2d-map dist): {:.4f}'.format(r))
+    print(
+        'Pearson r(RDM dist, 2d-map dist) for model-data dists: {:.4f}'.format(r_model_data))
+    print(
+        'Spearman r(RDM dist, 2d-map dist) for model-data dists: {:.4f}'.format(
+            Spearman_r_model_data))
+    # show Elastic plot
+    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
+    plot_model_map_elastic(locs2d, rdm_dists, names, colors)
+    plt.show()
+
+    # compute scale bar
+    approx_frac_of_max = 0.15
+    dists_2d_vec = ssd.pdist(locs2d)
+    rdm_dists_vec = ssd.squareform(rdm_dists)
+    distortion_facs = dists_2d_vec / rdm_dists_vec
+    percent_covered = 100
+    prop_cut = (1 - percent_covered / 100) / 2
+    qnts = np.quantile(distortion_facs, [prop_cut, 1 - prop_cut], axis=0)
+    np.max(distortion_facs)
+    dist_2d_max = np.max(dists_2d_vec)
+    scalebar_length = round(approx_frac_of_max * dist_2d_max * 10) / 10
+    scalebar_descr = '{:.1f} [a.u.]\n'.format(scalebar_length)
+    scalebar_descr += ('normalized-pattern Eucl. dist.')
+    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
+    plot_model_map(locs2d, significant, model_significant, model_below_lower_bound,
+                   eb_low_high, noise_halo_rad, scalebar_length, scalebar_descr, qnts,
+                   colors, label_orientation, names)
+    plt.show()
+
+
+def plot_model_map(locs2d, significant, model_significant, model_below_lower_bound,
+                   eb_low_high, noise_halo_rad, scalebar_length, scalebar_descr, qnts,
+                   colors=None, label_orientation='tangential', names=None):
+    """ Plots the model map result illustration """
+    n_models = locs2d.shape[0] - 1
+    # Define the model colors
+    colors = _parse_colors(colors, n_models)
+    # Draw the map with error bars
+    rdm_dot_size = 200  # size of dots representing RDMs [pt]
+    noise_halo_col = [0.7, 0.7, 0.7, 1]  # color of the noise halo
+    orbit_col = [0.85, 0.85, 0.85, 0.4]   # color of the orbits
+    ns_lw = 6  # line width for non-significance cords and rays
+    ns_col = [0, 0, 0, 0.15]  # color for non-significance cords and rays
+
+    # prepare axes for map
+    l, b, w, h = 0.15, 0.15, 0.85, 0.85
+    ax = plt.axes((l, b, w, h))
+    r_max = np.max(np.sqrt(np.sum(np.array(locs2d)**2, axis=1)))
+    clearance_fac = 1.08  # factor by which the maximum radius is multiplied
+    # to determine the inner edge of the model labels...
+    rng = r_max * clearance_fac  # ...and minimum extent shown in the axes.
+    # plt.axis([-rng, rng, -rng, rng])
+    # ensure axes shows the largest orbit completely
+    plt.scatter([-rng, 0, rng, 0], [0, rng, 0, -rng], c='none')
+
+    # plot non-significance arches
+    if significant is not None:
+        for i in range(n_models - 1):
+            for j in range(i + 1, n_models):
+                if not significant[i, j]:
+                    # draw non-significance arch
+                    xi, yi = np.array(locs2d[i + 1]).squeeze()
+                    xj, yj = np.array(locs2d[j + 1]).squeeze()
+                    rad_i, rad_j = np.sqrt(xi**2 + yi**2), np.sqrt(xj**2 + yj**2)
+                    angle_i, angle_j = np.arctan2(xi, yi), np.arctan2(xj, yj)
+                    angle_diff = angle_j - angle_i  # pos if angle_j is greater
+                    if (angle_diff > 0 and abs(angle_diff) <= np.pi) or \
+                       (angle_diff <= 0 and abs(angle_diff) > np.pi):
+                        # clockwise from i to j is shorter
+                        angles = np.linspace(
+                            angle_i,
+                            angle_i + min(
+                                abs(angle_diff),
+                                2 * np.pi - abs(angle_diff)),
+                            360)
+                        radii = np.linspace(rad_i, rad_j, 360)
+                    else:
+                        # clockwise from j to i is shorter
+                        angles = np.linspace(
+                            angle_j, angle_j + min(
+                                abs(angle_diff),
+                                2 * np.pi - abs(angle_diff)),
+                            360)
+                        radii = np.linspace(rad_j, rad_i, 360)
+                    xx, yy = np.sin(angles) * radii, np.cos(angles) * radii
+                    plt.plot(xx, yy, color=ns_col, linewidth=ns_lw)
+
+    # plot orbits, relatedness and distinctness tests, error bars, and model
+    # labels
+    for model_i in range(n_models):
+        # orbits
+        v = locs2d[model_i + 1]
+        rad = np.sqrt(v * v.T)
+        orbit = plt.Circle((0, 0), rad, color='none', ec=orbit_col, lw=1, zorder=12)
+        ax.add_artist(orbit)
+        # plt.plot(v[0, 0], v[0, 1], ms=200, color='r')
+
+        # indicate whether model RDM is significantly related to data RDM
+        if model_significant is not None:
+            if not model_significant[model_i]:
+                x0, y0 = locs2d[model_i + 1, 0], locs2d[model_i + 1, 1]
+                x1, y1 = np.array((x0, y0)) / np.sqrt(np.sum(
+                    x0**2 + y0**2)) * r_max * clearance_fac * 0.98
+                # plot outer non-significance ray
+                plt.plot([x0, x1], [y0, y1], color=ns_col, linewidth=ns_lw)
+
+        # indicate whether model RDM is significantly distinct from data RDM
+        if model_below_lower_bound is not None:
+            if not model_below_lower_bound[model_i]:
+                x, y = locs2d[model_i + 1, 0], locs2d[model_i + 1, 1]
+                # plot outer non-significance ray
+                plt.plot([0, x], [0, y], color=ns_col, linewidth=ns_lw)
+
+        # error bars
+        vn = v / rad
+        edlx = vn[0, 0] * eb_low_high[:, model_i]
+        edly = vn[0, 1] * eb_low_high[:, model_i]
+        plt.plot(edlx, edly, color=colors[model_i], zorder=15)
+
+        # model labels
+        if names is not None:
+            tx = vn[0, 0] * r_max * clearance_fac
+            ty = vn[0, 1] * r_max * clearance_fac
+            if label_orientation == 'tangential':
+                angle_deg = (np.arctan2(vn[0, 1], vn[0, 0]) /
+                             np.pi * 180) % 180 - 90  # tangential
+                horAlign = 'center'
+                verAlign = 'bottom' if ty > 0 else 'top'
+            elif label_orientation == 'radial':
+                angle_deg = (np.arctan2(vn[0, 1], vn[0, 0]) /
+                             np.pi * 180 - 90) % 180 - 90  # radial
+                horAlign = 'right' if tx <= 1e-5 else 'left'
+                verAlign = 'center'
+            plt.text(tx, ty, names[model_i], va=verAlign, ha=horAlign,
+                     rotation_mode='anchor', family='sans-serif', size=fs,
+                     rotation=angle_deg)
+
+    # plot the data RDM with its noise halo
+    noise_halo = plt.Circle((0, 0), noise_halo_rad,
+                            color=noise_halo_col, zorder=5)
+    ax.add_artist(noise_halo)
+    plt.scatter(0, 0, s=rdm_dot_size, c='k', zorder=20)
+
+    # plot the model RDMs
+    x, y = np.array(locs2d[1:, 0]).squeeze(), np.array(locs2d[1:, 1]).squeeze()
+    plt.scatter(x, y, s=rdm_dot_size, c=colors, zorder=15)
+
+    # add a scalebar
+    plt.plot([-rng, -rng + scalebar_length],
+             [-rng, -rng], color='k', linewidth=6)
+    plt.plot([-rng + scalebar_length * qnts[0], -rng + scalebar_length * qnts[1]], [-rng, -rng],
+             color=[0.5, 0.5, 0.5], linewidth=2)
+    plt.text(-rng + 0.5 * scalebar_length, -rng * 1.02, scalebar_descr, va='top', ha='center',
+             family='sans-serif', size=fs_small)
+    plt.axis('equal')
+    plt.axis('off')
+
+
+def _parse_colors(colors, n_models):
+    """ parses a color argument into an array of RGB values
+    """
+    if colors is None:  # no color passed...
+        colors = np.array([0, 0.4, 0.9, 1])[None, :]  # use default blue
+    elif isinstance(colors, cm.colors.LinearSegmentedColormap):
+        cmap = cm.get_cmap(colors)
+        colors = cmap(np.linspace(0, 1, 100))[np.newaxis, :, :3].squeeze()
+    colors = np.array([np.array(col) for col in colors])
+    if colors.ndim == 1:  # one color passed...
+        n_col, n_chan = 1, colors.shape[0]
+        colors = colors.reshape(n_col, n_chan)
+    elif colors.ndim == 2 and colors.shape[0] == 1:
+        n_col, n_chan = colors.shape
+    else:  # multiple colors passed...
+        n_col, n_chan = colors.shape
+        if n_col == n_models:  # one color passed for each model...
+            cols2 = colors
+        else:  # number of colors passed does not match number of models...
+            # interpolate colors to define a color for each model
+            cols2 = np.empty((n_models, n_chan))
+            for c in range(n_chan):
+                cols2[:, c] = np.interp(np.arange(n_models),
+                                        np.arange(n_col) /
+                                        (n_col - 1) * (n_models - 1),
+                                        colors[:, c])
+        colors = cols2
+    # if there is no alpha channel, make opaque
+    if colors.shape[1] == 3:
+        colors = np.concatenate((colors, np.ones((colors.shape[0], 1))),
+                                axis=1)
+    if colors.shape[0] == 1:
+        colors = np.tile(colors, (n_models, 1))
+    return colors
+
+
+def _get_description(test_pair_comparisons, multiple_pair_testing, error_bars,
+                     test_above_0, test_below_noise_ceil,
+                     cv_method, method,
+                     alpha, n_tests, n_models, n_bootstraps):
+    inference_descr = ''
+    if test_pair_comparisons:
+        inference_descr += 'Model comparisons: two-tailed, '
+    if multiple_pair_testing.lower() == 'bonferroni' or \
+       multiple_pair_testing.lower() == 'fwer':
+        inference_descr += ('p < {:<.5g}'.format(alpha) +
+                            ', Bonferroni-corrected for ' +
+                            str(n_tests) +
+                            ' model-pair comparisons')
+    elif multiple_pair_testing.lower() == 'fdr':
+        inference_descr += ('FDR q < {:<.5g}'.format(alpha) +
+                            ' (' + str(n_tests) +
+                            ' model-pair comparisons)')
+    else:
+        inference_descr = (inference_descr +
+                           'p < {:<.5g}'.format(alpha) +
+                           ', uncorrected (' + str(n_tests) +
+                           ' model-pair comparisons)')
+    if cv_method in ['bootstrap_rdm', 'bootstrap_pattern', 'bootstrap_crossval']:
+        inference_descr = inference_descr + \
+            '\nInference by bootstrap resampling ' + \
+            '({:<,.0f}'.format(n_bootstraps) + ' bootstrap samples) of '
+    if cv_method == 'bootstrap_rdm':
+        inference_descr = inference_descr + 'subjects. '
+    elif cv_method == 'bootstrap_pattern':
+        inference_descr = inference_descr + 'experimental conditions. '
+    elif cv_method in ['bootstrap', 'bootstrap_crossval']:
+        inference_descr = inference_descr + \
+            'subjects and experimental conditions. '
+
+    # %% Print description of inferential methods
+    inference_descr += '\nError bars indicate the'
+    if error_bars[0:2].lower() == 'ci':
+        if len(error_bars) == 2:
+            CI_percent = 95.0
+        else:
+            CI_percent = float(error_bars[2:])
+        inference_descr += ' ' + str(CI_percent) + '% confidence interval.'
+    elif error_bars.lower() == 'sem':
+        inference_descr += ' standard error of the mean.'
+    if test_above_0 or test_below_noise_ceil:
+        inference_descr += '\nOne-sided comparisons of each model performance '
+    if test_above_0:
+        inference_descr += 'against 0 '
+    if test_above_0 and test_below_noise_ceil:
+        inference_descr += 'and '
+    if test_below_noise_ceil:
+        inference_descr += 'against the lower-bound estimate of the noise ceiling '
+    if test_above_0 or test_below_noise_ceil:
+        inference_descr += ('are Bonferroni-corrected for ' +
+                            str(n_models) + ' models.\n')
+    inference_descr += 'Inter-RDM distances were measured by the '
+    if method == 'corr':
+        inference_descr += (
+            'Pearson correlation distance '
+            + '(proportional to squared Euclidean distance'
+            + ' after RDM centering and divisive normalization).')
+    elif method == 'cosine':
+        inference_descr += (
+            'cosine distance '
+            + '(proportional to squared Euclidean distance after RDM divisive normalizaton). ')
+    else:
+        raise Exception('rsatoolbox.vis.map_model_comparison: result.method ' +
+                        method + ' not yet handled.')
+    inference_descr += 'Inter-RDM distances are mapped as '
+    inference_descr += (
+        'Euclidean distance '
+        + '(proportional to the square root of correlation or cosine distance'
+        + ' if RDMs were appropriately normalized).')
+    return inference_descr
+
+
+def custom_MDS(rdm_dists, n_init=100, n_iter=500, verbose=0):
+    """ Custom multidimensional scaling
+
+    Performs multidimensional scaling (MDS) using the metric stress cost
+    function (sum of squared distance deviations) for the intermodel RDM
+    distances while exactly preserving the model-data RDM distances.
+    Assumes that the data RDM has index 0 in the passed vectorized matrix of
+    RDM distances (second-order distances). The data RDM is placed at the
+    origin. The best fitting model is placed straight above it at the exact
+    RDM distance. Since the remaining models must be placed at radii exactly
+    matching the model-data RDM distances, only the angles are free parameters.
+    The remaining model-RDMs are placed in random order. Each is placed
+    by line search to minimize the sum of squared deviations from the RDM-
+    distance to the already placed models. After this initial round of
+    placements, each model (except the best one, which remains in its initial
+    position) is re-placed in random order. Models are adjusted until
+    convergence in random order. The entire process, including the
+    initialization, is repeated n_init times. The best arrangement (the one
+    with minimum sum of squared deviations) is returned.
+
+    """
+    # Preparations
+    n_models = rdm_dists.shape[0] - 1
+    best_model_i = np.argmin(rdm_dists[0, 1:])
+    other_model_is = [i for i in range(0, n_models) if i != best_model_i]
+    print(rdm_dists.shape)
+    rdm_dists_vec = ssd.squareform(rdm_dists)
+    ssqd_min = np.inf
+    two = 2  # minimizes sum(abs(errors)**two)
+
+    # Run repeatedly with random initialization
+    if verbose > 0:
+        iterator = trange(n_init)
+    else:
+        iterator = range(n_init)
+    for _ in iterator:
+        locs2d = np.full((n_models + 1, 2), np.NaN)
+        # place data RDM at the origin and the best model straight above it
+        locs2d[0] = 0, 0
+        # ...and best model straight above it
+        locs2d[best_model_i + 1] = 0, rdm_dists[0, best_model_i + 1]
+        # Initialize by locally optimal placing of each of the remaining models
+        # in random order
+        rand_perm_other_model_is = np.random.permutation(other_model_is)
+        for model_i in rand_perm_other_model_is:
+            locs2d[model_i + 1,
+                   :] = place_model(model_i, locs2d, rdm_dists, 1, two)
+        # Initialize by placing the remaining models at random angles
+        # (This random-angle approach is less reliable than the random-order initialization above.)
+        # for model_i in other_model_is:
+        #     angle = np.random.rand() * 2 * np.pi
+        #     rad = rdm_dists[0, model_i+1]
+        #     locs2d[model_i+1, :] = np.sin(angle) * rad, np.cos(angle) * rad
+
+        # Adjust each model (except the best) in random order
+        n_scales, n_scales_max = 1, 2
+        for iter_i in range(n_iter):
+            locs2d_prev = locs2d.copy()
+            rand_perm_other_model_is = np.random.permutation(other_model_is)
+            for model_i in rand_perm_other_model_is:
+                locs2d[model_i + 1,
+                       :] = place_model(model_i, locs2d, rdm_dists, n_scales, two)
+            if np.max(abs(locs2d_prev - locs2d)) < 1e-8:
+                if verbose > 0:
+                    print('MDS converged after {:.0f} iterations using {:.0f} scales.'.format(
+                        iter_i, n_scales))
+                n_scales += 1
+                if n_scales > n_scales_max:
+                    break
+        dists_2d = ssd.pdist(locs2d, metric='euclidean')
+        ssqd = np.sum(abs(rdm_dists_vec - dists_2d)**two)
+        if ssqd < ssqd_min:
+            ssqd_min = ssqd
+            # r = np.corrcoef(rdm_dists_vec, dists_2d)[0, 1]
+            r_0fixed = np.sum(rdm_dists_vec * dists_2d) \
+                / (np.sum(rdm_dists_vec ** 2) + np.sum(dists_2d ** 2))
+            locs2d_best = locs2d.copy()
+            if verbose > 0:
+                print(' SSQD: {:.4f}, corr_0fixed(RDM dist, map dist): {:.4f}'.format(
+                    ssqd, r_0fixed))
+        if iter_i == n_iter - 1:
+            print(' MDS did not converge. Doubling number of iterations.')
+            n_iter *= 2
+    return np.matrix(locs2d_best)
+
+
+def place_model(model_i, locs2d, rdm_dists, n_scales=3, two=2):
+    """ place a model
+
+    Args:
+        model_i : int
+            which model to update.
+        locs2d : np.array
+            The current positions in 2D space.
+        rdm_dists : np.array
+            The desired distances between models.
+        n_scales : int, optional
+            How many scales to test at. The default is 3.
+        two : TYPE, optional
+            Exponent for the error weighting. The default is 2.
+
+    """
+    n_angles = 180
+    n_models = rdm_dists.shape[0] - 1
+    radius = rdm_dists[0, model_i + 1]
+    start, stop = 0, 2 * np.pi
+
+    for _ in range(n_scales):
+        angles = np.linspace(start, stop, n_angles)
+        cand_locs = np.concatenate([np.sin(angles)[:, None], np.cos(angles)[:, None]], axis=1) \
+            * radius
+        dists_2d = np.sqrt(np.sum(
+            (cand_locs.reshape(n_angles, 1, 2) -
+             locs2d.reshape(1, n_models + 1, 2)) ** 2,
+            axis=2))
+        ssqd = np.nansum(abs(
+            dists_2d - rdm_dists[model_i + 1, :].reshape(1, n_models + 1)) ** two, axis=1)
+        best_angle_i = np.argmin(ssqd)
+        start, stop = angles[(best_angle_i + np.array([-1, 1])) % n_angles]
+        n_angles = int(n_angles / 3)
+
+    return cand_locs[best_angle_i, :]
+
+
+def plot_model_map_elastic(locs2d, rdm_dists, names, colors=None):
+    """ Plots the models with an indiciation how strongly their distances were
+    distorted.
+    """
+    # parsing input
+    n_models = locs2d.shape[0] - 1
+    dists_2d_vec = ssd.pdist(locs2d, metric='euclidean')
+    dists_2d = ssd.squareform(dists_2d_vec)
+    rdm_dists_vec = ssd.squareform(rdm_dists)
+    # Define the model colors
+    colors = _parse_colors(colors, n_models)
+    # distance-distortion plot
+    r_max = np.max(np.sqrt(np.sum(np.array(locs2d)**2, axis=1)))
+    rng = r_max * 1.2  # ...and minimum extent shown in the axes.
+    # plt.axis([-rng, rng, -rng, rng])
+    # ensure axes shows the largest orbit completely
+    plt.scatter([-rng, 0, rng, 0], [0, rng, 0, -rng], c='none')
+
+    # model labels
+    clearance_fac = 1.05  # factor by which the maximum radius is multiplied
+    for model_i in range(n_models):
+        v = locs2d[model_i + 1]
+        rad = np.sqrt(v * v.T)
+        vn = v / rad
+        tx = vn[0, 0] * r_max * clearance_fac
+        ty = vn[0, 1] * r_max * clearance_fac
+        angle_deg = (np.arctan2(vn[0, 1], vn[0, 0]) /
+                     np.pi * 180) % 180 - 90  # tangential
+        horAlign = 'center'
+        verAlign = 'bottom' if ty > 0 else 'top'
+        plt.text(tx, ty, names[model_i], va=verAlign, ha=horAlign,
+                 rotation_mode='anchor', family='sans-serif',
+                 size=fs, rotation=angle_deg)
+
+    # plot the data RDM
+    rdm_dot_size = 50
+    plt.scatter(0, 0, s=rdm_dot_size, c='k', zorder=20)
+
+    # plot the distortion-indicating rubberbands
+    stretched_col = np.array([0, 0.5, 1])
+    squeezed_col = np.array([0.8, 0, 0])
+    correct_col = np.array([0.5, 0.5, 0.5])
+    lw_undistorted = 3
+
+    # slackstringplot(rdm_dists)
+    for i in range(n_models - 1):
+        for j in range(i + 1, n_models):
+            distortion = dists_2d[i + 1, j + 1] - rdm_dists[i + 1, j + 1]
+            w = abs(distortion) / np.max(abs(dists_2d_vec - rdm_dists_vec))
+            if distortion <= 0:
+                col = w * squeezed_col + (1 - w) * correct_col
+            else:
+                col = w * stretched_col + (1 - w) * correct_col
+            # if abs(distortion) < 0.1:
+            #     col = correct_col
+            # else:
+            #     col = stretched_col if distortion > 0 else squeezed_col
+            lw = lw_undistorted / dists_2d[i +
+                                           1, j + 1] * rdm_dists[i + 1, j + 1]
+            plt.plot(locs2d[[i + 1, j + 1], 0], locs2d[[i + 1, j + 1], 1],
+                     color=col, linewidth=lw, solid_capstyle='round')
+
+    # plot the model RDMs
+    x, y = np.array(locs2d[1:, 0]).squeeze(), np.array(locs2d[1:, 1]).squeeze()
+    plt.scatter(x, y, s=rdm_dot_size, c=colors, zorder=10)
+
+    plt.title('Rubberband plot of inter-RDM-distance distortions\n' +
+              '(thin & blue: stretched, thick & red: squeezed)', fontsize=fs_large)
+    plt.axis('equal')
+    plt.axis('off')
+
+
+def show_Shepard_plot(locs2d, rdm_dists, colors=None):
+    """ Show shepard plot """
+    rdm_dists_vec = ssd.squareform(rdm_dists)
+    n_models = locs2d.shape[0] - 1
+    dists_2d_vec = ssd.pdist(locs2d, metric='euclidean')
+    mx = max(dists_2d_vec)
+    # Define the model colors
+    colors = _parse_colors(colors, n_models)
+
+    r = np.corrcoef(rdm_dists_vec, dists_2d_vec)[0, 1]
+    r_model_data = np.corrcoef(
+        rdm_dists_vec[:n_models], dists_2d_vec[:n_models])[0, 1]
+    Spearman_r_model_data = sst.spearmanr(rdm_dists_vec[:n_models],
+                                          dists_2d_vec[:n_models]).correlation
+
+    # Shepard plot
+    plt.figure(figsize=(fig_width, fig_width), dpi=dpi)
+    plt.plot((0, mx), (0, mx), color=[0.5, 0.5, 0.5, 0.5])
+    plt.plot(rdm_dists_vec, dists_2d_vec, '.', color=[0, 0, 0, 0.8],
+             mec=[1, 1, 1], ms=20, label='model-model distances')
+    plt.plot(rdm_dists_vec[:n_models], dists_2d_vec[:n_models], '.',
+             color=[0.8, 0, 0], mec=[1, 1, 1, 1], ms=20, label='model-data distances')
+    plt.title('Shepard plot\n(Pearson r: {:.3f}, Pearson r for model-data dists: {:.1f})'.format(
+        r, r_model_data), fontsize=fs_large)
+    plt.xlabel('distance between RDMs', fontsize=fs)
+    plt.ylabel('2d distance', fontsize=fs)
+    plt.xticks(fontsize=fs_small)
+    plt.yticks(fontsize=fs_small)
+    plt.axis([0, mx, 0, mx])
+    plt.gca().set_aspect('equal', adjustable='box')
+    plt.legend(fontsize=fs)
+
+    # plot distortion factor range
+    approx_frac_of_max = 0.15
+    distortion_facs = dists_2d_vec / rdm_dists_vec
+    percent_covered = 100
+    prop_cut = (1 - percent_covered / 100) / 2
+    qnts = np.quantile(distortion_facs, [prop_cut, 1 - prop_cut], axis=0)
+    dist_2d_max = np.max(dists_2d_vec)
+    scalebar_length = round(approx_frac_of_max * dist_2d_max * 10) / 10
+    plt.plot([0, mx], [0, mx * qnts[0]],
+             color=[0.5, 0.5, 0.5, 0.2], linewidth=1)
+    plt.plot([0, mx], [0, mx * qnts[1]],
+             color=[0.5, 0.5, 0.5, 0.2], linewidth=1)
+    plt.plot([scalebar_length, scalebar_length], [0, scalebar_length],
+             color=[0, 0, 0, 0.7], linewidth=6)
+    plt.plot([scalebar_length, scalebar_length],
+             [scalebar_length * qnts[0], scalebar_length * qnts[1]],
+             color=[0.5, 0.5, 0.5], linewidth=2)
+    return r, r_model_data, Spearman_r_model_data
+
+
+def _correct_model_dist(rdms_data, method='corr'):
+    if rdms_data is None:
+        print('No data RDMs passed. Omitting noise correction.'
+              + ' Data-model RDM distances will be positively biased.')
+        return 1
+    N = rdms_data.n_rdm
+    if method in ['corr', 'cosine']:
+        rdms = rdms_data.dissimilarities
+        if method == 'corr':
+            rdms -= rdms.mean(axis=1, keepdims=True)
+        rdms /= np.sqrt(np.einsum('ij,ij->i', rdms, rdms))[:, None]
+        mean_rdm = rdms.mean(axis=0, keepdims=True)
+        mean_rdm /= np.sqrt(np.sum(mean_rdm**2))
+        dist = np.sum((rdms - mean_rdm)**2) / (N-1)
+        print(
+            '\nFormula-based average distance: {:.4f}'.format(dist))
+        correction = 2 / (2 - dist)
+    else:
+        raise Exception(
+            'rsatoolbox.vis.map_model_comparison:'
+            + ' RDM comparison method must be "corr" or "cosine" for current implementation.')
+    return correction
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/model_plot.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/model_plot.py`

 * *Files identical despite different names*

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/rdm_comparison.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/rdm_comparison.py`

 * *Ordering differences only*

 * *Files 18% similar despite different names*

```diff
@@ -1,577 +1,577 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Created on 2020-09-17
-
-@author: caiw
-"""
-
-from typing import Tuple, List, Union, Dict, Optional
-
-import numpy as np
-from numpy import fill_diagonal, array
-from matplotlib import pyplot, rcParams
-from matplotlib.axes import Axes
-from matplotlib.figure import Figure
-from matplotlib.gridspec import GridSpec
-from scipy.spatial.distance import squareform
-from scipy.special import comb
-
-from rsatoolbox.rdm import RDMs
-from rsatoolbox.util.matrix import square_category_binary_mask, \
-    square_between_category_binary_mask
-from rsatoolbox.util.rdm_utils import category_condition_idxs
-
-
-_Colour = Tuple[float, float, float]
-
-_default_colour = "#107ab0"  # xkcd:nice blue
-_legend_linespacing = 0.02
-
-
-def rdm_comparison_scatterplot(rdms,
-                               show_marginal_distributions: bool = True,
-                               show_identity_line: bool = True,
-                               show_legend: bool = True,
-                               highlight_selector: Union[str, List[int]] = None,
-                               highlight_categories: List = None,
-                               colors: Dict[str, _Colour] = None,
-                               axlim: Tuple[float, float] = None,
-                               hist_bins: int = 30,
-                               ):
-    """
-    Plot dissimilarities for 2 or more RDMs
-
-    Args:
-        rdms (RDMs object or list-like of 2 RDMs objects):
-            If one RDMs object supplied, each RDM within is compared against
-            each other.
-            If two RDMs objects supplied (as list, tuple, etc.), each RDM in the
-            first is compared against each RDM in the second
-        show_marginal_distributions (bool):
-            True (default): Show marginal distributions.
-            False: Don't.
-        show_identity_line (bool):
-            True (default): Show identity line in each graph.
-            False: Don't.
-        show_legend (bool):
-            True (default): Show a coloured legend for highlighted groups.
-            False: Don't.
-            Only honoured alongside `highlight_categories`.
-        highlight_selector (Optional. str or List[int]):
-            EITHER: A RDMs.pattern_descriptor defining category labelling for
-                    conditions.
-                OR: A list of ints providing category labels for each condition.
-            If None or not supplied, no categories will be highlighted, in which
-            case `highlight_categories` must also be None.
-        highlight_categories (Optional. List):
-            List of category labels to highlight. Must be compatible with
-            `highlight_selector`.
-            Colours within each and between each pair of categories will be
-            highlighted.
-        colors: (Optional. Dict):
-            Dict mapping category labels to RGB 3-tuples of floats (values range
-            01).  Between-category colours will be interpolated midpoints
-            between category colours.
-            If None (the default), default colours will be selected.
-            Only used if `highlight_categories` is not None.
-        axlim (Optional. Tuple[float, float]):
-            Set the axis limits for the figure.
-            If None or not supplied, axis limits will be automatically
-            determined.
-        hist_bins (int, default 30):
-            The number of bins to use in the histogram.
-
-    Returns:
-        matplotlib.pyplot.Figure containing the scatter plot (not shown).
-
-    """
-
-    rdms_x, rdms_y = _handle_args_rdms(rdms)
-    category_idxs: Optional[Dict[str, List[int]]]
-    category_idxs = _handle_args_highlight_categories(highlight_selector,
-                                                      highlight_categories,
-                                                      rdms_x)
-    show_legend = _handle_args_legend(show_legend, highlight_categories)
-
-    if colors is None and highlight_categories is not None:
-        colors = {
-            highlight_category: _default_colour
-            for highlight_category in highlight_categories
-        }
-
-    n_rdms_x, n_rdms_y = len(rdms_x), len(rdms_y)
-
-    if show_legend:
-        legend_height = _legend_linespacing * (
-            # Within-category lines
-            len(highlight_selector) +
-            # Between-category lines
-            comb(len(highlight_selector), 2)
-        )
-    else:
-        legend_height = None
-    gridspec = _set_up_gridspec(n_rdms_x, n_rdms_y, show_marginal_distributions,
-                                legend_height)
-
-    fig: Figure = pyplot.figure(figsize=(8, 8))
-
-    # To share x and y axes when using gridspec you need to specify which axis
-    # to use as references. The reference axes will be those in the first column
-    # and those in the last row.
-    reference_axis = None
-    # Remember axes for scatter plots now so we can draw to them all later
-    scatter_axes: List[Axes] = []
-    for scatter_col_idx, rdm_for_col in enumerate(rdms_x):
-        is_leftmost_col = (scatter_col_idx == 0)
-        if show_marginal_distributions:
-            # distributions show in the first column, so need to bump the column
-            # index
-            scatter_col_idx += 1
-        # Since matplotlib ordering is left-to-right, top-to-bottom, we need to
-        # process the rows in reverse to get the correct reference axis.
-        for scatter_row_idx in reversed(range(n_rdms_y)):
-            is_bottom_row = (scatter_row_idx == n_rdms_y - 1)
-
-            # RDMs objects aren't iterators, so while we can do `for r in rdms`,
-            # we can't do `reversed(rdms)`.
-            # Hence we have to pull the rdm out by its index.
-            rdm_for_row = rdms_y[scatter_row_idx]
-
-            if reference_axis is None:
-                sub_axis: Axes = fig.add_subplot(gridspec[scatter_row_idx,
-                                                          scatter_col_idx])
-                reference_axis = sub_axis
-            else:
-                sub_axis: Axes = fig.add_subplot(gridspec[scatter_row_idx,
-                                                          scatter_col_idx],
-                                                 sharex=reference_axis,
-                                                 sharey=reference_axis)
-
-            _do_scatter_plot(sub_axis, rdm_for_row, rdm_for_col,
-                             highlight_categories, category_idxs,
-                             highlight_selector, colors)
-
-            if is_bottom_row:
-                sub_axis.set_xlabel(f"{rdm_for_col.rdm_descriptors['name'][0]}"
-                                    f" dissimilarity")
-            if is_leftmost_col:
-                sub_axis.set_ylabel(f"{rdm_for_row.rdm_descriptors['name'][0]}"
-                                    f" dissimilarity")
-
-            scatter_axes.append(sub_axis)
-
-            _format_sub_axes(sub_axis, is_bottom_row, is_leftmost_col)
-
-    if show_marginal_distributions:
-        _do_show_marginal_distributions(fig, reference_axis, gridspec,
-                                        rdms_x, rdms_y, hist_bins,
-                                        highlight_categories, category_idxs,
-                                        colors)
-
-    if show_identity_line:
-        _do_show_identity_line(reference_axis, scatter_axes)
-
-    if axlim is not None:
-        _set_axes_limits(axlim, reference_axis)
-
-    if show_legend:
-        _do_show_legend(highlight_categories, colors)
-
-    return fig
-
-
-def _handle_args_highlight_categories(highlight_category_selector,
-                                      highlight_categories,
-                                      reference_rdms
-                                      ) -> Optional[Dict[str, List[int]]]:
-    # Handle category highlighting args
-    _msg_arg_highlight = "Arguments `highlight_selector` and " \
-                         "`highlight_categories` must be compatible."
-    try:
-        if highlight_category_selector is None:
-            assert highlight_categories is None
-            # If we get here we'll never use this value, but we need to satisfy
-            # the static analyser that it's initialised under all code paths..
-            category_idxs = None
-        else:
-            assert highlight_categories is not None
-            category_idxs = category_condition_idxs(reference_rdms,
-                                                    highlight_category_selector)
-            assert all(c in category_idxs.keys() for c in highlight_categories)
-    except AssertionError as exc:
-        raise ValueError(_msg_arg_highlight) from exc
-    return category_idxs
-
-
-def _handle_args_legend(show_legend, highlight_categories) -> bool:
-    if show_legend:
-        if highlight_categories is None:
-            # Can't show the legend without highlighted categories
-            show_legend = False
-    return show_legend
-
-
-def _handle_args_rdms(rdms):
-    _msg_arg_rdms = "Argument `rdms` must be an RDMs or pair of RDMs objects."
-
-    rdms_x: RDMs  # RDM for the x-axis, or RDMs for facet columns
-    rdms_y: RDMs  # RDM for the y-axis, or RDMs for facet rows
-    try:
-        if isinstance(rdms, RDMs):
-            # 1 supplied
-            rdms_x, rdms_y = rdms, rdms
-        else:
-            # Check that only 2 supplied
-            assert len(rdms) == 2
-            rdms_x, rdms_y = rdms[0], rdms[1]
-        assert len(rdms_x) > 0
-        assert len(rdms_y) > 0
-    except TypeError as exc:
-        raise ValueError(_msg_arg_rdms) from exc
-    except AssertionError as exc:
-        raise ValueError(_msg_arg_rdms) from exc
-    return rdms_x, rdms_y
-
-
-def _format_sub_axes(sub_axis, is_bottom_row: bool, is_leftmost_col: bool):
-    # Square axes
-    # sub_axis.set_aspect('equal', adjustable='box')
-
-    # Hide the right and top spines
-    sub_axis.spines['right'].set_visible(False)
-    sub_axis.spines['top'].set_visible(False)
-
-    # Hide all but the outermost ticklabels
-    if not is_bottom_row:
-        pyplot.setp(sub_axis.get_xticklabels(), visible=False)
-    if not is_leftmost_col:
-        pyplot.setp(sub_axis.get_yticklabels(), visible=False)
-
-
-def _set_axes_limits(axlim, reference_axis):
-    reference_axis.set_xlim(axlim[0], axlim[1])
-    reference_axis.set_ylim(axlim[0], axlim[1])
-
-
-def _set_up_gridspec(n_rdms_x, n_rdms_y,
-                     show_marginal_distributions, legend_height):
-    grid_n_rows = n_rdms_y
-    grid_n_cols = n_rdms_x
-    grid_width_ratios = tuple(6 for _ in range(grid_n_cols))
-    grid_height_ratios = tuple(6 for _ in range(grid_n_rows))
-    if show_marginal_distributions:
-        # Add extra row & col for marginal distributions
-        grid_n_rows += 1
-        grid_n_cols += 1
-        grid_width_ratios = (1, *grid_width_ratios)
-        grid_height_ratios = (*grid_height_ratios, 1)
-    if legend_height is not None:
-        gridspec = GridSpec(
-            nrows=grid_n_rows,
-            ncols=grid_n_cols,
-            width_ratios=grid_width_ratios,
-            height_ratios=grid_height_ratios,
-            wspace=.3, hspace=.3,
-            top=1-_legend_linespacing, left=_legend_linespacing,
-            bottom=legend_height,
-        )
-    else:
-        gridspec = GridSpec(
-            nrows=grid_n_rows,
-            ncols=grid_n_cols,
-            width_ratios=grid_width_ratios,
-            height_ratios=grid_height_ratios,
-        )
-    return gridspec
-
-
-def _do_scatter_plot(sub_axis, rdm_for_row, rdm_for_col, highlight_categories,
-                     category_idxs, highlight_category_selector, colors):
-
-    # First plot dissimilarities within all stimuli
-    full_marker_size = rcParams["lines.markersize"] ** 2
-    sub_axis.scatter(x=rdm_for_col.get_vectors(),
-                     y=rdm_for_row.get_vectors(),
-                     color=_default_colour,
-                     s=full_marker_size,
-                     cmap=None)
-    if highlight_category_selector is not None:
-
-        within_category_idxs = _get_within_category_idxs(
-            highlight_categories=highlight_categories,
-            category_idxs=category_idxs,
-            n_cond=rdm_for_row.n_cond)
-
-        between_category_idxs = _get_between_category_idxs(
-            category_idxs=category_idxs,
-            highlight_categories=highlight_categories,
-            n_cond=rdm_for_row.n_cond)
-
-        dissims_within, dissims_between = _split_dissimilarities_within_between(
-            dissimilarities_for_row=rdm_for_row.get_vectors(),
-            dissimilarities_for_col=rdm_for_col.get_vectors(),
-            within_category_idxs=within_category_idxs,
-            between_category_idxs=between_category_idxs,
-        )
-
-        # Plot between highlighted categories
-        colours_between = _colours_between_categories(highlight_categories,
-                                                      colors)
-        for categories in between_category_idxs.keys():
-            sub_axis.scatter(x=dissims_between[categories][0],
-                             y=dissims_between[categories][1],
-                             color=colours_between[categories],
-                             # Slightly smaller, so the points for all still
-                             # shows
-                             s=full_marker_size * 0.5,
-                             cmap=None)
-
-        # Plot within highlighted categories
-        for category_name in within_category_idxs.keys():
-            sub_axis.scatter(x=dissims_within[category_name][0],
-                             y=dissims_within[category_name][1],
-                             color=colors[category_name],
-                             # Slightly smaller still, so the points for all and
-                             # between still show
-                             s=full_marker_size * 0.3,
-                             cmap=None)
-
-
-def _do_show_identity_line(reference_axis, scatter_axes):
-    for ax in scatter_axes:
-        # Prevent autoscale, else plotting from the origin causes the axes to
-        # rescale
-        ax.autoscale(False)
-        ax.plot([reference_axis.get_xlim()[0], reference_axis.get_xlim()[1]],
-                [reference_axis.get_ylim()[0], reference_axis.get_ylim()[1]],
-                # Grey line in the background
-                "0.5", zorder=-1)
-
-
-def _do_show_marginal_distributions(fig, reference_axis, gridspec,
-                                    rdms_x, rdms_y, hist_bins,
-                                    highlight_categories, category_idxs,
-                                    colors):
-
-    # Add marginal distributions along the x axis
-    reference_hist = None
-    for col_idx, rdm_for_col in enumerate(rdms_x):
-        if reference_hist is None:
-            hist_axis: Axes = fig.add_subplot(gridspec[-1, col_idx + 1],
-                                              sharex=reference_axis)
-            reference_hist = hist_axis
-        else:
-            hist_axis: Axes = fig.add_subplot(gridspec[-1, col_idx + 1],
-                                              sharex=reference_axis,
-                                              sharey=reference_hist)
-
-        # Plot all dissims
-        hist_axis.hist(rdm_for_col.get_vectors().flatten(),
-                       histtype='step',
-                       fill=False,
-                       orientation='vertical',
-                       bins=hist_bins,
-                       color=_default_colour)
-
-        if highlight_categories is not None:
-            # Plot within dissims
-            within_category_idxs = _get_within_category_idxs(
-                highlight_categories, category_idxs, rdm_for_col.n_cond)
-            for category_name, idxs in within_category_idxs.items():
-                hist_axis.hist(rdm_for_col.dissimilarities[idxs],
-                               histtype='step', fill=False,
-                               orientation='vertical', bins=hist_bins,
-                               color=colors[category_name])
-
-            # Plot between dissims
-            between_category_idxs = _get_between_category_idxs(
-                category_idxs, highlight_categories, rdm_for_col.n_cond)
-            colours_between = _colours_between_categories(highlight_categories,
-                                                          colors)
-            for categories, idxs in between_category_idxs.items():
-                hist_axis.hist(rdm_for_col.dissimilarities[idxs],
-                               histtype='step', fill=False,
-                               orientation='vertical', bins=hist_bins,
-                               color=colours_between[categories])
-
-        hist_axis.xaxis.set_visible(False)
-        hist_axis.yaxis.set_visible(False)
-        hist_axis.set_frame_on(False)
-    # Flip to pointing downwards
-    reference_hist.set_ylim(hist_axis.get_ylim()[::-1])
-
-    # Add marginal distributions along the y axis
-    reference_hist = None
-    for row_idx, rdm_for_row in enumerate(rdms_y):
-        if reference_hist is None:
-            hist_axis: Axes = fig.add_subplot(gridspec[row_idx, 0],
-                                              sharey=reference_axis)
-            reference_hist = hist_axis
-        else:
-            hist_axis: Axes = fig.add_subplot(gridspec[row_idx, 0],
-                                              sharey=reference_axis,
-                                              sharex=reference_hist)
-
-        # Plot all dissims
-        hist_axis.hist(rdm_for_row.get_vectors().flatten(), histtype='step',
-                       fill=False, orientation='horizontal',
-                       bins=hist_bins)
-
-        if highlight_categories is not None:
-            # Plot within dissims
-            within_category_idxs = _get_within_category_idxs(
-                highlight_categories, category_idxs, rdm_for_row.n_cond)
-            for category_name, idxs in within_category_idxs.items():
-                hist_axis.hist(rdm_for_row.dissimilarities[idxs],
-                               histtype='step', fill=False,
-                               orientation='horizontal', bins=hist_bins,
-                               color=colors[category_name])
-
-            # Plot between dissims
-            between_category_idxs = _get_between_category_idxs(
-                category_idxs, highlight_categories, rdm_for_row.n_cond)
-            colours_between = _colours_between_categories(
-                highlight_categories, colors)
-            for categories, idxs in between_category_idxs.items():
-                hist_axis.hist(rdm_for_row.dissimilarities[idxs],
-                               histtype='step', fill=False,
-                               orientation='horizontal', bins=hist_bins,
-                               color=colours_between[categories])
-
-        hist_axis.xaxis.set_visible(False)
-        hist_axis.yaxis.set_visible(False)
-        hist_axis.set_frame_on(False)
-    # Flip to pointing leftwards
-    reference_hist.set_xlim(hist_axis.get_xlim()[::-1])
-
-
-def _do_show_legend(highlight_categories, colors):
-    colours_between = _colours_between_categories(highlight_categories, colors)
-    legend_text = [("All dissimilarities", _default_colour)]
-    for category_name, colour in colors.items():
-        legend_text.append((f"Within-{category_name} dissimilarities", colour))
-    for categories, colour in colours_between.items():
-        assert len(categories) == 2
-        category_1, category_2 = tuple(categories)
-        legend_text.append((
-            f"Between {category_1}{category_2} dissimilarities",
-            colour
-        ))
-    line_i = 1
-    for t, c in sorted(legend_text, key=lambda p: p[0]):
-        pyplot.figtext(x=_legend_linespacing,
-                       y=(len(legend_text) - line_i + 1) * _legend_linespacing,
-                       s=t, color=c, horizontalalignment='left')
-        line_i += 1
-    pyplot.subplots_adjust(bottom=_legend_linespacing * (len(legend_text) + 1))
-
-
-def _get_within_category_idxs(
-        highlight_categories: List[str],
-        category_idxs: Dict[str, List[int]],
-        n_cond: int) -> Dict[str, List[int]]:
-
-    # category name -> [idxs]
-    idxs_within: Dict[str, List[int]] = {}
-
-    for category_name in highlight_categories:
-        # Get UTV binary mask for within-category dissims
-        square_mask = square_category_binary_mask(
-            category_idxs=category_idxs[category_name], size=n_cond)
-        # We don't use diagonal entries, but they must be 0 for squareform to
-        # work
-        fill_diagonal(square_mask, False)  # in place
-        idxs_within[category_name] = squareform(square_mask)[np.newaxis]
-
-    return idxs_within
-
-
-def _get_between_category_idxs(category_idxs, highlight_categories, n_cond
-                               ) -> Dict[frozenset, List[int]]:
-    # {category1, category2} -> [idxs]
-    idxs_between: Dict[frozenset, List[int]] = {}
-    exhausted_categories = []
-    for category_1_name in highlight_categories:
-        for category_2_name in highlight_categories:
-            # Don't do between a category and itself
-            if category_1_name == category_2_name:
-                continue
-            # Don't double-count between-category dissims; just restrict to UTV
-            if category_2_name in exhausted_categories:
-                continue
-
-            categories = frozenset({category_1_name, category_2_name})
-            idxs_between[categories] = squareform(
-                square_between_category_binary_mask(
-                    category_1_idxs=category_idxs[category_1_name],
-                    category_2_idxs=category_idxs[category_2_name],
-                    size=n_cond))[np.newaxis]
-        exhausted_categories.append(category_1_name)
-    return idxs_between
-
-
-def _split_dissimilarities_within_between(
-        dissimilarities_for_row: array,
-        dissimilarities_for_col: array,
-        within_category_idxs,
-        between_category_idxs):
-    """
-    Splits dissimilarities into within/between category dissimilarities for
-    highlighted categories.
-    """
-
-    # Within categories
-    # category name -> (xs, ys)
-    within_category_dissims: Dict[str, Tuple[List[float], List[float]]]
-    within_category_dissims = {
-        category_name: (
-            dissimilarities_for_col[idxs],  # x
-            dissimilarities_for_row[idxs],  # y
-        )
-        for category_name, idxs in within_category_idxs.items()
-    }
-
-    # Between categories
-    # {category1, category2} -> (xs, ys)
-    between_category_dissims: Dict[frozenset, Tuple[List[float], List[float]]]
-    between_category_dissims = {
-        categories: (
-                dissimilarities_for_col[idxs],  # x
-                dissimilarities_for_row[idxs],  # y
-        )
-        for categories, idxs in between_category_idxs.items()
-    }
-    return within_category_dissims, between_category_dissims
-
-
-def _colours_between_categories(highlight_categories, colours):
-
-    # {category1, category2} -> colour
-    between_category_colours: Dict[frozenset, _Colour] = {}
-
-    exhausted_categories = []
-    for category_1_name in highlight_categories:
-        for category_2_name in highlight_categories:
-            if category_1_name == category_2_name:
-                continue
-            if category_2_name in exhausted_categories:
-                continue
-            categories = frozenset({category_1_name, category_2_name})
-            between_category_colours[categories] = _blend_rgb_colours(
-                colours[category_1_name],
-                colours[category_2_name]
-            )
-        exhausted_categories.append(category_1_name)
-
-    return between_category_colours
-
-
-def _blend_rgb_colours(color, other_colour, method: str = "midpoint"):
-    if method == "midpoint":
-        return (
-            (color[0] + other_colour[0]) / 2,  # R
-            (color[1] + other_colour[1]) / 2,  # G
-            (color[2] + other_colour[2]) / 2,  # B
-        )
-    raise NotImplementedError()
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Created on 2020-09-17
+
+@author: caiw
+"""
+
+from typing import Tuple, List, Union, Dict, Optional
+
+import numpy as np
+from numpy import fill_diagonal, array
+from matplotlib import pyplot, rcParams
+from matplotlib.axes import Axes
+from matplotlib.figure import Figure
+from matplotlib.gridspec import GridSpec
+from scipy.spatial.distance import squareform
+from scipy.special import comb
+
+from rsatoolbox.rdm import RDMs
+from rsatoolbox.util.matrix import square_category_binary_mask, \
+    square_between_category_binary_mask
+from rsatoolbox.util.rdm_utils import category_condition_idxs
+
+
+_Colour = Tuple[float, float, float]
+
+_default_colour = "#107ab0"  # xkcd:nice blue
+_legend_linespacing = 0.02
+
+
+def rdm_comparison_scatterplot(rdms,
+                               show_marginal_distributions: bool = True,
+                               show_identity_line: bool = True,
+                               show_legend: bool = True,
+                               highlight_selector: Union[str, List[int]] = None,
+                               highlight_categories: List = None,
+                               colors: Dict[str, _Colour] = None,
+                               axlim: Tuple[float, float] = None,
+                               hist_bins: int = 30,
+                               ):
+    """
+    Plot dissimilarities for 2 or more RDMs
+
+    Args:
+        rdms (RDMs object or list-like of 2 RDMs objects):
+            If one RDMs object supplied, each RDM within is compared against
+            each other.
+            If two RDMs objects supplied (as list, tuple, etc.), each RDM in the
+            first is compared against each RDM in the second
+        show_marginal_distributions (bool):
+            True (default): Show marginal distributions.
+            False: Don't.
+        show_identity_line (bool):
+            True (default): Show identity line in each graph.
+            False: Don't.
+        show_legend (bool):
+            True (default): Show a coloured legend for highlighted groups.
+            False: Don't.
+            Only honoured alongside `highlight_categories`.
+        highlight_selector (Optional. str or List[int]):
+            EITHER: A RDMs.pattern_descriptor defining category labelling for
+                    conditions.
+                OR: A list of ints providing category labels for each condition.
+            If None or not supplied, no categories will be highlighted, in which
+            case `highlight_categories` must also be None.
+        highlight_categories (Optional. List):
+            List of category labels to highlight. Must be compatible with
+            `highlight_selector`.
+            Colours within each and between each pair of categories will be
+            highlighted.
+        colors: (Optional. Dict):
+            Dict mapping category labels to RGB 3-tuples of floats (values range
+            01).  Between-category colours will be interpolated midpoints
+            between category colours.
+            If None (the default), default colours will be selected.
+            Only used if `highlight_categories` is not None.
+        axlim (Optional. Tuple[float, float]):
+            Set the axis limits for the figure.
+            If None or not supplied, axis limits will be automatically
+            determined.
+        hist_bins (int, default 30):
+            The number of bins to use in the histogram.
+
+    Returns:
+        matplotlib.pyplot.Figure containing the scatter plot (not shown).
+
+    """
+
+    rdms_x, rdms_y = _handle_args_rdms(rdms)
+    category_idxs: Optional[Dict[str, List[int]]]
+    category_idxs = _handle_args_highlight_categories(highlight_selector,
+                                                      highlight_categories,
+                                                      rdms_x)
+    show_legend = _handle_args_legend(show_legend, highlight_categories)
+
+    if colors is None and highlight_categories is not None:
+        colors = {
+            highlight_category: _default_colour
+            for highlight_category in highlight_categories
+        }
+
+    n_rdms_x, n_rdms_y = len(rdms_x), len(rdms_y)
+
+    if show_legend:
+        legend_height = _legend_linespacing * (
+            # Within-category lines
+            len(highlight_selector) +
+            # Between-category lines
+            comb(len(highlight_selector), 2)
+        )
+    else:
+        legend_height = None
+    gridspec = _set_up_gridspec(n_rdms_x, n_rdms_y, show_marginal_distributions,
+                                legend_height)
+
+    fig: Figure = pyplot.figure(figsize=(8, 8))
+
+    # To share x and y axes when using gridspec you need to specify which axis
+    # to use as references. The reference axes will be those in the first column
+    # and those in the last row.
+    reference_axis = None
+    # Remember axes for scatter plots now so we can draw to them all later
+    scatter_axes: List[Axes] = []
+    for scatter_col_idx, rdm_for_col in enumerate(rdms_x):
+        is_leftmost_col = (scatter_col_idx == 0)
+        if show_marginal_distributions:
+            # distributions show in the first column, so need to bump the column
+            # index
+            scatter_col_idx += 1
+        # Since matplotlib ordering is left-to-right, top-to-bottom, we need to
+        # process the rows in reverse to get the correct reference axis.
+        for scatter_row_idx in reversed(range(n_rdms_y)):
+            is_bottom_row = (scatter_row_idx == n_rdms_y - 1)
+
+            # RDMs objects aren't iterators, so while we can do `for r in rdms`,
+            # we can't do `reversed(rdms)`.
+            # Hence we have to pull the rdm out by its index.
+            rdm_for_row = rdms_y[scatter_row_idx]
+
+            if reference_axis is None:
+                sub_axis: Axes = fig.add_subplot(gridspec[scatter_row_idx,
+                                                          scatter_col_idx])
+                reference_axis = sub_axis
+            else:
+                sub_axis: Axes = fig.add_subplot(gridspec[scatter_row_idx,
+                                                          scatter_col_idx],
+                                                 sharex=reference_axis,
+                                                 sharey=reference_axis)
+
+            _do_scatter_plot(sub_axis, rdm_for_row, rdm_for_col,
+                             highlight_categories, category_idxs,
+                             highlight_selector, colors)
+
+            if is_bottom_row:
+                sub_axis.set_xlabel(f"{rdm_for_col.rdm_descriptors['name'][0]}"
+                                    f" dissimilarity")
+            if is_leftmost_col:
+                sub_axis.set_ylabel(f"{rdm_for_row.rdm_descriptors['name'][0]}"
+                                    f" dissimilarity")
+
+            scatter_axes.append(sub_axis)
+
+            _format_sub_axes(sub_axis, is_bottom_row, is_leftmost_col)
+
+    if show_marginal_distributions:
+        _do_show_marginal_distributions(fig, reference_axis, gridspec,
+                                        rdms_x, rdms_y, hist_bins,
+                                        highlight_categories, category_idxs,
+                                        colors)
+
+    if show_identity_line:
+        _do_show_identity_line(reference_axis, scatter_axes)
+
+    if axlim is not None:
+        _set_axes_limits(axlim, reference_axis)
+
+    if show_legend:
+        _do_show_legend(highlight_categories, colors)
+
+    return fig
+
+
+def _handle_args_highlight_categories(highlight_category_selector,
+                                      highlight_categories,
+                                      reference_rdms
+                                      ) -> Optional[Dict[str, List[int]]]:
+    # Handle category highlighting args
+    _msg_arg_highlight = "Arguments `highlight_selector` and " \
+                         "`highlight_categories` must be compatible."
+    try:
+        if highlight_category_selector is None:
+            assert highlight_categories is None
+            # If we get here we'll never use this value, but we need to satisfy
+            # the static analyser that it's initialised under all code paths..
+            category_idxs = None
+        else:
+            assert highlight_categories is not None
+            category_idxs = category_condition_idxs(reference_rdms,
+                                                    highlight_category_selector)
+            assert all(c in category_idxs.keys() for c in highlight_categories)
+    except AssertionError as exc:
+        raise ValueError(_msg_arg_highlight) from exc
+    return category_idxs
+
+
+def _handle_args_legend(show_legend, highlight_categories) -> bool:
+    if show_legend:
+        if highlight_categories is None:
+            # Can't show the legend without highlighted categories
+            show_legend = False
+    return show_legend
+
+
+def _handle_args_rdms(rdms):
+    _msg_arg_rdms = "Argument `rdms` must be an RDMs or pair of RDMs objects."
+
+    rdms_x: RDMs  # RDM for the x-axis, or RDMs for facet columns
+    rdms_y: RDMs  # RDM for the y-axis, or RDMs for facet rows
+    try:
+        if isinstance(rdms, RDMs):
+            # 1 supplied
+            rdms_x, rdms_y = rdms, rdms
+        else:
+            # Check that only 2 supplied
+            assert len(rdms) == 2
+            rdms_x, rdms_y = rdms[0], rdms[1]
+        assert len(rdms_x) > 0
+        assert len(rdms_y) > 0
+    except TypeError as exc:
+        raise ValueError(_msg_arg_rdms) from exc
+    except AssertionError as exc:
+        raise ValueError(_msg_arg_rdms) from exc
+    return rdms_x, rdms_y
+
+
+def _format_sub_axes(sub_axis, is_bottom_row: bool, is_leftmost_col: bool):
+    # Square axes
+    # sub_axis.set_aspect('equal', adjustable='box')
+
+    # Hide the right and top spines
+    sub_axis.spines['right'].set_visible(False)
+    sub_axis.spines['top'].set_visible(False)
+
+    # Hide all but the outermost ticklabels
+    if not is_bottom_row:
+        pyplot.setp(sub_axis.get_xticklabels(), visible=False)
+    if not is_leftmost_col:
+        pyplot.setp(sub_axis.get_yticklabels(), visible=False)
+
+
+def _set_axes_limits(axlim, reference_axis):
+    reference_axis.set_xlim(axlim[0], axlim[1])
+    reference_axis.set_ylim(axlim[0], axlim[1])
+
+
+def _set_up_gridspec(n_rdms_x, n_rdms_y,
+                     show_marginal_distributions, legend_height):
+    grid_n_rows = n_rdms_y
+    grid_n_cols = n_rdms_x
+    grid_width_ratios = tuple(6 for _ in range(grid_n_cols))
+    grid_height_ratios = tuple(6 for _ in range(grid_n_rows))
+    if show_marginal_distributions:
+        # Add extra row & col for marginal distributions
+        grid_n_rows += 1
+        grid_n_cols += 1
+        grid_width_ratios = (1, *grid_width_ratios)
+        grid_height_ratios = (*grid_height_ratios, 1)
+    if legend_height is not None:
+        gridspec = GridSpec(
+            nrows=grid_n_rows,
+            ncols=grid_n_cols,
+            width_ratios=grid_width_ratios,
+            height_ratios=grid_height_ratios,
+            wspace=.3, hspace=.3,
+            top=1-_legend_linespacing, left=_legend_linespacing,
+            bottom=legend_height,
+        )
+    else:
+        gridspec = GridSpec(
+            nrows=grid_n_rows,
+            ncols=grid_n_cols,
+            width_ratios=grid_width_ratios,
+            height_ratios=grid_height_ratios,
+        )
+    return gridspec
+
+
+def _do_scatter_plot(sub_axis, rdm_for_row, rdm_for_col, highlight_categories,
+                     category_idxs, highlight_category_selector, colors):
+
+    # First plot dissimilarities within all stimuli
+    full_marker_size = rcParams["lines.markersize"] ** 2
+    sub_axis.scatter(x=rdm_for_col.get_vectors(),
+                     y=rdm_for_row.get_vectors(),
+                     color=_default_colour,
+                     s=full_marker_size,
+                     cmap=None)
+    if highlight_category_selector is not None:
+
+        within_category_idxs = _get_within_category_idxs(
+            highlight_categories=highlight_categories,
+            category_idxs=category_idxs,
+            n_cond=rdm_for_row.n_cond)
+
+        between_category_idxs = _get_between_category_idxs(
+            category_idxs=category_idxs,
+            highlight_categories=highlight_categories,
+            n_cond=rdm_for_row.n_cond)
+
+        dissims_within, dissims_between = _split_dissimilarities_within_between(
+            dissimilarities_for_row=rdm_for_row.get_vectors(),
+            dissimilarities_for_col=rdm_for_col.get_vectors(),
+            within_category_idxs=within_category_idxs,
+            between_category_idxs=between_category_idxs,
+        )
+
+        # Plot between highlighted categories
+        colours_between = _colours_between_categories(highlight_categories,
+                                                      colors)
+        for categories in between_category_idxs.keys():
+            sub_axis.scatter(x=dissims_between[categories][0],
+                             y=dissims_between[categories][1],
+                             color=colours_between[categories],
+                             # Slightly smaller, so the points for all still
+                             # shows
+                             s=full_marker_size * 0.5,
+                             cmap=None)
+
+        # Plot within highlighted categories
+        for category_name in within_category_idxs.keys():
+            sub_axis.scatter(x=dissims_within[category_name][0],
+                             y=dissims_within[category_name][1],
+                             color=colors[category_name],
+                             # Slightly smaller still, so the points for all and
+                             # between still show
+                             s=full_marker_size * 0.3,
+                             cmap=None)
+
+
+def _do_show_identity_line(reference_axis, scatter_axes):
+    for ax in scatter_axes:
+        # Prevent autoscale, else plotting from the origin causes the axes to
+        # rescale
+        ax.autoscale(False)
+        ax.plot([reference_axis.get_xlim()[0], reference_axis.get_xlim()[1]],
+                [reference_axis.get_ylim()[0], reference_axis.get_ylim()[1]],
+                # Grey line in the background
+                "0.5", zorder=-1)
+
+
+def _do_show_marginal_distributions(fig, reference_axis, gridspec,
+                                    rdms_x, rdms_y, hist_bins,
+                                    highlight_categories, category_idxs,
+                                    colors):
+
+    # Add marginal distributions along the x axis
+    reference_hist = None
+    for col_idx, rdm_for_col in enumerate(rdms_x):
+        if reference_hist is None:
+            hist_axis: Axes = fig.add_subplot(gridspec[-1, col_idx + 1],
+                                              sharex=reference_axis)
+            reference_hist = hist_axis
+        else:
+            hist_axis: Axes = fig.add_subplot(gridspec[-1, col_idx + 1],
+                                              sharex=reference_axis,
+                                              sharey=reference_hist)
+
+        # Plot all dissims
+        hist_axis.hist(rdm_for_col.get_vectors().flatten(),
+                       histtype='step',
+                       fill=False,
+                       orientation='vertical',
+                       bins=hist_bins,
+                       color=_default_colour)
+
+        if highlight_categories is not None:
+            # Plot within dissims
+            within_category_idxs = _get_within_category_idxs(
+                highlight_categories, category_idxs, rdm_for_col.n_cond)
+            for category_name, idxs in within_category_idxs.items():
+                hist_axis.hist(rdm_for_col.dissimilarities[idxs],
+                               histtype='step', fill=False,
+                               orientation='vertical', bins=hist_bins,
+                               color=colors[category_name])
+
+            # Plot between dissims
+            between_category_idxs = _get_between_category_idxs(
+                category_idxs, highlight_categories, rdm_for_col.n_cond)
+            colours_between = _colours_between_categories(highlight_categories,
+                                                          colors)
+            for categories, idxs in between_category_idxs.items():
+                hist_axis.hist(rdm_for_col.dissimilarities[idxs],
+                               histtype='step', fill=False,
+                               orientation='vertical', bins=hist_bins,
+                               color=colours_between[categories])
+
+        hist_axis.xaxis.set_visible(False)
+        hist_axis.yaxis.set_visible(False)
+        hist_axis.set_frame_on(False)
+    # Flip to pointing downwards
+    reference_hist.set_ylim(hist_axis.get_ylim()[::-1])
+
+    # Add marginal distributions along the y axis
+    reference_hist = None
+    for row_idx, rdm_for_row in enumerate(rdms_y):
+        if reference_hist is None:
+            hist_axis: Axes = fig.add_subplot(gridspec[row_idx, 0],
+                                              sharey=reference_axis)
+            reference_hist = hist_axis
+        else:
+            hist_axis: Axes = fig.add_subplot(gridspec[row_idx, 0],
+                                              sharey=reference_axis,
+                                              sharex=reference_hist)
+
+        # Plot all dissims
+        hist_axis.hist(rdm_for_row.get_vectors().flatten(), histtype='step',
+                       fill=False, orientation='horizontal',
+                       bins=hist_bins)
+
+        if highlight_categories is not None:
+            # Plot within dissims
+            within_category_idxs = _get_within_category_idxs(
+                highlight_categories, category_idxs, rdm_for_row.n_cond)
+            for category_name, idxs in within_category_idxs.items():
+                hist_axis.hist(rdm_for_row.dissimilarities[idxs],
+                               histtype='step', fill=False,
+                               orientation='horizontal', bins=hist_bins,
+                               color=colors[category_name])
+
+            # Plot between dissims
+            between_category_idxs = _get_between_category_idxs(
+                category_idxs, highlight_categories, rdm_for_row.n_cond)
+            colours_between = _colours_between_categories(
+                highlight_categories, colors)
+            for categories, idxs in between_category_idxs.items():
+                hist_axis.hist(rdm_for_row.dissimilarities[idxs],
+                               histtype='step', fill=False,
+                               orientation='horizontal', bins=hist_bins,
+                               color=colours_between[categories])
+
+        hist_axis.xaxis.set_visible(False)
+        hist_axis.yaxis.set_visible(False)
+        hist_axis.set_frame_on(False)
+    # Flip to pointing leftwards
+    reference_hist.set_xlim(hist_axis.get_xlim()[::-1])
+
+
+def _do_show_legend(highlight_categories, colors):
+    colours_between = _colours_between_categories(highlight_categories, colors)
+    legend_text = [("All dissimilarities", _default_colour)]
+    for category_name, colour in colors.items():
+        legend_text.append((f"Within-{category_name} dissimilarities", colour))
+    for categories, colour in colours_between.items():
+        assert len(categories) == 2
+        category_1, category_2 = tuple(categories)
+        legend_text.append((
+            f"Between {category_1}{category_2} dissimilarities",
+            colour
+        ))
+    line_i = 1
+    for t, c in sorted(legend_text, key=lambda p: p[0]):
+        pyplot.figtext(x=_legend_linespacing,
+                       y=(len(legend_text) - line_i + 1) * _legend_linespacing,
+                       s=t, color=c, horizontalalignment='left')
+        line_i += 1
+    pyplot.subplots_adjust(bottom=_legend_linespacing * (len(legend_text) + 1))
+
+
+def _get_within_category_idxs(
+        highlight_categories: List[str],
+        category_idxs: Dict[str, List[int]],
+        n_cond: int) -> Dict[str, List[int]]:
+
+    # category name -> [idxs]
+    idxs_within: Dict[str, List[int]] = {}
+
+    for category_name in highlight_categories:
+        # Get UTV binary mask for within-category dissims
+        square_mask = square_category_binary_mask(
+            category_idxs=category_idxs[category_name], size=n_cond)
+        # We don't use diagonal entries, but they must be 0 for squareform to
+        # work
+        fill_diagonal(square_mask, False)  # in place
+        idxs_within[category_name] = squareform(square_mask)[np.newaxis]
+
+    return idxs_within
+
+
+def _get_between_category_idxs(category_idxs, highlight_categories, n_cond
+                               ) -> Dict[frozenset, List[int]]:
+    # {category1, category2} -> [idxs]
+    idxs_between: Dict[frozenset, List[int]] = {}
+    exhausted_categories = []
+    for category_1_name in highlight_categories:
+        for category_2_name in highlight_categories:
+            # Don't do between a category and itself
+            if category_1_name == category_2_name:
+                continue
+            # Don't double-count between-category dissims; just restrict to UTV
+            if category_2_name in exhausted_categories:
+                continue
+
+            categories = frozenset({category_1_name, category_2_name})
+            idxs_between[categories] = squareform(
+                square_between_category_binary_mask(
+                    category_1_idxs=category_idxs[category_1_name],
+                    category_2_idxs=category_idxs[category_2_name],
+                    size=n_cond))[np.newaxis]
+        exhausted_categories.append(category_1_name)
+    return idxs_between
+
+
+def _split_dissimilarities_within_between(
+        dissimilarities_for_row: array,
+        dissimilarities_for_col: array,
+        within_category_idxs,
+        between_category_idxs):
+    """
+    Splits dissimilarities into within/between category dissimilarities for
+    highlighted categories.
+    """
+
+    # Within categories
+    # category name -> (xs, ys)
+    within_category_dissims: Dict[str, Tuple[List[float], List[float]]]
+    within_category_dissims = {
+        category_name: (
+            dissimilarities_for_col[idxs],  # x
+            dissimilarities_for_row[idxs],  # y
+        )
+        for category_name, idxs in within_category_idxs.items()
+    }
+
+    # Between categories
+    # {category1, category2} -> (xs, ys)
+    between_category_dissims: Dict[frozenset, Tuple[List[float], List[float]]]
+    between_category_dissims = {
+        categories: (
+                dissimilarities_for_col[idxs],  # x
+                dissimilarities_for_row[idxs],  # y
+        )
+        for categories, idxs in between_category_idxs.items()
+    }
+    return within_category_dissims, between_category_dissims
+
+
+def _colours_between_categories(highlight_categories, colours):
+
+    # {category1, category2} -> colour
+    between_category_colours: Dict[frozenset, _Colour] = {}
+
+    exhausted_categories = []
+    for category_1_name in highlight_categories:
+        for category_2_name in highlight_categories:
+            if category_1_name == category_2_name:
+                continue
+            if category_2_name in exhausted_categories:
+                continue
+            categories = frozenset({category_1_name, category_2_name})
+            between_category_colours[categories] = _blend_rgb_colours(
+                colours[category_1_name],
+                colours[category_2_name]
+            )
+        exhausted_categories.append(category_1_name)
+
+    return between_category_colours
+
+
+def _blend_rgb_colours(color, other_colour, method: str = "midpoint"):
+    if method == "midpoint":
+        return (
+            (color[0] + other_colour[0]) / 2,  # R
+            (color[1] + other_colour[1]) / 2,  # G
+            (color[2] + other_colour[2]) / 2,  # B
+        )
+    raise NotImplementedError()
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/rdm_plot.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/rdm_plot.py`

 * *Ordering differences only*

 * *Files 16% similar despite different names*

```diff
@@ -1,544 +1,544 @@
-#!/usr/bin/env python3
-# -*- coding: utf-8 -*-
-"""
-Plot showing an RDMs object
-"""
-
-from __future__ import annotations
-import collections
-from typing import TYPE_CHECKING, Union, Tuple, Optional
-import pkg_resources
-import numpy as np
-import matplotlib
-import matplotlib.pyplot as plt
-import rsatoolbox.rdm
-from rsatoolbox import vis
-from rsatoolbox.vis.colors import rdm_colormap_classic
-if TYPE_CHECKING:
-    import numpy.typing as npt
-    import pathlib
-    from matplotlib.axes._axes import Axes
-    from matplotlib.colors import Colormap
-
-RDM_STYLE = pkg_resources.resource_filename('rsatoolbox.vis', 'rdm.mplstyle')
-
-
-def show_rdm(
-    rdm: rsatoolbox.rdm.RDMs,
-    pattern_descriptor: str = None,
-    cmap: Union[str, Colormap] = 'bone',
-    rdm_descriptor: str = None,
-    n_column: int = None,
-    n_row: int = None,
-    show_colorbar: str = None,
-    gridlines: npt.ArrayLike = None,
-    num_pattern_groups: int = None,
-    figsize: Tuple[float, float] = None,
-    nanmask: npt.ArrayLike = None,
-    style: Union[str, pathlib.Path] = RDM_STYLE,
-    vmin: float = None,
-    vmax: float = None,
-    icon_spacing: float = 1.0,
-    linewidth: float = 0.5,
-) -> Tuple[
-    matplotlib.figure.Figure, npt.ArrayLike, collections.defaultdict
-]:
-    """show_rdm. Heatmap figure for RDMs instance, with one panel per RDM.
-
-    Args:
-        rdm (rsatoolbox.rdm.RDMs): RDMs object to be plotted.
-        pattern_descriptor (str): Key into rdm.pattern_descriptors to use for axis
-            labels.
-        cmap (str or Colormap): Colormap to be used.
-            Either the name of a Matplotlib built-in colormap, a Matplotlib
-            Colormap compatible object, or 'classic' for the matlab toolbox
-            colormap. Defaults to 'bone'.
-        rdm_descriptor (str): Key for rdm_descriptor to use as panel title, or
-            str for direct labeling.
-        n_column (int): Number of columns in subplot arrangement.
-        n_row (int): Number of rows in subplot arrangement.
-        show_colorbar (str): Set to 'panel' or 'figure' to display a colorbar. If
-            'panel' a colorbar is added next to each RDM. If 'figure' a shared colorbar
-            (and scale) is used across panels.
-        gridlines (npt.ArrayLike): Set to add gridlines at these positions. If
-            num_pattern_groups is defined this is used to infer gridlines.
-        num_pattern_groups (int): Number of rows/columns for any image labels. Also
-            determines gridlines frequency by default (so e.g., num_pattern_groups=3
-            with results in gridlines every 3 rows/columns).
-        figsize (Tuple[float, float]): mpl.Figure argument. By default we
-            auto-scale to achieve a figure that fits on a standard A4 / US Letter page
-            in portrait orientation.
-        nanmask (npt.ArrayLike): boolean mask defining RDM elements to suppress
-            (by default, the diagonals).
-        style (Union[str, pathlib.Path]): Path to mplstyle file that controls
-            various figure aesthetics (default rsatoolbox/vis/rdm.mplstyle).
-        vmin (float): Minimum intensity for colorbar mapping. matplotlib imshow
-            argument.
-        vmax (float): Maximum intensity for colorbar mapping. matplotlib imshow
-            argument.
-        icon_spacing (float): control spacing of image labels - 1. means no gap (the
-            default), 1.1 means pad 10%, .9 means overlap 10% etc.
-        linewidth (float): Width of connecting lines from icon labels (if used) to axis
-            margin.  The default is 0.5 - set to 0. to disable the lines.
-
-    Returns:
-        Tuple[matplotlib.figure.Figure, npt.ArrayLike, collections.defaultdict]:
-
-        Tuple of
-
-            - Handle to created figure.
-            - Subplot axis handles from plt.subplots.
-            - Nested dict containing handles to all other plotted
-              objects (icon labels, colorbars, etc). The keys at the first level are the
-              axis and figure handles.
-
-    """
-
-    if show_colorbar and show_colorbar not in ("panel", "figure"):
-        raise ValueError(
-            f"show_colorbar can be None, panel or figure, got: {show_colorbar}"
-        )
-    if nanmask is None:
-        nanmask = np.eye(rdm.n_cond, dtype=bool)
-    n_panel = rdm.n_rdm
-    if show_colorbar == "figure":
-        n_panel += 1
-        # need to keep track of global CB limits
-        if any(var is None for var in [vmin, vmax]):
-            # need to load the RDMs here (expensive)
-            rdmat = rdm.get_matrices()
-            if vmin is None:
-                vmin = rdmat[:, (nanmask == False)].min()
-            if vmax is None:
-                vmax = rdmat[:, (nanmask == False)].max()
-    if n_column is None and n_row is None:
-        n_column = np.ceil(np.sqrt(n_panel))
-    if n_row is None:
-        n_row = np.ceil(n_panel / n_column)
-    if n_column is None:
-        n_column = np.ceil(n_panel / n_row)
-    if (n_column * n_row) < rdm.n_rdm:
-        raise ValueError(
-            f"invalid n_row*n_column specification for {n_panel} rdms: {n_row}*{n_column}"
-        )
-    if figsize is None:
-        # scale with number of RDMs, up to a point (the intersection of A4 and us
-        # letter)
-        figsize = (min(2 * n_column, 8.3), min(2 * n_row, 11))
-    if not np.any(gridlines):
-        # empty list to disable gridlines
-        gridlines = []
-        if num_pattern_groups:
-            # grid by pattern groups if they exist and explicit grid setting does not
-            gridlines = np.arange(
-                num_pattern_groups - 0.5, rdm.n_cond + 0.5, num_pattern_groups
-            )
-    if num_pattern_groups is None or num_pattern_groups == 0:
-        num_pattern_groups = 1
-    # we don't necessarily have the same number of RDMs as panels, so need to stop the
-    # loop when we've plotted all the RDMs
-    rdms_gen = (this_rdm for this_rdm in rdm)
-    # return values are
-    # image, axis, colorbar, x_labels, y_labels
-    # some are global for figure, others local. Perhaps dicts indexed by axis is easiest
-    return_handles = collections.defaultdict(dict)
-    with plt.style.context(style):
-        fig, ax_array = plt.subplots(
-            nrows=int(n_row),
-            ncols=int(n_column),
-            sharex=True,
-            sharey=True,
-            squeeze=False,
-            figsize=figsize,
-        )
-        # reverse panel order so unfilled rows are at top instead of bottom
-        ax_array = ax_array[::-1]
-        for row_ind, row in enumerate(ax_array):
-            for col_ind, panel in enumerate(row):
-                try:
-                    return_handles[panel]["image"] = show_rdm_panel(
-                        next(rdms_gen),
-                        ax=panel,
-                        cmap=cmap,
-                        nanmask=nanmask,
-                        rdm_descriptor=rdm_descriptor,
-                        gridlines=gridlines,
-                        vmin=vmin,
-                        vmax=vmax,
-                    )
-                except StopIteration:
-                    # hide empty panels
-                    panel.set_visible(False)
-                    continue
-                if show_colorbar == "panel":
-                    # needs to happen before labels because it resizes the axis
-                    return_handles[panel]["colorbar"] = _rdm_colorbar(
-                        mappable=return_handles[panel]["image"],
-                        fig=fig,
-                        ax=panel,
-                        title=rdm.dissimilarity_measure,
-                    )
-                if col_ind == 0 and pattern_descriptor:
-                    return_handles[panel]["y_labels"] = add_descriptor_y_labels(
-                        rdm,
-                        pattern_descriptor,
-                        ax=panel,
-                        num_pattern_groups=num_pattern_groups,
-                        icon_spacing=icon_spacing,
-                        linewidth=linewidth,
-                    )
-                if row_ind == 0 and pattern_descriptor:
-                    return_handles[panel]["x_labels"] = add_descriptor_x_labels(
-                        rdm,
-                        pattern_descriptor,
-                        ax=panel,
-                        num_pattern_groups=num_pattern_groups,
-                        icon_spacing=icon_spacing,
-                        linewidth=linewidth,
-                    )
-        if show_colorbar == "figure":
-            # key challenge is to obtain a similarly-sized colorbar to the 'panel' case
-            # BUT positioned centered on the reserved subplot axes
-            cbax_parent = ax_array[-1, -1]
-            cbax_parent_orgpos = cbax_parent.get_position(original=True)
-            # use last instance of 'image' (should all be yoked at this point)
-            return_handles[fig]["colorbar"] = _rdm_colorbar(
-                mappable=return_handles[ax_array[0][0]]["image"],
-                fig=fig,
-                ax=cbax_parent,
-                title=rdm.dissimilarity_measure,
-            )
-            cbax_pos = return_handles[fig]["colorbar"].ax.get_position()
-            # halfway through panel, less the width/height of the colorbar itself
-            x0 = (
-                cbax_parent_orgpos.x0
-                + cbax_parent_orgpos.width / 2
-                - cbax_pos.width / 2
-            )
-            y0 = (
-                cbax_parent_orgpos.y0
-                + cbax_parent_orgpos.height / 2
-                - cbax_pos.height / 2
-            )
-            return_handles[fig]["colorbar"].ax.set_position(
-                [x0, y0, cbax_pos.width, cbax_pos.height]
-            )
-
-    return fig, ax_array, return_handles
-
-
-def _rdm_colorbar(
-    mappable: matplotlib.cm.ScalarMappable = None,
-    fig: matplotlib.figure.Figure = None,
-    ax: Axes = None,
-    title: str = None,
-) -> matplotlib.colorbar.Colorbar:
-    """_rdm_colorbar. Add vertically-oriented, small colorbar to rdm figure. Used
-    internally by show_rdm.
-
-    Args:
-        mappable (matplotlib.cm.ScalarMappable): Typically plt.imshow instance.
-        fig (matplotlib.figure.Figure): Matplotlib figure handle.
-        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
-        title (str): Title string for the colorbar (positioned top, left aligned).
-
-    Returns:
-        matplotlib.colorbar.Colorbar: Matplotlib handle.
-    """
-    cb = fig.colorbar(
-        mappable=mappable,
-        ax=ax,
-        shrink=0.25,
-        aspect=5,
-        ticks=matplotlib.ticker.LinearLocator(numticks=3),
-    )
-    cb.ax.set_title(title, loc="left", fontdict=dict(fontweight="normal"))
-    return cb
-
-
-def show_rdm_panel(
-    rdm: rsatoolbox.rdm.RDMs,
-    ax: Optional[Axes] = None,
-    cmap: Union[str, Colormap] = 'bone',
-    nanmask: npt.ArrayLike = None,
-    rdm_descriptor: str = None,
-    gridlines: npt.ArrayLike = None,
-    vmin: float = None,
-    vmax: float = None,
-) -> matplotlib.image.AxesImage:
-    """show_rdm_panel. Add RDM heatmap to the axis ax.
-
-    Args:
-        rdm (rsatoolbox.rdm.RDMs): RDMs object to be plotted (n_rdm must be 1).
-        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
-        cmap (str or Colormap): Colormap to be used.
-            Either the name of a Matplotlib built-in colormap, a Matplotlib
-            Colormap compatible object, or 'classic' for the matlab toolbox
-            colormap. Defaults to 'bone'.
-        nanmask (npt.ArrayLike): boolean mask defining RDM elements to suppress
-            (by default, the diagonals).
-        rdm_descriptor (str): Key for rdm_descriptor to use as panel title, or
-            str for direct labeling.
-        gridlines (npt.ArrayLike): Set to add gridlines at these positions.
-        vmin (float): Minimum intensity for colorbar mapping. matplotlib imshow
-            argument.
-        vmax (float): Maximum intensity for colorbar mapping. matplotlib imshow
-            argument.
-
-    Returns:
-        matplotlib.image.AxesImage: Matplotlib handle.
-    """
-    if rdm.n_rdm > 1:
-        raise ValueError("expected single rdm - use show_rdm for multi-panel figures")
-    if ax is None:
-        ax = plt.gca()
-    if cmap == 'classic':
-        cmap = rdm_colormap_classic()
-    if nanmask is None:
-        nanmask = np.eye(rdm.n_cond, dtype=bool)
-    if not np.any(gridlines):
-        gridlines = []
-    rdmat = rdm.get_matrices()[0, :, :]
-    if np.any(nanmask):
-        rdmat[nanmask] = np.nan
-    image = ax.imshow(
-        rdmat, cmap=cmap, vmin=vmin, vmax=vmax,
-        interpolation='none')
-    ax.set_xlim(-0.5, rdm.n_cond - 0.5)
-    ax.set_ylim(rdm.n_cond - 0.5, -0.5)
-    ax.xaxis.set_ticks(gridlines)
-    ax.yaxis.set_ticks(gridlines)
-    ax.xaxis.set_ticklabels([])
-    ax.yaxis.set_ticklabels([])
-    ax.xaxis.set_ticks(np.arange(rdm.n_cond), minor=True)
-    ax.yaxis.set_ticks(np.arange(rdm.n_cond), minor=True)
-    # hide minor ticks by default
-    ax.xaxis.set_tick_params(length=0, which="minor")
-    ax.yaxis.set_tick_params(length=0, which="minor")
-    if rdm_descriptor in rdm.rdm_descriptors:
-        ax.set_title(rdm.rdm_descriptors[rdm_descriptor][0])
-    else:
-        ax.set_title(rdm_descriptor)
-    return image
-
-
-def add_descriptor_x_labels(
-    rdm: rsatoolbox.rdm.RDMs,
-    pattern_descriptor: str,
-    ax: Axes = None,
-    num_pattern_groups: int = None,
-    icon_spacing: float = 1.0,
-    linewidth: float = 0.5,
-) -> list:
-    """add_descriptor_x_labels. Add labels to the X axis in ax by accessing the
-    rdm.pattern_descriptors dict with the pattern_descriptor key.
-
-    Args:
-        rdm (rsatoolbox.rdm.RDMs): RDMs instance to annotate.
-        pattern_descriptor (str): dict key for the rdm.pattern_descriptors dict.
-        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
-        num_pattern_groups (int): Number of rows/columns for any image labels.
-        icon_spacing (float): control spacing of image labels - 1. means no gap (the
-            default), 1.1 means pad 10%, .9 means overlap 10% etc.
-        linewidth (float): Width of connecting lines from icon labels (if used) to axis
-            margin.  The default is 0.5 - set to 0. to disable the lines.
-
-    Returns:
-        list: Tick label handles.
-    """
-    if ax is None:
-        ax = plt.gca()
-    return _add_descriptor_labels(
-        rdm,
-        pattern_descriptor,
-        "x_tick_label",
-        ax.xaxis,
-        num_pattern_groups=num_pattern_groups,
-        icon_spacing=icon_spacing,
-        linewidth=linewidth,
-        horizontalalignment="center",
-    )
-
-
-def add_descriptor_y_labels(
-    rdm: rsatoolbox.rdm.RDMs,
-    pattern_descriptor: str,
-    ax: Axes = None,
-    num_pattern_groups: int = None,
-    icon_spacing: float = 1.0,
-    linewidth: float = 0.5,
-) -> list:
-    """add_descriptor_y_labels. Add labels to the Y axis in ax by accessing the
-    rdm.pattern_descriptors dict with the pattern_descriptor key.
-
-    Args:
-        rdm (rsatoolbox.rdm.RDMs): RDMs instance to annotate.
-        pattern_descriptor (str): dict key for the rdm.pattern_descriptors dict.
-        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
-        num_pattern_groups (int): Number of rows/columns for any image labels.
-        icon_spacing (float): control spacing of image labels - 1. means no gap (the
-            default), 1.1 means pad 10%, .9 means overlap 10% etc.
-        linewidth (float): Width of connecting lines from icon labels (if used) to axis
-            margin.  The default is 0.5 - set to 0. to disable the lines.
-
-    Returns:
-        list: Tick label handles.
-    """
-    if ax is None:
-        ax = plt.gca()
-    return _add_descriptor_labels(
-        rdm,
-        pattern_descriptor,
-        "y_tick_label",
-        ax.yaxis,
-        num_pattern_groups=num_pattern_groups,
-        icon_spacing=icon_spacing,
-        linewidth=linewidth,
-        horizontalalignment="right",
-    )
-
-
-def _add_descriptor_labels(
-    rdm: rsatoolbox.rdm.RDMs,
-    pattern_descriptor: str,
-    icon_method: str,
-    axis: Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis],
-    num_pattern_groups: int = None,
-    icon_spacing: float = 1.0,
-    linewidth: float = 0.5,
-    horizontalalignment: str = "center",
-) -> list:
-    """_add_descriptor_labels. Used internally by add_descriptor_y_labels and
-    add_descriptor_x_labels.
-
-    Args:
-        rdm (rsatoolbox.rdm.RDMs): RDMs instance to annotate.
-        pattern_descriptor (str): dict key for the rdm.pattern_descriptors dict.
-        icon_method (str): method to access on Icon instances (typically y_tick_label or
-            x_tick_label).
-        axis (Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis]): Axis to add
-            tick labels to.
-        num_pattern_groups (int): Number of rows/columns for any image labels.
-        icon_spacing (float): control spacing of image labels - 1. means no gap (the
-            default), 1.1 means pad 10%, .9 means overlap 10% etc.
-        linewidth (float): Width of connecting lines from icon labels (if used) to axis
-            margin.  The default is 0.5 - set to 0. to disable the lines.
-        horizontalalignment (str): Horizontal alignment of text tick labels.
-
-    Returns:
-        list: Tick label handles.
-    """
-    descriptor_arr = np.asarray(rdm.pattern_descriptors[pattern_descriptor])
-    if isinstance(descriptor_arr[0], vis.Icon):
-        return _add_descriptor_icons(
-            descriptor_arr,
-            icon_method,
-            n_cond=rdm.n_cond,
-            ax=axis.axes,
-            icon_spacing=icon_spacing,
-            num_pattern_groups=num_pattern_groups,
-            linewidth=linewidth,
-        )
-    is_x_axis = "x" in icon_method
-    return _add_descriptor_text(
-        descriptor_arr,
-        axis=axis,
-        horizontalalignment=horizontalalignment,
-        is_x_axis=is_x_axis,
-    )
-
-
-def _add_descriptor_text(
-    descriptor_arr: npt.ArrayLike,
-    axis: Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis],
-    horizontalalignment: str = "center",
-    is_x_axis: bool = False,
-) -> list:
-    """_add_descriptor_text. Used internally by _add_descriptor_labels to add vanilla
-    Matplotlib-based text labels to the X or Y axis.
-
-    Args:
-        descriptor_arr (npt.ArrayLike): np.Array-like version of the labels.
-        axis (Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis]): handle for
-            the relevant axis (ax.xaxis or ax.yaxis).
-        horizontalalignment (str): Horizontal alignment of text tick labels.
-        is_x_axis (bool): If set, rotate the text labels 60 degrees to reduce overlap on
-            the X axis.
-
-    Returns:
-        list: Tick label handles.
-    """
-    # vanilla matplotlib-based
-    # need to ensure the minor ticks have some length
-    axis.set_tick_params(length=matplotlib.rcParams["xtick.minor.size"], which="minor")
-    label_handles = axis.set_ticklabels(
-        descriptor_arr,
-        verticalalignment="center",
-        horizontalalignment=horizontalalignment,
-        minor=True,
-    )
-    if is_x_axis:
-        plt.setp(
-            axis.get_ticklabels(minor=True),
-            rotation=60,
-            ha="right",
-            rotation_mode="anchor",
-        )
-    return label_handles
-
-
-def _add_descriptor_icons(
-    descriptor_arr: npt.ArrayLike,
-    icon_method: str,
-    n_cond: int,
-    ax: Axes = None,
-    num_pattern_groups: int = None,
-    icon_spacing: float = 1.0,
-    linewidth: float = 0.5,
-) -> list:
-    """_add_descriptor_icons. Used internally by _add_descriptor_labels to add
-    Icon-based labels to the X or Y axis.
-
-    Args:
-        descriptor_arr (npt.ArrayLike): np.Array-like version of the labels.
-        icon_method (str): method to access on Icon instances (typically y_tick_label or
-            x_tick_label).
-        n_cond (int): Number of conditions in the RDM (usually from RDMs.n_cond).
-        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle.
-        num_pattern_groups (int): Number of rows/columns for any image labels.
-        icon_spacing (float): control spacing of image labels - 1. means no gap (the
-            default), 1.1 means pad 10%, .9 means overlap 10% etc.
-        linewidth (float): Width of connecting lines from icon labels (if used) to axis
-            margin.  The default is 0.5 - set to 0. to disable the lines.
-
-    Returns:
-        list: Tick label handles.
-    """
-    # annotated labels with Icon
-    n_to_fit = np.ceil(n_cond / num_pattern_groups)
-    # work out sizing of icons
-    im_max_pix = 20.
-    if descriptor_arr[0].final_image:
-        # size by image
-        im_width_pix = max(this_desc.final_image.width for this_desc in descriptor_arr)
-        im_height_pix = max(this_desc.final_image.height for this_desc in descriptor_arr)
-        im_max_pix = max(im_width_pix, im_height_pix) * icon_spacing
-    ax.figure.canvas.draw()
-    extent = ax.get_window_extent(ax.figure.canvas.get_renderer())
-    ax_size_pix = max((extent.width, extent.height))
-    size = (ax_size_pix / n_to_fit) / im_max_pix
-    # from proportion of original size to figure pixels
-    offset = im_max_pix * size
-    label_handles = []
-    for group_ind in range(num_pattern_groups - 1, -1, -1):
-        position = offset * 0.2 + offset * group_ind
-        ticks = np.arange(group_ind, n_cond, num_pattern_groups)
-        label_handles.append(
-            [
-                getattr(this_desc, icon_method)(
-                    this_x, size, offset=position, linewidth=linewidth, ax=ax,
-                )
-                for (this_x, this_desc) in zip(ticks, descriptor_arr[ticks])
-            ]
-        )
-    return label_handles
+#!/usr/bin/env python3
+# -*- coding: utf-8 -*-
+"""
+Plot showing an RDMs object
+"""
+
+from __future__ import annotations
+import collections
+from typing import TYPE_CHECKING, Union, Tuple, Optional
+import pkg_resources
+import numpy as np
+import matplotlib
+import matplotlib.pyplot as plt
+import rsatoolbox.rdm
+from rsatoolbox import vis
+from rsatoolbox.vis.colors import rdm_colormap_classic
+if TYPE_CHECKING:
+    import numpy.typing as npt
+    import pathlib
+    from matplotlib.axes._axes import Axes
+    from matplotlib.colors import Colormap
+
+RDM_STYLE = pkg_resources.resource_filename('rsatoolbox.vis', 'rdm.mplstyle')
+
+
+def show_rdm(
+    rdm: rsatoolbox.rdm.RDMs,
+    pattern_descriptor: str = None,
+    cmap: Union[str, Colormap] = 'bone',
+    rdm_descriptor: str = None,
+    n_column: int = None,
+    n_row: int = None,
+    show_colorbar: str = None,
+    gridlines: npt.ArrayLike = None,
+    num_pattern_groups: int = None,
+    figsize: Tuple[float, float] = None,
+    nanmask: npt.ArrayLike = None,
+    style: Union[str, pathlib.Path] = RDM_STYLE,
+    vmin: float = None,
+    vmax: float = None,
+    icon_spacing: float = 1.0,
+    linewidth: float = 0.5,
+) -> Tuple[
+    matplotlib.figure.Figure, npt.ArrayLike, collections.defaultdict
+]:
+    """show_rdm. Heatmap figure for RDMs instance, with one panel per RDM.
+
+    Args:
+        rdm (rsatoolbox.rdm.RDMs): RDMs object to be plotted.
+        pattern_descriptor (str): Key into rdm.pattern_descriptors to use for axis
+            labels.
+        cmap (str or Colormap): Colormap to be used.
+            Either the name of a Matplotlib built-in colormap, a Matplotlib
+            Colormap compatible object, or 'classic' for the matlab toolbox
+            colormap. Defaults to 'bone'.
+        rdm_descriptor (str): Key for rdm_descriptor to use as panel title, or
+            str for direct labeling.
+        n_column (int): Number of columns in subplot arrangement.
+        n_row (int): Number of rows in subplot arrangement.
+        show_colorbar (str): Set to 'panel' or 'figure' to display a colorbar. If
+            'panel' a colorbar is added next to each RDM. If 'figure' a shared colorbar
+            (and scale) is used across panels.
+        gridlines (npt.ArrayLike): Set to add gridlines at these positions. If
+            num_pattern_groups is defined this is used to infer gridlines.
+        num_pattern_groups (int): Number of rows/columns for any image labels. Also
+            determines gridlines frequency by default (so e.g., num_pattern_groups=3
+            with results in gridlines every 3 rows/columns).
+        figsize (Tuple[float, float]): mpl.Figure argument. By default we
+            auto-scale to achieve a figure that fits on a standard A4 / US Letter page
+            in portrait orientation.
+        nanmask (npt.ArrayLike): boolean mask defining RDM elements to suppress
+            (by default, the diagonals).
+        style (Union[str, pathlib.Path]): Path to mplstyle file that controls
+            various figure aesthetics (default rsatoolbox/vis/rdm.mplstyle).
+        vmin (float): Minimum intensity for colorbar mapping. matplotlib imshow
+            argument.
+        vmax (float): Maximum intensity for colorbar mapping. matplotlib imshow
+            argument.
+        icon_spacing (float): control spacing of image labels - 1. means no gap (the
+            default), 1.1 means pad 10%, .9 means overlap 10% etc.
+        linewidth (float): Width of connecting lines from icon labels (if used) to axis
+            margin.  The default is 0.5 - set to 0. to disable the lines.
+
+    Returns:
+        Tuple[matplotlib.figure.Figure, npt.ArrayLike, collections.defaultdict]:
+
+        Tuple of
+
+            - Handle to created figure.
+            - Subplot axis handles from plt.subplots.
+            - Nested dict containing handles to all other plotted
+              objects (icon labels, colorbars, etc). The keys at the first level are the
+              axis and figure handles.
+
+    """
+
+    if show_colorbar and show_colorbar not in ("panel", "figure"):
+        raise ValueError(
+            f"show_colorbar can be None, panel or figure, got: {show_colorbar}"
+        )
+    if nanmask is None:
+        nanmask = np.eye(rdm.n_cond, dtype=bool)
+    n_panel = rdm.n_rdm
+    if show_colorbar == "figure":
+        n_panel += 1
+        # need to keep track of global CB limits
+        if any(var is None for var in [vmin, vmax]):
+            # need to load the RDMs here (expensive)
+            rdmat = rdm.get_matrices()
+            if vmin is None:
+                vmin = rdmat[:, (nanmask == False)].min()
+            if vmax is None:
+                vmax = rdmat[:, (nanmask == False)].max()
+    if n_column is None and n_row is None:
+        n_column = np.ceil(np.sqrt(n_panel))
+    if n_row is None:
+        n_row = np.ceil(n_panel / n_column)
+    if n_column is None:
+        n_column = np.ceil(n_panel / n_row)
+    if (n_column * n_row) < rdm.n_rdm:
+        raise ValueError(
+            f"invalid n_row*n_column specification for {n_panel} rdms: {n_row}*{n_column}"
+        )
+    if figsize is None:
+        # scale with number of RDMs, up to a point (the intersection of A4 and us
+        # letter)
+        figsize = (min(2 * n_column, 8.3), min(2 * n_row, 11))
+    if not np.any(gridlines):
+        # empty list to disable gridlines
+        gridlines = []
+        if num_pattern_groups:
+            # grid by pattern groups if they exist and explicit grid setting does not
+            gridlines = np.arange(
+                num_pattern_groups - 0.5, rdm.n_cond + 0.5, num_pattern_groups
+            )
+    if num_pattern_groups is None or num_pattern_groups == 0:
+        num_pattern_groups = 1
+    # we don't necessarily have the same number of RDMs as panels, so need to stop the
+    # loop when we've plotted all the RDMs
+    rdms_gen = (this_rdm for this_rdm in rdm)
+    # return values are
+    # image, axis, colorbar, x_labels, y_labels
+    # some are global for figure, others local. Perhaps dicts indexed by axis is easiest
+    return_handles = collections.defaultdict(dict)
+    with plt.style.context(style):
+        fig, ax_array = plt.subplots(
+            nrows=int(n_row),
+            ncols=int(n_column),
+            sharex=True,
+            sharey=True,
+            squeeze=False,
+            figsize=figsize,
+        )
+        # reverse panel order so unfilled rows are at top instead of bottom
+        ax_array = ax_array[::-1]
+        for row_ind, row in enumerate(ax_array):
+            for col_ind, panel in enumerate(row):
+                try:
+                    return_handles[panel]["image"] = show_rdm_panel(
+                        next(rdms_gen),
+                        ax=panel,
+                        cmap=cmap,
+                        nanmask=nanmask,
+                        rdm_descriptor=rdm_descriptor,
+                        gridlines=gridlines,
+                        vmin=vmin,
+                        vmax=vmax,
+                    )
+                except StopIteration:
+                    # hide empty panels
+                    panel.set_visible(False)
+                    continue
+                if show_colorbar == "panel":
+                    # needs to happen before labels because it resizes the axis
+                    return_handles[panel]["colorbar"] = _rdm_colorbar(
+                        mappable=return_handles[panel]["image"],
+                        fig=fig,
+                        ax=panel,
+                        title=rdm.dissimilarity_measure,
+                    )
+                if col_ind == 0 and pattern_descriptor:
+                    return_handles[panel]["y_labels"] = add_descriptor_y_labels(
+                        rdm,
+                        pattern_descriptor,
+                        ax=panel,
+                        num_pattern_groups=num_pattern_groups,
+                        icon_spacing=icon_spacing,
+                        linewidth=linewidth,
+                    )
+                if row_ind == 0 and pattern_descriptor:
+                    return_handles[panel]["x_labels"] = add_descriptor_x_labels(
+                        rdm,
+                        pattern_descriptor,
+                        ax=panel,
+                        num_pattern_groups=num_pattern_groups,
+                        icon_spacing=icon_spacing,
+                        linewidth=linewidth,
+                    )
+        if show_colorbar == "figure":
+            # key challenge is to obtain a similarly-sized colorbar to the 'panel' case
+            # BUT positioned centered on the reserved subplot axes
+            cbax_parent = ax_array[-1, -1]
+            cbax_parent_orgpos = cbax_parent.get_position(original=True)
+            # use last instance of 'image' (should all be yoked at this point)
+            return_handles[fig]["colorbar"] = _rdm_colorbar(
+                mappable=return_handles[ax_array[0][0]]["image"],
+                fig=fig,
+                ax=cbax_parent,
+                title=rdm.dissimilarity_measure,
+            )
+            cbax_pos = return_handles[fig]["colorbar"].ax.get_position()
+            # halfway through panel, less the width/height of the colorbar itself
+            x0 = (
+                cbax_parent_orgpos.x0
+                + cbax_parent_orgpos.width / 2
+                - cbax_pos.width / 2
+            )
+            y0 = (
+                cbax_parent_orgpos.y0
+                + cbax_parent_orgpos.height / 2
+                - cbax_pos.height / 2
+            )
+            return_handles[fig]["colorbar"].ax.set_position(
+                [x0, y0, cbax_pos.width, cbax_pos.height]
+            )
+
+    return fig, ax_array, return_handles
+
+
+def _rdm_colorbar(
+    mappable: matplotlib.cm.ScalarMappable = None,
+    fig: matplotlib.figure.Figure = None,
+    ax: Axes = None,
+    title: str = None,
+) -> matplotlib.colorbar.Colorbar:
+    """_rdm_colorbar. Add vertically-oriented, small colorbar to rdm figure. Used
+    internally by show_rdm.
+
+    Args:
+        mappable (matplotlib.cm.ScalarMappable): Typically plt.imshow instance.
+        fig (matplotlib.figure.Figure): Matplotlib figure handle.
+        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
+        title (str): Title string for the colorbar (positioned top, left aligned).
+
+    Returns:
+        matplotlib.colorbar.Colorbar: Matplotlib handle.
+    """
+    cb = fig.colorbar(
+        mappable=mappable,
+        ax=ax,
+        shrink=0.25,
+        aspect=5,
+        ticks=matplotlib.ticker.LinearLocator(numticks=3),
+    )
+    cb.ax.set_title(title, loc="left", fontdict=dict(fontweight="normal"))
+    return cb
+
+
+def show_rdm_panel(
+    rdm: rsatoolbox.rdm.RDMs,
+    ax: Optional[Axes] = None,
+    cmap: Union[str, Colormap] = 'bone',
+    nanmask: npt.ArrayLike = None,
+    rdm_descriptor: str = None,
+    gridlines: npt.ArrayLike = None,
+    vmin: float = None,
+    vmax: float = None,
+) -> matplotlib.image.AxesImage:
+    """show_rdm_panel. Add RDM heatmap to the axis ax.
+
+    Args:
+        rdm (rsatoolbox.rdm.RDMs): RDMs object to be plotted (n_rdm must be 1).
+        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
+        cmap (str or Colormap): Colormap to be used.
+            Either the name of a Matplotlib built-in colormap, a Matplotlib
+            Colormap compatible object, or 'classic' for the matlab toolbox
+            colormap. Defaults to 'bone'.
+        nanmask (npt.ArrayLike): boolean mask defining RDM elements to suppress
+            (by default, the diagonals).
+        rdm_descriptor (str): Key for rdm_descriptor to use as panel title, or
+            str for direct labeling.
+        gridlines (npt.ArrayLike): Set to add gridlines at these positions.
+        vmin (float): Minimum intensity for colorbar mapping. matplotlib imshow
+            argument.
+        vmax (float): Maximum intensity for colorbar mapping. matplotlib imshow
+            argument.
+
+    Returns:
+        matplotlib.image.AxesImage: Matplotlib handle.
+    """
+    if rdm.n_rdm > 1:
+        raise ValueError("expected single rdm - use show_rdm for multi-panel figures")
+    if ax is None:
+        ax = plt.gca()
+    if cmap == 'classic':
+        cmap = rdm_colormap_classic()
+    if nanmask is None:
+        nanmask = np.eye(rdm.n_cond, dtype=bool)
+    if not np.any(gridlines):
+        gridlines = []
+    rdmat = rdm.get_matrices()[0, :, :]
+    if np.any(nanmask):
+        rdmat[nanmask] = np.nan
+    image = ax.imshow(
+        rdmat, cmap=cmap, vmin=vmin, vmax=vmax,
+        interpolation='none')
+    ax.set_xlim(-0.5, rdm.n_cond - 0.5)
+    ax.set_ylim(rdm.n_cond - 0.5, -0.5)
+    ax.xaxis.set_ticks(gridlines)
+    ax.yaxis.set_ticks(gridlines)
+    ax.xaxis.set_ticklabels([])
+    ax.yaxis.set_ticklabels([])
+    ax.xaxis.set_ticks(np.arange(rdm.n_cond), minor=True)
+    ax.yaxis.set_ticks(np.arange(rdm.n_cond), minor=True)
+    # hide minor ticks by default
+    ax.xaxis.set_tick_params(length=0, which="minor")
+    ax.yaxis.set_tick_params(length=0, which="minor")
+    if rdm_descriptor in rdm.rdm_descriptors:
+        ax.set_title(rdm.rdm_descriptors[rdm_descriptor][0])
+    else:
+        ax.set_title(rdm_descriptor)
+    return image
+
+
+def add_descriptor_x_labels(
+    rdm: rsatoolbox.rdm.RDMs,
+    pattern_descriptor: str,
+    ax: Axes = None,
+    num_pattern_groups: int = None,
+    icon_spacing: float = 1.0,
+    linewidth: float = 0.5,
+) -> list:
+    """add_descriptor_x_labels. Add labels to the X axis in ax by accessing the
+    rdm.pattern_descriptors dict with the pattern_descriptor key.
+
+    Args:
+        rdm (rsatoolbox.rdm.RDMs): RDMs instance to annotate.
+        pattern_descriptor (str): dict key for the rdm.pattern_descriptors dict.
+        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
+        num_pattern_groups (int): Number of rows/columns for any image labels.
+        icon_spacing (float): control spacing of image labels - 1. means no gap (the
+            default), 1.1 means pad 10%, .9 means overlap 10% etc.
+        linewidth (float): Width of connecting lines from icon labels (if used) to axis
+            margin.  The default is 0.5 - set to 0. to disable the lines.
+
+    Returns:
+        list: Tick label handles.
+    """
+    if ax is None:
+        ax = plt.gca()
+    return _add_descriptor_labels(
+        rdm,
+        pattern_descriptor,
+        "x_tick_label",
+        ax.xaxis,
+        num_pattern_groups=num_pattern_groups,
+        icon_spacing=icon_spacing,
+        linewidth=linewidth,
+        horizontalalignment="center",
+    )
+
+
+def add_descriptor_y_labels(
+    rdm: rsatoolbox.rdm.RDMs,
+    pattern_descriptor: str,
+    ax: Axes = None,
+    num_pattern_groups: int = None,
+    icon_spacing: float = 1.0,
+    linewidth: float = 0.5,
+) -> list:
+    """add_descriptor_y_labels. Add labels to the Y axis in ax by accessing the
+    rdm.pattern_descriptors dict with the pattern_descriptor key.
+
+    Args:
+        rdm (rsatoolbox.rdm.RDMs): RDMs instance to annotate.
+        pattern_descriptor (str): dict key for the rdm.pattern_descriptors dict.
+        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle. plt.gca() by default.
+        num_pattern_groups (int): Number of rows/columns for any image labels.
+        icon_spacing (float): control spacing of image labels - 1. means no gap (the
+            default), 1.1 means pad 10%, .9 means overlap 10% etc.
+        linewidth (float): Width of connecting lines from icon labels (if used) to axis
+            margin.  The default is 0.5 - set to 0. to disable the lines.
+
+    Returns:
+        list: Tick label handles.
+    """
+    if ax is None:
+        ax = plt.gca()
+    return _add_descriptor_labels(
+        rdm,
+        pattern_descriptor,
+        "y_tick_label",
+        ax.yaxis,
+        num_pattern_groups=num_pattern_groups,
+        icon_spacing=icon_spacing,
+        linewidth=linewidth,
+        horizontalalignment="right",
+    )
+
+
+def _add_descriptor_labels(
+    rdm: rsatoolbox.rdm.RDMs,
+    pattern_descriptor: str,
+    icon_method: str,
+    axis: Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis],
+    num_pattern_groups: int = None,
+    icon_spacing: float = 1.0,
+    linewidth: float = 0.5,
+    horizontalalignment: str = "center",
+) -> list:
+    """_add_descriptor_labels. Used internally by add_descriptor_y_labels and
+    add_descriptor_x_labels.
+
+    Args:
+        rdm (rsatoolbox.rdm.RDMs): RDMs instance to annotate.
+        pattern_descriptor (str): dict key for the rdm.pattern_descriptors dict.
+        icon_method (str): method to access on Icon instances (typically y_tick_label or
+            x_tick_label).
+        axis (Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis]): Axis to add
+            tick labels to.
+        num_pattern_groups (int): Number of rows/columns for any image labels.
+        icon_spacing (float): control spacing of image labels - 1. means no gap (the
+            default), 1.1 means pad 10%, .9 means overlap 10% etc.
+        linewidth (float): Width of connecting lines from icon labels (if used) to axis
+            margin.  The default is 0.5 - set to 0. to disable the lines.
+        horizontalalignment (str): Horizontal alignment of text tick labels.
+
+    Returns:
+        list: Tick label handles.
+    """
+    descriptor_arr = np.asarray(rdm.pattern_descriptors[pattern_descriptor])
+    if isinstance(descriptor_arr[0], vis.Icon):
+        return _add_descriptor_icons(
+            descriptor_arr,
+            icon_method,
+            n_cond=rdm.n_cond,
+            ax=axis.axes,
+            icon_spacing=icon_spacing,
+            num_pattern_groups=num_pattern_groups,
+            linewidth=linewidth,
+        )
+    is_x_axis = "x" in icon_method
+    return _add_descriptor_text(
+        descriptor_arr,
+        axis=axis,
+        horizontalalignment=horizontalalignment,
+        is_x_axis=is_x_axis,
+    )
+
+
+def _add_descriptor_text(
+    descriptor_arr: npt.ArrayLike,
+    axis: Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis],
+    horizontalalignment: str = "center",
+    is_x_axis: bool = False,
+) -> list:
+    """_add_descriptor_text. Used internally by _add_descriptor_labels to add vanilla
+    Matplotlib-based text labels to the X or Y axis.
+
+    Args:
+        descriptor_arr (npt.ArrayLike): np.Array-like version of the labels.
+        axis (Union[matplotlib.axis.XAxis, matplotlib.axis.YAxis]): handle for
+            the relevant axis (ax.xaxis or ax.yaxis).
+        horizontalalignment (str): Horizontal alignment of text tick labels.
+        is_x_axis (bool): If set, rotate the text labels 60 degrees to reduce overlap on
+            the X axis.
+
+    Returns:
+        list: Tick label handles.
+    """
+    # vanilla matplotlib-based
+    # need to ensure the minor ticks have some length
+    axis.set_tick_params(length=matplotlib.rcParams["xtick.minor.size"], which="minor")
+    label_handles = axis.set_ticklabels(
+        descriptor_arr,
+        verticalalignment="center",
+        horizontalalignment=horizontalalignment,
+        minor=True,
+    )
+    if is_x_axis:
+        plt.setp(
+            axis.get_ticklabels(minor=True),
+            rotation=60,
+            ha="right",
+            rotation_mode="anchor",
+        )
+    return label_handles
+
+
+def _add_descriptor_icons(
+    descriptor_arr: npt.ArrayLike,
+    icon_method: str,
+    n_cond: int,
+    ax: Axes = None,
+    num_pattern_groups: int = None,
+    icon_spacing: float = 1.0,
+    linewidth: float = 0.5,
+) -> list:
+    """_add_descriptor_icons. Used internally by _add_descriptor_labels to add
+    Icon-based labels to the X or Y axis.
+
+    Args:
+        descriptor_arr (npt.ArrayLike): np.Array-like version of the labels.
+        icon_method (str): method to access on Icon instances (typically y_tick_label or
+            x_tick_label).
+        n_cond (int): Number of conditions in the RDM (usually from RDMs.n_cond).
+        ax (matplotlib.axes._axes.Axes): Matplotlib axis handle.
+        num_pattern_groups (int): Number of rows/columns for any image labels.
+        icon_spacing (float): control spacing of image labels - 1. means no gap (the
+            default), 1.1 means pad 10%, .9 means overlap 10% etc.
+        linewidth (float): Width of connecting lines from icon labels (if used) to axis
+            margin.  The default is 0.5 - set to 0. to disable the lines.
+
+    Returns:
+        list: Tick label handles.
+    """
+    # annotated labels with Icon
+    n_to_fit = np.ceil(n_cond / num_pattern_groups)
+    # work out sizing of icons
+    im_max_pix = 20.
+    if descriptor_arr[0].final_image:
+        # size by image
+        im_width_pix = max(this_desc.final_image.width for this_desc in descriptor_arr)
+        im_height_pix = max(this_desc.final_image.height for this_desc in descriptor_arr)
+        im_max_pix = max(im_width_pix, im_height_pix) * icon_spacing
+    ax.figure.canvas.draw()
+    extent = ax.get_window_extent(ax.figure.canvas.get_renderer())
+    ax_size_pix = max((extent.width, extent.height))
+    size = (ax_size_pix / n_to_fit) / im_max_pix
+    # from proportion of original size to figure pixels
+    offset = im_max_pix * size
+    label_handles = []
+    for group_ind in range(num_pattern_groups - 1, -1, -1):
+        position = offset * 0.2 + offset * group_ind
+        ticks = np.arange(group_ind, n_cond, num_pattern_groups)
+        label_handles.append(
+            [
+                getattr(this_desc, icon_method)(
+                    this_x, size, offset=position, linewidth=linewidth, ax=ax,
+                )
+                for (this_x, this_desc) in zip(ticks, descriptor_arr[ticks])
+            ]
+        )
+    return label_handles
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox/vis/scatter_plot.py` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox/vis/scatter_plot.py`

 * *Ordering differences only*

 * *Files 15% similar despite different names*

```diff
@@ -1,217 +1,217 @@
-from __future__ import annotations
-from typing import TYPE_CHECKING, Optional
-import math
-import matplotlib.pyplot
-import sklearn.manifold
-import numpy
-from rsatoolbox.util.vis_utils import weight_to_matrices, Weighted_MDS
-from rsatoolbox.vis.icon import Icon
-if TYPE_CHECKING:
-    from rsatoolbox.rdm import RDMs
-    from numpy.typing import NDArray
-    from matplotlib.figure import Figure
-seed = numpy.random.RandomState(seed=1)
-
-
-def show_scatter(
-        rdms: RDMs,
-        coords: NDArray,
-        rdm_descriptor: Optional[str]=None,
-        pattern_descriptor: Optional[str]=None,
-        icon_size: float=0.1
-    ) -> Figure:
-    """Draw a 2-dimensional scatter plot based on the provided coordinates
-
-    Args:
-        rdms (RDMs): The RDMs object to display
-        coords (NDArray): Array of x and y coordinates for each
-            pattern (patterns x 2)
-        rdm_descriptor: (Optional[str]): If provided, this will be used as
-            title for each individual RDM.
-        pattern_descriptor (Optional[str]): If provided, the chosen pattern
-            descriptor will be printed adjacent to each point in the plot
-        icon_size: relative size of icons if the pattern descriptor chosen
-            is of type Icon
-
-    Returns:
-        Figure: A matplotlib figure in which the plot is drawn
-    """
-    frac, n = math.modf(math.sqrt(rdms.n_rdm))
-    nrows, ncols = math.floor(n), math.floor(n)
-    if frac > 0:
-        nrows += 1
-    if frac > 0.5:
-        ncols += 1
-    fig, axes = matplotlib.pyplot.subplots(nrows=nrows, ncols=ncols)
-    axes = numpy.array(axes)  ## it's now an array even if there's only one
-    for r, ax in enumerate(axes.ravel()):
-
-        if r > (rdms.n_rdm - 1):
-            ## fewer rdms than rows x cols, hide the remaining axes
-            ax.axis('off')
-            break
-
-        ax.scatter(coords[r, :, 0], coords[r, :, 1])
-        ax.set_xlim(coords.min()*0.95, coords.max()*1.05)
-        ax.set_ylim(coords.min()*0.95, coords.max()*1.05)
-
-        ## RDM names
-        if rdm_descriptor is not None:
-            ax.set_title(rdms.rdm_descriptors[rdm_descriptor][r])
-
-        ## print labels next to dots
-        if pattern_descriptor is not None:
-            for p in range(coords.shape[1]):
-                pat_desc = rdms.pattern_descriptors[pattern_descriptor][p]
-                pat_coords = (coords[r, p, 0], coords[r, p, 1])
-                if isinstance(pat_desc, Icon):
-                    pat_desc.plot(pat_coords[0], pat_coords[1], ax=ax, size=icon_size)
-                else:
-                    label = ax.annotate(pat_desc, pat_coords)
-                    label.set_alpha(.6)
-
-        ## turn off all axis ticks and labels
-        ax.tick_params(axis='both', which='both', bottom=False, top=False,
-            right=False, left=False, labelbottom=False, labeltop=False,
-            labelleft=False, labelright=False)
-    return fig
-
-def show_2d(
-        rdms: RDMs,
-        method: str,
-        weights: Optional[NDArray]=None,
-        rdm_descriptor: Optional[str]=None,
-        pattern_descriptor: Optional[str]=None,
-        icon_size: float=0.1
-    ) -> Figure:
-    """Draw a scatter plot of the RDMs reduced to two dimensions
-
-    Args:
-        rdms (RDMs): The RDMs object to display
-        method (str): One of 'MDS', 't-SNE', 'Isomap'.
-        weights: Optional array of weights (vector per RDM)
-        rdm_descriptor: (Optional[str]): If provided, this will be used as
-            title for each individual RDM.
-        pattern_descriptor (Optional[str]): If provided, the chosen pattern
-            descriptor will be printed adjacent to each point in the plot
-        icon_size: relative size of icons if the pattern descriptor chosen
-            is of type Icon
-
-    Returns:
-        Figure: A matplotlib figure in which the plot is drawn
-    """
-    if method == 'MDS':
-        MDS = sklearn.manifold.MDS if weights is None else Weighted_MDS
-        embedding = MDS(
-            n_components=2,
-            random_state=seed,
-            dissimilarity='precomputed',
-            # normalized_stress='auto' # drop:py37
-        )
-    elif method == 't-SNE':
-        embedding = sklearn.manifold.TSNE(n_components=2)
-    elif method == 'Isomap':
-        embedding = sklearn.manifold.Isomap(n_components=2)
-    else:
-        raise NotImplementedError('Unknown method: ' + str(method))
-    rdm_mats = rdms.get_matrices()
-    coords = numpy.full((rdms.n_rdm, rdms.n_cond, 2), numpy.nan)
-    for r in range(rdms.n_rdm):
-        fitKwargs = dict()
-        if weights is not None:
-            fitKwargs['weight'] = weight_to_matrices(weights)[r, :, :]
-        coords[r, :, :] = embedding.fit_transform(rdm_mats[r, :, :], **fitKwargs)
-    return show_scatter(
-        rdms,
-        coords,
-        rdm_descriptor=rdm_descriptor,
-        pattern_descriptor=pattern_descriptor,
-        icon_size=icon_size
-    )
-
-def show_MDS(
-        rdms: RDMs,
-        weights: Optional[NDArray]=None,
-        rdm_descriptor: Optional[str]=None,
-        pattern_descriptor: Optional[str]=None,
-        icon_size: float=0.1
-    ) -> Figure:
-    """Draw a scatter plot based on Multidimensional Scaling dimensionality reduction
-
-    Args:
-        rdms (RDMs): The RDMs object to display
-        weights: Optional array of weights (vector per RDM)
-        rdm_descriptor: (Optional[str]): If provided, this will be used as
-            title for each individual RDM.
-        pattern_descriptor (Optional[str]): If provided, the chosen pattern
-            descriptor will be printed adjacent to each point in the plot
-        icon_size: relative size of icons if the pattern descriptor chosen
-            is of type Icon
-
-    Returns:
-        Figure: A matplotlib figure in which the plot is drawn
-    """
-    return show_2d(
-        rdms,
-        method='MDS',
-        weights=weights,
-        rdm_descriptor=rdm_descriptor,
-        pattern_descriptor=pattern_descriptor,
-        icon_size=icon_size
-    )
-
-def show_tSNE(
-        rdms: RDMs,
-        rdm_descriptor: Optional[str]=None,
-        pattern_descriptor: Optional[str]=None,
-        icon_size: float=0.1
-    ) -> Figure:
-    """Draw a scatter plot based on t-SNE dimensionality reduction
-
-    Args:
-        rdms (RDMs): The RDMs object to display
-        rdm_descriptor: (Optional[str]): If provided, this will be used as
-            title for each individual RDM.
-        pattern_descriptor (Optional[str]): If provided, the chosen pattern
-            descriptor will be printed adjacent to each point in the plot
-        icon_size: relative size of icons if the pattern descriptor chosen
-            is of type Icon
-
-    Returns:
-        Figure: A matplotlib figure in which the plot is drawn
-    """
-    return show_2d(
-        rdms,
-        method='t-SNE',
-        rdm_descriptor=rdm_descriptor,
-        pattern_descriptor=pattern_descriptor,
-        icon_size=icon_size
-    )
-
-def show_iso(
-        rdms: RDMs,
-        rdm_descriptor: Optional[str]=None,
-        pattern_descriptor: Optional[str]=None,
-        icon_size: float=0.1
-    ) -> Figure:
-    """Draw a scatter plot based on Isomap dimensionality reduction
-
-    Args:
-        rdms (RDMs): The RDMs object to display
-        rdm_descriptor: (Optional[str]): If provided, this will be used as
-            title for each individual RDM.
-        pattern_descriptor (Optional[str]): If provided, the chosen pattern
-            descriptor will be printed adjacent to each point in the plot
-        icon_size: relative size of icons if the pattern descriptor chosen
-            is of type Icon
-
-    Returns:
-        Figure: A matplotlib figure in which the plot is drawn
-    """
-    return show_2d(
-        rdms,
-        method='Isomap',
-        rdm_descriptor=rdm_descriptor,
-        pattern_descriptor=pattern_descriptor,
-        icon_size=icon_size
-    )
+from __future__ import annotations
+from typing import TYPE_CHECKING, Optional
+import math
+import matplotlib.pyplot
+import sklearn.manifold
+import numpy
+from rsatoolbox.util.vis_utils import weight_to_matrices, Weighted_MDS
+from rsatoolbox.vis.icon import Icon
+if TYPE_CHECKING:
+    from rsatoolbox.rdm import RDMs
+    from numpy.typing import NDArray
+    from matplotlib.figure import Figure
+seed = numpy.random.RandomState(seed=1)
+
+
+def show_scatter(
+        rdms: RDMs,
+        coords: NDArray,
+        rdm_descriptor: Optional[str]=None,
+        pattern_descriptor: Optional[str]=None,
+        icon_size: float=0.1
+    ) -> Figure:
+    """Draw a 2-dimensional scatter plot based on the provided coordinates
+
+    Args:
+        rdms (RDMs): The RDMs object to display
+        coords (NDArray): Array of x and y coordinates for each
+            pattern (patterns x 2)
+        rdm_descriptor: (Optional[str]): If provided, this will be used as
+            title for each individual RDM.
+        pattern_descriptor (Optional[str]): If provided, the chosen pattern
+            descriptor will be printed adjacent to each point in the plot
+        icon_size: relative size of icons if the pattern descriptor chosen
+            is of type Icon
+
+    Returns:
+        Figure: A matplotlib figure in which the plot is drawn
+    """
+    frac, n = math.modf(math.sqrt(rdms.n_rdm))
+    nrows, ncols = math.floor(n), math.floor(n)
+    if frac > 0:
+        nrows += 1
+    if frac > 0.5:
+        ncols += 1
+    fig, axes = matplotlib.pyplot.subplots(nrows=nrows, ncols=ncols)
+    axes = numpy.array(axes)  ## it's now an array even if there's only one
+    for r, ax in enumerate(axes.ravel()):
+
+        if r > (rdms.n_rdm - 1):
+            ## fewer rdms than rows x cols, hide the remaining axes
+            ax.axis('off')
+            break
+
+        ax.scatter(coords[r, :, 0], coords[r, :, 1])
+        ax.set_xlim(coords.min()*0.95, coords.max()*1.05)
+        ax.set_ylim(coords.min()*0.95, coords.max()*1.05)
+
+        ## RDM names
+        if rdm_descriptor is not None:
+            ax.set_title(rdms.rdm_descriptors[rdm_descriptor][r])
+
+        ## print labels next to dots
+        if pattern_descriptor is not None:
+            for p in range(coords.shape[1]):
+                pat_desc = rdms.pattern_descriptors[pattern_descriptor][p]
+                pat_coords = (coords[r, p, 0], coords[r, p, 1])
+                if isinstance(pat_desc, Icon):
+                    pat_desc.plot(pat_coords[0], pat_coords[1], ax=ax, size=icon_size)
+                else:
+                    label = ax.annotate(pat_desc, pat_coords)
+                    label.set_alpha(.6)
+
+        ## turn off all axis ticks and labels
+        ax.tick_params(axis='both', which='both', bottom=False, top=False,
+            right=False, left=False, labelbottom=False, labeltop=False,
+            labelleft=False, labelright=False)
+    return fig
+
+def show_2d(
+        rdms: RDMs,
+        method: str,
+        weights: Optional[NDArray]=None,
+        rdm_descriptor: Optional[str]=None,
+        pattern_descriptor: Optional[str]=None,
+        icon_size: float=0.1
+    ) -> Figure:
+    """Draw a scatter plot of the RDMs reduced to two dimensions
+
+    Args:
+        rdms (RDMs): The RDMs object to display
+        method (str): One of 'MDS', 't-SNE', 'Isomap'.
+        weights: Optional array of weights (vector per RDM)
+        rdm_descriptor: (Optional[str]): If provided, this will be used as
+            title for each individual RDM.
+        pattern_descriptor (Optional[str]): If provided, the chosen pattern
+            descriptor will be printed adjacent to each point in the plot
+        icon_size: relative size of icons if the pattern descriptor chosen
+            is of type Icon
+
+    Returns:
+        Figure: A matplotlib figure in which the plot is drawn
+    """
+    if method == 'MDS':
+        MDS = sklearn.manifold.MDS if weights is None else Weighted_MDS
+        embedding = MDS(
+            n_components=2,
+            random_state=seed,
+            dissimilarity='precomputed',
+            # normalized_stress='auto' # drop:py37
+        )
+    elif method == 't-SNE':
+        embedding = sklearn.manifold.TSNE(n_components=2)
+    elif method == 'Isomap':
+        embedding = sklearn.manifold.Isomap(n_components=2)
+    else:
+        raise NotImplementedError('Unknown method: ' + str(method))
+    rdm_mats = rdms.get_matrices()
+    coords = numpy.full((rdms.n_rdm, rdms.n_cond, 2), numpy.nan)
+    for r in range(rdms.n_rdm):
+        fitKwargs = dict()
+        if weights is not None:
+            fitKwargs['weight'] = weight_to_matrices(weights)[r, :, :]
+        coords[r, :, :] = embedding.fit_transform(rdm_mats[r, :, :], **fitKwargs)
+    return show_scatter(
+        rdms,
+        coords,
+        rdm_descriptor=rdm_descriptor,
+        pattern_descriptor=pattern_descriptor,
+        icon_size=icon_size
+    )
+
+def show_MDS(
+        rdms: RDMs,
+        weights: Optional[NDArray]=None,
+        rdm_descriptor: Optional[str]=None,
+        pattern_descriptor: Optional[str]=None,
+        icon_size: float=0.1
+    ) -> Figure:
+    """Draw a scatter plot based on Multidimensional Scaling dimensionality reduction
+
+    Args:
+        rdms (RDMs): The RDMs object to display
+        weights: Optional array of weights (vector per RDM)
+        rdm_descriptor: (Optional[str]): If provided, this will be used as
+            title for each individual RDM.
+        pattern_descriptor (Optional[str]): If provided, the chosen pattern
+            descriptor will be printed adjacent to each point in the plot
+        icon_size: relative size of icons if the pattern descriptor chosen
+            is of type Icon
+
+    Returns:
+        Figure: A matplotlib figure in which the plot is drawn
+    """
+    return show_2d(
+        rdms,
+        method='MDS',
+        weights=weights,
+        rdm_descriptor=rdm_descriptor,
+        pattern_descriptor=pattern_descriptor,
+        icon_size=icon_size
+    )
+
+def show_tSNE(
+        rdms: RDMs,
+        rdm_descriptor: Optional[str]=None,
+        pattern_descriptor: Optional[str]=None,
+        icon_size: float=0.1
+    ) -> Figure:
+    """Draw a scatter plot based on t-SNE dimensionality reduction
+
+    Args:
+        rdms (RDMs): The RDMs object to display
+        rdm_descriptor: (Optional[str]): If provided, this will be used as
+            title for each individual RDM.
+        pattern_descriptor (Optional[str]): If provided, the chosen pattern
+            descriptor will be printed adjacent to each point in the plot
+        icon_size: relative size of icons if the pattern descriptor chosen
+            is of type Icon
+
+    Returns:
+        Figure: A matplotlib figure in which the plot is drawn
+    """
+    return show_2d(
+        rdms,
+        method='t-SNE',
+        rdm_descriptor=rdm_descriptor,
+        pattern_descriptor=pattern_descriptor,
+        icon_size=icon_size
+    )
+
+def show_iso(
+        rdms: RDMs,
+        rdm_descriptor: Optional[str]=None,
+        pattern_descriptor: Optional[str]=None,
+        icon_size: float=0.1
+    ) -> Figure:
+    """Draw a scatter plot based on Isomap dimensionality reduction
+
+    Args:
+        rdms (RDMs): The RDMs object to display
+        rdm_descriptor: (Optional[str]): If provided, this will be used as
+            title for each individual RDM.
+        pattern_descriptor (Optional[str]): If provided, the chosen pattern
+            descriptor will be printed adjacent to each point in the plot
+        icon_size: relative size of icons if the pattern descriptor chosen
+            is of type Icon
+
+    Returns:
+        Figure: A matplotlib figure in which the plot is drawn
+    """
+    return show_2d(
+        rdms,
+        method='Isomap',
+        rdm_descriptor=rdm_descriptor,
+        pattern_descriptor=pattern_descriptor,
+        icon_size=icon_size
+    )
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/PKG-INFO` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/PKG-INFO`

 * *Files 10% similar despite different names*

```diff
@@ -1,83 +1,83 @@
-Metadata-Version: 2.1
-Name: rsatoolbox
-Version: 0.1.3.dev51
-Summary: Representational Similarity Analysis (RSA) in Python
-Author: rsatoolbox authors
-License: MIT License
-        
-        Copyright (c) 2019 rsatoolbox authors
-        
-        Permission is hereby granted, free of charge, to any person obtaining a copy
-        of this software and associated documentation files (the "Software"), to deal
-        in the Software without restriction, including without limitation the rights
-        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-        copies of the Software, and to permit persons to whom the Software is
-        furnished to do so, subject to the following conditions:
-        
-        The above copyright notice and this permission notice shall be included in all
-        copies or substantial portions of the Software.
-        
-        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-        SOFTWARE.
-        
-Project-URL: homepage, https://github.com/rsagroup/rsatoolbox
-Project-URL: documentation, https://rsatoolbox.readthedocs.io/
-Keywords: neuroscience
-Classifier: Programming Language :: Python
-Classifier: License :: OSI Approved :: MIT License
-Classifier: Operating System :: OS Independent
-Classifier: Development Status :: 4 - Beta
-Classifier: Topic :: Scientific/Engineering
-Classifier: Intended Audience :: Science/Research
-Classifier: Programming Language :: Python :: 3.7
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Requires-Python: <3.12,>=3.7
-Description-Content-Type: text/x-rst
-License-File: LICENSE
-License-File: AUTHORS
-
-# Representational Similarity Analysis 3.0
-
-[![Documentation Status](https://readthedocs.org/projects/rsatoolbox/badge/?version=latest)](https://rsatoolbox.readthedocs.io/en/latest/?badge=latest)
-[![PyPI version](https://badge.fury.io/py/rsatoolbox.svg)](https://badge.fury.io/py/rsatoolbox)
-[![Codacy Badge](https://app.codacy.com/project/badge/Grade/626ca9ec9f75485a9f73783c02710b1f)](https://www.codacy.com/gh/rsagroup/rsatoolbox?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=rsagroup/rsatoolbox&amp;utm_campaign=Badge_Grade)
-[![CodeFactor](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox/badge)](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox)
-[![codecov](https://codecov.io/gh/rsagroup/rsatoolbox/branch/master/graph/badge.svg)](https://codecov.io/gh/rsagroup/rsatoolbox)
-
-
-Conceived during the RSA retreat 2019 in Blue Mountains.
-
-[Documentation](https://rsatoolbox.readthedocs.io/)
-
-
-#### Getting Started
-
-To install the latest stable version of rsatoolbox with pip:
-
-```sh
-pip install rsatoolbox
-```
-
-or with conda:
-
-```sh
-conda install -c conda-forge rsatoolbox
-```
-
-
-here is a simple code sample:
-
-```python
-import numpy, rsatoolbox
-data = rsatoolbox.data.Dataset(numpy.random.rand(10, 5))
-rdms = rsatoolbox.rdm.calc_rdm(data)
-rsatoolbox.vis.show_rdm(rdms)
-```
+Metadata-Version: 2.1
+Name: rsatoolbox
+Version: 0.1.3.dev56
+Summary: Representational Similarity Analysis (RSA) in Python
+Author: rsatoolbox authors
+License: MIT License
+        
+        Copyright (c) 2019 rsatoolbox authors
+        
+        Permission is hereby granted, free of charge, to any person obtaining a copy
+        of this software and associated documentation files (the "Software"), to deal
+        in the Software without restriction, including without limitation the rights
+        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+        copies of the Software, and to permit persons to whom the Software is
+        furnished to do so, subject to the following conditions:
+        
+        The above copyright notice and this permission notice shall be included in all
+        copies or substantial portions of the Software.
+        
+        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+        SOFTWARE.
+        
+Project-URL: homepage, https://github.com/rsagroup/rsatoolbox
+Project-URL: documentation, https://rsatoolbox.readthedocs.io/
+Keywords: neuroscience
+Classifier: Programming Language :: Python
+Classifier: License :: OSI Approved :: MIT License
+Classifier: Operating System :: OS Independent
+Classifier: Development Status :: 4 - Beta
+Classifier: Topic :: Scientific/Engineering
+Classifier: Intended Audience :: Science/Research
+Classifier: Programming Language :: Python :: 3.7
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Requires-Python: <3.12,>=3.7
+Description-Content-Type: text/x-rst
+License-File: LICENSE
+License-File: AUTHORS
+
+# Representational Similarity Analysis 3.0
+
+[![Documentation Status](https://readthedocs.org/projects/rsatoolbox/badge/?version=latest)](https://rsatoolbox.readthedocs.io/en/latest/?badge=latest)
+[![PyPI version](https://badge.fury.io/py/rsatoolbox.svg)](https://badge.fury.io/py/rsatoolbox)
+[![Codacy Badge](https://app.codacy.com/project/badge/Grade/626ca9ec9f75485a9f73783c02710b1f)](https://www.codacy.com/gh/rsagroup/rsatoolbox?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=rsagroup/rsatoolbox&amp;utm_campaign=Badge_Grade)
+[![CodeFactor](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox/badge)](https://www.codefactor.io/repository/github/rsagroup/rsatoolbox)
+[![codecov](https://codecov.io/gh/rsagroup/rsatoolbox/branch/master/graph/badge.svg)](https://codecov.io/gh/rsagroup/rsatoolbox)
+
+
+Conceived during the RSA retreat 2019 in Blue Mountains.
+
+[Documentation](https://rsatoolbox.readthedocs.io/)
+
+
+#### Getting Started
+
+To install the latest stable version of rsatoolbox with pip:
+
+```sh
+pip install rsatoolbox
+```
+
+or with conda:
+
+```sh
+conda install -c conda-forge rsatoolbox
+```
+
+
+here is a simple code sample:
+
+```python
+import numpy, rsatoolbox
+data = rsatoolbox.data.Dataset(numpy.random.rand(10, 5))
+rdms = rsatoolbox.rdm.calc_rdm(data)
+rsatoolbox.vis.show_rdm(rdms)
+```
```

### Comparing `rsatoolbox-0.1.3.dev51/src/rsatoolbox.egg-info/SOURCES.txt` & `rsatoolbox-0.1.3.dev56/src/rsatoolbox.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -9,15 +9,14 @@
 README.md
 codecov.yml
 pyproject.toml
 requirements.txt
 setup.cfg
 setup.py
 .conda/meta.yaml
-src/rsatoolbox/cengine/similarity.pyx
 src/rsatoolbox/__init__.py
 src/rsatoolbox/test.py
 src/rsatoolbox.egg-info/PKG-INFO
 src/rsatoolbox.egg-info/SOURCES.txt
 src/rsatoolbox.egg-info/dependency_links.txt
 src/rsatoolbox.egg-info/requires.txt
 src/rsatoolbox.egg-info/top_level.txt
```

